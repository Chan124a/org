* n-gram
** 介绍
N-Gram是一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。

每一个字节片段称为gram，对所有gram的出现频度进行统计，并且按照事先设定好的阈值进行过滤，形成关键gram列表，也就是这个文本的向量特征空间，列表中的每一种gram就是一个特征向量维度。

该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram。

** 平滑技术
** 参考文章
[[https://zhuanlan.zhihu.com/p/32829048][自然语言处理中N-Gram模型介绍]]
* 梯度累加(Gradient Accumulation)

#+DOWNLOADED: screenshot @ 2021-12-05 19:13:38
[[file:images/机器学习/梯度累加(Gradient_Accumulation)/2021-12-05_19-13-38_screenshot.png]]
 上图也是某种意义上的梯度累加：一般是直接加总或者取平均，这样操作是scale了，其实影响不大，只是确保loss计算时的value不至于太大。batchsize超过64的情况不多(batchsize太大会有副作用)，这时候优化的粒度没那么细，scale操作适当又做了惩罚。可能在某些时候比不加收敛更快

我们在训练神经网络的时候，超参数batch size的大小会对最终的模型效果产生很大的影响。一定条件下，batch size设置的越大，模型就会越稳定。batch size的值通常设置在 8-32 之间，但是当我们做一些计算量需求大的任务(例如语义分割、GAN等)或者输入图片尺寸太大的时候，我们的batch size往往只能设置为2或者4，否则就会出现 “CUDA OUT OF MEMORY” 的不可抗力报错。

贫穷是促进人类进步的阶梯，如何在有限的计算资源的条件下，训练时采用更大的batch size呢？这就是梯度累加(Gradient Accumulation)技术了。

我们以Pytorch为例，一个神经网络的训练过程通常如下：

#+begin_src bash
for i, (inputs, labels) in enumerate(trainloader):
    optimizer.zero_grad()                   # 梯度清零
    outputs = net(inputs)                   # 正向传播
    loss = criterion(outputs, labels)       # 计算损失
    loss.backward()                         # 反向传播，计算梯度
    optimizer.step()                        # 更新参数
    if (i+1) % evaluation_steps == 0:
        evaluate_model()
#+END_SRC
  从代码中可以很清楚地看到神经网络是如何做到训练的：
1.将前一个batch计算之后的网络梯度清零
2.正向传播，将数据传入网络，得到预测结果
3.根据预测结果与label，计算损失值
4.利用损失进行反向传播，计算参数梯度
5.利用计算的参数梯度更新网络参数

  下面来看梯度累加是如何做的：
#+begin_src bash
for i, (inputs, labels) in enumerate(trainloader):
    outputs = net(inputs)                   # 正向传播
    loss = criterion(outputs, labels)       # 计算损失函数
    loss = loss / accumulation_steps        # 损失标准化
    loss.backward()                         # 反向传播，计算梯度
    if (i+1) % accumulation_steps == 0:
        optimizer.step()                    # 更新参数
        optimizer.zero_grad()               # 梯度清零
        if (i+1) % evaluation_steps == 0:
            evaluate_model()
#+END_SRC
1.正向传播，将数据传入网络，得到预测结果
2.根据预测结果与label，计算损失值
3.利用损失进行反向传播，计算参数梯度
4.重复1-3，不清空梯度，而是将梯度累加
5.梯度累加达到固定次数之后，更新参数，然后将梯度清零

  总结来讲，梯度累加就是每计算一个batch的梯度，不进行清零，而是做梯度的累加，当累加到一定的次数之后，再更新网络参数，然后将梯度清零。

  通过这种参数延迟更新的手段，可以实现与采用大batch size相近的效果。在平时的实验过程中，我一般会采用梯度累加技术，大多数情况下，采用梯度累加训练的模型效果，要比采用小batch size训练的模型效果要好很多。
** 参考资料
[[https://www.cnblogs.com/sddai/p/14598018.html][梯度累加(Gradient Accumulation)]]
* 相关技术方案参考
1. 达观的解决方案：http://www.datagrand.com/blog/search-query.html
方法：基于字之间书写过程中的“距离”，搜索可能的正确词语；基于语言模型修正。
数据：爬虫系统爬取优质词条词典、拼音纠错词典、从检索结果是否很差推测是否有错，不需要标注语料。

2. 百度AI提及的解决方案：https://mp.weixin.qq.com/s/r0kWgPHKthPgGqTbVc3lKw
将错误分为3类：用词错误，音近、词近；文法/句法错误；知识性错误
技术：基于机器翻译的方法，主要思想是把纠错看成同种语言中错误句子翻译为正确句子的过程，大规模训练语言模型后，以机器翻译任务形式进行微调训练，需要标注语料。
纠错系统：ECNet框架，线下挖掘句对、词对齐、phrase抽取，线上检测后候选排序。Restricted-V NEC，端到端机器翻译。

3. 腾讯内部搜索平台部提及的解决方案：https://cloud.tencent.com/developer/article/1030059
分为召回层和决策层，召回层包含不同领域的错字检测模块，决策层基于用户、字形字音等分类，需要标注语料。

4. 开源方案：https://github.com/shibing624/pycorrector
基于规则：先用外部分词工具分词，若某个字的语言模型得分或某个词不在词典中，则判定错误，再基于词典、字典替换该字，使用语言模型判断最好的替换。

5. 基于深度模型：
RNN-CRF：阿里巴巴2016中文语法纠错比赛CGED2018第一名方案
conv_seq2seq：NLPCC-2018第三名方案
BERT：基于MASK输出获得错词判断。
* 文本表示方法
** 传统
*** one-hot
*** IF-IDF
** 分布式
1. 基于共现矩阵的分布式词向量
2. 基于聚类的分布式词向量
3. 基于神经网络进行词嵌入

*** word2vec

**** CBOW 

**** Skip-Gram
** Glove词嵌入

* 子词切分技术
未知词问题
** 基于拷贝的方法
** 基于子词单元的方法
* 数据集
** NLPCC 2018 GEC
NLPCC 2018 GEC 训练集（http://tcci.ccf.org.cn/conference/2018/taskdata.php） 来自于 NLPCC 2018 GEC 共享任务测评，官方数据的原始格式为：

（1）每行是对一个原始输入的校对，每行可能包含0个（即原始输入为正确句子）、1 个或多个平行句子对
（2）各行包括 4 个字段，各字段间用制表符（tab）分隔

样例：
#+BEGIN_EXAMPLE
1 1 我在家里一个人学习中文。 我在家里自学中文。
2 0 我是里阿德，来自以色列。
1 3 这个软件让我们什么有趣的事都记录。 这个软件讓我们能把任何有趣的事都记录下來。 这个软件能让我们把有趣的事都记录下来。 这个软件能让我们把任何有趣的事都记录。
1 1 两位易知基金代表访白目的为连接两国高等院校。 两位易知基金代表访白目的为开展两国高等院校的合作。
#+END_EXAMPLE
各字段含义：
#+BEGIN_EXAMPLE
字段序号	字段名	字段意义
1	sens_id	句子在短文中的索引，从 1 开始
2	num_correct	目标句子的个数
3	orig_sen	原始句子
4	corrections	若干目标句子（如果 num_correct !=0）
#+END_EXAMPLE
** lang8
目前中英文公开的最大规模平行校对语料（中文的 NLPCC 2018 GEC 数据集， 英文的 Lang-8 语料）都源自于 Lang-8 网站（https://lang-8.com/）

Lang-8 是一个多 语言学习平台，由原生语言人士（native speakers）挑选学习者写的短文（作文）进行修 改

当前公开的较大规模平行语料都以类似众包的形式得到，数据质量并不高
** 中文维基百科语料
可以从维基 dump 网站（https://dumps.wikimedia.org）下载 XML 格式的中文维基百科语料（https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2），得到1.65G的压缩包

XML 文件中的 text 标签内部即为文章内容，由于维基百科语料属于网络文本，需要对其进行必要的处理与清洗

该语料可供后续使用
** 汉语水平考试(HSK)
HSK（汉语水平考试的拼音缩写）语料来自北京语言大学（BLCU）的“HSK动态作文语料库”

“HSK动态作文语料库”是母语非汉语的外国人参加高等汉语水平考试作文考试的答卷语料库，收集了 1992-2005 年的部分外国考生的作文答卷。语料库 1.0 版收入语料 10740 篇，约 400 万字

本次使用的 HSK 平行数据来自 NLPCC 2018 GEC 共享任务 BLCU 团队 github 上开源的项目（https://github.com/blcu-nlp/NLPCC_2018_TASK2_GEC） ，该平行语料质量较高，且已经预处理完毕，共计有 156870 个平行句子对
** SIGHAN Bake-off: Chinese Spelling Check Task.
拼写纠错 (Spelling Correction)任务的目标是在文本中查找并更正拼写错误 (typographical errors),这类错误通常发生在外观、发音相似的字符之间.

示例

输入:
1986年毕业于国防科技大学计算机应用专业，获学时学位。

输出:
1986年毕业于国防科技大学计算机应用专业，获学士学位。
(时 -> 士)

标准评价指标:

拼写纠错 (Spelling Correction)任务通常使用accuracy, precision, recall, 以及F1 score指标进行评测.这些指标可以是字符层面(character level)的或者是句子层面(sentence level)的.通常会分别对识别(Detection)和纠正(Correction)进行评估.
- 识别(Detection): 识别一段文字中所有拼写错误字符的位置，应该做到与正确参考 (gold standard) 相同.
- 纠正(Correction): 识别的错误字符以及纠正错误字符，应与正确参考 (gold standard)相同.

版本: SIGHAN 数据集有3个版本 ([[http://anthology.aclweb.org/W/W15/W15-3106.pdf][2015]], [[http://anthology.aclweb.org/W/W14/W14-6820.pdf][2014]], [[http://anthology.aclweb.org/W/W13/W13-4406.pdf][2013]])
综述论文: [[http://anthology.aclweb.org/W/W15/W15-3106.pdf][Tseng et. al. (2015]])
Licence: http://nlp.ee.ncu.edu.tw/resource/csc_download.html (academic use only)

| Source                                         | # sentence pairs | # chars | #拼写错误 | 字符集 | 主题 (Genre)                            |
|------------------------------------------------+------------------+---------+-----------+--------+-----------------------------------------|
| SIGHAN 2015 Training data (Tseng et. al. 2015) | 2,334            | 146,076 | 2,594     | 繁体   | 第二语言学习 (second-language learning) |
| SIGHAN 2014 Training data (Yu et. al. 2014)    | 6,526            | 324,342 | 10,087    | 繁体   | 第二语言学习 (second-language learning) |
| SIGHAN 2013 Training data (Wu et. al. 2013)    | 350              | 17,220  | 350       | 繁体   | 第二语言学习 (second-language learning) |

| test set                         | # sentence pairs | # characters | #拼写错误 (chars) | 字符集 | 主题 (Genre)                            |
|----------------------------------+------------------+--------------+-------------------+--------+-----------------------------------------|
| SIGHAN 2015 (Tseng et. al. 2015) | 1,100            | 33,711       |               715 | 繁体   | 第二语言学习 (second-language learning) |
| SIGHAN 2014 (Yu et. al. 2014)    | 1,062            | 53,114       |               792 | 繁体   | 第二语言学习 (second-language learning) |
| SIGHAN 2013 (Wu et. al. 2013)    | 2,000            | 143,039      |             1,641 | 繁体   | 第二语言学习 (second-language learning) |

** CoNLL-2003数据集
CoNLL-2003命名实体数据集[下载]是用于CoNLL-2003共享任务，由八个文件组成，涵盖两种语言:英语和德语。

每种语言都包含：训练集、开发集、测试集、无标签数据；
1. 训练集：用于模型学习训练
2. 开发集：用于模型学习过程中调参
3. 测试集：用于结果的测试

注意：其中无标签数据较大【未标注数据包含1700万个token(英语)和1400万个token(德语)】，其他数据集都比较小

英文数据取自Reuters Corpus，该数据集由路透社从1996年8月到1997年8月的新闻故事组成；

具体的数据详细信息如下：

（1）个数据集中的文章、句子、词语数量
|        | 文章数 | 句子数 | 词语数 |
| 训练集 |    946 |  14987 | 203621 |
| 开发集 |    216 |   3466 |  51362 |
| 测试集 |    231 |   3684 |  46435 |
（2）各数据集中的实体数量分布情况

|        | 地名 | 人名 | 组织名 | 其他实体 |
| 训练集 | 7140 | 6600 |   6321 |     3438 |
| 开发集 | 1837 | 1842 |   1341 |      922 |
| 测试集 | 1668 | 1617 |   1661 |      702 |
数据样例如下（假设实体没有循环和交叉）：
#+BEGIN_EXAMPLE
词       词性   词块   实体
U.N.     NNP   I-NP  I-ORG
official NN    I-NP  O
Ekeus    NNP   I-NP  I-PER
heads    VBZ   I-VP  O
for      IN    I-PP  O
Baghdad  NNP   I-NP  I-LOC
. . O O
#+END_EXAMPLE
这是其训练集中某个部分。

通过其官网介绍，可知改数据集第一例是单词，第二列是词性，第三列是语法快，第四列是实体标签。在NER任务中，只关心第一列和第四列。实体类别标注采用BIO标注法，

该数据的加载方式在transformers库中进行了封装，我们可以通过以下语句进行数据加载：
#+begin_src bash
from datasets import load_dataset
datasets = load_dataset("conll2003")
#+END_SRC
给定一个数据切分的key（train、validation或者test）和下标即可查看数据。
#+begin_src bash
datasets["train"][0]
#{'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],
# 'id': '0',
# 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],
# 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],
# 'tokens': ['EU',
#  'rejects',
#  'German',
#  'call',
#  'to',
#  'boycott',
#  'British',
#  'lamb',
#  '.']}
#+END_SRC
所有的数据标签都已经被编码成了整数，可以直接被预训练transformer模型使用。这些整数的编码所对应的实际类别储存在features中。
#+begin_src bash
datasets["train"].features[f"ner_tags"]
#Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)
#+END_SRC
以NER任务为例，0对应的标签类别是”O“， 1对应的是”B-PER“等等。

”O“表示没有特别实体（no special entity/other）。本例包含4种有价值实体类别分别是（PER、ORG、LOC，MISC），每一种实体类别又分别有B-（实体开始的token）前缀和I-（实体中间的token）前缀。
- PER for person
- ORG for organization
- LOC for location
- MISC for miscellaneous
#+begin_src bash
label_list = datasets["train"].features[f"{task}_tags"].feature.names
label_list
#['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
#+END_SRC
下面的函数将从数据集里随机选择几个例子进行展示：
#+begin_src bash
from datasets import ClassLabel, Sequence
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=4):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):
            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])
    display(HTML(df.to_html()))
#+END_SRC
#+begin_src bash
show_random_elements(datasets["train"])
#+END_SRC

#+DOWNLOADED: screenshot @ 2021-12-13 15:06:03
[[file:images/机器学习/数据集/2021-12-13_15-06-03_screenshot.png]]


*** 参考文章
[[https://blog.csdn.net/Elvira521yan/article/details/118028020][【NLP公开数据集】 CoNLL-2003数据集]]
[[https://blog.csdn.net/StarLib/article/details/107350559][命名实体识别学习-数据集介绍-conll03]]
* 并行数据
指的是一句错误的语句和一句纠正的语句
* 论文调研
** 基于Transformer增强架构的中文语法纠错方法_王辰成

动态残差结构：在transformer模型的encoder和decoder分别加入残差模块

基于腐化语料的单语数据增强算法：在语料中按照不同的概率添加多种类型的语料错误，以此创建更多的平行语料


** Overview of NLPTEA-2020 Shared Task for Chinese Grammatical Error Diagnosis

像bert之类的预训练模型取得了很好的效果，取代了CGED2017、2018的standard pipe-line以及biLSTM+CRF方法。

像resnet之类的图像卷积方法第一次出现在该任务。

大多的组都用了数据增强的方法（Hybrid methods based on pre-trained model）

* 序列标注（Sequence Tagging）
[[https://zhuanlan.zhihu.com/p/268579769][序列标注]]里有很多相关链接

序列标注（Sequence Tagging）是NLP中最基础的任务，应用十分广泛，如分词、词性标注（POS tagging）、命名实体识别（Named Entity Recognition，NER）、关键词抽取、语义角色标注（Semantic Role Labeling）、槽位抽取（Slot Filling）等实质上都属于序列标注的范畴。

序列标注一般可以分为两类：
1. 原始标注（Raw labeling）：每个元素都需要被标注为一个标签。
2. 联合标注（Joint segmentation and labeling）：所有的分段被标注为同样的标签。

token级别的分类任务通常指的是为文本中的每一个token预测一个标签结果。比如命名实体识别任务：
#+DOWNLOADED: screenshot @ 2021-12-09 09:50:48
[[file:images/机器学习/序列标注（Sequence_Tagging）/2021-12-09_09-50-48_screenshot.png]]
常见的token级别分类任务:
- NER (Named-entity recognition 名词-实体识别) 分辨出文本中的名词和实体 (person人名, organization组织机构名, location地点名...).
- POS (Part-of-speech tagging词性标注) 根据语法对token进行词性标注 (noun名词, verb动词, adjective形容词...)
- Chunk (Chunking短语组块) 将同一个短语的tokens组块放在一起。

举个NER和联合标注的例子。一个句子为：Yesterday , George Bush gave a speech. 其中包括一个命名实体：George Bush。我们希望将标签“人名”标注到整个短语“George Bush”中，而不是将两个词分别标注。这就是联合标注。
** 简介
序列标注问题可以认为是分类问题的一个推广，或者是更复杂的结构预测（structure prediction）问题的简单形式。

序列标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测。

首先给定一个训练集
#+DOWNLOADED: screenshot @ 2021-12-08 21:59:27
[[file:images/机器学习/序列标注（Sequence_Tagging）/2021-12-08_21-59-27_screenshot.png]]
** 标签方案
标签方案给出了标记的定义方式，常用的标签方案可以参考这篇文章，这里我们介绍一下其中的“BIO”方案。

B，即Begin，表示开始；I，即Intermediate，表示中间；O，即Other，表示其它，用于标记无关字符。

以NER问题为例，对于命名实体“ORG（组织名）”，在BIO标签方案下的标记集合为B_ORG、I_ORG、O。
*** BIO标注
解决联合标注问题的最简单的方法，就是将其转化为原始标注问题。标准做法就是使用BIO标注。

BIO标注：将每个元素标注为“B-X”、“I-X”或者“O”。其中，“B-X”表示此元素所在的片段属于X类型并且此元素在此片段的开头，“I-X”表示此元素所在的片段属于X类型并且此元素在此片段的中间位置，“O”表示不属于任何类型。

比如，我们将 X 表示为名词短语（Noun Phrase, NP），则BIO的三个标记为：
1. B-NP：名词短语的开头
2. I-NP：名词短语的中间
3. O：不是名词短语

因此可以将一段话划分为如下结果;
#+DOWNLOADED: screenshot @ 2021-12-08 22:13:23
[[file:images/机器学习/序列标注（Sequence_Tagging）/2021-12-08_22-13-23_screenshot.png]]
我们可以进一步将BIO应用到NER中，来定义所有的命名实体（人名、组织名、地点、时间等），那么我们会有许多 B 和 I 的类别，如 B-PERS、I-PERS、B-ORG、I-ORG等。然后可以得到以下结果：

#+DOWNLOADED: screenshot @ 2021-12-08 22:15:36
[[file:images/机器学习/序列标注（Sequence_Tagging）/2021-12-08_22-15-36_screenshot.png]]
*** 举例：命名实体识别 
一段待标注的序列X=x1,x2,...,xn
- B - Begin，表示开始
- I - Intermediate，为中间字
- E - End，表示结尾
- S - Single，表示单个字符
- O - Other，表示其他，用于标记无关字符

常见标签方案通常为三标签或者五标签法：

IOB - 对于文本块的第一个字符用B标注，文本块的其它字符用I标注，非文本块字符用O标注

IOBES：
- B，即Begin，表示开始
- I，即Intermediate，表示中间
- E，即End，表示结尾
- S，即Single，表示单个字符
- O，即Other，表示其他，用于标记无关字符

这样的tag并不是固定的，根据任务不同还可以对标签有一系列灵活的变化或扩展。对于分词任务，我们可以用同样的标注方式来标注每一个词的开头、结尾，或单字。如词性标注中，我们可以将标签定义为：n、v、adj...而对于更细类别的命名实体识别任务，我们在定义的标签之后加上一些后缀，如：B-Person、B-Location...这都可以根据你的实际任务来自行选择。

处理序列标注问题的常用模型包括隐马尔可夫模型（HMM）、条件随机场（CRF）、BiLSTM + CRF。
** 建模方式
解决序列标注问题的方式大体可分为两种，一种是概率图模型，另一种是深度学习模型。
*** 概率图模型
从发展时间和应用效果来看，依次是HMM、MEMM、CRF。
**** HMM
**** MEMM
**** CRF
*** 深度学习模型
**** DNN
按照前面所说的，可以把序列标注问题当作分类问题来解，此时相当于对输入观测序列的每个token进行一次多分类，DNN自然可以派上用场。大致流程如下图所示：

#+DOWNLOADED: screenshot @ 2021-12-08 22:03:24
[[file:images/机器学习/序列标注（Sequence_Tagging）/2021-12-08_22-03-24_screenshot.png]]

这里的DNN，通常采用LSTM、GRU、Transformer等。
**** DNN+CRF
仅用DNN的情况下会存在一个问题，由于每个输出标记的预测是独立的，可能会导致出现类似“B_PER I_LOC”的标记序列。因此，考虑引入CRF学习标记之间的约束关系，（也可以采用其它的方式，如beam search解码，但是CRF将标记序列作为整体看待，效果更好）。大致流程如下图所示：

#+DOWNLOADED: screenshot @ 2021-12-08 22:04:20
[[file:images/机器学习/序列标注（Sequence_Tagging）/2021-12-08_22-04-20_screenshot.png]]
在这里，DNN的输出是当前时刻对所有标记的打分，对应CRF打分函数中的 [公式] ，打分函数中的另一项 [公式] 则作为CRF层的参数进行学习（随机初始化），由此可以通过最大化似然概率进行模型训练，训练完成后，使用Viterbi算法即可完成预测。

序列标注模型的框架基本上可以分为特征提取层和序列标注层。在DNN+CRF模型中，DNN是特征提取层，CRF是序列标注层；而在仅用CRF的传统机器学习模型中，特征提取层由人工模板特征来替代，因此它的特点是task-specific，较之于DNN而言，不利于迁移和泛化。

当然，随着BERT这种大规模语料pretrain模型的出现，DNN能够抽取的特征变得丰富，完全能够捕捉到标记序列间的依赖关系，CRF层也就非必需了
** BERT中进行NER为什么没有使用CRF
BERT中进行NER为什么没有使用CRF，我们使用DL进行序列标注问题的时候CRF是必备么？

之所以在序列标注模型中引入CRF，是为了建模标注序列内部的依赖或约束，常见的任务就是NER了。以IOBES标注方案为例，显然一个B标注后面不能紧跟着一个S标注或者O标注。如果直接在神经网络上层用softmax为每个词产生一个标注，那么很可能产出的标注序列是invalid的。所以一种方案就是在得到神经网络上层表示后，作每个标注决策时“偷看”一下前面的标注，也就是套用一个可学习的CRF层来获得整个序列上最大似然的结果（与之相对的是为每个单词独立作决策）。但是标注序列的依赖不一定需要CRF这样显式的模型。如果神经网络的结构够强，CRF可能是不必要的，甚至表现要更差。需要看到的是：如果结构合理，神经网络是具有捕捉标注序列中的依赖的潜能的；CRF中的马尔科夫假设对决策过程是有影响/限制的；Viterbi解码计算复杂度不低，所以CRF训练起来计算量不小。总而言之，如果有结构合理的神经网络，nn+CRF的模式就不是必需。可以参考一下西湖大学张岳老师发在EMNLP2019的这篇Hierarchically-Refined Label Attention Network for Sequence Labeling
*** 参考文章
[[https://www.zhihu.com/question/358892919][BERT中进行NER为什么没有使用CRF，我们使用DL进行序列标注问题的时候CRF是必备么？]]
** 参考文章
[[https://zhuanlan.zhihu.com/p/268579769][序列标注]]
[[https://www.cnblogs.com/shona/p/12121473.html][NLP | 序列标注 总结]]
* BiLSTM + CRF做NER
** BILSTM + CRF介绍
*** 介绍
 基于神经网络的方法，在命名实体识别任务中非常流行和普遍。 如果你不知道Bi-LSTM和CRF是什么，你只需要记住他们分别是命名实体识别模型中的两个层。

 我们假设我们的数据集中有两类实体——人名和地名，与之相对应在我们的训练数据集中，有五类标签：
 #+BEGIN_EXAMPLE
 B-Person， I- Person，B-Organization，I-Organization
 #+END_EXAMPLE
 假设句子x由五个字符w1,w2,w3,w4,w5组成，其中【w1,w2】为人名类实体，【w3】为地名类实体，其他字符标签为“O”。
*** BiLSTM-CRF模型
 以下将给出模型的结构：
 第一，句子x中的每一个单元都代表着由字嵌入或词嵌入构成的向量。其中，字嵌入是随机初始化的，词嵌入是通过数据训练得到的。所有的嵌入在训练过程中都会调整到最优。
 第二，这些字或词嵌入为BiLSTM-CRF模型的输入，输出的是句子x中每个单元的标签。

 #+DOWNLOADED: screenshot @ 2021-12-16 22:12:52
 [[file:images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BiLSTM_+_CRF%E5%81%9ANER/2021-12-16_22-12-52_screenshot.png]]

 尽管一般不需要详细了解BiLSTM层的原理，但是为了更容易知道CRF层的运行原理，我们需要知道BiLSTM的输出层。
 #+DOWNLOADED: screenshot @ 2021-12-16 22:13:13
 [[file:images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BiLSTM_+_CRF%E5%81%9ANER/2021-12-16_22-13-13_screenshot.png]]

 如上图所示，BiLSTM层的输出为每一个标签的预测分值，例如，对于单元w0,BiLSTM层输出的是
 #+BEGIN_EXAMPLE
 1.5 (B-Person), 0.9 (I-Person), 0.1 (B-Organization), 0.08 (I-Organization) 0.05 (O).
 #+END_EXAMPLE

 这些分值将作为CRF的输入。 ### 1.3 如果没有CRF层会怎样

 你也许已经发现了，即使没有CRF层，我们也可以训练一个BiLSTM命名实体识别模型，如图所示：
 #+DOWNLOADED: screenshot @ 2021-12-16 22:13:30
 [[file:images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BiLSTM_+_CRF%E5%81%9ANER/2021-12-16_22-13-30_screenshot.png]]
 由于BiLSTM的输出为单元的每一个标签分值，我们可以挑选分值最高的一个作为该单元的标签。例如，对于单元w0,“B-Person”有最高分值—— 1.5，因此我们可以挑选“B-Person”作为w0的预测标签。同理，我们可以得到w1——“I-Person”，w2—— “O” ，w3——“B-Organization”，w4——“O”。

 虽然我们可以得到句子x中每个单元的正确标签，但是我们不能保证标签每次都是预测正确的。例如，图4.中的例子，标签序列是“I-Organization I-Person” and “B-Organization I-Person”，很显然这是错误的。

 #+DOWNLOADED: screenshot @ 2021-12-16 22:13:47
 [[file:images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BiLSTM_+_CRF%E5%81%9ANER/2021-12-16_22-13-47_screenshot.png]]
*** CRF层能从训练数据中获得约束性的规则
 CRF层可以为最后预测的标签添加一些约束来保证预测的标签是合法的。在训练数据训练过程中，这些约束可以通过CRF层自动学习到。 这些约束可以是： I：句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-” II：标签“B-label1 I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-Person I-Person” 是合法的序列, 但是“B-Person I-Organization” 是非法标签序列. III：标签序列“O I-label” is 非法的.实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说,有效的标签序列应该是“O B-label”。 有了这些约束，标签序列预测中非法序列出现的概率将会大大降低。
** 标签的score和损失函数的定义

** 参考文章
[[https://zhuanlan.zhihu.com/p/59845590][Pytorch BiLSTM + CRF做NER]]
* fp16
* set.seed()设置种子的作用
主要作用：可重现一样的结果
R语言中set.seed()作用是设定生成随机数的种子，目的是为了让结果具有重复性，重现结果。

* 汉语词性对照表
| 代码 | 名称       | 说明                                                 | 举例                                                                 |
|------+------------+------------------------------------------------------+----------------------------------------------------------------------|
| a    | 形容词     | 取英语形容词adjective的第1个字母                     | 最/d 大/a 的/u                                                       |
| ad   | 副形词     | 直接作状语的形容词.形容词代码a和副词代码d并在一起    | 一定/d 能够/v 顺利/ad 实现/v 。/w                                    |
| ag   | 形语素     | 形容词性语素。形容词代码为a，语素代码ｇ前面置以a     | 喜/v 煞/ag 人/n                                                      |
| an   | 名形词     | 具有名词功能的形容词。形容词代码a和名词代码n并在一起 | 人民/n 的/u 根本/a 利益/n 和/c 国家/n 的/u 安稳/an 。/w              |
| b    | 区别词     | 取汉字“别”的声母                                     | 副/b 书记/n 王/nr 思齐/nr                                            |
| c    | 连词       | 取英语连词conjunction的第1个字母                     | 全军/n 和/c 武警/n 先进/a 典型/n 代表/n                              |
| d    | 副词       | 取adverb的第2个字母，因其第1个字母已用于形容词       | 两侧/f 台柱/n 上/ 分别/d 雄踞/v 着/u                                 |
| dg   | 副语素     | 副词性语素。副词代码为d，语素代码ｇ前面置以d         | 用/v 不/d 甚/dg 流利/a 的/u 中文/nz 主持/v 节目/n 。/w               |
| e    | 叹词       | 取英语叹词exclamation的第1个字母                     | 嗬/e ！/w                                                            |
| f    | 方位词     | 取汉字“方” 的声母                                    | 从/p 一/m 大/a 堆/q 档案/n 中/f 发现/v 了/u                          |
| g    | 语素       | 绝大多数语素都能作为合成词的“词根”，取汉字“根”的声母 | 例如dg 或ag                                                          |
| h    | 前接成分   | 取英语head的第1个字母                                | 目前/t 各种/r 非/h 合作制/n 的/u 农产品/n                            |
| i    | 成语       | 取英语成语idiom的第1个字母                           | 提高/v 农民/n 讨价还价/i 的/u 能力/n 。/w                            |
| j    | 简称略语   | 取汉字“简”的声母                                     | 民主/ad 选举/v 村委会/j 的/u 工作/vn                                 |
| k    | 后接成分   |                                                      | 权责/n 明确/a 的/u 逐级/d 授权/v 制/k                                |
| l    | 习用语     | 习用语尚未成为成语，有点“临时性”，取“临”的声母       | 是/v 建立/v 社会主义/n 市场经济/n 体制/n 的/u 重要/a 组成部分/l 。/w |
| m    | 数词       | 取英语numeral的第3个字母，n，u已有他用               | 科学技术/n 是/v 第一/m 生产力/n                                      |
| n    | 名词       | 取英语名词noun的第1个字母                            | 希望/v 双方/n 在/p 市政/n 规划/vn                                    |
| ng   | 名语素     | 名词性语素。名词代码为n，语素代码ｇ前面置以n         | 就此/d 分析/v 时/Ng 认为/v                                           |
| nr   | 人名       | 名词代码n和“人(ren)”的声母并在一起                   | 建设部/nt 部长/n 侯/nr 捷/nr                                         |
| ns   | 地名       | 名词代码n和处所词代码s并在一起                       | 北京/ns 经济/n 运行/vn 态势/n 喜人/a                                 |
| nt   | 机构团体   | “团”的声母为t，名词代码n和t并在一起                  | [冶金/n 工业部/n 洛阳/ns 耐火材料/l 研究院/n]nt                      |
| nx   | 字母专名   |                                                      | ＡＴＭ/nx 交换机/n                                                   |
| nz   | 其他专名   | “专”的声母的第1个字母为z，名词代码n和z并在一起       | 德士古/nz 公司/n                                                     |
| o    | 拟声词     | 取英语拟声词onomatopoeia的第1个字母                  | 汩汩/o 地/u 流/v 出来/v                                              |
| p    | 介词       | 取英语介词prepositional的第1个字母                   | 往/p 基层/n 跑/v 。/w                                                |
| q    | 量词       | 取英语quantity的第1个字母                            | 不止/v 一/m 次/q 地/u 听到/v ，/w                                    |
| r    | 代词       | 取英语代词pronoun的第2个字母,因p已用于介词           | 有些/r 部门/n                                                        |
| s    | 处所词     | 取英语space的第1个字母                               | 移居/v 海外/s 。/w                                                   |
| t    | 时间词     | 取英语time的第1个字母                                | 当前/t 经济/n 社会/n 情况/n                                          |
| tg   | 时语素     | 时间词性语素。时间词代码为t,在语素的代码g前面置以t   | 秋/Tg 冬/tg 连/d 旱/a                                                |
| u    | 助词       | 取英语助词auxiliary 的第2个字母,因a已用于形容词      | 工作/vn 的/u 政策/n                                                  |
| ud   | 结构助词   |                                                      | 有/v 心/n 栽/v 得/ud 梧桐树/n                                        |
| ug   | 时态助词   |                                                      | 你/r 想/v 过/ug 没有/v                                               |
| uj   | 结构助词的 |                                                      | 迈向/v 充满/v 希望/n 的/uj 新/a 世纪/n                               |
| ul   | 时态助词了 |                                                      | 完成/v 了/ ul                                                        |
| uv   | 结构助词地 |                                                      | 满怀信心/l 地/uv 开创/v 新/a 的/u 业绩/n                             |
| uz   | 时态助词着 |                                                      | 眼看/v 着/uz                                                         |
| v    | 动词       |                                                      | 举行/v 老/a 干部/n 迎春/vn 团拜会/n                                  |
| vd   | 副动词     |                                                      | 强调/vd 指出/v                                                       |
| vg   | 动语素     | 动词性语素。动词代码为v。在语素的代码g前面置以V      | 做好/v 尊/vg 干/j 爱/v 兵/n 工作/vn                                  |
| vn   | 名动词     | 指具有名词功能的动词。动词和名词的代码并在一起       | 股份制/n 这种/r 企业/n 组织/vn 形式/n ，/w                           |
| w    | 标点符号   |                                                      | 生产/v 的/u ５Ｇ/nx 、/w ８Ｇ/nx 型/k 燃气/n 热水器/n                |
| x    | 非语素字   | 非语素字只是一个符号，字母x通常用于代表未知数、符号  |                                                                      |
| y    | 语气词     | 取汉字“语”的声母                                     | 已经/d ３０/m 多/m 年/q 了/y 。/w                                    |
| z    | 状态词     | 取汉字“状”的声母的前一个字母                         | 势头/n 依然/z 强劲/a ；/w                                            |
* 依存句法分析
句法分析（syntactic parsing）是自然语言处理中的关键技术之一，它是对输入的文本句子进行分析以得到句子的句法结构的处理过程。对句法结构进行分析，一方面是语言理解的自身需求，句法分析是语言理解的重要一环，另一方面也为其它自然语言处理任务提供支持。例如句法驱动的统计机器翻译需要对源语言或目标语言（或者同时两种语言）进行句法分析；语义分析通常以句法分析的输出结果作为输入以便获得更多的指示信息。
** 句法分析任务
根据句法结构的表示形式不同，最常见的句法分析任务可以分为以下三种：
- 句法结构分析（syntactic structure parsing），又称短语结构分析（phrase structure parsing），也叫成分句法分析（constituent syntactic parsing）。作用是识别出句子中的短语结构以及短语之间的层次句法关系。

- 依存关系分析，又称依存句法分析（dependency syntactic parsing），简称依存分析，作用是识别句子中词汇与词汇之间的相互依存关系。

- 深层文法句法分析，即利用深层文法，例如词汇化树邻接文法（Lexicalized Tree Adjoining Grammar， LTAG）、词汇功能文法（Lexical Functional Grammar， LFG）、组合范畴文法（Combinatory Categorial Grammar， CCG）等，对句子进行深层的句法以及语义分析。

** 依存句法定义
百度百科定义：依存句法是由法国语言学家L.Tesniere最先提出。它将句子分析成一颗依存句法树，描述出各个词语之间的依存关系。也即指出了词语之间在句法上的搭配关系，这种搭配关系是和语义相关联的。

维基百科定义：The dependency-based parse trees of dependency grammars see all nodes as terminal, which means they do not acknowledge the distinction between terminal and non-terminal categories. They are simpler on average than constituency-based parse trees because they contain fewer nodes.

在自然语言处理中，用词与词之间的依存关系来描述语言结构的框架称为依存语法（dependence grammar），又称从属关系语法。利用依存句法进行句法分析也是自然语言理解的重要技术之一。

** 依存分析的一些重要概念：

依存句法认为“谓语”中的动词是一个句子的中心，其他成分与动词直接或间接地产生联系。

依存句法理论中，“依存”指词与词之间支配与被支配的关系，这种关系不是对等的，这种关系具有方向。确切的说，处于支配地位的成分称之为支配者（governor，regent，head），而处于被支配地位的成分称之为从属者（modifier，subordinate，dependency）。

依存语法本身没有规定要对依存关系进行分类，但为了丰富依存结构传达的句法信息，在实际应用中，一般会给依存树的边加上不同的标记。

依存语法存在一个共同的基本假设：句法结构本质上包含词和词之间的依存（修饰）关系。一个依存关系连接两个词，分别是核心词（head）和依存词（dependent）。依存关系可以细分为不同的类型，表示两个词之间的具体句法关系。
* LTP
** 分词标注集
| 标记 | 含义     | 举例       |
| B    | 词首     | __中__国   |
| I    | 词中     | 哈__工__大 |
| E    | 词尾     | 科__学__   |
| S    | 单字成词 | 的         |
** 词性标注集
LTP 使用的是863词性标注集，其各个词性含义如下表。

| Tag | Description         | Example    | Tag | Description       | Example    |
| a   | adjective           | 美丽       | ni  | organization name | 保险公司   |
| b   | other noun-modifier | 大型, 西式 | nl  | location noun     | 城郊       |
| c   | conjunction         | 和, 虽然   | ns  | geographical name | 北京       |
| d   | adverb              | 很         | nt  | temporal noun     | 近日, 明代 |
| e   | exclamation         | 哎         | nz  | other proper noun | 诺贝尔奖   |
| g   | morpheme            | 茨, 甥     | o   | onomatopoeia      | 哗啦       |
| h   | prefix              | 阿, 伪     | p   | preposition       | 在, 把     |
| i   | idiom               | 百花齐放   | q   | quantity          | 个         |
| j   | abbreviation        | 公检法     | r   | pronoun           | 我们       |
| k   | suffix              | 界, 率     | u   | auxiliary         | 的, 地     |
| m   | number              | 一, 第一   | v   | verb              | 跑, 学习   |
| n   | general noun        | 苹果       | wp  | punctuation       | ，。！     |
| nd  | direction noun      | 右侧       | ws  | foreign words     | CPU        |
| nh  | person name         | 杜甫, 汤姆 | x   | non-lexeme        | 萄, 翱     |
|     |                     |            | z   | descriptive words | 瑟瑟，匆匆 |
** 命名实体识别标注集
NE识别模块的标注结果采用O-S-B-I-E标注形式，其含义为

| 标记 | 含义                 |
| O    | 这个词不是NE         |
| S    | 这个词单独构成一个NE |
| B    | 这个词为一个NE的开始 |
| I    | 这个词为一个NE的中间 |
| E    | 这个词位一个NE的结尾 |
LTP中的NE 模块识别三种NE，分别如下：

| 标记 | 含义   |
| Nh   | 人名   |
| Ni   | 机构名 |
| Ns   | 地名   |

** 依存句法关系
| 关系类型   | Tag | Description               | Example                   |
|------------+-----+---------------------------+---------------------------|
| 主谓关系   | SBV | subject-verb              | 我送她一束花 (我 <– 送)   |
| 动宾关系   | VOB | 直接宾语，verb-object     | 我送她一束花 (送 –> 花)   |
| 间宾关系   | IOB | 间接宾语，indirect-object | 我送她一束花 (送 –> 她)   |
| 前置宾语   | FOB | 前置宾语，fronting-object | 他什么书都读 (书 <– 读)   |
| 兼语       | DBL | double                    | 他请我吃饭 (请 –> 我)     |
| 定中关系   | ATT | attribute                 | 红苹果 (红 <– 苹果)       |
| 状中结构   | ADV | adverbial                 | 非常美丽 (非常 <– 美丽)   |
| 动补结构   | CMP | complement                | 做完了作业 (做 –> 完)     |
| 并列关系   | COO | coordinate                | 大山和大海 (大山 –> 大海) |
| 介宾关系   | POB | preposition-object        | 在贸易区内 (在 –> 内)     |
| 左附加关系 | LAD | left adjunct              | 大山和大海 (和 <– 大海)   |
| 右附加关系 | RAD | right adjunct             | 孩子们 (孩子 –> 们)       |
| 独立结构   | IS  | independent structure     | 两个单句在结构上彼此独立  |
| 核心关系   | HED | head                      | 指整个句子的核心          |
** 语义角色类型

| 语义角色类型 | 说明                                        |
| ADV          | adverbial, default tag ( 附加的，默认标记 ) |
| BNE          | beneﬁciary ( 受益人 )                       |
| CND          | condition ( 条件 )                          |
| DIR          | direction ( 方向 )                          |
| DGR          | degree ( 程度 )                             |
| EXT          | extent ( 扩展 )                             |
| FRQ          | frequency ( 频率 )                          |
| LOC          | locative ( 地点 )                           |
| MNR          | manner ( 方式 )                             |
| PRP          | purpose or reason ( 目的或原因 )            |
| TMP          | temporal ( 时间 )                           |
| TPC          | topic ( 主题 )                              |
| CRD          | coordinated arguments ( 并列参数 )          |
| PRD          | predicate ( 谓语动词 )                      |
| PSR          | possessor ( 持有者 )                        |
| PSE          | possessee ( 被持有 )                        |
