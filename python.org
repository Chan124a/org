
#+OPTIONS: ^:nil

* 基本数据类型
** 字符串
字符创用单引号 =‘ ‘= 、双引号 =“ “= 、三引号 =‘‘‘ ‘‘‘= 表示

单引号可以嵌入到双引号和三引号，双引号可以嵌入到三引号里面

三引号可以用于表示分行的字符串
#+BEGIN_SRC python :results output :exports both
str2="a 'b' c"
print(str2) 

str3='''he said:''hello'' '''
print(str3)
#+END_SRC

#+RESULTS:
: a 'b' c
: he said:''hello'' 
*** split()方法
 Python split() 通过指定分隔符对字符串进行切片，如果参数 num 有指定值，则分隔 num+1 个子字符串
 #+BEGIN_SRC python
 str.split(str="", num=string.count(str)).
 #+END_SRC
 - str -- 分隔符，默认为所有的空字符，包括空格、换行(\n)、制表符(\t)等。
 - num -- 分割次数。默认为 -1, 即分隔所有。
**** 例子
 #+BEGIN_SRC python
 #!/usr/bin/python
 # -*- coding: UTF-8 -*-
 
 str = "Line1-abcdef \nLine2-abc \nLine4-abcd";
 print(str.split());       # 以空格为分隔符，包含 \n
 print(str.split(' ', 1 )); # 以空格为分隔符，分隔成两个
 #+END_SRC
 以上实例输出结果如下：
 #+BEGIN_SRC python
 ['Line1-abcdef', 'Line2-abc', 'Line4-abcd']
 ['Line1-abcdef', '\nLine2-abc \nLine4-abcd']
 #+END_SRC
 以下实例以 # 号为分隔符，指定第二个参数为 1，返回两个参数列表。
 #+BEGIN_SRC python
 #!/usr/bin/python
 # -*- coding: UTF-8 -*-
 
 txt = "Google#Runoob#Taobao#Facebook"
 
 # 第二个参数为 1，返回两个参数列表
 x = txt.split("#", 1)
 
 print(x)
 #+END_SRC
 以上实例输出结果如下：
 #+BEGIN_SRC python
 ['Google', 'Runoob#Taobao#Facebook']
 #+END_SRC

*** strip()方法
Python strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。

注意：该方法只能删除开头或是结尾的字符，不能删除中间部分的字符。

str.strip([chars]);

参数：
chars -- 移除字符串头尾指定的字符序列。

#+BEGIN_SRC python
#!/usr/bin/python
# -*- coding: UTF-8 -*-
 
str = "00000003210Runoob01230000000"; 
print str.strip( '0' );  # 去除首尾字符 0
 
 
str2 = "   Runoob      ";   # 去除首尾空格
print str2.strip();
#+END_SRC

#+BEGIN_EXAMPLE
以上实例输出结果如下：

3210Runoob0123
Runoob
#+END_EXAMPLE
*** join() 方法
使用哈希字符作为分隔符，将元组中的所有项目连接到字符串中：
#+BEGIN_SRC python
myTuple = ("Bill", "Steve", "Elon")

x = "#".join(myTuple)

print(x)  # Bill#Steve#Elon
#+END_SRC
*** in
in这个词在字符串操作中是一个布尔操作符，它读取两个字符串，如果前者的字符串为后者所包含，就返回真，否则为假，看例子：
#+begin_src python
>>> 'a' in 'banana'

True

>>> 'app' in 'banana'

False
#+END_SRC
*** startswith()方法
startswith() 方法用于检查字符串是否是以指定子字符串开头，如果是则返回 True，否则返回 False。如果参数 beg 和 end 指定值，则在指定范围内检查。

~str.startswith(str, beg=0,end=len(string))~

参数：
- str -- 检测的字符串。
- strbeg -- 可选参数用于设置字符串检测的起始位置。
- strend -- 可选参数用于设置字符串检测的结束位置。

返回值：
如果检测到字符串则返回True，否则返回False。

**** 例子
#+begin_src python
#!/usr/bin/python

str = "this is string example....wow!!!";
print str.startswith( 'this' );
print str.startswith( 'is', 2, 4 );
print str.startswith( 'this', 2, 4 );
#+END_SRC

以上实例输出结果如下：
#+BEGIN_EXAMPLE
True
True
False
#+END_EXAMPLE

** 布尔值
python中可以直接用 True、False 表示布尔值（请注意大小写），也可以通过布尔运算计算出来。
** 空值
python中用None表示空值
** 列表
列表的数据项不需要具有相同的类型

创建一个列表，只要把逗号分隔的不同的数据项使用方括号括起来即可。如下所示：
#+BEGIN_SRC python
list1 = ['physics', 'chemistry', 1997, 2000]
list2 = [1, 2, 3, 4, 5 ]
list3 = ["a", "b", "c", "d"]
#+END_SRC
*** Python列表脚本操作符
列表对 + 和 * 的操作符与字符串相似。+ 号用于组合列表，* 号用于重复列表。

如下所示：
| Python 表达式                | 结果                         | 描述                 |
|------------------------------+------------------------------+----------------------|
| len([1, 2, 3])               | 3                            | 长度                 |
| [1, 2, 3] + [4, 5, 6]        | [1, 2, 3, 4, 5, 6]           | 组合                 |
| ['Hi!'] * 4                  | ['Hi!', 'Hi!', 'Hi!', 'Hi!'] | 重复                 |
| 3 in [1, 2, 3]               | True                         | 元素是否存在于列表中 |
| for x in [1, 2, 3]: print x, | 1 2 3                        | 迭代                 |
*** 列表生成式
列表生成式即List Comprehensions，是Python内置的非常简单却强大的可以用来创建list的生成式。

举个例子，要生成list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]可以用list(range(1, 11))：
#+BEGIN_SRC python
>>> list(range(1, 11))
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
#+END_SRC
但如果要生成[1x1, 2x2, 3x3, ..., 10x10]怎么做？方法一是循环：
#+BEGIN_SRC python
>>> L = []
>>> for x in range(1, 11):
...    L.append(x * x)
...
>>> L
[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]
#+END_SRC
但是循环太繁琐，而列表生成式则可以用一行语句代替循环生成上面的list：
#+BEGIN_SRC python
>>> [x * x for x in range(1, 11)]
[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]
#+END_SRC
写列表生成式时，把要生成的元素x * x放到前面，后面跟for循环，就可以把list创建出来，十分有用，多写几次，很快就可以熟悉这种语法。

for循环后面还可以加上if判断，这样我们就可以筛选出仅偶数的平方：
#+BEGIN_SRC python
>>> [x * x for x in range(1, 11) if x % 2 == 0]
[4, 16, 36, 64, 100]
#+END_SRC
for循环后面还可以加上if判断，这样我们就可以筛选出仅偶数的平方：
#+BEGIN_SRC python
>>> [x * x for x in range(1, 11) if x % 2 == 0]
[4, 16, 36, 64, 100]
#+END_SRC
还可以使用两层循环，可以生成全排列：
#+BEGIN_SRC python
>>> [m + n for m in 'ABC' for n in 'XYZ']
['AX', 'AY', 'AZ', 'BX', 'BY', 'BZ', 'CX', 'CY', 'CZ']
#+END_SRC
三层和三层以上的循环就很少用到了。

运用列表生成式，可以写出非常简洁的代码。例如，列出当前目录下的所有文件和目录名，可以通过一行代码实现：
#+BEGIN_SRC python
>>> import os # 导入os模块，模块的概念后面讲到
>>> [d for d in os.listdir('.')] # os.listdir可以列出文件和目录
['.emacs.d', '.ssh', '.Trash', 'Adlm', 'Applications', 'Desktop', 'Documents', 'Downloads', 'Library', 'Movies', 'Music', 'Pictures', 'Public', 'VirtualBox VMs', 'Workspace', 'XCode']
#+END_SRC
for循环其实可以同时使用两个甚至多个变量，比如dict的items()可以同时迭代key和value：
#+BEGIN_SRC python
>>> d = {'x': 'A', 'y': 'B', 'z': 'C' }
>>> for k, v in d.items():
...     print(k, '=', v)
...
y = B
x = A
z = C
#+END_SRC
因此，列表生成式也可以使用两个变量来生成list：
#+BEGIN_SRC python
>>> d = {'x': 'A', 'y': 'B', 'z': 'C' }
>>> [k + '=' + v for k, v in d.items()]
['y=B', 'x=A', 'z=C']
#+END_SRC
最后把一个list中所有的字符串变成小写：
#+BEGIN_SRC python
>>> L = ['Hello', 'World', 'IBM', 'Apple']
>>> [s.lower() for s in L]
['hello', 'world', 'ibm', 'apple']
#+END_SRC
**** if ... else
使用列表生成式的时候，有些童鞋经常搞不清楚if...else的用法。

例如，以下代码正常输出偶数：
#+BEGIN_SRC python
>>> [x for x in range(1, 11) if x % 2 == 0]
[2, 4, 6, 8, 10]
#+END_SRC
但是，我们不能在最后的if加上else：
#+BEGIN_SRC python
>>> [x for x in range(1, 11) if x % 2 == 0 else 0]
  File "<stdin>", line 1
    [x for x in range(1, 11) if x % 2 == 0 else 0]
                                              ^
SyntaxError: invalid syntax
#+END_SRC
这是因为跟在for后面的if是一个筛选条件，不能带else，否则如何筛选？

另一些童鞋发现把if写在for前面必须加else，否则报错：
#+BEGIN_SRC python
>>> [x if x % 2 == 0 for x in range(1, 11)]
  File "<stdin>", line 1
    [x if x % 2 == 0 for x in range(1, 11)]
                       ^
SyntaxError: invalid syntax
#+END_SRC
这是因为for前面的部分是一个表达式，它必须根据x计算出一个结果。因此，考察表达式：x if x % 2 == 0，它无法根据x计算出结果，因为缺少else，必须加上else：
#+BEGIN_SRC python
>>> [x if x % 2 == 0 else -x for x in range(1, 11)]
[-1, 2, -3, 4, -5, 6, -7, 8, -9, 10]
#+END_SRC
上述for前面的表达式x if x % 2 == 0 else -x才能根据x计算出确定的结果。

可见，在一个列表生成式中，for前面的if ... else是表达式，而for后面的if是过滤条件，不能带else。
*** [None]的说明
#+BEGIN_SRC python
>>> a=[None]
>>> a.append(1)
>>> a
[None, 1]
>>> len(a)  #注意None是有占一个空间的
2

#+END_SRC
*** pop()
pop() 函数用于移除列表中的一个元素（默认最后一个元素），并且返回该元素的值。
#+BEGIN_SRC python
list.pop([index=-1])
#+END_SRC
- obj -- 可选参数，要移除列表元素的索引值，不能超过列表总长度，默认为 index=-1，删除最后一个列表值。

该方法返回从列表中移除的元素对象。
#+BEGIN_SRC python
list1 = ['Google', 'Runoob', 'Taobao']
list_pop=list1.pop(1)
print "删除的项为 :", list_pop
print "列表现在为 : ", list1
#+END_SRC
以上实例输出结果如下：
#+BEGIN_SRC python
删除的项为 : Runoob
列表现在为 :  ['Google', 'Taobao']
#+END_SRC

*** extend()
extend() 函数用于在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）。
#+BEGIN_SRC python
#!/usr/bin/python

aList = [123, 'xyz', 'zara', 'abc', 123];
bList = [2009, 'manni'];
aList.extend(bList)

print "Extended List : ", aList ;
#+END_SRC
以上实例输出结果如下：
#+BEGIN_EXAMPLE
Extended List :  [123, 'xyz', 'zara', 'abc', 123, 2009, 'manni']
#+END_EXAMPLE

** 字典(Dictionary)
*** items()方法
 ~items()~ 函数以列表返回可遍历的(键, 值) 元组数组。
 #+BEGIN_SRC python
 #!/usr/bin/python
 # coding=utf-8
 dict = {'Google': 'www.google.com', 'Runoob': 'www.runoob.com', 'taobao': 'www.taobao.com'}

 print "字典值 : %s" %  dict.items()
 
 # 遍历字典列表
 for key,values in  dict.items():
     print key,values
 #+END_SRC
 输出结果为：
 #+BEGIN_SRC bash
 字典值 : [('Google', 'www.google.com'), ('taobao', 'www.taobao.com'), ('Runoob', 'www.runoob.com')]
 Google www.google.com
 taobao www.taobao.com
 Runoob www.runoob.com
 #+END_SRC
*** pop()
 删除字典给定键 key 及对应的值，返回值为被删除的值。若字典中没有对应的值，则返回key后的值：

 例子：
 #+BEGIN_SRC python
 #在字典kwargs中删除num_train_samples对应的值并将删除的值返回给A，若字典kwargs中没有num_train_samples对应的值，则将1000赋值给A
 A = kwargs.pop("num_train_samples", 1000)
 #+END_SRC
*** get()
~dict.get(key, default=None)~
返回指定键的值，如果键不在字典中返回默认值 None 或者设置的默认值。
**** 参数
- key -- 字典中要查找的键。
- default -- 如果指定键的值不存在时，返回该默认值。
**** 例子
#+BEGIN_SRC python
dict = {'Name': 'Runoob', 'Age': 27}

print "Value : %s" %  dict.get('Age')
print "Value : %s" %  dict.get('Sex', "Never")

#Value : 27
#Value : Never
#+END_SRC

* 基本数据类型转换
Python 中基本数据类型转换的方法有下面几个。

| 方法                   | 说明                                                  |
|------------------------+-------------------------------------------------------|
| int(x [,base ])        | 将x转换为一个整数                                     |
| float(x )              | 将x转换到一个浮点数                                   |
| complex(real [,imag ]) | 创建一个复数                                          |
| str(x )                | 将对象 x 转换为字符串                                 |
| repr(x )               | 将对象 x 转换为表达式字符串                           |
| eval(str )             | 用来计算在字符串中的有效 Python 表达式,并返回一个对象 |
| tuple(s )              | 将序列 s 转换为一个元组                               |
| list(s )               | 将序列 s 转换为一个列表                               |
| chr(x )                | 将一个整数转换为一个字符                              |
| unichr(x )             | 将一个整数转换为 Unicode 字符                         |
| ord(x )                | 将一个字符转换为它的整数值                            |
| hex(x )                | 将一个整数转换为一个十六进制字符串                    |
| oct(x )                | 将一个整数转换为一个八进制字符串                      |

注：在 Python 3 里，只有一种整数类型 int，表示为长整型，没有 python2 中的 Long。
* 变量
Python 是不用声明数据类型的。在 Python 中 = 是赋值语句，跟其他的编程语言也是一样的，因为 Python 定义变量时不需要声明数据类型，因此可以把任意的数据类型赋值给变量，且同一个变量可以反复赋值，而且可以是不同的数据类型。

这种变量本身类型不固定的语言称之为动态语言，与之对应的是静态语言。静态语言在定义变量时必须指定变量类型，如果赋值的时候类型不匹配，就会报错。例如 Java 是静态语言。
** 变量的指向问题
这个跟c语言是一样的
#+BEGIN_SRC python :results output :exports both
a="hello python"
b=a
a=123
print(b)
#+END_SRC

#+RESULTS:
: hello python
** 多个变量赋值
Python 允许同时为多个变量赋值。例如：
#+BEGIN_SRC python :results output :exports both
a = b = c = 1
#+END_SRC

当然也可以为多个对象指定多个变量。例如：
#+BEGIN_SRC python :exports both
a, b, c = 1, 2, "liangdianshui"
#+END_SRC
以上实例，两个整型对象 1 和 2 的分配给变量 a 和 b，字符串对象 "liangdianshui" 分配给变量 c。
* 内置方法
** enumerate()
enumerate 函数用于遍历序列中的元素以及它们的下标
#+BEGIN_SRC python
enumerate(sequence, [start=0])
#+END_SRC
*** 参数
- sequence -- 一个序列、迭代器或其他支持迭代对象。
- start -- 下标起始位置。
*** 返回值
返回 enumerate(枚举) 对象。
*** 样例
#+BEGIN_SRC python
>>>seasons = ['Spring', 'Summer', 'Fall', 'Winter']
>>> list(enumerate(seasons))
[(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]
>>> list(enumerate(seasons, start=1))       # 下标从 1 开始
[(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')]

#+END_SRC
#+BEGIN_SRC python
>>>seq = ['one', 'two', 'three']
>>> for i, element in enumerate(seq):
...     print i, element
... 
0 one
1 two
2 three

#+END_SRC
** open
python open() 函数用于打开一个文件，创建一个 **file** 对象，相关的方法才可以调用它进行读写。
#+BEGIN_SRC python
open(name[, mode[, buffering]])
#示例
open(path, "r")
#+END_SRC
*** 参数
- name : 一个包含了你要访问的文件名称的字符串值。
- mode : mode 决定了打开文件的模式：只读，写入，追加等。所有可取值见如下的完全列表。这个参数是非强制的，默认文件访问模式为只读(r)。
- buffering : 如果 buffering 的值被设为 0，就不会有寄存。如果 buffering 的值取 1，访问文件时会寄存行。如果将 buffering 的值设为大于 1 的整数，表明了这就是的寄存区的缓冲大小。如果取负值，寄存区的缓冲大小则为系统默认。
*** 打开文件的模式
| 模式 | 描述                                                                                                                                                               |
| r    | 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。                                                                                                   |
| rb   | 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。                                                                                       |
| r+   | 打开一个文件用于读写。文件指针将会放在文件的开头。                                                                                                                 |
| rb+  | 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。                                                                                                     |
| w    | 打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。                                           |
| wb   | 以二进制格式打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。                               |
| w+   | 打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。                                             |
| wb+  | 以二进制格式打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。                                 |
| a    | 打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。             |
| ab   | 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 |
| a+   | 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。                                 |
| ab+  | 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。                                             |
**** 参考  
[[https://www.runoob.com/python/python-func-open.html][菜鸟教程]]

*** file 对象方法
- file.read([size])：size 未指定则返回整个文件，如果文件大小 >2 倍内存则有问题，f.read()读到文件尾时返回""(空字串)。
- file.readline()：返回一行。
- file.readlines([size]) ：返回包含size行的列表, size 未指定则返回全部行。
- for line in f: print line ：通过迭代器访问。
- f.write("hello\n")：如果要写入字符串以外的数据,先将他转换为字符串。
- f.tell()：返回一个整数,表示当前文件指针的位置(就是到文件头的比特数)。
- f.seek(偏移量,[起始位置])：用来移动文件指针。偏移量: 单位为比特，可正可负. 起始位置: 0 - 文件头, 默认值; 1 - 当前位置; 2 - 文件尾
- f.close() 关闭文件
** print
打印 Hello Python ，注意必须加单引号
#+BEGIN_SRC python
print ('hello python')
#+END_SRC

#+RESULTS:
: None
** super()
super() 函数是用于调用父类(超类)的一个方法。
#+BEGIN_SRC python
super(type[, object-or-type])
#+END_SRC
- type -- 类。
- object-or-type -- 类，一般是 self

Python3.x 和 Python2.x 的一个区别是: Python 3 可以使用直接使用 super().xxx 代替 super(Class, self).xxx :
#+BEGIN_SRC python
#!/usr/bin/python
# -*- coding: UTF-8 -*-
 
class FooParent(object):
    def __init__(self):
        self.parent = 'I\'m the parent.'
        print ('Parent')
    
    def bar(self,message):
        print ("%s from Parent" % message)
 
class FooChild(FooParent):
    def __init__(self):
        # super(FooChild,self) 首先找到 FooChild 的父类（就是类 FooParent），然后把类 FooChild 的对象转换为类 FooParent 的对象
        super(FooChild,self).__init__()    
        print ('Child')
        
    def bar(self,message):
        super(FooChild, self).bar(message)
        print ('Child bar fuction')
        print (self.parent)
 
if __name__ == '__main__':
    fooChild = FooChild()
    fooChild.bar('HelloWorld')
#+END_SRC
执行结果：
#+BEGIN_SRC bash
Parent
Child
HelloWorld from Parent
Child bar fuction
I'm the parent.
#+END_SRC
** hasattr()
~hasattr(object, name)~

hasattr() 函数用于判断对象是否包含对应的属性。如果对象有该属性返回 True，否则返回 False。
*** 参数
- object -- 对象。
- name -- 字符串，属性名。
*** 实例
#+BEGIN_SRC python
class Coordinate:
    x = 10
    y = -5
    z = 0
 
point1 = Coordinate() 
print(hasattr(point1, 'x'))
print(hasattr(point1, 'y'))
print(hasattr(point1, 'z'))
print(hasattr(point1, 'no'))  # 没有该属性
#+END_SRC
输出结果：
#+BEGIN_SRC bash
True
True
True
False
#+END_SRC
** getattr()
~getattr(object, name[, default])~
getattr() 函数用于返回一个对象属性值。
*** 参数
- object: 对象。
- name: 字符串，对象属性。
- default : 默认返回值，如果不提供该参数，在没有对应属性时，将触发 AttributeError。
*** 返回值
返回对象属性值。
*** 例子
#+begin_src bash
>>>class A(object):
...     bar = 1
... 
>>> a = A()
>>> getattr(a, 'bar')        # 获取属性 bar 值
1
>>> getattr(a, 'bar2')       # 属性 bar2 不存在，触发异常
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'A' object has no attribute 'bar2'
>>> getattr(a, 'bar2', 3)    # 属性 bar2 不存在，但设置了默认值
3
>>>
#+END_SRC
** range()
~range(start, stop[, step])~

*** 参数
- start: 计数从 start 开始。默认是从 0 开始。例如range（5）等价于range（0， 5）;
- stop: 计数到 stop 结束，但不包括 stop。例如：range（0， 5） 是[0, 1, 2, 3, 4]没有5
- step：步长，默认为1。例如：range（0， 5） 等价于 range(0, 5, 1)
*** 实例
#+BEGIN_SRC python
>>>range(10)        # 从 0 开始到 10
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> range(1, 11)     # 从 1 开始到 11
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
>>> range(0, 30, 5)  # 步长为 5
[0, 5, 10, 15, 20, 25]
>>> range(0, 10, 3)  # 步长为 3
[0, 3, 6, 9]
>>> range(0, -10, -1) # 负数
[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]
>>> range(0)
[]
>>> range(1, 0)
[]
#+END_SRC
** filter() 
filter() 函数用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表。

该接收两个参数，第一个为函数，第二个为序列，序列的每个元素作为参数传递给函数进行判断，然后返回 True 或 False，最后将返回 True 的元素放到新列表中。

注意: Python2.7 返回列表，Python3.x 返回迭代器对象，具体内容可以查看：Python3 filter() 函数

以下是 filter() 方法的语法:

filter(function, iterable)

参数
- function -- 判断函数, 如果参数为None的话，则默认会去除序列中所有值为假的元素
- iterable -- 可迭代对象。

#+BEGIN_EXAMPLE
在 Python中，认为以下值为假:

None       # None值
False       # False值
0              # 数值零不管它是int,float还是complex类型
'',(),[]        # 任何一个空的序列
{}             # 空的集合
#+END_EXAMPLE

返回值：
返回列表

关于filter()方法, python3和python2有一点不同

Python2.x 中返回的是过滤后的列表, 而 Python3 中返回到是一个 filter 类。

filter 类实现了 __iter__ 和 __next__ 方法, 可以看成是一个迭代器, 有惰性运算的特性, 相对 Python2.x 提升了性能, 可以节约内存。
#+BEGIN_SRC python
a = filter(lambda x: x % 2 == 0, range(10))
print(a)
#+END_SRC
输出
#+BEGIN_EXAMPLE
<filter object at 0x0000022EC66BB128>
#+END_EXAMPLE

*** 实例
以下展示了使用 filter 函数的实例：

过滤出列表中的所有奇数：
#+BEGIN_SRC python
#!/usr/bin/python
# -*- coding: UTF-8 -*-
 
def is_odd(n):
    return n % 2 == 1
 
newlist = filter(is_odd, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
print(newlist)
#+END_SRC
输出结果 ：
#+BEGIN_EXAMPLE
[1, 3, 5, 7, 9]
#+END_EXAMPLE

过滤出1~100中平方根是整数的数：
#+BEGIN_SRC python
#!/usr/bin/python
# -*- coding: UTF-8 -*-
 
import math
def is_sqr(x):
    return math.sqrt(x) % 1 == 0
 
newlist = filter(is_sqr, range(1, 101))
print(newlist)
#+END_SRC
输出结果 ：
#+BEGIN_EXAMPLE
[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]
#+END_EXAMPLE

** any()
any() 函数用于判断给定的可迭代参数 iterable 是否全部为 False，则返回 False，如果有一个为 True，则返回 True。

元素除了是 0、空、FALSE 外都算 TRUE。

函数等价于：
#+begin_src python
def any(iterable):
    for element in iterable:
        if element:
            return True
    return False
#+END_SRC

* 条件语句
** if
#+BEGIN_SRC python
if(...):
    comment...

#+END_SRC
* 三元表达式
#+BEGIN_SRC python
a = 1
b = 2
h = ""
#如果if后面的条件成立,则h=a-b,否则h=a+b
h = a-b if a>b else a+b
print(h)
#+END_SRC
* 参数
** 默认参数
#+BEGIN_SRC python
def test_defargs(one, two = 2):
   print 'Required argument: ', one
   print 'Optional argument: ', two

test_defargs(1)
# result:
# Required argument: 1
# Optional argument: 2

test_defargs(1, 3)
# result:
# Required argument: 1
# Optional argument: 3

#+END_SRC
** 可变参数
你可以将不定数量的参数传递给一个函数。不定的意思是：预先并不知道, 函数使用者会传递多少个参数给你, 所以在这个场景下使用这两个关键字。其实并不是必须写成 *args 和 **kwargs。  *(星号) 才是必须的. 你也可以写成 *ar  和 **k 。而写成 *args 和**kwargs 只是一个通俗的命名约定。

其中，*args是可变的positional arguments列表，**kwargs是可变的keyword arguments列表。并且，*args必须位于**kwargs之前，因为positional arguments必须位于keyword arguments之前。
- ~*args~ 表示任何多个无名参数，它本质是一个 tuple
- ~**kwargs~ 表示关键字参数，它本质上是一个 dict

下面一个例子使用*args，同时包含一个必须的参数：
#+BEGIN_SRC python
def test_args(first, *args):
   print 'Required argument: ', first
   for v in args:
      print 'Optional argument: ', v

test_args(1, 2, 3, 4)
# result:
# Required argument: 1
# Optional argument:  2
# Optional argument:  3
# Optional argument:  4

#+END_SRC
下面一个例子使用 ~**kwargs~, 同时包含一个必须的参数和*args列表：
#+BEGIN_SRC python
def test_kwargs(first, *args, **kwargs):
   print 'Required argument: ', first
   for v in args:
      print 'Optional argument (*args): ', v
   for k, v in kwargs.items():
      print 'Optional argument %s (*kwargs): %s' % (k, v)

test_kwargs(1, 2, 3, 4, k1=5, k2=6)
# results:
# Required argument:  1
# Optional argument (*args):  2
# Optional argument (*args):  3
# Optional argument (*args):  4
# Optional argument k2 (*kwargs): 6
# Optional argument k1 (*kwargs): 5

#+END_SRC
~*args~ 和 ~**kwargs~ 语法不仅可以在函数定义中使用，同样可以在函数调用的时候使用。不同的是，如果说在函数定义的位置使用*args和**kwargs是一个将参数pack的过程，那么在函数调用的时候就是一个将参数unpack的过程了。下面使用一个例子来加深理解：
#+BEGIN_SRC python
def test_args(first, second, third, fourth, fifth):
    print('First argument: ', first)
    print('Second argument: ', second)
    print('Third argument: ', third)
    print('Fourth argument: ', fourth)
    print('Fifth argument: ', fifth)

# Use *args
args = [1, 2, 3, 4, 5]
test_args(*args)
# results:
# First argument:  1
# Second argument:  2
# Third argument:  3
# Fourth argument:  4
# Fifth argument:  5

# Use **kwargs
kwargs = {
    'first': 1,   #注意这里的first、second等等都要和函数定义里的参数名一样，否则会报错
    'second': 2,
    'third': 3,
    'fourth': 4,
    'fifth': 5
}

test_args(**kwargs)
# results:
# First argument:  1
# Second argument:  2
# Third argument:  3
# Fourth argument:  4
# Fifth argument:  5

#+END_SRC
** positional argument vs keyword argument
positional argument位置参数，是指用相对位置指代参数。关键字参数（keyword argument），见名知意使用关键字指代参数。

位置参数或者按顺序传递参数，或者使用名字，使用名字时，对顺序没有要求。
#+BEGIN_SRC python
def fn(a, b, c=1):
    return a*b+c
print(fn(1, 2))          # 3, positional(a, b) and default(c)
print(fn(1, 2, 3))       # 5, positional(a, b)
print(fn(c=5, b=2, a=2)) # 9, named(b=2, a=2)
print(fn(c=5, 1, 2))     # syntax error
print(fn(b=2, a=2))      # 5, named(b=2, a=2) and default
print(fn(5, c=2, b=1))   # 7, positional(a), named(b).
print(fn(8, b=0))        # 1, positional(a), named(b), default(c=1)
#+END_SRC

* // 与 / 的含义
在 Python 2.2  ：要引用： from __future__ import division
  
" / "就表示 浮点数除法，返回浮点结果;" // "表示整数除法。

Python 3以后  ：

" / "就表示 浮点数除法，返回浮点结果;" // "表示整数除法。
* Python3 序列解包(*号的作用)
序列解包是 Python 3.0 之后才有的语法。
** 序列解包
那什么是序列解包呢？先看一个例子：
#+BEGIN_SRC bash
> a, b, c = 1, 2, 3
> a
1
> b
2
> c
3
#+END_SRC
这种方法并不限于列表和元组，而是适用于任意序列类型（甚至包括字符串和字节序列）。只要赋值运算符左边的变量数目与序列中的元素数目相等，你都可以用这种方法将元素序列解包到另一组变量中。

可以利用 ~*~ 表达式获取单个变量中的多个元素，只要它的解释没有歧义即可。

~*~ 获取的值默认为 list

获取剩余部分：
#+BEGIN_SRC bash
> a, b, *c = 0, 1, 2, 3
> a
0
> b
1
> c       
[2, 3]
#+END_SRC
获取中间部分：
#+BEGIN_SRC bash
> a, *b, c = 0, 1, 2, 3
> a
0
> b
[1, 2]
> c
3
#+END_SRC
如果左值比右值要多，那么带 * 的变量默认为空
#+BEGIN_SRC bash
> a, b, *c = 0, 1
> a
0
> b
1
> c
[]

> a, *b, c = 0, 1
> a
0
> b
[]
> c
1
#+END_SRC
嵌套解包:
#+BEGIN_SRC bash
> (a, b), (c, d) = (1, 2), (3, 4)
> a
1
> b
2
> c
3
> d
4
> a, b, c, d
(1, 2, 3, 4)
#+END_SRC
** 用在函数形参里
如：*parameter是用来接受任意多个参数并将其放在一个元组中。
#+BEGIN_SRC base
>>> def demo(*p):
 print(p)

>>> demo(1,2,3)
(1, 2, 3)
#+END_SRC

** 用在调用函数实参里
函数在调用多个参数时，在列表、元组、集合、字典及其他可迭代对象作为实参，并在前面加 *

如 ~*（1,2,3）~ 解释器将自动进行解包然后传递给多个单变量参数（参数个数要对应相等）。
#+BEGIN_SRC bash
>>> def d(a,b,c):
    print(a,b,c)
>>> d(1,2,3)
1 2 3
>>> a=[1,2,3]
>>> b=[1,2,3]
>>> c=[1,2,3]
>>> d(a,b,c)
[1,2,3] [1,2,3] [1,2,3]

>>> d(*a)
1 2 3
#+END_SRC
** 实战例子
假如一个字符串 'ABCDEFGH'，要输出下列格式:
#+BEGIN_SRC bash
A ['B', 'C', 'D', 'E', 'F', 'G', 'H']
B ['C', 'D', 'E', 'F', 'G', 'H']
C ['D', 'E', 'F', 'G', 'H']
D ['E', 'F', 'G', 'H']
E ['F', 'G', 'H']
F ['G', 'H']
G ['H']
H []
#+END_SRC
一般处理过程：
#+BEGIN_SRC bash
> s = 'ABCDEFGH'
> while s:
     x, s = s[0], list(s[1:])
     print(x, s)

A ['B', 'C', 'D', 'E', 'F', 'G', 'H']
B ['C', 'D', 'E', 'F', 'G', 'H']
C ['D', 'E', 'F', 'G', 'H']
D ['E', 'F', 'G', 'H']
E ['F', 'G', 'H']
F ['G', 'H']
G ['H']
H []

#+END_SRC
使用序列解包的方法：
#+BEGIN_SRC bash
> s = 'ABCDEFGH'
> while s:
     x, *s = s
     print(x, s)
 
A ['B', 'C', 'D', 'E', 'F', 'G', 'H']
B ['C', 'D', 'E', 'F', 'G', 'H']
C ['D', 'E', 'F', 'G', 'H']
D ['E', 'F', 'G', 'H']
E ['F', 'G', 'H']
F ['G', 'H']
G ['H']
H []
#+END_SRC

* 双星号（**）的作用
** 表示取幂
第一种情况：用在两表达式的中间，*表示乘法，**表示取幂，如：
#+BEGIN_SRC base 
>>> 2*5
10
>>> 2**7
128
#+END_SRC
** 用在变量前面
1. 向函数传递参数，将变量中可迭代对象的元素拆解出来，作为独立的参数第传给函数，如：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20201.jpg @ 2020-11-08 18:46:42
[[file:asdf/2020-11-08_18-46-42_20201.jpg]]

对于**，变量为字典，将其拆解出来，单独传给函数。如：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/v2-d6669f8dfb6a267e993214d5a725e7fa_720w.jpg @ 2020-11-08 18:47:42
[[file:asdf/2020-11-08_18-47-42_v2-d6669f8dfb6a267e993214d5a725e7fa_720w.jpg]]

2.在函数定义中使用，收集参数。将参数捕捉到一个元组中，如：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/v2-9e7ae9491d016162fe9c7aaab4d0a16e_720w.jpg @ 2020-11-08 18:49:18
[[file:asdf/2020-11-08_18-49-18_v2-9e7ae9491d016162fe9c7aaab4d0a16e_720w.jpg]]
该用法允许我们创建自己的函数时，可以接收任意数量的参数。

双星号可以将参数捕捉到字典中，如：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/v2-74c0f1151574fc5bbafa12b9c375ca41_720w.jpg @ 2020-11-08 18:49:42
[[file:asdf/2020-11-08_18-49-42_v2-74c0f1151574fc5bbafa12b9c375ca41_720w.jpg]]

* 类
Python的类提供了面向对象编程的所有标准特性：类继承机制允许多个基类，派生类可以覆盖它基类的任何方法，一个方法可以调用基类中相同名称的的方法。对象可以包含任意数量和类型的数据。和模块一样，类也拥有 Python 天然的动态特性：它们在运行时创建，可以在创建后修改。
** 类的定义
#+BEGIN_SRC python
class Student(object):
	pass
#+END_SRC
(object)表示该类从哪个类继承下来的，Object类是所有类都会继承的类。
#+BEGIN_SRC pyhton
class MyClass:
    """A simple example class"""
	i=12345
    def f(self):
        return 'hello world'
#+END_SRC
** 类对象
类对象支持两种操作：属性引用和实例化。
*** 属性引用
属性引用的标准语法：obj.name,如：MyClass.i和MyClass.f

MyClass.__doc__这也是有效引用，将返回所属类的文档字符串: "A simple example class"
*** 实例化
x=MyClass()

~__init__~ 是python中的一个内置方法，可以用来初始化类的状态，实际上就是为了能够给类传入参数。没有 ~__init__~ ，就没办法给类传入参数了。
#+BEGIN_SRC python
class Student(object):
        def __init__(self):  #这个可以用来初始化类的状态，实例化时可以不用传入参数。
                self.data=[]
#+END_SRC

#+BEGIN_SRC python
class Student(object):
	def __inin__(self,name,score):
		self.name=name
		self.score=score
#+END_SRC

\under\under{}init\under\under{}方法的第一参数永远为self，表示类 *实例* 本身

定义了\under\under{}init\under\under{}方法，创建实例时就不能传入空的参数，必须传入与\under\under{}init\under\under{}匹配的参数，但是self不需要传，python解释器会自己添加。
#+BEGIN_SRC python
student=Student("Hugh",99) #实例化
#+END_SRC

** classmethod 修饰符
classmethod 修饰符对应的函数不需要实例化，不需要 self 参数，但第一个参数需要是表示自身类的 cls 参数，可以来调用类的属性，类的方法，实例化对象等。

以下实例展示了 classmethod 的使用方法：
#+BEGIN_SRC python
#!/usr/bin/python
# -*- coding: UTF-8 -*-
 
class A(object):
    bar = 1
    def func1(self):  
        print ('foo') 
    @classmethod
    def func2(cls):
        print ('func2')
        print (cls.bar)
        cls().func1()   # 调用 foo 方法
 
A.func2()   
#+END_SRC

#+BEGIN_EXAMPLE
func2
1
foo
#+END_EXAMPLE
* cls含义及使用方法
** cls含义
python中cls代表的是类的本身，相对应的self则是类的一个实例对象。
** cls用法

cls可以在静态方法中使用，并通过cls()方法来实例化一个对象。
#+begin_src python
class Person(object):
    def __init__(self, name, age):
        self.name = name
        self.age = age
        print('self:', self)

    # 定义一个build方法，返回一个person实例对象，这个方法等价于Person()。
    @classmethod
    def build(cls):
        # cls()等于Person()
        p = cls("Tom", 18)
        print('cls:', cls)
        return p


if __name__ == '__main__':
    person = Person.build()
    print(person, person.name, person.age)
#+END_SRC
输出结果
#+BEGIN_EXAMPLE
self: <__main__.Person object at 0x000001FE88E4A280>
cls: <class '__main__.Person'>
<__main__.Person object at 0x000001FE88E4A280> Tom 18
#+END_EXAMPLE
* 函数修饰符@
Python函数修饰符，“@”，与其说是修饰函数倒不如说是引用、调用它修饰的函数。

举个栗子，下面的一段代码，里面两个函数，没有被调用，也会有输出结果：
#+begin_src python
def test(f):
    print "before ..."
    f()
    print "after ..."
 
@test
def func():
    print "func was called"
#+END_SRC
直接运行，输出结果：
#+BEGIN_EXAMPLE
before ...
func was called
after ...
#+END_EXAMPLE
上面代码可以看出来，只定义了两个函数： test和func。没有地方调用它们。如果没有“@test”，运行应该是没有任何输出的。

但是，Python解释器读到函数修饰符“@”的时候，后面步骤会是这样了：
1. 去调用 test函数，test函数的入口参数就是那个叫“func”的函数；
2. test函数被执行，入口参数的（也就是func函数）会被调用（执行）；

换言之，修饰符带的那个函数的入口参数，就是下面的那个整个的函数。有点儿类似JavaScript里面的 function a (function () { ... });

再来看一个例子：
#+begin_src python
def test(func):
    func()
    print "call test"
 
def test1(f):
    f()
    print "call test1"
     
def main():
    @test
    def fun():
        print "call fun"
        @test1
        def fun1():
            print "call fun1"
main()
#+END_SRC
输出结果：
#+BEGIN_EXAMPLE
call fun
call fun1
call test1
call test
#+END_EXAMPLE
需要注意的：
1. 函数先定义，再修饰它；反之会编译器不认识；
2. 修饰符“@”后面必须是之前定义的某一个函数；
3. 每个函数可以有多个修饰符。
* @修饰符用法
** @property
先来看下一般情况下class类函数，定义一个Student类，stu实例化对象，输出结果显示xiaoming字样。
#+begin_src python
class Student():
    def __init__(self, name):
        self._name = name
    def sname(self):
        return self._name
stu = Student('xiaoming')
print(stu.sname())
#+END_SRC
增加@property之后，输出结果也显示xiaoming字样，与上面输出结果一致，但调用的时候，stu.sname后面省了一个括号@property可以使类方法转化成属性，直接通过方法名来访问方法
#+begin_src python
class Student():
    def __init__(self, name):
        self._name = name
    @property
    def sname(self):
        return self._name
stu = Student('xiaoming')
print(stu.sname)  //注意这里sname后面不用带括号
#+END_SRC
如果这时你想修改sname属性，使用下面语句，你会发现报错
#+begin_src python
stu.sname='zhangsan'

Traceback (most recent call last):
  File "D:/pythonstudy/data_ap/test.py", line 120, in <module>
    stu.sname='zhangsan'
AttributeError: can't set attribute
#+END_SRC
但是这时如果使用@函数名.setter，可以重新设置属性值
#+begin_src python
class Student():
    def __init__(self, name):
        self._name = name
    @property
    def sname(self):
        return self._name
    @sname.setter
    def sname(self, value):
        self._sname = value
stu = Student('xiaoming')
stu.sname='zhangsan'
print(stu.sname)
#+END_SRC
** @classmethod
@classmethod将对象方法转化成类方法，并注意类方法函数参数写法上需要加上cls作为参数，但调用的时候不用加参数。
#+begin_src python
class MyClass():
    @classmethod
    def thisIsClassMethod(cls,parameter):
        print("this is a class method")
        print(cls.__name__)
        print(type(cls)) #打印类型为classobj
if __name__ == "__main__":
    MyClass.thisIsClassMethod(None) 
    print(type(MyClass))
'''
this is a class method
MyClass
<class 'type'>
<class 'type'>
'''
#+END_SRC
** @staticmethod
这时将对象方法转化成静态方法，可以被类直接调用
#+begin_src python
class MyClass:
    @staticmethod
    def thisIsStaticMethod():
        print("This is static method")
if __name__ == "__main__":
    MyClass.thisIsStaticMethod()

#This is static method
#+END_SRC
** 保护核心代码
func是核心代码，不能更改，但想要在该代码基础上扩展功能。这时重新定义一个两层函数，内层函数的输出是一个函数对象，并在func函数前面增加@deco进行修饰
#+begin_src python
import time
def deco(func):
    def wrapper():
        startTime = time.time()
        func()
        endTime = time.time()
        msecs = (endTime - startTime)*1000
        print("time is %d ms" %msecs)
    return wrapper
@deco
def func():
    print("hello")
    time.sleep(1)
    print("world")
if __name__ == '__main__':
    f = func
    f()
'''
hello
time is 1000 ms
world
'''
#+END_SRC

如果核心代码带参数，只需要在内层函数中增加参数即可
#+begin_src python
def deco(func):
    def wrapper(a,b):
        startTime = time.time()
        func(a,b)
        endTime = time.time()
        msecs = (endTime - startTime)*1000
        print("time is %d ms" %msecs)
    return wrapper
@deco
def func(a,b):
    print("hello")
    time.sleep(1)
    print("result is %d" % (a + b))
if __name__ == '__main__':
    f = func
    f(2,4)
'''
hello
result is 6
time is 1000 ms
'''
#+END_SRC
如果参数的个数不确定的情况下，使用*args，可以理解为一个数组，**kwargs可以理解为一个键值对

如果参数的个数不确定的:
#+begin_src python
    def wrapper(*args, **kwargs):
        startTime = time.time()
        func(*args, **kwargs)
        endTime = time.time()
        msecs = (endTime - startTime)*1000
        print("time is %d ms" %msecs)
    return wrapper
@deco
def func(a,b):
    print("hello，here is a func for add :")
    time.sleep(1)
    print("result is %d" %(a+b))
@deco
def func2(a,b,c):
    print("hello，here is a func for add :")
    time.sleep(1)
    print("result is %d" %(a+b+c))
if __name__ == '__main__':
    f = func
    func2(3,4,5)
    f(3,4)
'''
hello，here is a func for add :
result is 12
time is 1000 ms
hello，here is a func for add :
result is 7
time is 1000 ms
'''
#+END_SRC

** 类装饰器
[[https://zhuanlan.zhihu.com/p/93846887][参考文档]]

类也可以作为装饰器，使用起来可能比函数装饰器更方便。首先看下面一个简单的例子：
#+begin_src python
class myDecorator(object):
    def __init__(self, f):
        print("inside myDecorator.__init__()")
        f() # Prove that function definition has completed
    def __call__(self):
        print("inside myDecorator.__call__()")
​
@myDecorator
def aFunction():
    print("inside aFunction()")
​
print("Finished decorating aFunction()")
​
aFunction()
#+END_SRC
例子中函数aFunction()就使用了类myDecorator作为装饰器修饰。例子的输出结果如下：
#+BEGIN_EXAMPLE
inside myDecorator.__init__()
inside aFunction()
Finished decorating aFunction()
inside myDecorator.__call__()
#+END_EXAMPLE
可以看出，在aFunction()函数声明处进入了类myDecorator的__init__()方法，但要注意，从第2个输出可以看出，此时函数aFunction()的定义已经完成了，在__init__()中调用的输入参数f()，实际上是调用了aFunction()函数。至此aFunction()函数的声明完成，包括装饰器声明的部分，然后输出了第3个输出。最后执行aFunction()时，可以看出实际上是执行了类myDecorator的__call__()方法（定义了__call__()方法的类的对象可以像函数一样被调用，此时调用的是对象的__call__()方法）。

这个例子其实不难理解，因为根据装饰器语法的含义，下面的代码：
#+begin_src python
@myDecorator
def aFunction():
    # 。。。
#+END_SRC

等价于
#+begin_src python
def aFunction():
    # 。。。

aFunction = myDecorator(aFunction)
#+END_SRC
因此被装饰后的函数aFunction()实际上已经是类myDecorator的对象。当再调用aFunction()函数时，实际上就是调用类myDecorator的对象，因此会调用到类myDecorator的__call__()方法。

因此使用类作为装饰器装饰函数来对函数添加一些额外的属性或功能时，一般会在类的__init__()方法中记录传入的函数，再在__call__()调用修饰的函数及其它额外处理。

下面是一个简单的例子：
#+begin_src python
class entryExit(object):
    def __init__(self, f):
        self.f = f
    def __call__(self):
        print("Entering", self.f.__name__)
        self.f()
        print("Exited", self.f.__name__)
​
@entryExit
def func1():
    print("inside func1()")
​
@entryExit
def func2():
    print("inside func2()")
​
func1()
func2()
#+END_SRC
例子的输出：
#+BEGIN_EXAMPLE
Entering func1
inside func1()
Exited func1
Entering func2
inside func2()
Exited func2
#+END_EXAMPLE

* 模块与包
** 模块简介
在 Python 中，一个 =.py= 文件就称之为一个模块（Module）

使用模块还可以避免函数名和变量名冲突。相同名字的函数和变量完全可以分别存在不同的模块中，因此，我们自己在编写模块时，不必考虑名字会与其他模块冲突。但是也要注意，尽量不要与内置函数名字冲突。

Python 本身就内置了很多非常有用的模块，比如我的 Python 安装目录是默认的安装目录，在 C:\Users\Administrator\AppData\Local\Programs\Python\Python36 ，然后找到 Lib 目录，就可以发现里面全部都是 =.py= 文件.这些 =.py= 文件就是模块了。

模块可以分为标准库模块和自定义模块，Lib 目录下的都是标准库模块
** 模块的使用
*** import
导入一个模块的方法我们使用的是 import 关键字，这样做是导入了这个模块.这里需要注意了，这样做只是导入了模块，并没有导入模块中具体的某个属性或方法的。

=import= 的语法基本如下：
#+BEGIN_SRC python
import module1[, module2[,... moduleN]
#+END_SRC
一个模块只会被导入一次，不管你执行了多少次 import。这样可以防止导入模块被一遍又一遍地执行。

当我们使用 import 语句的时候，Python 解释器会根据 Python 的搜索路径去寻找文件.

搜索路径是在 Python 编译或安装的时候确定的，安装新的库应该也会修改。搜索路径被存储在sys 模块中的 path 变量 。事实上，也可以通过定义环境变量的方式来确定搜索路径。

我们可以查一下路径：
#+BEGIN_SRC python
#!/usr/bin/env python
# -*- coding: UTF-8 -*-
import sys
print(sys.path)
#+END_SRC


*** from ... import
from ... import 可以直接导入某个模块中的属性和方法

语法如下:
#+BEGIN_SRC python
直接导入某个模块中的属性和方法
#+END_SRC

*** *** from ··· import *
这个语句可以把某个模块中的所有方法属性都导入。
#+BEGIN_SRC python
#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
from sys import *
print(version)
print(executable)
#+END_SRC

* Python中的with-as用法
** 介绍
有一些任务，可能事先需要设置，事后做清理工作。对于这种场景，Python的with语句提供了一种非常方便的处理方式。一个很好的例子是文件处理，你需要获取一个文件句柄，从文件中读取数据，然后关闭文件句柄。

如果不用with语句，代码如下：
#+BEGIN_SRC python
file = open("/tmp/foo.txt")
data = file.read()
file.close()
#+END_SRC
这里有两个问题。一是可能忘记关闭文件句柄；二是文件读取数据发生异常，没有进行任何处理。下面是处理异常的加强版本：
#+BEGIN_SRC python
file = open("/tmp/foo.txt")
try:
    data = file.read()
finally:
    file.close()
#+END_SRC
虽然这段代码运行良好，但是太冗长了。这时候就是with一展身手的时候了。除了有更优雅的语法，with还可以很好的处理上下文环境产生的异常。下面是with版本的代码：
#+BEGIN_SRC python
with open("/tmp/foo.txt") as file:
    data = file.read()
#+END_SRC
** with如何工作
这看起来充满魔法，但不仅仅是魔法，Python对with的处理还很聪明。基本思想是with所求值的对象必须有一个enter()方法，一个exit()方法。

紧跟with后面的语句被求值后，返回对象的enter()方法被调用，这个方法的返回值将被赋值给as后面的变量。当with后面的代码块全部被执行完之后，将调用前面返回对象的exit()方法。

下面例子可以具体说明with如何工作：
#+BEGIN_SRC python

#!/usr/bin/env python
# with_example01.py
 
class Sample:
    def __enter__(self):
        print "In __enter__()"
        return "Foo"
 
    def __exit__(self, type, value, trace):
        print "In __exit__()"
 
def get_sample():
    return Sample()
 
with get_sample() as sample:
    print "sample:", sample

#+END_SRC
代码输出结果如下：
#+BEGIN_SRC python
In __enter__()
sample: Foo
In __exit__()
#+END_SRC
正如你看到的，
1. enter()方法被执行
2. enter()方法返回的值 - 这个例子中是"Foo"，赋值给变量'sample'
3. 执行代码块，打印变量"sample"的值为 "Foo"
4. exit()方法被调用

with真正强大之处是它可以处理异常。可能你已经注意到Sample类的exit方法有三个参数- val, type 和 trace。 这些参数在异常处理中相当有用。我们来改一下代码，看看具体如何工作的
#+BEGIN_SRC python
#!/usr/bin/env python
# with_example02.py
 
class Sample:
    def __enter__(self):
        return self
 
    def __exit__(self, type, value, trace):
        print "type:", type
        print "value:", value
        print "trace:", trace
 
    def do_something(self):
        bar = 1/0
        return bar + 10
 
with Sample() as sample:
    sample.do_something()
#+END_SRC
代码运行结果如下：
#+BEGIN_SRC python
bash-3.2$ ./with_example02.py
type: <type 'exceptions.ZeroDivisionError'>
value: integer division or modulo by zero
trace: <traceback object at 0x1004a8128>
Traceback (most recent call last):
  File "./with_example02.py", line 19, in <module>
    sample.do_something()
  File "./with_example02.py", line 15, in do_something
    bar = 1/0
ZeroDivisionError: integer division or modulo by zero
#+END_SRC
这个例子中，with后面的get_sample()变成了Sample()。这没有任何关系，只要紧跟with后面的语句所返回的对象有enter()和exit()方法即可。此例中，Sample()的enter()方法返回新创建的Sample对象，并赋值给变量sample。

实际上，在with后面的代码块抛出任何异常时，exit()方法被执行。正如例子所示，异常抛出时，与之关联的type，value和stack trace传给exit()方法，因此抛出的ZeroDivisionError异常被打印出来了。开发库时，清理资源，关闭文件等等操作，都可以放在exit方法当中。

因此，Python的with语句是提供一个有效的机制，让代码更简练，同时在异常产生时，清理工作更简单。

* py文件的两种执行方式
一个python文件通常有两种使用方法，第一是作为脚本直接执行，第二是 import 到其他的 python 脚本中被调用（模块重用）执行。
~if __name__== 'main':~ 的作用就是控制这两种情况执行代码的过程，在 ~if __name__== 'main':~ 下的代码只有在第一种情况下（即文件作为脚本直接执行）才会被执行，而 ~import~ 到其他脚本中是不会被执行的。

- 直接执行
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20190510141202522.png @ 2020-06-03 22:00:47
[[file:py%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8F/2020-06-03_22-00-47_20190510141202522.png]]

直接执行 test.py，结果如下图，可以成功 print 两行字符串。即， ~if __name__=="__main__":~ 语句之前和之后的代码都被执行。

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20190510141303114.png @ 2020-06-03 22:00:59
[[file:py%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8F/2020-06-03_22-00-59_20190510141303114.png]]

- import 执行
然后在同一文件夹新建名称为 import_test.py 的脚本，输入如下代码：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20190510141602268.png @ 2020-06-03 22:01:45
[[file:py%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8F/2020-06-03_22-01-45_20190510141602268.png]]

执行 import_test.py 脚本，输出结果如下：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20190510141624918.png @ 2020-06-03 22:02:00
[[file:py%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8F/2020-06-03_22-02-00_20190510141624918.png]]

只输出了第一行字符串。即， ~if __name__=="__main__":~ 之前的语句被执行，之后的没有被执行。

** ~if __name__ == '__main__':~ 的运行原理
每个python模块（python文件，也就是此处的 test.py 和 import_test.py）都包含内置的变量 ~__name__~ ，当该模块被直接执行的时候， ~__name__~ 等于文件名（包含后缀 .py ）；如果该模块 import 到其他模块中，则该模块的 ~__name__~ 等于模块名称（不包含后缀.py）。

而 ~__main__~ 始终指当前执行模块的名称（包含后缀.py）。进而当模块被直接执行时， ~__name__ == 'main'~ 结果为真。

为了进一步说明，我们在 test.py 脚本的 if __name__=="__main__": 之前加入 print(__name__)，即将 __name__ 打印出来。文件内容和结果如下：


#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20190510142230219.png @ 2020-06-04 08:41:55
[[file:py%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8F/2020-06-04_08-41-55_20190510142230219.png]]

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20190510142253482.png @ 2020-06-04 08:42:02
[[file:py%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8F/2020-06-04_08-42-02_20190510142253482.png]]

可以看出，此时变量__name__的值为"__main__"。

再执行 import_test.py，执行结果如下：

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20190510142441889.png @ 2020-06-04 08:42:40
[[file:py%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8F/2020-06-04_08-42-40_20190510142441889.png]]

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20190510142452571.png @ 2020-06-04 08:42:47
[[file:py%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8F/2020-06-04_08-42-47_20190510142452571.png]]

此时，test.py中的__name__变量值为 test，不满足 __name__=="__main__" 的条件，因此，无法执行其后的代码。
* 切片（splice）操作
** 基本索引
负数下标索引，即：index可以取为负数，当其为-n时，对倒数第n个元素进行索引。我们用一张表格值观展示a的索引范围。
| a中元素  |   0 |  1 |  2 |  3 |  4 |  5 |  6 |  7 |  8 |  9 |
|----------+-----+----+----+----+----+----+----+----+----+----|
| 非负下标 |   0 |  1 |  2 |  3 |  4 |  5 |  6 |  7 |  8 |  9 |
| 负数下标 | -10 | -9 | -8 | -7 | -6 | -5 | -4 | -3 | -2 | -1 |
非负下标索引和负数下标索引共同构成了Python索引的有效范围：​。有效范围的概念对切片的理解非常重要，在基本索引中，索引超出有效范围时会抛出IndexError异常：
#+BEGIN_SRC python
>>> a
 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> a[-1]
9
>>> a[10]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
IndexError: list index out of range
>>> a[-11]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
IndexError: list index out of range
#+END_SRC

** 简单切片
简单切片指的是这样的切片形式：a[start:stop]，其行为是得到下标在这样一个前闭后开区间范围内的元素，其中start和stop为负数时，简单看作是负数下标对应的位置即可：
#+BEGIN_SRC python
>>> a
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> a[2:3]
[2]
>>> a[5:9]
[5, 6, 7, 8]
>>> a[5:-1]
[5, 6, 7, 8]
>>> a[-5:9]
[5, 6, 7, 8]
>>> a[-5:-1]
[5, 6, 7, 8]
>>> a[-1:-5]
[]
#+END_SRC
** 超出有效索引范围
当start或stop超出上文提到的有效索引范围​时，切片操作不会抛出异常，而是进行截断。可以这样去理解截断机制：我们假象把索引范围扩充到全体整数，只不过小于​或大于​的区域对应空元素，在这个扩充后的数轴上进行切片，只需把最终结果中的所有空元素忽略即可。

来看几个具体的例子
#+BEGIN_SRC python
 >>> a
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> a[-100:5]
[0, 1, 2, 3, 4]
>>> a[5:100]
[5, 6, 7, 8, 9]
>>> a[-100:100]
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> a[100:1000]
[]
#+END_SRC
另外，如果start的位置比stop还靠后怎么办？Python还是不会抛出异常，而是直接返回空序列：
#+BEGIN_SRC python
>>> a
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> a[6:5]
[]
#+END_SRC
** 缺省
start和stop都是可以缺省的，在缺省的情况下，Python的行为是尽可能取最大区间，具体来说：

按照扩充索引范围的观点，start的缺省值是无穷小(​)，stop的缺省值是无穷大(​)。
#+BEGIN_SRC python
>>> a
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> a[:5]
[0, 1, 2, 3, 4]
>>> a[5:]
[5, 6, 7, 8, 9]
>>> a[100:]
[]
#+END_SRC
** 扩展切片
早期的Python解释器仅支持上述a[start:stop]形式的基本切片，后来加入了下面要介绍的切片形式，扩展切片的名称也流传下来，实际上不用担心，这早已是Python所支持的标准语法。

扩展切片指的是这样的切片形式：a[start:stop:step]，其中step是一个非零整数，即比简单切片多了调整步长的功能，此时切片的行为可概括为：从start对应的位置出发，以step为步长索引序列，直至越过stop对应的位置，且不包括stop本身。事实上，简单切片就是step=1的扩展切片的特殊情况。需要详细解释的是step分别为正数和负数的两种情况。

step 缺省值为1
*** step为正数
当step为正数时，切片行为很容易理解，start和stop的截断和缺省规则也与简单切片完全一致：
#+BEGIN_SRC python
>>> a
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> a[0:6:2]
[0, 2, 4]
>>> a[::2]
[0, 2, 4, 6, 8]
>>> a[:-2:2]
[0, 2, 4, 6]
>>> a[4::2]
[4, 6, 8]
>>> a[4::]
[4, 5, 6, 7, 8, 9]
#+END_SRC
*** step为负数
当step为负数时，切片将其解释为从start出发以步长 |step| 逆序索引序列，此时，start和stop的截断依然遵循前述规则，但缺省发生一点变化，因为我们说过，在缺省的情况下，Python的行为是尽可能取最大区间，此时访问是逆序的，start应尽量取大，stop应尽量取小，才能保证区间最大，因此：

按照扩充索引范围的观点，start的缺省值是无穷大(​)，stop的缺省值是无穷小(​)
#+BEGIN_SRC python
>>> a
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> a[5::-1]
[5, 4, 3, 2, 1, 0]
>>> a[:4:-2]
[9, 7, 5]
>>> a[::-1]
[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
#+END_SRC
* 注释
** 单行注释
Python中单行注释以 =#= 开头，例如
#+BEGIN_SRC python
# 这是一个注释
print("Hello, World!")
#+END_SRC
** 多行注释
多行注释用三个单引号 ''' 或者三个双引号 """ 将注释括起来，例如:
#+BEGIN_SRC python
#!/usr/bin/python3 
'''
这是多行注释，用三个单引号
这是多行注释，用三个单引号 
这是多行注释，用三个单引号
'''

"""
这是多行注释，用三个双引号
这是多行注释，用三个双引号 
这是多行注释，用三个双引号
"""

print("Hello, World!")
#+END_SRC
* 编码问题
最早的Python 只支持 ASCII 编码，普通的字符串 'ABC' 在 Python 内部都是 ASCII 编码的。

Python 在后来添加了对 Unicode 的支持，以 Unicode 表示的字符串用u'...'表示。

不过在最新的 Python 3 版本中，字符串是以 Unicode 编码的，也就是说，Python 的字符串支持多语言。就像上面的例子一样，我的代码中没有加u'...'，也能正常显示。

不过由于 Python 源代码也是一个文本文件，所以，当你的源代码中包含中文的时候，在保存源代码时，就需要务必指定保存为 UTF-8 编码。当Python 解释器读取源代码时，为了让它按 UTF-8 编码读取，我们通常在文件开头写上这两行：
#+BEGIN_SRC python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#+END_SRC

第一行注释是为了告诉 Linux/OS X 系统，这是一个 Python 可执行程序，Windows 系统会忽略这个注释；

第二行注释是为了告诉 Python 解释器，按照 UTF-8 编码读取源代码，否则，你在源代码中写的中文输出可能会有乱码。

申明了 UTF-8 编码并不意味着你的 .py 文件就是 UTF-8 编码的，必须并且要确保文本编辑器正在使用 UTF-8 without BOM 编码

* del用法
python中的del用法比较特殊，新手学习往往产生误解，弄清del的用法，可以帮助深入理解python的内存方面的问题。

python的del不同于C的free和C++的delete。

由于python都是引用，而python有GC机制，所以，del语句作用在变量上，而不是数据对象上。
#+BEGIN_SRC python
if __name__=='__main__':  
    a=1       # 对象 1 被 变量a引用，对象1的引用计数器为1  
    b=a       # 对象1 被变量b引用，对象1的引用计数器加1  
    c=a       #1对象1 被变量c引用，对象1的引用计数器加1  
    del a     #删除变量a，解除a对1的引用  
    del b     #删除变量b，解除b对1的引用  
    print(c)  #最终变量c仍然引用1  
#+END_SRC
del删除的是变量，而不是数据。
#+BEGIN_SRC python 
if __name__=='__main__':  
    li=[1,2,3,4,5]  #列表本身不包含数据1,2,3,4,5，而是包含变量：li[0] li[1] li[2] li[3] li[4]   
    first=li[0]     #拷贝列表，也不会有数据对象的复制，而是创建新的变量引用  
    del li[0]  
    print(li)      #输出[2, 3, 4, 5]  
    print(first)   #输出 1  
#+END_SRC

* lambda表达式
** lambda简介

先来看一段代码示例：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20190207214329509.jpg @ 2020-11-08 12:53:08
[[file:ambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/2020-11-08_12-53-08_20190207214329509.jpg]]
第一行是lambda声明，x,y相当于传入的参数，整个函数会返回x+y的值。lambda作为一个表达式，定义了一个匿名函数，上例的代码x，y为入口参数，x+y为函数体。在这里lambda简化了函数定义的书写形式。

python允许用lambda关键字创造匿名函数。匿名是不需要以标准的方式来声明，比如说使用 def 语句。（除非赋值给一个局部变量，这样的对象也不会在任何的名字空间内创建名字，上面的例子中会创建名字。)

作为函数，它们也能有参数。一个完整的 lambda"语句"代表了一个表达式，这个表达式的定义体必须和声明放在同一行。语法如下：

lambda [arg1[, arg2, … argN]]: expression

参数是可选的，如果使用的参数话，参数通常也是表达式的一部分

lambda使用可以加参数也可以不加参数

无参数：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20190207214435106.jpg @ 2020-11-08 12:56:14
[[file:ambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/2020-11-08_12-56-14_20190207214435106.jpg]]

lambda通常用来编写跳转表（jump table），也就是行为的列表或字典，能够按照需要执行相应的动作。如下段代码所示。
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20190207214542358.jpg @ 2020-11-08 12:56:36
[[file:ambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/2020-11-08_12-56-36_20190207214542358.jpg]]

** 为什么要使用lambda？

1、lambda函数主要用来写一些小体量的一次性函数，避免污染环境，同时也能简化代码。

2、lambda起到了一种函数速写的作用，允许在使用的代码内嵌入一个函数的定义。他们完全是可选的（你总是能够使用def来替代它们），但是你仅需要嵌入小段可执行代码的情况下它们会带来一个更简洁的代码结构。

例如：

map( lambda x: x*x, [y for y in range(10)] )

这个写法要好过

def sq(x):

return x * x

map(sq, [y for y in range(10)])
** lambda匿名函数与def区别
lambda 和def它两个的基本用法差不多，参数都是可选，也都会返回对象

如下：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20190207214400126.jpg @ 2020-11-08 12:54:04
[[file:ambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/2020-11-08_12-54-04_20190207214400126.jpg]]

不同之处：

lambda可以定义一个匿名函数，而def定义的函数必须有一个名字。这应该是lambda与def两者最大的区别。

lambda是一个表达式，而不是一个语句（lambda is an expression, not a statement.）因此，lambda能够出现在Python语法不允许def出现的地方——例如，在一个列表常量中或者函数调用的参数中。

lambda的主体是一个单个的表达式，而不是一个代码块。（lambda’s body is a single expression, not a block of statements.），lambda里面的语句相当于def中return中的代码一样。只能是简单的表达式，所以说lambda的能力小于def，在lambda中只能使用简单的语法，不能使用if else while return等语句

它的设计理念为：lambda是一个为编写简单的函数而设计的，而def用来处理更大的任务。（lambda is designed for coding simple functions, and def handles larger tasks.）

* __dict__属性
在 Python 类的内部，无论是类属性还是实例属性，都是以字典的形式进行存储的，其中属性名作为键，而值作为该键对应的值。

为了方便用户查看类中包含哪些属性，Python 类提供了 __dict__ 属性。需要注意的一点是，该属性可以用类名或者类的实例对象来调用，用类名直接调用 __dict__，会输出该由类中所有类属性组成的字典；而使用类的实例对象调用 __dict__，会输出由类中所有实例属性组成的字典。

举个例子：
#+BEGIN_SRC python 
class CLanguage:
    a = 1
    b = 2
    def __init__ (self):
        self.name = "C语言中文网"
        self.add = "http://c.biancheng.net"
#通过类名调用__dict__
print(CLanguage.__dict__)

#通过类实例对象调用 __dict__
clangs = CLanguage()
print(clangs.__dict__)
#+END_SRC
程序输出结果为：
#+BEGIN_EXAMPLE
{'__module__': '__main__', 'a': 1, 'b': 2, '__init__': <function CLanguage.__init__ at 0x0000022C69833E18>, '__dict__': <attribute '__dict__' of 'CLanguage' objects>, '__weakref__': <attribute '__weakref__' of 'CLanguage' objects>, '__doc__': None}
{'name': 'C语言中文网', 'add': 'http://c.biancheng.net'}
#+END_EXAMPLE
不仅如此，对于具有继承关系的父类和子类来说，父类有自己的 __dict__，同样子类也有自己的 __dict__，它不会包含父类的 __dict__。例如：
#+BEGIN_SRC python
class CLanguage:
    a = 1
    b = 2
    def __init__ (self):
        self.name = "C语言中文网"
        self.add = "http://c.biancheng.net"
       
class CL(CLanguage):
    c = 1
    d = 2
    def __init__ (self):
        self.na = "Python教程"
        self.ad = "http://c.biancheng.net/python"
#父类名调用__dict__
print(CLanguage.__dict__)
#子类名调用__dict__
print(CL.__dict__)
#父类实例对象调用 __dict__
clangs = CLanguage()
print(clangs.__dict__)
#子类实例对象调用 __dict__
cl = CL()
print(cl.__dict__)
#+END_SRC
运行结果为：
#+BEGIN_EXAMPLE
{'__module__': '__main__', 'a': 1, 'b': 2, '__init__': <function CLanguage.__init__ at 0x000001721A853E18>, '__dict__': <attribute '__dict__' of 'CLanguage' objects>, '__weakref__': <attribute '__weakref__' of 'CLanguage' objects>, '__doc__': None}
{'__module__': '__main__', 'c': 1, 'd': 2, '__init__': <function CL.__init__ at 0x000001721CD15510>, '__doc__': None}
{'name': 'C语言中文网', 'add': 'http://c.biancheng.net'}
{'na': 'Python教程', 'ad': 'http://c.biancheng.net/python'}
#+END_EXAMPLE
显然，通过子类直接调用的 __dict__ 中，并没有包含父类中的 a 和 b 类属性；同样，通过子类对象调用的 __dict__，也没有包含父类对象拥有的 name 和 add 实例属性。

除此之外，借助由类实例对象调用 __dict__ 属性获取的字典，可以使用字典的方式对其中实例属性的值进行修改，例如：
#+BEGIN_SRC python
class CLanguage:
    a = "aaa"
    b = 2
    def __init__ (self):
        self.name = "C语言中文网"
        self.add = "http://c.biancheng.net"
#通过类实例对象调用 __dict__
clangs = CLanguage()
print(clangs.__dict__)
clangs.__dict__['name'] = "Python教程"
print(clangs.name)
#+END_SRC
程序运行结果为：
#+BEGIN_EXAMPLE
{'name': 'C语言中文网', 'add': 'http://c.biancheng.net'}
Python教程
#+END_EXAMPLE
注意，无法通过类似的方式修改类变量的值。
* 异常处理
** 什么是异常？
异常即是一个事件，该事件会在程序执行过程中发生，影响了程序的正常执行。

一般情况下，在Python无法正常处理程序时就会发生一个异常。

异常是Python对象，表示一个错误。

当Python脚本发生异常时我们需要捕获处理它，否则程序会终止执行。
** 异常处理
捕捉异常可以使用try/except语句。

try/except语句用来检测try语句块中的错误，从而让except语句捕获异常信息并处理。

如果你不想在异常发生时结束你的程序，只需在try里捕获它。

以下为简单的try....except...else的语法：
#+BEGIN_SRC python
try:
<语句>        #运行别的代码
except <名字>：
<语句>        #如果在try部份引发了'name'异常
except <名字>，<数据>:
<语句>        #如果引发了'name'异常，获得附加的数据
else:
<语句>        #如果没有异常发生
#+END_SRC
try的工作原理是，当开始一个try语句后，python就在当前程序的上下文中作标记，这样当异常出现时就可以回到这里，try子句先执行，接下来会发生什么依赖于执行时是否出现异常。
- 如果当try后的语句执行时发生异常，python就跳回到try并执行第一个匹配该异常的except子句，异常处理完毕，控制流就通过整个try语句（除非在处理异常时又引发新的异常）。
- 如果在try后的语句里发生了异常，却没有匹配的except子句，异常将被递交到上层的try，或者到程序的最上层（这样将结束程序，并打印默认的出错信息）。
- 如果在try子句执行时没有发生异常，python将执行else语句后的语句（如果有else的话），然后控制流通过整个try语句。

*** 使用except而不带任何异常类型

你可以不带任何异常类型使用except，如下实例：
#+BEGIN_SRC python
try:
    正常的操作
   ......................
except:
    发生异常，执行这块代码
   ......................
else:
    如果没有异常执行这块代码
#+END_SRC
以上方式try-except语句捕获所有发生的异常。但这不是一个很好的方式，我们不能通过该程序识别出具体的异常信息。因为它捕获所有的异常。
*** 例子
下面是简单的例子，它打开一个文件，在该文件中的内容写入内容，且并未发生异常：
#+BEGIN_SRC python
#!/usr/bin/python
# -*- coding: UTF-8 -*-

try:
    fh = open("testfile", "w")
    fh.write("这是一个测试文件，用于测试异常!!")
except IOError:
    print "Error: 没有找到文件或读取文件失败"
else:
    print "内容写入文件成功"
    fh.close()

#+END_SRC
以上程序输出结果：
#+BEGIN_SRC python
$ python test.py 
内容写入文件成功
$ cat testfile       # 查看写入的内容
这是一个测试文件，用于测试异常!!
#+END_SRC

* pass语句
Python pass 是空语句，是为了保持程序结构的完整性。

pass 不做任何事情，一般用做占位语句。
#+BEGIN_SRC python
#!/usr/bin/python
# -*- coding: UTF-8 -*- 
 
# 输出 Python 的每个字母
for letter in 'Python':
   if letter == 'h':
      pass
      print '这是 pass 块'
   print '当前字母 :', letter
 
print "Good bye!"
#+END_SRC
以上实例执行结果：
#+BEGIN_EXAMPLE
当前字母 : P
当前字母 : y
当前字母 : t
这是 pass 块
当前字母 : h
当前字母 : o
当前字母 : n
Good bye!
#+END_EXAMPLE

在python2.0中是不能定义空函数的，必须用pass填充：
#+BEGIN_SRC python
def sample(n_samples):
    pass
#+END_SRC
该处的 pass 便是占据一个位置，当你没有想好函数的内容是可以用 pass 填充，使程序可以正常运行。

在 Python3.x 的时候 pass 可以写或不写:
#+BEGIN_SRC python
def function():
    # 在Python3.x的时候pass可以写或不写
    pass
#+END_SRC

* __init__.py的用法
0、__init__.py
在Python工程里，当python检测到一个目录下存在__init__.py文件时，python就会把它当成一个模块(module)。Module跟C＋＋的命名空间和Java的Package的概念很像，都是为了科学地组织化工程，管理命名空间。

__init__.py可以是一个空文件，也可以有非常丰富的内容。本文将举一个非常简单的例子，来介绍__init__.py的用法；在本文的最后，我将会再简单介绍__init__.py的设计理念。

1、一个普通的四则运算模块
在不利用__init__.py的情况下，我们来看一个四则运算的例子。我们的工程目录结构如下图所示：

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-08_17-01-47.png @ 2021-10-08 17:02:18
[[file:__init__.py%E7%9A%84%E7%94%A8%E6%B3%95/2021-10-08_17-02-18_Snipaste_2021-10-08_17-01-47.png]]
如上图，其中，main.py是程序入口，我们用了不同的方式来import四则运算的各个子模块。arithmetic模块实现四则运算；为了展示需要，我们将加减乘除四种运算分别放在不同的代码中。

每个文件的代码如下:
#+BEGIN_SRC python
#
# @file main.py
#

import arithmetic.add
import arithmetic.sub as sub

from arithmetic.mul import mul
from arithmetic import dev

def letscook(x, y, oper):
    r = 0
    if oper == "+":
        r = arithmetic.add.add(x, y)
    elif oper == "-":
        r = sub.sub(x, y)
    elif oper == "*":
        r = mul(x, y)
    else:
        r = dev.dev(x, y)

    print("{} {} {} = {}".format(x, oper, y, r))

x, y = 3, 8

letscook(x, y, "+")
letscook(x, y, "-")
letscook(x, y, "*")
letscook(x, y, "/")
#+END_SRC

#+begin_src python
#
# @file add.py
#

def add(a, b):
    return a + b

#+END_SRC
#+begin_src python
#
# @file sub.py
#

def sub(a, b):
    return a - b
#+END_SRC
#+begin_src python
#
# @file mul.py
# 

def mul(a, b):
    return a * b
#+END_SRC
#+begin_src python
#
# @file dev.py
#

def dev(a, b):
    return a / b
#+END_SRC
从代码可以看出，为了使用arithmetic中的某个子模块（main.py中我们展示了四种不同的方式），我们必须使用非常繁琐的import语句；在使用的时候，也有可能需要使用非常繁琐的表达式。如果我们在不同的地方使用arithmetic的子模块，都需要写这么繁琐的import或者使用表达式，你可能会觉得心累。这就是为什么我们需要利用__init__.py来简化我们的使用。
2、利用__init__.py
还是第1小节中的工程目录结构，实现同样的功能，我们在__init__.py中编写了一些代码；同样，我们对main.py进行了一些适当的修改。

修改后__init__.py和main.py的代码如下：
#+begin_src python
#
# @file main.py
#

import arithmetic as a4

def letscook(x, y, oper):
    r = 0
    if oper == "+":
        r = a4.add(x, y)
    elif oper == "-":
        r = a4.sub(x, y)
    elif oper == "*":
        r = a4.mul(x, y)
    else:
        r = a4.dev(x, y)

    print("{} {} {} = {}".format(x, oper, y, r))

x, y = 3, 8

letscook(x, y, "+")
letscook(x, y, "-")
letscook(x, y, "*")
letscook(x, y, "/")
#+END_SRC
#+begin_src python
#
# @file __init__.py
#

import arithmetic.add
import arithmetic.sub
import arithmetic.mul
import arithmetic.dev

add = arithmetic.add.add
sub = arithmetic.sub.sub
mul = arithmetic.mul.mul
dev = arithmetic.dev.dev
#+END_SRC

在__init__.py中， 我们import了arithmetic下的所有子模块，并在__init__.py中给各个子模块的核心功能取了新的名字，作为arithmetic模块的变量。所以我们在main.py中import了arithmetic模块之后，就可以直接进行使用了。如果你使用from arithmetic import * 语句，那么我们就可以使用add、sub、mul、dev，连a4都省了。

3、__init__.py的设计原则

__init__.py的原始使命是声明一个模块，所以它可以是一个空文件。在__init__.py中声明的所有类型和变量，就是其代表的模块的类型和变量，第2小节就是利用这个原理，为四则运算的4个子模块声明了新的变量。我们在利用__init__.py时，应该遵循如下几个原则：

A、不要污染现有的命名空间。模块一个目的，是为了避免命名冲突，如果你在种用__init__.py时违背这个原则，是反其道而为之，就没有必要使用模块了。

B、利用__init__.py对外提供类型、变量和接口，对用户隐藏各个子模块的实现。一个模块的实现可能非常复杂，你需要用很多个文件，甚至很多子模块来实现，但用户可能只需要知道一个类型和接口。就像我们的arithmetic例子中，用户只需要知道四则运算有add、sub、mul、dev四个接口，却并不需要知道它们是怎么实现的，也不想去了解arithmetic中是如何组织各个子模块的。由于各个子模块的实现有可能非常复杂，而对外提供的类型和接口有可能非常的简单，我们就可以通过这个方式来对用户隐藏实现，同时提供非常方便的使用。

C、只在__init__.py中导入有必要的内容，不要做没必要的运算。像我们的例子，import arithmetic语句会执行__ini__.py中的所有代码。如果我们在__init__.py中做太多事情，每次import都会有额外的运算，会造成没有必要的开销。一句话，__init__.py只是为了达到B中所表述的目的，其它事情就不要做啦。

* 查看当前导入(引入)模块信息
使用不带参的dir函数dir()

例如小编这里导入以下几个模块。

import sys,os,re,time,wmi

导入这4个模块。

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-16_16-12-57.png @ 2021-10-16 21:12:58
[[file:%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8D%E5%AF%BC%E5%85%A5(%E5%BC%95%E5%85%A5)%E6%A8%A1%E5%9D%97%E4%BF%A1%E6%81%AF/2021-10-16_21-12-58_Snipaste_2021-10-16_16-12-57.png]]
此时我们要查看导入的模块信息——使用不带参的dir函数。

dir()

可以看到，没有_开头和结尾的数据，就是我们导入的模块，其实_开头和结尾的也是模块，只不过是python解释器导入的模块。
#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-16_21-13-20.png @ 2021-10-16 21:13:25
[[file:%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8D%E5%AF%BC%E5%85%A5(%E5%BC%95%E5%85%A5)%E6%A8%A1%E5%9D%97%E4%BF%A1%E6%81%AF/2021-10-16_21-13-25_Snipaste_2021-10-16_21-13-20.png]]


* Optional和带默认值的参数
** 带默认值的参数
在Python中的类或者函数中，若参数在声明时附带了它的默认值，则在实例化或调用时，可以选择性地为该参数赋值，例如：
#+begin_src python
#默认值参数
def foo_v1(a: int, b: int = 1):
    print(a + b)
#未给b传入实参时，采用默认值    
foo_v1(2)

# 输出
# >>> 3
#+END_SRC
【 注意： 规定默认值时，不一定要声明变量所属的类型（说到底Python中的对象、变量名只是一个指针或者说地址罢了），Python是一门动态语言，它总会在Python解释器进程运行的时候去动态地判定一个变量赋值的类型，而之所以在代码中声明静态类型则是为了减少人为错误而提供相应的类型或错误提示，但并不会影响Python的运行！】
** Typing.Optional类
可选类型，作用几乎和带默认值的参数等价，不同的是使用Optional会告诉你的IDE或者框架：这个参数除了给定的默认值外还可以是None，而且使用有些静态检查工具如mypy时，对 a: int =None这样类似的声明可能会提示报错，但使用a :Optional[int] = None不会。

以下为Python Documents对它的解释：

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-08_22-54-30.png @ 2021-11-11 16:12:46
[[file:Optional%E5%92%8C%E5%B8%A6%E9%BB%98%E8%AE%A4%E5%80%BC%E7%9A%84%E5%8F%82%E6%95%B0/2021-11-11_16-12-46_Snipaste_2021-11-08_22-54-30.png]]

Optional[X]等价于Union[X, None]

看个例子：
#+begin_src python
#Optional
from typing import Optional

def foo_v2(a: int, b: Optional[int] = None):
    if b:
        print(a + b)
    else:
        print("parameter b is a NoneType!")

#只传入a位置的实参
foo_v2(2)

# 输出
>>> parameter b is a NoneType!
#+END_SRC
当你给Optional参数的默认值不为None时，看看Optional为IDE的带来的提示：
#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-11_16-13-21.png @ 2021-11-11 16:13:26
[[file:Optional%E5%92%8C%E5%B8%A6%E9%BB%98%E8%AE%A4%E5%80%BC%E7%9A%84%E5%8F%82%E6%95%B0/2021-11-11_16-13-26_Snipaste_2021-11-11_16-13-21.png]]
这意味着在这个函数中b是一个可选参数,并且提示你它的默认值可以为None。

* field
field 在 dataclasses 里面是比较重要的功能， 用于初处理定义的参数非常有用
在 PEP 557 中是这样描述 field 的

Field objects describe each defined field. These objects are created internally, and are returned by the fields() module-level method (see below). Users should never instantiate a Field object directly.
大致意思就是 Field 对象是用于描述定义的字段的，这些对象是内部定义好了的。然后由 field() 方法返回，用户不用直接实例化 Field。
我们先看看 field 是如何使用的
#+begin_src python
from dataclasses import dataclass, field


@dataclass
class A:
    a: str = field(default="123")
#+END_SRC
可以用于设立默认值，和 a: str = "123" 一个效果，那为什么我们还需要 field 呢？
因为 field 的功能远不止这一个设置默认值，他还有很多有用的功能

1. 设置是否加载到 __init__ 里面去
#+begin_src python
@dataclass
class A:
    a: int
    b: int = field(default=10, init=False)
a = A(1) # 注意，实例化 A 的时候只需要一个参数，赋给 a 的

#+END_SRC
等价于:
#+begin_src python
class A:
    b = 10
    def __init__(self, a: int):
        self.a = a
#+END_SRC

2. 设置是否成为 __repr__ 返回参数
我们在之前实例化 A 的时候，把实例化对象打印出来的话，是这样的：

A(a=1, b=10)

那如果我们不想把特定的对象打印出来，可以这样写:
#+begin_src python
@dataclass
class A:
    a: int
    b: int = field(default=1, repr=False)

a = A(1)
print(a)
#+END_SRC

这时候，打印的结果为 A(a=1)

3. 设置是否计算 hash 的对象之一
#+begin_src python
a: int = field(hash=False)
#+END_SRC

4. 设置是否成为和其他类进行对比的值之一
#+begin_src python
a: int = field(compare=False)
#+END_SRC

5. 定义 field 信息
#+begin_src python
from dataclasses import field, dataclass, fields
@dataclass
class A:
    a: int = field(metadata={"name": "a"}) # metadata 需要接受一个映射对象，也就是 python 的字典

metadata = fields(A)
print(metadata)
#+END_SRC
打印的结果是
#+BEGIN_EXAMPLE
(Field(name='a',type=<class 'int'>,default=<dataclasses._MISSING_TYPE object at 0x10f2fe748>,default_factory=<dataclasses._MISSING_TYPE object at 0x10f2fe748>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'name': 'a'}),_field_type=_FIELD),)
#+END_EXAMPLE
是一个 tuple，第一个即是 a 字段的 field 定义

可以通过 metadata[0].metadata["name"] 获取值

6. 自定义处理定义的参数

有些字段需要我们进行一些预处理，不用传递初始值，由其他函数返回

我们可以这么写
#+begin_src python
def value():
    return "123"

@dataclass
class A:
    a: str = field(default_factory=value)

print(A().a) # 实例化 A 的时候已经可以不传递值了
#+END_SRC
打印的结果是 '123'
* numpy库 
** numpy.fromfile
由一个text或者binary文件创建数组
#+BEGIN_SRC python
numpy.fromfile(file, dtype=float, count=-1, sep='', offset=0)
#+END_SRC

- **file** : file or str or Path
  Open file object or filename.
- **dtype** : data-type   
  Data type of the returned array. For binary files, it is used to determine the size and byte-order of the items in the file.
- **count** : int
  Number of items to read. `-1` means all items (i.e., the complete file).
- **sep** : str
  Separator between items if file is a text file. Empty (“”) separator means the file should be treated as binary. Spaces (” “) in the separator match zero or more whitespace characters. A separator consisting only of spaces must match at least one whitespace.
- **offset** : int
  The offset (in bytes) from the file’s current position. Defaults to 0. Only permitted for binary files.
** np.argsort()
~numpy.argsort(a, axis=-1, kind=’quicksort’, order=None)~

功能: 将矩阵a按照axis排序，并返回排序后的下标

参数: a:输入矩阵， axis:需要排序的维度

返回值: 输出排序后的下标
*** 例子
#+BEGIN_SRC python
import numpy as np
x = np.array([1,4,3,-1,6,9])
x.argsort()
# array([3, 0, 1, 2, 4, 5], dtype=int64)
#+END_SRC

** numpy.bincount
~numpy.bincount(x, weights=None, minlength=0)~

Count number of occurrences of each value in array of non-negative ints.

The number of bins (of size 1) is one larger than the largest value in x. If minlength is specified, there will be at least this number of bins in the output array (though it will be longer if necessary, depending on the contents of x). Each bin gives the number of occurrences of its index value in x. If weights is specified the input array is weighted by it, i.e. if a value n is found at position i, out[n] += weight[i] instead of out[n] += 1.

*** 参数
- x:array_like, 1 dimension, nonnegative ints
Input array.

- weights:array_like, optional
Weights, array of the same shape as x.

- minlength:int, optional
A minimum number of bins for the output array.

*** 返回值
out：ndarray of ints
The result of binning the input array. The length of out is equal to np.amax(x)+1.
*** 例子
它大致说bin的数量比x中的最大值大1，每个bin给出了它的索引值在x中出现的次数。下面，我举个例子让大家更好的理解一下：
#+BEGIN_SRC python
# 我们可以看到x中最大的数为7，因此bin的数量为8，那么它的索引值为0->7
x = np.array([0, 1, 1, 3, 2, 1, 7])
# 索引0出现了1次，索引1出现了3次......索引5出现了0次......
np.bincount(x)
#因此，输出结果为：array([1, 3, 1, 1, 0, 0, 0, 1])

# 我们可以看到x中最大的数为7，因此bin的数量为8，那么它的索引值为0->7
x = np.array([7, 6, 2, 1, 4])
# 索引0出现了0次，索引1出现了1次......索引5出现了0次......
np.bincount(x)
#输出结果为：array([0, 1, 1, 0, 1, 0, 1, 1])
#+END_SRC
下面，我来解释一下weights这个参数。文档说，如果weights参数被指定，那么x会被它加权，也就是说，如果值n发现在位置i，那么out[n] += weight[i]而不是out[n] += 1.**因此，我们weights的大小必须与x相同，否则报错。**下面，我举个例子让大家更好的理解一下：
#+BEGIN_SRC python
w = np.array([0.3, 0.5, 0.2, 0.7, 1., -0.6])
# 我们可以看到x中最大的数为4，因此bin的数量为5，那么它的索引值为0->4
x = np.array([2, 1, 3, 4, 4, 3])
# 索引0 -> 0
# 索引1 -> w[1] = 0.5
# 索引2 -> w[0] = 0.3
# 索引3 -> w[2] + w[5] = 0.2 - 0.6 = -0.4
# 索引4 -> w[3] + w[4] = 0.7 + 1 = 1.7
np.bincount(x,  weights=w)
# 因此，输出结果为：array([ 0. ,  0.5,  0.3, -0.4,  1.7])
#+END_SRC
最后，我们来看一下minlength这个参数。文档说，如果minlength被指定，那么输出数组中bin的数量至少为它指定的数（如果必要的话，bin的数量会更大，这取决于x）。下面，我举个例子让大家更好的理解一下：
#+BEGIN_SRC python
# 我们可以看到x中最大的数为3，因此bin的数量为4，那么它的索引值为0->3
x = np.array([3, 2, 1, 3, 1])
# 本来bin的数量为4，现在我们指定了参数为7，因此现在bin的数量为7，所以现在它的索引值为0->6
np.bincount(x, minlength=7)
# 因此，输出结果为：array([0, 2, 1, 2, 0, 0, 0])

# 我们可以看到x中最大的数为3，因此bin的数量为4，那么它的索引值为0->3
x = np.array([3, 2, 1, 3, 1])
# 本来bin的数量为4，现在我们指定了参数为1，那么它指定的数量小于原本的数量，因此这个参数失去了作用，索引值还是0->3
np.bincount(x, minlength=1)
# 因此，输出结果为：array([0, 2, 1, 2])
#+END_SRC
** numpy.array_split
~numpy.array_split(ary, indices_or_sections, axis=0)[source]~

Split an array into multiple sub-arrays.

Please refer to the split documentation. The only difference between these functions is that array_split allows indices_or_sections to be an integer that does not equally divide the axis. For an array of length l that should be split into n sections, it returns l % n sub-arrays of size l//n + 1 and the rest of size l//n.
*** 例子
#+BEGIN_SRC python 
>>>x = np.arange(8.0)
>>>np.array_split(x, 3)
    [array([0.,  1.,  2.]), array([3.,  4.,  5.]), array([6.,  7.])]
>>>x = np.arange(7.0)
>>>np.array_split(x, 3)
    [array([0.,  1.,  2.]), array([3.,  4.]), array([5.,  6.])]
#+END_SRC
** numpy.vstack
~numpy.vstack(tup)~
Stack arrays in sequence vertically (row wise).

Take a sequence of arrays and stack them vertically to make a single array. Rebuild arrays divided by vsplit.

This function continues to be supported for backward compatibility, but you should prefer np.concatenate or np.stack. The np.stack function was added in NumPy 1.10.

*** nParameters:
tup : sequence of ndarrays

Tuple containing arrays to be stacked. The arrays must have the same shape along all but the first axis.

*** Returns:
stacked : ndarray

The array formed by stacking the given arrays.
*** 例子
#+BEGIN_SRC python
>>> a = np.array([1, 2, 3])
>>> b = np.array([2, 3, 4])
>>> np.vstack((a,b))
array([[1, 2, 3],
       [2, 3, 4]])
>>> a = np.array([[1], [2], [3]])
>>> b = np.array([[2], [3], [4]])
>>> np.vstack((a,b))
array([[1],
       [2],
       [3],
       [2],
       [3],
       [4]])
#+END_SRC
** numpy.hstack
~numpy.hstack(tup)~
Stack arrays in sequence horizontally (column wise).

Take a sequence of arrays and stack them horizontally to make a single array. Rebuild arrays divided by hsplit.

This function continues to be supported for backward compatibility, but you should prefer np.concatenate or np.stack. The np.stack function was added in NumPy 1.10.

*** Parameters:	
tup : sequence of ndarrays

All arrays must have the same shape along all but the second axis.

*** Returns:	
stacked : ndarray

The array formed by stacking the given arrays.
*** 例子
#+BEGIN_SRC python
>>> a = np.array((1,2,3))
>>> b = np.array((2,3,4))
>>> np.hstack((a,b))
array([1, 2, 3, 2, 3, 4])
>>> a = np.array([[1],[2],[3]])
>>> b = np.array([[2],[3],[4]])
>>> np.hstack((a,b))
array([[1, 2],
       [2, 3],
       [3, 4]])


>>> a=np.array([[1,2],[3,4],[5,6]])
>>> np.hstack(a)
array([1, 2, 3, 4, 5, 6])
>>> np.vstack(a)
array([[1, 2],
       [3, 4],
       [5, 6]])
#+END_SRC
** np.random
*** np.random.choice
~numpy.random.choice(a, size=None, replace=True, p=None)~
从a(只要是ndarray都可以，但必须是一维的)中随机抽取数字，并组成指定大小(size)的数组

replace:True表示可以取相同数字，False表示不可以取相同数字

数组p：与数组a相对应，表示取数组a中每个元素的概率，默认为选取每个元素的概率相同。
**** 例子
产生随机数
#+BEGIN_SRC python
>>>np.random.choice(5)#从[0, 5)中随机输出一个随机数
#相当于np.random.randint(0, 5)
	2

>>>np.random.choice(5, 3)#在[0, 5)内输出3个数字并组成一维数组（ndarray）
#相当于np.random.randint(0, 5, 3)
	array([1, 4, 1])
#+END_SRC
从数组、列表或元组中随机抽取

注意：不管是什么，它必须是一维的！
#+BEGIN_SRC python 
L = [1, 2, 3, 4, 5]#list列表
T = (2, 4, 6, 2)#tuple元组
A = np.array([4, 2, 1])#numpy,array数组,必须是一维的
A0 = np.arange(10).reshape(2, 5)#二维数组会报错

>>>np.random.choice(L, 5)
	array([3, 5, 2, 1, 5])
	
>>>np.random.choice(T, 5)
	array([2, 2, 2, 4, 2])
 
>>>np.random.choice(A, 5)
	array([1, 4, 2, 2, 1])

>>>np.random.choice(A0, 5)#如果是二维数组，会报错
	ValueError: 'a' must be 1-dimensional
#+END_SRC
参数replace

用来设置是否可以取相同元素：True表示可以取相同数字；False表示不可以取相同数字。默认是True
#+BEGIN_SRC python 
np.random.choice(5, 6, replace=True)#可以看到有相同元素
	array([3, 4, 1, 1, 0, 3])
np.random.choice(5, 6, replace=False)#会报错，因为五个数字中取六个，不可能不取到重复的数字
	ValueError: Cannot take a larger sample than population when 'replace=False'
#+END_SRC
参数p

p实际是个数组，大小（size）应该与指定的a相同，用来规定选取a中每个元素的概率，默认为概率相同
#+BEGIN_SRC python 
>>> aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']
>>> np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])
	array(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'], dtype='|S11')
#可以看到，‘pooh’被选取的概率明显比其他几个高很多
#+END_SRC
*** numpy.random.normal
~numpy.random.normal(loc=0.0, scale=1.0, size=None)~
生成正太分布

Draw random samples from a normal (Gaussian) distribution.
**** Parameters:	
- loc : float
Mean (“centre”) of the distribution.
- scale : float
Standard deviation (spread or “width”) of the distribution.
- size : int or tuple of ints, optional
Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.

** np.max
~np.max(a, axis=None, out=None, keepdims=False)~

求序列的最值

最少接受一个参数

axis默认为axis=0即列向,如果axis=1即横向

ex:

>> np.max([-2, -1, 0, 1, 2])
2
** np.maximum
~np.maximum(X, Y, out=None)~

X和Y逐位进行比较,选择最大值.

最少接受两个参数

ex:

>> np.maximum([-3, -2, 0, 1, 2], 0)
array([0, 0, 0, 1, 2])
** np.sum
sum(a)默认为对输入参数中的所有元素进行求和
#+BEGIN_SRC python 
>>> a
array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
>>> np.sum(a)
66


>>> b=np.arange(12)
>>> b=b.reshape(2,6)
>>> b
array([[ 0,  1,  2,  3,  4,  5],
       [ 6,  7,  8,  9, 10, 11]])
>>> np.sum(b)
66
#+END_SRC
sum()输入参数带有axis时，将按照指定axis进行对应求和
#+BEGIN_SRC python 
>>> b=np.arange(12)
>>> b=b.reshape(2,6)
>>> b
array([[ 0,  1,  2,  3,  4,  5],
       [ 6,  7,  8,  9, 10, 11]])
>>> np.sum(b)	#默认对所有元素进行求和
66
>>> np.sum(b,axis=0)		#在第一个轴展开方向上求和
array([ 6,  8, 10, 12, 14, 16])
>>> np.sum(b,axis=1)
array([15, 51])
#+END_SRC
sum()输入参数axis为多个轴时，则依次按要求在axis上进行多次求和
#+BEGIN_SRC python 
>>> a=np.arange(12).reshape(2,2,3)
>>> a
array([[[ 0,  1,  2],
        [ 3,  4,  5]],

       [[ 6,  7,  8],
        [ 9, 10, 11]]])

>>> b=np.sum(a,axis=(0,1))	#分别在axis=0 和 1两个方向上进行求和
>>> b
array([18, 22, 26])
>>> b=np.sum(a,axis=(1,2))
>>> b
array([15, 51])

>>> b=np.sum(a,axis=(0,1,2))	#由于a为3为矩阵，所有在三个axis上分别求和相当于对所有元素进行求和 
>>> b
66
#+END_SRC

** numpy.linspace
~numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0)~
用于创建等差数列

Return evenly spaced numbers over a specified interval.

Returns num evenly spaced samples, calculated over the interval [start, stop].

The endpoint of the interval can optionally be excluded.

Changed in version 1.16.0: Non-scalar start and stop are now supported.
*** 参数
- start：array_like
The starting value of the sequence.
- stop：array_like
The end value of the sequence, unless endpoint is set to False. In that case, the sequence consists of all but the last of num + 1 evenly spaced samples, so that stop is excluded. Note that the step size changes when endpoint is False.
- num：int, optional
Number of samples to generate. Default is 50. Must be non-negative.
- endpoint：bool, optional
If True, stop is the last sample. Otherwise, it is not included. Default is True.
- retstep：bool, optional
If True, return (samples, step), where step is the spacing between samples.
- dtype：dtype, optional
The type of the output array. If dtype is not given, infer the data type from the other input arguments.
New in version 1.9.0.
- axis：int, optional
The axis in the result to store the samples. Relevant only if start or stop are array-like. By default (0), the samples will be along a new axis inserted at the beginning. Use -1 to get an axis at the end.
New in version 1.16.0.
*** Returns
- samples：ndarray
There are num equally spaced samples in the closed interval [start, stop] or the half-open interval [start, stop) (depending on whether endpoint is True or False).
- step：float, optional
Only returned if retstep is True
Size of spacing between samples.
*** 例子
#+BEGIN_SRC bash
>>> np.linspace(2.0, 3.0, num=5)
array([2.  , 2.25, 2.5 , 2.75, 3.  ])
>>> np.linspace(2.0, 3.0, num=5, endpoint=False)
array([2. ,  2.2,  2.4,  2.6,  2.8])
>>> np.linspace(2.0, 3.0, num=5, retstep=True)
(array([2.  ,  2.25,  2.5 ,  2.75,  3.  ]), 0.25)
#+END_SRC
** numpy.logspace
~numpy.logspace(start, stop, num, endpoint, base, dtype)~
对数等比数列
*** 参数
- start：代表间隔的起始值。
- stop：代表以区间为基础的间隔的停止值。
- num：范围之间的值数。
- endpoint:是否包含结束值
- base：指定对数的底，默认为10
- dtype：代表数组项的数据类型。
*** 例子
#+BEGIN_SRC python
b = np.logspace(0,5,6,base=2)    # base指定对数的底为2
#结果为array([ 1.,  2.,  4.,  8., 16., 32.])
a = np.logspace(0,3,4) 
#结果为array([   1.,   10.,  100., 1000.])
#+END_SRC
** numpy.mean()
~numpy.mean(a, axis=None, dtype=None, out=None, keepdims=<no value>)~
Compute the arithmetic mean along the specified axis.

Returns the average of the array elements. The average is taken over the flattened array by default, otherwise over the specified axis. float64 intermediate and return values are used for integer inputs.

*** Parameters
- a：array_like
Array containing numbers whose mean is desired. If a is not an array, a conversion is attempted.
- axis：None or int or tuple of ints, optional
Axis or axes along which the means are computed. The default is to compute the mean of the flattened array.

New in version 1.7.0.

If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before.

- dtype：data-type, optional
Type to use in computing the mean. For integer inputs, the default is float64; for floating point inputs, it is the same as the input dtype.

- out：ndarray, optional
Alternate output array in which to place the result. The default is None; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See ufuncs-output-type for more details.

- keepdims：bool, optional
If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.

If the default value is passed, then keepdims will not be passed through to the mean method of sub-classes of ndarray, however any non-default value will be. If the sub-class’ method does not implement keepdims any exceptions will be raised.
*** Returns
m:ndarray, see dtype parameter above

If out=None, returns a new array containing the mean values, otherwise a reference to the output array is returned.
*** 例子
#+BEGIN_SRC python
>>> x=np.array([[6,12,18],[24,30,36]])
>>> b=np.mean(x,axis=0)
array([15., 21., 27.])
>>> c=np.mean(x,axis=1)
array([12., 30.])

>>> b.shape  #注意b和c的shape都为（3，）
(3,)
>>> c.shape
(2,)

>>> d=b.reshape(1,-1)
>>> d
array([[15., 21., 27.]])
>>> d.shape
(1, 3)
#+END_SRC
** numpy.std
~numpy.std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>)~
Compute the standard deviation along the specified axis.

Returns the standard deviation, a measure of the spread of a distribution, of the array elements. The standard deviation is computed for the flattened array by default, otherwise over the specified axis.

*** Parameters
- a:array_like
Calculate the standard deviation of these values.

- axis:None or int or tuple of ints, optional
Axis or axes along which the standard deviation is computed. The default is to compute the standard deviation of the flattened array.

New in version 1.7.0.

If this is a tuple of ints, a standard deviation is performed over multiple axes, instead of a single axis or all the axes as before.

- dtype:dtype, optional
Type to use in computing the standard deviation. For arrays of integer type the default is float64, for arrays of float types it is the same as the array type.

- out:ndarray, optional
Alternative output array in which to place the result. It must have the same shape as the expected output but the type (of the calculated values) will be cast if necessary.

- ddof:int, optional
Means Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. By default ddof is zero.

- keepdims:bool, optional
If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.

If the default value is passed, then keepdims will not be passed through to the std method of sub-classes of ndarray, however any non-default value will be. If the sub-class’ method does not implement keepdims any exceptions will be raised.

**** Returns
standard_deviation:ndarray, see dtype parameter above.

If out is None, return a new array containing the standard deviation, otherwise return a reference to the output array.
** numpy.var
~numpy.var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>)~
Compute the variance along the specified axis.

Returns the variance of the array elements, a measure of the spread of a distribution. The variance is computed for the flattened array by default, otherwise over the specified axis.

*** Parameters
- a:array_like
Array containing numbers whose variance is desired. If a is not an array, a conversion is attempted.

- axis:None or int or tuple of ints, optional
Axis or axes along which the variance is computed. The default is to compute the variance of the flattened array.

New in version 1.7.0.

If this is a tuple of ints, a variance is performed over multiple axes, instead of a single axis or all the axes as before.

- dtype:data-type, optional
Type to use in computing the variance. For arrays of integer type the default is float64; for arrays of float types it is the same as the array type.

- out:ndarray, optional
Alternate output array in which to place the result. It must have the same shape as the expected output, but the type is cast if necessary.

- ddof:int, optional
“Delta Degrees of Freedom”: the divisor used in the calculation is N - ddof, where N represents the number of elements. By default ddof is zero.

- keepdims:bool, optional
If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.

If the default value is passed, then keepdims will not be passed through to the var method of sub-classes of ndarray, however any non-default value will be. If the sub-class’ method does not implement keepdims any exceptions will be raised.

*** Returns
variance:ndarray, see dtype parameter above

If out=None, returns a new array containing the variance; otherwise, a reference to the output array is returned.
** np.pad()
~pad(array, pad_width, mode, **kwargs)~

返回值：数组
*** 参数
- array——表示需要填充的数组；
- pad_width——表示每个轴（axis）边缘需要填充的数值数目。
参数输入方式为：（(before_1, after_1), … (before_N, after_N)），其中(before_1, after_1)表示第1轴两边缘分别填充before_1个和after_1个数值。取值为：{sequence, array_like, int}

- mode——表示填充的方式（取值：str字符串或用户提供的函数）,总共有11种填充模式；
*** 填充方式
- ‘constant’——表示连续填充相同的值，每个轴可以分别指定填充值，constant_values=（x, y）时前面用x填充，后面用y填充，缺省值填充0
- ‘edge’——表示用边缘值填充
- ‘linear_ramp’——表示用边缘递减的方式填充
- ‘maximum’——表示最大值填充
- ‘mean’——表示均值填充
- ‘median’——表示中位数填充
- ‘minimum’——表示最小值填充
- ‘reflect’——表示对称填充
- ‘symmetric’——表示对称填充
- ‘wrap’——表示用原数组后面的值填充前面，前面的值填充后面
*** 例子
**** 常数填充模式——’constant’
#+BEGIN_SRC python
>>> A = np.arange(95,99).reshape(2,2)    #原始输入数组
>>> A
array([[95, 96],
       [97, 98]])

#在数组A的边缘填充constant_values指定的数值
#（3,2）表示在A的第[0]轴填充（二维数组中，0轴表示行），即在0轴前面填充3个宽度的0，比如数组A中的95,96两个元素前面各填充了3个0；在后面填充2个0，比如数组A中的97,98两个元素后面各填充了2个0
#（2,3）表示在A的第[1]轴填充（二维数组中，1轴表示列），即在1轴前面填充2个宽度的0，后面填充3个宽度的0
>>> np.pad(A,((3,2),(2,3)),'constant',constant_values = (0,0))  #constant_values表示填充值，且(before，after)的填充值等于（0,0）
array([[ 0,  0,  0,  0,  0,  0,  0],
       [ 0,  0,  0,  0,  0,  0,  0],
       [ 0,  0,  0,  0,  0,  0,  0],
       [ 0,  0, 95, 96,  0,  0,  0],
       [ 0,  0, 97, 98,  0,  0,  0],
       [ 0,  0,  0,  0,  0,  0,  0],
       [ 0,  0,  0,  0,  0,  0,  0]])

#填充时，从前面轴，往后面轴依次填充
>>> np.pad(A,((3,2),(2,3)),'constant',constant_values = (-2,2))   #填充值，前面填充改为-2，后面填充改为2
array([[-2, -2, -2, -2,  2,  2,  2],
       [-2, -2, -2, -2,  2,  2,  2],
       [-2, -2, -2, -2,  2,  2,  2],
       [-2, -2, 95, 96,  2,  2,  2],
       [-2, -2, 97, 98,  2,  2,  2],
       [-2, -2,  2,  2,  2,  2,  2],
       [-2, -2,  2,  2,  2,  2,  2]])

>>> np.pad(A,((3,2),(2,3)),'constant',constant_values = ((0,0),(1,2)))    #0轴和1轴分别填充不同的值，先填充0轴，后填充1轴，存在1轴填充覆盖0轴填充的情形
array([[ 1,  1,  0,  0,  2,  2,  2],
       [ 1,  1,  0,  0,  2,  2,  2],
       [ 1,  1,  0,  0,  2,  2,  2],
       [ 1,  1, 95, 96,  2,  2,  2],
       [ 1,  1, 97, 98,  2,  2,  2],
       [ 1,  1,  0,  0,  2,  2,  2],
       [ 1,  1,  0,  0,  2,  2,  2]])

>>> np.pad(A,((3,2),(2,3)),'constant')     #,constant_values 缺省，则默认填充均为0
array([[ 0,  0,  0,  0,  0,  0,  0],
       [ 0,  0,  0,  0,  0,  0,  0],
       [ 0,  0,  0,  0,  0,  0,  0],
       [ 0,  0, 95, 96,  0,  0,  0],
       [ 0,  0, 97, 98,  0,  0,  0],
       [ 0,  0,  0,  0,  0,  0,  0],
       [ 0,  0,  0,  0,  0,  0,  0]])
#+END_SRC
**** 边缘值填充模式——’edge’
#+BEGIN_SRC python 
>>>B = np.arange(1,5).reshape(2,2)  #原始输入数组
>>>B
array([[1, 2],
       [3, 4]])

>>>np.pad(B,((1,2),(2,1)),'edge')   #注意先填充0轴，后面填充1轴，依次填充
array([[1, 1, 1, 2, 2],
       [1, 1, 1, 2, 2],
       [3, 3, 3, 4, 4],
       [3, 3, 3, 4, 4],
       [3, 3, 3, 4, 4]])
#+end_src
**** 边缘最大值填充模式——’maximum’
#+BEGIN_SRC python 
>>> B = np.arange(1,5).reshape(2,2)  #原始输入数组
>>> B
array([[1, 2],
       [3, 4]])

>>>np.pad(B,((1,2),(2,1)),'maximum')    #maximum填充模式还有其他控制参数，比如stat_length，详细见numpy库
array([[4, 4, 3, 4, 4],
       [2, 2, 1, 2, 2],
       [4, 4, 3, 4, 4],
       [4, 4, 3, 4, 4],
       [4, 4, 3, 4, 4]])

>>> C = np.arange(0,9).reshape(3,3)  #原始输入数组
>>> C
array([[0, 1, 2],
       [3, 4, 5],
       [6, 7, 8]])

>>>np.pad(C,((3,2),(2,1)),'maximum')  
array([[8, 8, 6, 7, 8, 8],
       [8, 8, 6, 7, 8, 8],
       [8, 8, 6, 7, 8, 8],
       [2, 2, 0, 1, 2, 2],
       [5, 5, 3, 4, 5, 5],
       [8, 8, 6, 7, 8, 8],
       [8, 8, 6, 7, 8, 8],
       [8, 8, 6, 7, 8, 8]])
#+END_SRC
**** 官方例子
#+BEGIN_SRC python
>>> a = [1, 2, 3, 4, 5]

>>> np.pad(a, (2, 3), 'constant', constant_values=(4, 6))
array([4, 4, 1, ..., 6, 6, 6])

>>> np.pad(a, (2, 3), 'edge')
array([1, 1, 1, ..., 5, 5, 5])

>>> np.pad(a, (2, 3), 'linear_ramp', end_values=(5, -4))
array([ 5,  3,  1,  2,  3,  4,  5,  2, -1, -4])

>>>np.pad(a, (2,), 'maximum')
array([5, 5, 1, 2, 3, 4, 5, 5, 5])

>>> np.pad(a, (2,), 'mean')
array([3, 3, 1, 2, 3, 4, 5, 3, 3])

>>> np.pad(a, (2,), 'median')
array([3, 3, 1, 2, 3, 4, 5, 3, 3])

>>> a = [[1, 2], [3, 4]]

>>> np.pad(a, ((3, 2), (2, 3)), 'minimum')
array([[1, 1, 1, 2, 1, 1, 1],
       [1, 1, 1, 2, 1, 1, 1],
       [1, 1, 1, 2, 1, 1, 1],
       [1, 1, 1, 2, 1, 1, 1],
       [3, 3, 3, 4, 3, 3, 3],
       [1, 1, 1, 2, 1, 1, 1],
       [1, 1, 1, 2, 1, 1, 1]])

>>> a = [1, 2, 3, 4, 5]

>>> np.pad(a, (2, 3), 'reflect')
array([3, 2, 1, 2, 3, 4, 5, 4, 3, 2])
>>> np.pad(a, (2, 3), 'reflect', reflect_type='odd')
array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8])
>>> np.pad(a, (2, 3), 'symmetric')
array([2, 1, 1, 2, 3, 4, 5, 5, 4, 3])
>>> np.pad(a, (2, 3), 'symmetric', reflect_type='odd')
array([0, 1, 1, 2, 3, 4, 5, 5, 6, 7])
>>> np.pad(a, (2, 3), 'wrap')
array([4, 5, 1, 2, 3, 4, 5, 1, 2, 3])
>>> def pad_with(vector, pad_width, iaxis, kwargs):
       pad_value = kwargs.get('padder', 10)
       vector[:pad_width[0]] = pad_value
       vector[-pad_width[1]:] = pad_value
>>> a = np.arange(6)
>>> a = a.reshape((2, 3))
>>> np.pad(a, 2, pad_with)
array([[10, 10, 10, 10, 10, 10, 10],
       [10, 10, 10, 10, 10, 10, 10],
       [10, 10,  0,  1,  2, 10, 10],
       [10, 10,  3,  4,  5, 10, 10],
       [10, 10, 10, 10, 10, 10, 10],
       [10, 10, 10, 10, 10, 10, 10]])
>>> np.pad(a, 2, pad_with, padder=100)
array([[100, 100, 100, 100, 100, 100, 100],
       [100, 100, 100, 100, 100, 100, 100],
       [100, 100,   0,   1,   2, 100, 100],
       [100, 100,   3,   4,   5, 100, 100],
       [100, 100, 100, 100, 100, 100, 100],
       [100, 100, 100, 100, 100, 100, 100]])
#+END_SRC
** None的作用
在数组索引中，加入None就相当于在对应维度加一维
#+BEGIN_SRC python 
>>> import numpy as np
>a=[1,2,3,4]
>>> a=np.array(a)
>>> a
array([1, 2, 3, 4])
>>> b=a[:,None]
>>> b
array([[1],
       [2],
       [3],
       [4]])
>>> c=a[:,None,None]
>>> c
array([[[1]],

       [[2]],

       [[3]],

       [[4]]])

>>> a=np.ones((2,3))
>>> a
array([[1., 1., 1.],
       [1., 1., 1.]])
>>> b=a[:,None,:]
>>> b
array([[[1., 1., 1.]],

       [[1., 1., 1.]]])
>>> b=a[None,:,:]
>>> b
array([[[1., 1., 1.],
        [1., 1., 1.]]])


###在pytorch中：
>>> import torch as t
>>> a=t.from_numpy(a)
>>> a
tensor([[1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
>>> b=a[:,None,:]
>>> b
tensor([[[1., 1., 1.]],

        [[1., 1., 1.]]], dtype=torch.float64)
>>>
>>>
#+END_SRC
** keepdims的作用
当 keepidms=True,保持其二维或者三维的特性,(结果保持其原来维数)
#+BEGIN_SRC python
import numpy as np
a = np.array([[1,2],[3,4]])

# 按行相加，并且保持其二维特性
print(np.sum(a, axis=1, keepdims=True))

# 按行相加，不保持其二维特性
print(np.sum(a, axis=1))
#+END_SRC
输出
#+BEGIN_SRC python 
array([[3], [7]])
array([3, 7])
#+END_SRC

** np.transpose
~numpy.transpose(a, axes=None)~
Reverse or permute the axes of an array; returns the modified array.

For an array a with two axes, transpose(a) gives the matrix transpose.

*** Parameters
- a：array_like
Input array.

- axes：tuple or list of ints, optional
If specified, it must be a tuple or list which contains a permutation of [0,1,..,N-1] where N is the number of axes of a. The i’th axis of the returned array will correspond to the axis numbered axes[i] of the input. If not specified, defaults to range(a.ndim)[::-1], which reverses the order of the axes.

*** Returns
- p：ndarray
a with its axes permuted. A view is returned whenever possible.

*** examples
#+BEGIN_SRC python


>>>x = np.arange(4).reshape((2,2))
>>>x
array([[0, 1],
       [2, 3]])
>>>np.transpose(x)
array([[0, 2],
       [1, 3]])
>>>x = np.ones((1, 2, 3))
>>>np.transpose(x, (1, 0, 2)).shape
(2, 1, 3)
#+END_SRC
** numpy.prod
~numpy.prod(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>)~

Return the product of array elements over a given axis.

*** Parameters
- a : array_like
Input data.
- axis : None or int or tuple of ints, optional
Axis or axes along which a product is performed. The default, axis=None, will calculate the product of all the elements in the input array. If axis is negative it counts from the last to the first axis.

New in version 1.7.0.

If axis is a tuple of ints, a product is performed on all of the axes specified in the tuple instead of a single axis or all the axes as before.

- dtype : dtype, optional
The type of the returned array, as well as of the accumulator in which the elements are multiplied. The dtype of a is used by default unless a has an integer dtype of less precision than the default platform integer. In that case, if a is signed then the platform integer is used while if a is unsigned then an unsigned integer of the same precision as the platform integer is used.

- out : ndarray, optional
Alternative output array in which to place the result. It must have the same shape as the expected output, but the type of the output values will be cast if necessary.

- keepdims : bool, optional
If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.

If the default value is passed, then keepdims will not be passed through to the prod method of sub-classes of ndarray, however any non-default value will be. If the sub-class’ method does not implement keepdims any exceptions will be raised.

- initial : scalar, optional
The starting value for this product. See reduce for details.

New in version 1.15.0.

*** Returns
product_along_axis : ndarray, see dtype parameter above.

An array shaped as a but with the specified axis removed. Returns a reference to out if specified.
** numpy.ufunc.at
~ufunc.at(a, indices, b=None)~

Performs unbuffered in place operation on operand ‘a’ for elements specified by ‘indices’. For addition ufunc, this method is equivalent to a[indices] += b, except that results are accumulated for elements that are indexed more than once. For example, a[[0,0]] += 1 will only increment the first element once because of buffering, whereas add.at(a, [0,0], 1) will increment the first element twice.

New in version 1.8.0.

*** Parameters
- a : array_like
The array to perform in place operation on.

- indices : array_like or tuple

Array like index object or slice object for indexing into first operand. If first operand has multiple dimensions, indices can be a tuple of array like index objects or slice objects.

- b : array_like

Second operand for ufuncs requiring two operands. Operand must be broadcastable over first operand after indexing or slicing.

*** examples
Set items 0 and 1 to their negative values:
#+BEGIN_SRC python
>>> a = np.array([1, 2, 3, 4])
>>> np.negative.at(a, [0, 1])
>>> print(a)
array([-1, -2, 3, 4])
#+END_SRC
Increment items 0 and 1, and increment item 2 twice:
#+BEGIN_SRC pyhton
>>> a = np.array([1, 2, 3, 4])
>>> np.add.at(a, [0, 1, 2, 2], 1)
>>> print(a)
array([2, 3, 5, 4])
#+END_SRC
Add items 0 and 1 in first array to second array, and store results in first array:
#+BEGIN_SRC python
>>> a = np.array([1, 2, 3, 4])
>>> b = np.array([1, 2])
>>> np.add.at(a, [0, 1], b)
>>> print(a)
array([2, 4, 3, 4])
#+END_SRC

** 关于indexing的总结
*** Single element indexing
Single element indexing for a 1-D array is what one expects. It work exactly like that for other standard Python sequences. It is 0-based, and accepts negative indices for indexing from the end of the array.
#+BEGIN_SRC python
>>> x=np.arange(10)
>>> x
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
>>> x[2]
2
>>> x[-2]
8
#+END_SRC
Unlike lists and tuples, numpy arrays support multidimensional indexing for multidimensional arrays. That means that it is not necessary to separate each dimension’s index into its own set of square brackets.
#+BEGIN_SRC python
>>> x.shape=(2,5) #now x is 2-dimensional 
>>> x
array([[0, 1, 2, 3, 4],
       [5, 6, 7, 8, 9]])
>>> x[1,3]
8
>>> x[1,-1]
9
#+END_SRC
Note that if one indexes a multidimensional array with fewer indices than dimensions, one gets a subdimensional array. For example:
#+BEGIN_SRC python
>>> x[0]
array([0, 1, 2, 3, 4])
#+END_SRC
That is, each index specified selects the array corresponding to the rest of the dimensions selected. In the above example, choosing 0 means that the remaining dimension of length 5 is being left unspecified, and that what is returned is an array of that dimensionality and size. It must be noted that the returned array is not a copy of the original, but points to the same values in memory as does the original array. In this case, the 1-D array at the first position (0) is returned. So using a single index on the returned array, results in a single element being returned. That is:
#+BEGIN_SRC python
>>> x[0][2]
2
#+END_SRC
So note that =x[0,2] = x[0][2]= though the second case is more inefficient as a new temporary array is created after the first index that is subsequently indexed by 2.

Note to those used to IDL or Fortran memory order as it relates to indexing. NumPy uses C-order indexing. That means that the last index usually represents the most rapidly changing memory location, unlike Fortran or IDL, where the first index represents the most rapidly changing location in memory. This difference represents a great potential for confusion.
*** Other indexing options
It is possible to slice and stride arrays to extract arrays of the same number of dimensions, but of different sizes than the original. The slicing and striding works exactly the same way it does for lists and tuples except that they can be applied to multiple dimensions as well. A few examples illustrates best:
#+BEGIN_SRC python
>>> x = np.arange(10)
>>> x
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
>>> x[2:5]
array([2, 3, 4])
>>> x[:-7]
array([0, 1, 2])
>>> x[1:7:2]
array([1, 3, 5])
>>> y = np.arange(35).reshape(5,7)
>>> y
array([[ 0,  1,  2,  3,  4,  5,  6],
       [ 7,  8,  9, 10, 11, 12, 13],
       [14, 15, 16, 17, 18, 19, 20],
       [21, 22, 23, 24, 25, 26, 27],
       [28, 29, 30, 31, 32, 33, 34]])
>>> y[1:5:2,::3]
array([[ 7, 10, 13],
       [21, 24, 27]])
>>> y[[1,2],[3,4]]
array([10, 18])
>>> y[[1,2,0],[3,4,2]]  #第一个list作为行指标，第二个list作为列指标
array([10, 18,  2])
>>> y[(1,2),(2,3)]  #每个tuples作为坐标
array([ 9, 17])
#+END_SRC
Note that slices of arrays do not copy the internal array data but also produce new views of the original data.

It is possible to index arrays with other arrays for the purposes of selecting lists of values out of arrays into new arrays. There are two different ways of accomplishing this. One uses one or more arrays of index values. The other involves giving a boolean array of the proper shape to indicate the values to be selected. Index arrays are a very powerful tool that allow one to avoid looping over individual elements in arrays and thus greatly improve performance.

It is possible to use special features to effectively increase the number of dimensions in an array through indexing so the resulting array aquires the shape needed for use in an expression or with a specific function.
*** Index arrays
NumPy arrays may be indexed with other arrays (or any other sequence- like object that can be converted to an array, such as lists, with the exception of tuples; see the end of this document for why this is). The use of index arrays ranges from simple, straightforward cases to complex, hard-to-understand cases. For all cases of index arrays, what is returned is a copy of the original data, not a view as one gets for slices.

Index arrays must be of integer type. Each value in the array indicates which value in the array to use in place of the index. To illustrate:
#+BEGIN_SRC python
>>> x = np.arange(10,1,-1)
>>> x
array([10,  9,  8,  7,  6,  5,  4,  3,  2])
>>> x[np.array([3, 3, 1, 8])]
array([7, 7, 9, 2])

>>> d=np.arange(105).reshape(3,5,7)
>>> d
array([[[  0,   1,   2,   3,   4,   5,   6],
        [  7,   8,   9,  10,  11,  12,  13],
        [ 14,  15,  16,  17,  18,  19,  20],
        [ 21,  22,  23,  24,  25,  26,  27],
        [ 28,  29,  30,  31,  32,  33,  34]],

       [[ 35,  36,  37,  38,  39,  40,  41],
        [ 42,  43,  44,  45,  46,  47,  48],
        [ 49,  50,  51,  52,  53,  54,  55],
        [ 56,  57,  58,  59,  60,  61,  62],
        [ 63,  64,  65,  66,  67,  68,  69]],

       [[ 70,  71,  72,  73,  74,  75,  76],
        [ 77,  78,  79,  80,  81,  82,  83],
        [ 84,  85,  86,  87,  88,  89,  90],
        [ 91,  92,  93,  94,  95,  96,  97],
        [ 98,  99, 100, 101, 102, 103, 104]]])

>>> d[np.array([2,0])]
array([[[ 70,  71,  72,  73,  74,  75,  76],
        [ 77,  78,  79,  80,  81,  82,  83],
        [ 84,  85,  86,  87,  88,  89,  90],
        [ 91,  92,  93,  94,  95,  96,  97],
        [ 98,  99, 100, 101, 102, 103, 104]],

       [[  0,   1,   2,   3,   4,   5,   6],
        [  7,   8,   9,  10,  11,  12,  13],
        [ 14,  15,  16,  17,  18,  19,  20],
        [ 21,  22,  23,  24,  25,  26,  27],
        [ 28,  29,  30,  31,  32,  33,  34]]])
>>> d[np.array([[0,2],[2,2]])]
array([[[[  0,   1,   2,   3,   4,   5,   6],
         [  7,   8,   9,  10,  11,  12,  13],
         [ 14,  15,  16,  17,  18,  19,  20],
         [ 21,  22,  23,  24,  25,  26,  27],
         [ 28,  29,  30,  31,  32,  33,  34]],

        [[ 70,  71,  72,  73,  74,  75,  76],
         [ 77,  78,  79,  80,  81,  82,  83],
         [ 84,  85,  86,  87,  88,  89,  90],
         [ 91,  92,  93,  94,  95,  96,  97],
         [ 98,  99, 100, 101, 102, 103, 104]]],


       [[[ 70,  71,  72,  73,  74,  75,  76],
         [ 77,  78,  79,  80,  81,  82,  83],
         [ 84,  85,  86,  87,  88,  89,  90],
         [ 91,  92,  93,  94,  95,  96,  97],
         [ 98,  99, 100, 101, 102, 103, 104]],

        [[ 70,  71,  72,  73,  74,  75,  76],
         [ 77,  78,  79,  80,  81,  82,  83],
         [ 84,  85,  86,  87,  88,  89,  90],
         [ 91,  92,  93,  94,  95,  96,  97],
         [ 98,  99, 100, 101, 102, 103, 104]]]])
#+END_SRC
The index array consisting of the values 3, 3, 1 and 8 correspondingly create an array of length 4 (same as the index array) where each index is replaced by the value the index array has in the array being indexed.

Negative values are permitted and work as they do with single indices or slices:
#+BEGIN_SRC python
>>> x[np.array([3,3,-3,8])]
array([7, 7, 4, 2])
#+END_SRC
It is an error to have index values out of bounds:
#+BEGIN_SRC python
>>> x[np.array([3, 3, 20, 8])]
<type 'exceptions.IndexError'>: index 20 out of bounds 0<=index<9
#+END_SRC
Generally speaking, what is returned when index arrays are used is an array with the same shape as the index array, but with the type and values of the array being indexed. As an example, we can use a multidimensional index array instead:
#+BEGIN_SRC python
>>> x[np.array([[1,1],[2,3]])]
array([[9, 9],
       [8, 7]])
#+END_SRC
*** Indexing Multi-dimensional arrays
Things become more complex when multidimensional arrays are indexed, particularly with multidimensional index arrays. These tend to be more unusual uses, but they are permitted, and they are useful for some problems. We’ll start with the simplest multidimensional case (using the array y from the previous examples):
#+BEGIN_SRC python
>>> y
array([[ 0,  1,  2,  3,  4,  5,  6],
       [ 7,  8,  9, 10, 11, 12, 13],
       [14, 15, 16, 17, 18, 19, 20],
       [21, 22, 23, 24, 25, 26, 27],
       [28, 29, 30, 31, 32, 33, 34]])
>>> y[np.array([0,2,4]), np.array([0,1,2])]
array([ 0, 15, 30])

>>> d=np.arange(105).reshape(3,5,7)
>>> d
array([[[  0,   1,   2,   3,   4,   5,   6],
        [  7,   8,   9,  10,  11,  12,  13],
        [ 14,  15,  16,  17,  18,  19,  20],
        [ 21,  22,  23,  24,  25,  26,  27],
        [ 28,  29,  30,  31,  32,  33,  34]],

       [[ 35,  36,  37,  38,  39,  40,  41],
        [ 42,  43,  44,  45,  46,  47,  48],
        [ 49,  50,  51,  52,  53,  54,  55],
        [ 56,  57,  58,  59,  60,  61,  62],
        [ 63,  64,  65,  66,  67,  68,  69]],

       [[ 70,  71,  72,  73,  74,  75,  76],
        [ 77,  78,  79,  80,  81,  82,  83],
        [ 84,  85,  86,  87,  88,  89,  90],
        [ 91,  92,  93,  94,  95,  96,  97],
        [ 98,  99, 100, 101, 102, 103, 104]]])
>>> d[np.array([[0,2],[2,2]]),np.array([[0,2],[2,2]])]  #由于d有3个维度,(3,5,7),所以这里第一个指标np.array([[0,2],[2,2]]指的是第一个维度，np.array([[0,2],[2,2]]指的是第二个维度
array([[[ 0,  1,  2,  3,  4,  5,  6],
        [84, 85, 86, 87, 88, 89, 90]],

       [[84, 85, 86, 87, 88, 89, 90],
        [84, 85, 86, 87, 88, 89, 90]]])
>>> d[np.array([[0,2],[2,2]]),np.array([[0,2],[2,2]])]
array([[[ 0,  1,  2,  3,  4,  5,  6],
        [84, 85, 86, 87, 88, 89, 90]],

       [[84, 85, 86, 87, 88, 89, 90],
        [84, 85, 86, 87, 88, 89, 90]]])
>>> d[np.array([[0,2],[2,2]]),np.array([[0,2]])]   #这里第二个参数np.array([[0,2]])由于与第一个参数np.array([[0,2],[2,2]])不同shape,所以第二个参数会通过广播机制转化为np.array([[0,2],[0,2]]),然后继续操作
array([[[ 0,  1,  2,  3,  4,  5,  6],
        [84, 85, 86, 87, 88, 89, 90]],

       [[70, 71, 72, 73, 74, 75, 76],
        [84, 85, 86, 87, 88, 89, 90]]])
#+END_SRC
In this case, if the index arrays have a matching shape, and there is an index array for each dimension of the array being indexed, the resultant array has the same shape as the index arrays, and the values correspond to the index set for each position in the index arrays. In this example, the first index value is 0 for both index arrays, and thus the first value of the resultant array is y[0,0]. The next value is y[2,1], and the last is y[4,2].

If the index arrays do not have the same shape, there is an attempt to broadcast them to the same shape. If they cannot be broadcast to the same shape, an exception is raised:
#+BEGIN_SRC python
>>> y[np.array([0,2,4]), np.array([0,1])]
<type 'exceptions.ValueError'>: shape mismatch: objects cannot be
broadcast to a single shape
#+END_SRC
The broadcasting mechanism permits index arrays to be combined with scalars for other indices. The effect is that the scalar value is used for all the corresponding values of the index arrays:
#+BEGIN_SRC python
>>> y[np.array([0,2,4]), 1]
array([ 1, 15, 29])
#+END_SRC
Jumping to the next level of complexity, it is possible to only partially index an array with index arrays. It takes a bit of thought to understand what happens in such cases. For example if we just use one index array with y:
#+BEGIN_SRC python
>>> y[np.array([0,2,4])]
array([[ 0,  1,  2,  3,  4,  5,  6],
       [14, 15, 16, 17, 18, 19, 20],
       [28, 29, 30, 31, 32, 33, 34]])
#+END_SRC
What results is the construction of a new array where each value of the index array selects one row from the array being indexed and the resultant array has the resulting shape (number of index elements, size of row).

An example of where this may be useful is for a color lookup table where we want to map the values of an image into RGB triples for display. The lookup table could have a shape (nlookup, 3). Indexing such an array with an image with shape (ny, nx) with dtype=np.uint8 (or any integer type so long as values are with the bounds of the lookup table) will result in an array of shape (ny, nx, 3) where a triple of RGB values is associated with each pixel location.

In general, the shape of the resultant array will be the concatenation of the shape of the index array (or the shape that all the index arrays were broadcast to) with the shape of any unused dimensions (those not indexed) in the array being indexed.
*** Boolean or “mask” index arrays
Boolean arrays used as indices are treated in a different manner entirely than index arrays. Boolean arrays must be of the same shape as the initial dimensions of the array being indexed. In the most straightforward case, the boolean array has the same shape:
#+BEGIN_SRC python
>>> y = np.arange(35).reshape(5,7)
>>> y
array([[ 0,  1,  2,  3,  4,  5,  6],
       [ 7,  8,  9, 10, 11, 12, 13],
       [14, 15, 16, 17, 18, 19, 20],
       [21, 22, 23, 24, 25, 26, 27],
       [28, 29, 30, 31, 32, 33, 34]])
>>> b = y>20
>>> y[b]
array([21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34])
>>> b=y>20
>>> b
array([[False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False],
       [ True,  True,  True,  True,  True,  True,  True],
       [ True,  True,  True,  True,  True,  True,  True]])
>>> b[:,5]
array([False, False, False,  True,  True])
>>> y[b[:,5]]
array([[21, 22, 23, 24, 25, 26, 27],
       [28, 29, 30, 31, 32, 33, 34]])
#+END_SRC

Unlike in the case of integer index arrays, in the boolean case, the result is a 1-D array containing all the elements in the indexed array corresponding to all the true elements in the boolean array. The elements in the indexed array are always iterated and returned in row-major (C-style) order. The result is also identical to y[np.nonzero(b)]. As with index arrays, what is returned is a copy of the data, not a view as one gets with slices.

The result will be multidimensional if y has more dimensions than b. For example:
#+BEGIN_SRC python
>>> b[:,5] # use a 1-D boolean whose first dim agrees with the first dim of y
array([False, False, False,  True,  True], dtype=bool)
>>> y[b[:,5]]
array([[21, 22, 23, 24, 25, 26, 27],
       [28, 29, 30, 31, 32, 33, 34]])
#+END_SRC
Here the 4th and 5th rows are selected from the indexed array and combined to make a 2-D array.

In general, when the boolean array has fewer dimensions than the array being indexed, this is equivalent to y[b, ...], which means y is indexed by b followed by as many : as are needed to fill out the rank of y. Thus the shape of the result is one dimension containing the number of True elements of the boolean array, followed by the remaining dimensions of the array being indexed.

For example, using a 2-D boolean array of shape (2,3) with four True elements to select rows from a 3-D array of shape (2,3,5) results in a 2-D result of shape (4,5):
#+BEGIN_SRC python
>>> x = np.arange(30).reshape(2,3,5)
>>> x
array([[[ 0,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  9],
        [10, 11, 12, 13, 14]],
       [[15, 16, 17, 18, 19],
        [20, 21, 22, 23, 24],
        [25, 26, 27, 28, 29]]])
>>> b = np.array([[True, True, False], [False, True, True]])
>>> x[b]
array([[ 0,  1,  2,  3,  4],
       [ 5,  6,  7,  8,  9],
       [20, 21, 22, 23, 24],
       [25, 26, 27, 28, 29]])
#+END_SRC
For further details, consult the numpy reference documentation on array indexing.
*** Combining index arrays with slices
Index arrays may be combined with slices. For example:
#+BEGIN_SRC python
>>> y[np.array([0,2,4]),1:3]
array([[ 1,  2],
       [15, 16],
       [29, 30]])
#+END_SRC
In effect, the slice is converted to an index array np.array([[1,2]]) (shape (1,2)) that is broadcast with the index array to produce a resultant array of shape (3,2).

Likewise, slicing can be combined with broadcasted boolean indices:
#+BEGIN_SRC python
>>> y
array([[ 0,  1,  2,  3,  4,  5,  6],
       [ 7,  8,  9, 10, 11, 12, 13],
       [14, 15, 16, 17, 18, 19, 20],
       [21, 22, 23, 24, 25, 26, 27],
       [28, 29, 30, 31, 32, 33, 34]])
>>> y[b[:,5],1:3]
array([[22, 23],
       [29, 30]])
#+END_SRC
*** Structural indexing tools
To facilitate easy matching of array shapes with expressions and in assignments, the np.newaxis object can be used within array indices to add new dimensions with a size of 1. For example:
#+BEGIN_SRC python 
>>> y.shape
(5, 7)
>>> y[:,np.newaxis,:].shape
(5, 1, 7)
#+END_SRC
Note that there are no new elements in the array, just that the dimensionality is increased. This can be handy to combine two arrays in a way that otherwise would require explicitly reshaping operations. For example:
#+BEGIN_SRC python
>>> x = np.arange(5)
>>> x[:,np.newaxis] + x[np.newaxis,:]
array([[0, 1, 2, 3, 4],
       [1, 2, 3, 4, 5],
       [2, 3, 4, 5, 6],
       [3, 4, 5, 6, 7],
       [4, 5, 6, 7, 8]])
#+END_SRC
The ellipsis syntax maybe used to indicate selecting in full any remaining unspecified dimensions. For example:
#+BEGIN_SRC python
>>> z = np.arange(81).reshape(3,3,3,3)
>>> z[1,...,2]
array([[29, 32, 35],
       [38, 41, 44],
       [47, 50, 53]])
#+END_SRC
This is equivalent to:
#+BEGIN_SRC pytho
>>> z[1,:,:,2]
array([[29, 32, 35],
       [38, 41, 44],
       [47, 50, 53]])
#+END_SRC
*** Structural indexing tools
To facilitate easy matching of array shapes with expressions and in assignments, the np.newaxis object can be used within array indices to add new dimensions with a size of 1. For example:
#+BEGIN_SRC python
>>> y.shape
(5, 7)
>>> y[:,np.newaxis,:].shape
(5, 1, 7)
#+END_SRC
Note that there are no new elements in the array, just that the dimensionality is increased. This can be handy to combine two arrays in a way that otherwise would require explicitly reshaping operations. For example:
#+BEGIN_SRC python
>>> x = np.arange(5)
>>> x
array([0, 1, 2, 3, 4])
>>> x[:,np.newaxis]
array([[0],
       [1],
       [2],
       [3],
       [4]])
>>> x[np.newaxis,:]
array([[0, 1, 2, 3, 4]])
>>> x[:,np.newaxis] + x[np.newaxis,:]
array([[0, 1, 2, 3, 4],
       [1, 2, 3, 4, 5],
       [2, 3, 4, 5, 6],
       [3, 4, 5, 6, 7],
       [4, 5, 6, 7, 8]])
#+END_SRC
The ellipsis syntax maybe used to indicate selecting in full any remaining unspecified dimensions. For example:
#+BEGIN_SRC python 
>>> z = np.arange(81).reshape(3,3,3,3)
>>> z[1,...,2]
array([[29, 32, 35],
       [38, 41, 44],
       [47, 50, 53]])
#+END_SRC
This is equivalent to:
#+BEGIN_SRC python 
>>> z[1,:,:,2]
array([[29, 32, 35],
       [38, 41, 44],
       [47, 50, 53]])
#+END_SRC
*** Assigning values to indexed arrays
As mentioned, one can select a subset of an array to assign to using a single index, slices, and index and mask arrays. The value being assigned to the indexed array must be shape consistent (the same shape or broadcastable to the shape the index produces). For example, it is permitted to assign a constant to a slice:
#+BEGIN_SRC python 
>>> x = np.arange(10)
>>> x[2:7] = 1
#+END_SRC
or an array of the right size:
#+BEGIN_SRC python
>>> x[2:7] = np.arange(5)
#+END_SRC
Note that assignments may result in changes if assigning higher types to lower types (like floats to ints) or even exceptions (assigning complex to floats or ints):
#+BEGIN_SRC python
>>> x[1] = 1.2
>>> x[1]
1
>>> x[1] = 1.2j
<type 'exceptions.TypeError'>: can't convert complex to long; use long(abs(z))
#+END_SRC

Unlike some of the references (such as array and mask indices) assignments are always made to the original data in the array (indeed, nothing else would make sense!). Note though, that some actions may not work as one may naively expect. This particular example is often surprising to people:
#+BEGIN_SRC python 
>>> x = np.arange(0, 50, 10)
>>> x
array([ 0, 10, 20, 30, 40])
>>> x[np.array([1, 1, 3, 1])] += 1
>>> x
array([ 0, 11, 20, 31, 40])
#+END_SRC

Where people expect that the 1st location will be incremented by 3. In fact, it will only be incremented by 1. The reason is because a new array is extracted from the original (as a temporary) containing the values at 1, 1, 3, 1, then the value 1 is added to the temporary, and then the temporary is assigned back to the original array. Thus the value of the array at x[1]+1 is assigned to x[1] three times, rather than being incremented 3 times.
*** Dealing with variable numbers of indices within programs
The index syntax is very powerful but limiting when dealing with a variable number of indices. For example, if you want to write a function that can handle arguments with various numbers of dimensions without having to write special case code for each number of possible dimensions, how can that be done? If one supplies to the index a tuple, the tuple will be interpreted as a list of indices. For example (using the previous definition for the array z):
#+BEGIN_SRC python 
>>> z = np.arange(81).reshape(3,3,3,3)
>>> z
array([[[[ 0,  1,  2],
         [ 3,  4,  5],
         [ 6,  7,  8]],

        [[ 9, 10, 11],
         [12, 13, 14],
         [15, 16, 17]],

        [[18, 19, 20],
         [21, 22, 23],
         [24, 25, 26]]],


       [[[27, 28, 29],
         [30, 31, 32],
         [33, 34, 35]],

        [[36, 37, 38],
         [39, 40, 41],
         [42, 43, 44]],

        [[45, 46, 47],
         [48, 49, 50],
         [51, 52, 53]]],


       [[[54, 55, 56],
         [57, 58, 59],
         [60, 61, 62]],

        [[63, 64, 65],
         [66, 67, 68],
         [69, 70, 71]],

        [[72, 73, 74],
         [75, 76, 77],
         [78, 79, 80]]]])
>>> indices = (1,1,1,1)
>>> z[indices]
40
#+END_SRC
So one can use code to construct tuples of any number of indices and then use these within an index.

Slices can be specified within programs by using the slice() function in Python. For example:
#+BEGIN_SRC python 
>>> indices = (1,1,1,slice(0,2)) # same as [1,1,1,0:2]
>>> z[indices]
array([39, 40])
#+END_SRC
Likewise, ellipsis can be specified by code by using the Ellipsis object:
#+BEGIN_SRC python
>>> indices = (1, Ellipsis, 1) # same as [1,...,1]
>>> z[indices]
array([[28, 31, 34],
       [37, 40, 43],
       [46, 49, 52]])
#+END_SRC
For this reason it is possible to use the output from the np.where() function directly as an index since it always returns a tuple of index arrays.

Because the special treatment of tuples, they are not automatically converted to an array as a list would be. As an example:
#+BEGIN_SRC python
>>> z[[1,1,1,1]] # produces a large array
array([[[[27, 28, 29],
         [30, 31, 32],
         [33, 34, 35]],

        [[36, 37, 38],
         [39, 40, 41],
         [42, 43, 44]],

        [[45, 46, 47],
         [48, 49, 50],
         [51, 52, 53]]],


       [[[27, 28, 29],
         [30, 31, 32],
         [33, 34, 35]],

        [[36, 37, 38],
         [39, 40, 41],
         [42, 43, 44]],

        [[45, 46, 47],
         [48, 49, 50],
         [51, 52, 53]]],


       [[[27, 28, 29],
         [30, 31, 32],
         [33, 34, 35]],

        [[36, 37, 38],
         [39, 40, 41],
         [42, 43, 44]],

        [[45, 46, 47],
         [48, 49, 50],
         [51, 52, 53]]],


       [[[27, 28, 29],
         [30, 31, 32],
         [33, 34, 35]],

        [[36, 37, 38],
         [39, 40, 41],
         [42, 43, 44]],

        [[45, 46, 47],
         [48, 49, 50],
         [51, 52, 53]]]])
>>> z[(1,1,1,1)] # returns a single value
40
#+END_SRC

* ~__future__~ 模块
from __future__ import division
导入python未来支持的语言特征division(精确除法)，当我们没有在程序中导入该特征时，"/"操作符执行的是截断除法(Truncating Division),当我们导入精确除法之后，"/"执行的是精确除法，如下所示：

#+BEGIN_SRC bash
>>> 3/4
0
>>> from __future__ import division
>>> 3/4
0.75
#+END_SRC

导入精确除法后，若要执行截断除法，可以使用"//"操作符：
#+BEGIN_SRC bash
>>> 3//4
0
>>> 
#+END_SRC

一些将来特征如下：

| feature          | optional in | mandatory in | effect                                             |
|------------------+-------------+--------------+----------------------------------------------------|
| nested_scopes    | 2.1.0b1     |          2.2 | PEP 227: Statically Nested Scopes                  |
| generators       | 2.2.0a1     |          2.3 | PEP 255: Simple Generators                         |
| division         | 2.2.0a2     |          3.0 | PEP 238: Changing the Division Operator            |
| absolute_import  | 2.5.0a1     |          2.7 | PEP 328: Imports: Multi-Line and Absolute/Relative |
| with_statement   | 2.5.0a1     |          2.6 | PEP 343: The “with” Statement                      |
| print_function   | 2.6.0a2     |          3.0 | PEP 3105: Make print a function                    |
| unicode_literals | 2.6.0a2     |          3.0 | PEP 3112: Bytes literals in Python 3000            |

PEP：Python Enhancement Proposals

可以在这个地方找到很多PEP：http://www.python.org/dev/peps/ 里面还能看到许多提议的动机
* argparse 库
argparse是python的一个命令行解析包.argparse 模块可以让人轻松编写用户友好的命令行接口。程序定义它需要的参数，然后 argparse 将弄清如何从 sys.argv 解析出那些参数。 argparse 模块还会自动生成帮助和使用手册，并在用户给程序传入无效参数时报出错误信息。
[[rmail:https://docs.python.org/zh-cn/3.10/library/argparse.html#module-argparse][官方中文文档]]
** 使用示例1
我们常常可以把argparse的使用简化成下面四个步骤

1. import argparse

2. parser = argparse.ArgumentParser()

3. parser.add_argument()

4. parser.parse_args()

上面四个步骤解释如下：首先导入该模块；然后创建一个解析对象；然后向该对象中添加你要关注的命令行参数和选项，每一个add_argument方法对应一个你要关注的参数或选项；最后调用parse_args()方法进行解析；解析成功之后即可使用。

以下代码是一个 Python 程序，它获取一个整数列表并计算总和或者最大值：
#+BEGIN_SRC python
import argparse

parser = argparse.ArgumentParser(description='Process some integers.')
parser.add_argument('integers', metavar='N', type=int, nargs='+',
                    help='an integer for the accumulator')
parser.add_argument('--sum', dest='accumulate', action='store_const',
                    const=sum, default=max,
                    help='sum the integers (default: find the max)')

args = parser.parse_args()
print(args.accumulate(args.integers))
#+END_SRC
假设上面的 Python 代码保存在名为 prog.py 的文件中，它可以在命令行运行并提供有用的帮助消息：
#+BEGIN_SRC bash
$ python prog.py -h
usage: prog.py [-h] [--sum] N [N ...]

Process some integers.

positional arguments:
 N           an integer for the accumulator

optional arguments:
 -h, --help  show this help message and exit
 --sum       sum the integers (default: find the max)
#+END_SRC
当使用适当的参数运行时，它会输出命令行传入整数的总和或者最大值：
#+BEGIN_SRC bash
$ python prog.py 1 2 3 4
4

$ python prog.py 1 2 3 4 --sum
10
#+END_SRC
如果传入无效参数，则会报出错误：
#+BEGIN_SRC bash
$ python prog.py a b c
usage: prog.py [-h] [--sum] N [N ...]
prog.py: error: argument N: invalid int value: 'a'
#+END_SRC
** 使用示例2
下面是采用argparse从命令行获取用户名，该python的文件名为： ~fun_test.py~
#+BEGIN_SRC python
import argparse

def main():
    parser = argparse.ArgumentParser(description="Demo of argparse")
    parser.add_argument('-n','--name', default=' Li ')
    parser.add_argument('-y','--year', default='20')
    args = parser.parse_args()
    print(args)
    name = args.name
    year = args.year
    print('Hello {}  {}'.format(name,year))

if __name__ == '__main__':
    main()
#+END_SRC
在终端执行命令:
#+BEGIN_SRC bash
python fun_test.py
#+END_SRC
结果如下：
#+BEGIN_SRC bash
Namespace(name=' Li ', year='20')
Hello  Li   20
#+END_SRC
在上面的代码中，我们先导入了 ~argparse~ 这个包，然后包中的 ~ArgumentParser~ 类生成一个 ~parser~ 对象（好多博客中把这个叫做参数解析器），其中的 ~description~ 描述这个参数解析器是干什么的，当我们在命令行显示帮助信息的时候会看到 ~description~ 描述的信息。

接着我们通过对象的 ~add_argument~ 函数来增加参数。这里我们增加了两个参数 ~name~ 和 ~year~ ，其中 ~-n,--name~ 表示同一个参数，~default~ 参数表示我们在运行命令时若没有提供参数，程序会将此值当做参数值。

最后采用对象的 ~parse_args~ 获取解析的参数，由上图可以看到， ~Namespace~ 中有两个属性（也叫成员）这里要注意个问题，当 ~-~ 和 ~--~ 同时出现的时候，系统默认后者为参数名，前者不是，但是在命令行输入的时候没有这个区分接下来就是打印参数信息了。

当执行命令：
#+BEGIN_SRC bash
python fun_test.py -n chen --year 25
#+END_SRC
结果如下：
#+BEGIN_SRC bash
Namespace(name='chen', year='25')
Hello chen  25
#+END_SRC
当执行命令 ~python fun_test.py -h~ 可以查看帮助信息
#+BEGIN_SRC python
usage: fun_test.py [-h] [-n NAME] [-y YEAR]

Demo of argparse

optional arguments:
  -h, --help            show this help message and exit
  -n NAME, --name NAME
  -y YEAR, --year YEAR
#+END_SRC
** ArgumentParser对象
创建一个ArgumentParser对象
#+BEGIN_SRC python
ArgumentParser(prog=None, usage=None, description=None, epilog=None, parents=[], formatter_class=argparse.HelpFormatter, prefix_chars='-', fromfile_prefix_chars=None, argument_default=None, conflict_handler='error', add_help=True, allow_abbrev=True)
#+END_SRC
*** 参数
- prog - 程序的名称（默认：sys.argv[0]）
- usage - 描述程序用途的字符串（默认值：从添加到解析器的参数生成）
- description - 在参数帮助文档之前显示的文本（默认值：无）
- epilog - 在参数帮助文档之后显示的文本（默认值：无）
- parents - 一个 ArgumentParser 对象的列表，它们的参数也应包含在内
- formatter_class - 用于自定义帮助文档输出格式的类
- prefix_chars - 可选参数的前缀字符集合（默认值：'-'）
- fromfile_prefix_chars - 当需要从文件中读取其他参数时，用于标识文件名的前缀字符集合（默认值：None）
- argument_default - 参数的全局默认值（默认值： None）
- conflict_handler - 解决冲突选项的策略（通常是不必要的）
- add_help - 为解析器添加一个 -h/--help 选项（默认值： True）
- allow_abbrev - 如果缩写是无歧义的，则允许缩写长选项 （默认值：True）
**** 参考
[https://docs.python.org/zh-cn/3/library/argparse.html#prog]
** add_argument()
定义单个的命令行参数应当如何解析。给一个 ArgumentParser 添加程序参数信息是通过调用 add_argument() 方法完成的。
#+BEGIN_SRC python
add_argument(name or flags...[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest])

#+END_SRC
*** 参数
- name or flags - Either a name or a list of option strings, e.g. foo or -f, --foo.选项字符串的名字或者列表，例如 foo 或者 -f, --foo。
- action - The basic type of action to be taken when this argument is encountered at the command line.命令行遇到参数时的动作，默认值是 store。
- nargs - The number of command-line arguments that should be consumed.应该读取的命令行参数个数，可以是具体的数字，或者是?号，当不指定值时对于 Positional argument 使用 default，对于 Optional argument 使用 const；或者是 * 号，表示 0 或多个参数；或者是 + 号表示 1 或多个参数。
- const - A constant value required by some action and nargs selections.action 和 nargs 所需要的常量值。
- default - The value produced if the argument is absent from the command line.不指定参数时的默认值。
- type - The type to which the command-line argument should be converted.命令行参数应该被转换成的类型。
- choices - A container of the allowable values for the argument.参数可允许的值的一个容器。
- required - Whether or not the command-line option may be omitted (optionals only).
- help - A brief description of what the argument does.参数的帮助信息，当指定为 argparse.SUPPRESS 时表示不显示该参数的帮助信息.
- metavar - A name for the argument in usage messages.在 usage 说明中的参数名称，对于必选参数默认就是参数名称，对于可选参数默认是全大写的参数名称.
- dest - The name of the attribute to be added to the object returned by parse_args().解析后的参数名称，默认情况下，对于可选参数选取最长的名称，中划线转换为下划线.即执行args = parser.parse_args()后可以用arg.xxx 表示参数

*** metavar的作用
在--help中的参数名称

用于设定在帮助中显示的内容.如下面所示，设定metavar后，打开帮助时，会显示metavar设定的内容
#+BEGIN_SRC python
> import argparse
> parser = argparse.ArgumentParser()
> parser.add_argument('--foo',metavar='YYY')
> parser.add_argument('--o')
> parser.print_help()
usage: [-h] [--foo YYY] [--o O]

optional arguments:
  -h, --help  show this help message and exit
  --foo YYY
  --o O
#+END_SRC

*** choices的作用
限制参数的取值范围。

Some command-line arguments should be selected from a restricted set of values. These can be handled by passing a container object as the choices keyword argument to ~add_argument()~ . When the command line is parsed, argument values will be checked, and an error message will be displayed if the argument was not one of the acceptable values:
#+BEGIN_SRC python
>>> parser = argparse.ArgumentParser(prog='game.py')
>>> parser.add_argument('move', choices=['rock', 'paper', 'scissors'])
>>> parser.parse_args(['rock'])
Namespace(move='rock')
>>> parser.parse_args(['fire'])
usage: game.py [-h] {rock,paper,scissors}
game.py: error: argument move: invalid choice: 'fire' (choose from 'rock',
'paper', 'scissors')
#+END_SRC
检查完 ~type~ 后会检查 ~choices~

Note that inclusion in the choices container is checked after any type conversions have been performed, so the type of the objects in the choices container should match the type specified:
#+BEGIN_SRC python
>>> parser = argparse.ArgumentParser(prog='doors.py')
>>> parser.add_argument('door', type=int, choices=range(1, 4))
>>> print(parser.parse_args(['3']))
Namespace(door=3)
>>> parser.parse_args(['4'])
usage: doors.py [-h] {1,2,3}
doors.py: error: argument door: invalid choice: 4 (choose from 1, 2, 3)
#+END_SRC

** parse_args()
ArgumentParser 通过 parse_args() 方法解析参数。它将检查命令行，把每个参数转换为适当的类型然后调用相应的操作。将参数字符串转换为对象并分配namespace的属性

Convert argument strings to objects and assign them as attributes of the namespace. Return the populated namespace.
#+BEGIN_SRC python
parse_args(args=None, namespace=None)
#+END_SRC
下面是 parse_args() 不加任何参数的结果
#+BEGIN_SRC python
> parser = argparse.ArgumentParser(description="Demo of argparse")
> parser.add_argument('-n','--name', default=' Li ')
> parser.add_argument('-y','--year', default='20')
> args = parser.parse_args()
> print(args)
Namespace(name=' Li ', year='20')
#+END_SRC
*** 参数
- args - List of strings to parse. The default is taken from sys.argv.
- namespace - An object to take the attributes. The default is a new empty Namespace object.
*** Option value syntax
对参数赋值的多种方法

The parse_args() method supports several ways of specifying the value of an option (if it takes one). In the simplest case, the option and its value are passed as two separate arguments:
#+BEGIN_SRC python
>>> parser = argparse.ArgumentParser(prog='PROG')
>>> parser.add_argument('-x')
>>> parser.add_argument('--foo')
>>> parser.parse_args(['-x', 'X'])  #这种方法要把参数和值放在括号[ ] 里面
Namespace(foo=None, x='X')
>>> parser.parse_args(['--foo', 'FOO'])
Namespace(foo='FOO', x=None)
#+END_SRC
For long options (options with names longer than a single character), the option and value can also be passed as a single command-line argument, using = to separate them:
#+BEGIN_SRC python
>>> parser.parse_args(['--foo=FOO'])
Namespace(foo='FOO', x=None)
#+END_SRC
For short options (options only one character long), the option and its value can be concatenated:
#+BEGIN_SRC python
>>> parser.parse_args(['-xX'])
Namespace(foo=None, x='X')
#+END_SRC
Several short options can be joined together, using only a single - prefix, as long as only the last option (or none of them) requires a value:
#+BEGIN_SRC python
>>> parser = argparse.ArgumentParser(prog='PROG')
>>> parser.add_argument('-x', action='store_true')
>>> parser.add_argument('-y', action='store_true')
>>> parser.add_argument('-z')
>>> parser.parse_args(['-xyzZ'])    #只有最后的z会赋值为Z
Namespace(x=True, y=True, z='Z')
#+END_SRC
*** Invalid arguments
While parsing the command line, parse_args() checks for a variety of errors, including ambiguous options, invalid types, invalid options, wrong number of positional arguments, etc. When it encounters such an error, it exits and prints the error along with a usage message:
#+BEGIN_SRC python
>>> parser = argparse.ArgumentParser(prog='PROG')
>>> parser.add_argument('--foo', type=int)
>>> parser.add_argument('bar', nargs='?')

>>> # invalid type
>>> parser.parse_args(['--foo', 'spam'])
usage: PROG [-h] [--foo FOO] [bar]
PROG: error: argument --foo: invalid int value: 'spam'

>>> # invalid option
>>> parser.parse_args(['--bar'])
usage: PROG [-h] [--foo FOO] [bar]
PROG: error: no such option: --bar

>>> # wrong number of arguments
>>> parser.parse_args(['spam', 'badger'])
usage: PROG [-h] [--foo FOO] [bar]
PROG: error: extra arguments found: badger
#+END_SRC
*** Arguments containing -
参数名称包含-，即-x可以表示可选参数x，也可以表示位置参数-x;

The parse_args() method attempts to give errors whenever the user has clearly made a mistake, but some situations are inherently ambiguous. For example, the command-line argument -1 could either be an attempt to specify an option or an attempt to provide a positional argument. The parse_args() method is cautious here: positional arguments may only begin with - if they look like negative numbers and there are no options in the parser that look like negative numbers:
#+BEGIN_SRC python
>>> parser = argparse.ArgumentParser(prog='PROG')
>>> parser.add_argument('-x')
>>> parser.add_argument('foo', nargs='?')

>>> # no negative number options, so -1 is a positional argument
>>> parser.parse_args(['-x', '-1'])
Namespace(foo=None, x='-1')

>>> # no negative number options, so -1 and -5 are positional arguments
>>> parser.parse_args(['-x', '-1', '-5'])
Namespace(foo='-5', x='-1')

>>> parser = argparse.ArgumentParser(prog='PROG')
>>> parser.add_argument('-1', dest='one')
>>> parser.add_argument('foo', nargs='?')

>>> # negative number options present, so -1 is an option
>>> parser.parse_args(['-1', 'X'])
Namespace(foo=None, one='X')

>>> # negative number options present, so -2 is an option
>>> parser.parse_args(['-2'])
usage: PROG [-h] [-1 ONE] [foo]
PROG: error: no such option: -2

>>> # negative number options present, so both -1s are options
>>> parser.parse_args(['-1', '-1'])
usage: PROG [-h] [-1 ONE] [foo]
PROG: error: argument -1: expected one argument
#+END_SRC
If you have positional arguments that must begin with - and don’t look like negative numbers, you can insert the pseudo-argument '--' which tells parse_args() that everything after that is a positional argument:
#+BEGIN_SRC python
>>> parser.parse_args(['--', '-f'])
Namespace(foo='-f', one=None)
#+END_SRC
*** Argument abbreviations (prefix matching)¶
The ~parse_args()~ method by default allows long options to be abbreviated to a prefix, if the abbreviation is unambiguous (the prefix matches a unique option):
#+BEGIN_SRC python
>>> parser = argparse.ArgumentParser(prog='PROG')
>>> parser.add_argument('-bacon')
>>> parser.add_argument('-badger')
>>> parser.parse_args('-bac MMM'.split())  #前缀bac可以对应参数bacon
Namespace(bacon='MMM', badger=None)
>>> parser.parse_args('-bad WOOD'.split()) #前缀bad对应参数badger
Namespace(bacon=None, badger='WOOD')
>>> parser.parse_args('-ba BA'.split())  #发生错误，因为前缀ba可以对应bacon，也可对应badger
usage: PROG [-h] [-bacon BACON] [-badger BADGER]
PROG: error: ambiguous option: -ba could match -badger, -bacon
#+END_SRC
An error is produced for arguments that could produce more than one options. This feature can be disabled by setting allow_abbrev to False.
* torch
** torch.Tensor
torch.Tensor是一种包含单一数据类型元素的多维矩阵。torch.Tensor是默认的tensor类型（torch.FlaotTensor）的简称。

*** 张量数据类型
Torch定义了九种CPU tensor类型和九种GPU tensor类型：
| Data type                | dtype                         | CPU tensor         | GPU tensor              |
|--------------------------+-------------------------------+--------------------+-------------------------|
| 32-bit floating point    | torch.float32 or torch.float  | torch.FloatTensor  | torch.cuda.FloatTensor  |
| 64-bit floating point    | torch.float64 or torch.double | torch.DoubleTensor | torch.cuda.DoubleTensor |
| 16-bit floating point    | torch.float16 or torch.half   | torch.HalfTensor   | torch.cuda.HalfTensor   |
| 8-bit integer (unsigned) | torch.uint8                   | torch.ByteTensor   | torch.cuda.ByteTensor   |
| 8-bit integer (signed)   | torch.int8                    | torch.CharTensor   | torch.cuda.CharTensor   |
| 16-bit integer (signed)  | torch.int16 or torch.short    | torch.ShortTensor  | torch.cuda.ShortTensor  |
| 32-bit integer (signed)  | torch.int32 or torch.int      | torch.IntTensor    | torch.cuda.IntTensor    |
| 64-bit integer (signed)  | torch.int64 or torch.long     | torch.LongTensor   | torch.cuda.LongTensor   |
| Boolean                  | torch.bool                    | torch.BoolTensor   | torch.cuda.BoolTensor   |

*** 创建张量
#+BEGIN_QUOTE
torch.tensor() 始终复制data。 如果您具有张量data，而只想更改其requires_grad标志，请使用 requires_grad_() 或 detach() 以避免复制。 如果您有一个 numpy 数组并且想要避免复制，请使用 torch.as_tensor() 。
#+END_QUOTE

#+BEGIN_SRC bash
#可以通过python列表或np数组创建张量
>>> torch.tensor([[1., -1.], [1., -1.]])
tensor([[ 1.0000, -1.0000],
        [ 1.0000, -1.0000]])
>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))
tensor([[ 1,  2,  3],
        [ 4,  5,  6]])
#+END_SRC

可以通过将 torch.dtype 和 torch.device 传递给构造函数或张量创建操作来构造特定数据类型的张量：
#+BEGIN_SRC bash
>>> torch.zeros([2, 4], dtype=torch.int32)
tensor([[ 0,  0,  0,  0],
        [ 0,  0,  0,  0]], dtype=torch.int32)
>>> cuda0 = torch.device('cuda:0')
>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)
tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],
        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')
#+END_SRC
*** 访问张量
张量的内容可以使用 Python 的索引和切片符号来访问和修改：
#+BEGIN_SRC bash
>>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])
>>> print(x[1][2])
tensor(6)
>>> x[0][1] = 8
>>> print(x)
tensor([[ 1,  8,  3],
        [ 4,  5,  6]])
#+END_SRC

使用 torch.Tensor.item() 从张量中获取包含单个值的 Python 数字：
#+BEGIN_SRC bash
>>> x = torch.tensor([[1]])
>>> x
tensor([[ 1]])
>>> x.item()
1
>>> x = torch.tensor(2.5)
>>> x
tensor(2.5000)
>>> x.item()
2.5
#+END_SRC
** torch.device
torch.device代表将torch.Tensor分配到的设备的对象。

torch.device包含一个设备类型（'cpu'or'cuda'）和可选的设备的序号。如果设备序号不存在，则为当前设备.

一个 =torch.Tensor=的设备可以通过 =Tensor.device= 访问。

=torch.device= 可以通过字符串或字符串和设备编号构造.

通过字符串：
#+BEGIN_SRC bash
>>> torch.device('cuda:0')
device(type='cuda', index=0)
 
>>> torch.device('cpu')
device(type='cpu')
 
>>> torch.device('cuda')  # current cuda device
device(type='cuda')
#+END_SRC

通过字符串和设备编号构造:
#+BEGIN_SRC bash
>>> torch.device('cuda', 0)
device(type='cuda', index=0)
 
>>> torch.device('cpu', 0)
device(type='cpu', index=0)
#+END_SRC
** torch.utils.data
*** torch.utils.data.DataLoader
PyTorch中数据读取的一个重要接口是torch.utils.data.DataLoader，该接口定义在dataloader.py脚本中，只要是用PyTorch来训练模型基本都会用到该接口，该接口主要用来将自定义的数据读取接口的输出或者PyTorch已有的数据读取接口的输入按照batch size封装成Tensor，后续只需要再包装成Variable即可作为模型的输入，因此该接口有点承上启下的作用，比较重要。

#+BEGIN_SRC python
class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=<function default_collate>, pin_memory=False, drop_last=False)
#+END_SRC
**** 参数
- dataset (Dataset) 加载数据的数据集。这个就是PyTorch已有的数据读取接口（比如torchvision.datasets.ImageFolder）或者自定义的数据接口的输出，该输出要么是torch.utils.data.Dataset类的对象，要么是继承自torch.utils.data.Dataset类的自定义类的对象。
- batch_size (int, optional) – 每个batch加载多少个样本(默认: 1)。
- shuffle (bool, optional) – 设置为True时会在每个epoch重新打乱数据(默认: False).
- sampler (Sampler, optional) – 定义从数据集中提取样本的策略。如果指定，则忽略shuffle参数。
- num_workers (int, optional) – 用多少个子进程加载数据。0表示数据将在主进程中加载(默认: 0)
- collate_fn (callable, optional) – 用来处理不同情况下的输入dataset的封装，一般采用默认即可，除非你自定义的数据读取输出非常少见。
- pin_memory (bool, optional) – If True, the data loader will copy tensors into CUDA pinned memory before returning them
- drop_last (bool, optional) – 如果数据集大小不能被batch size整除，则设置为True后可删除最后一个不完整的batch。如果设为False并且数据集的大小不能被batch size整除，则最后一个batch将更小。(默认: False)


***** collate_fn
通过collate_fn函数可以对这些样本做进一步的处理(任何你想要的处理)，原则上返回值应当是一个有结构的batch。而DataLoader每次迭代的返回值就是collate_fn的返回值。实现自定义的batch输出


*** ImageFolder
#+BEGIN_SRC python
ImageFolder(root,transform=None,target_transform=None,loader=default_loader)
#+END_SRC
root : 在指定的root路径下面寻找图片
transform: 对PIL Image进行转换操作,transform 输入是loader读取图片返回的对象
target_transform :对label进行变换
loader: 指定加载图片的函数，默认操作是读取PIL image对象
*** random_split
#+BEGIN_SRC python
class torch.utils.data.random_split(dataset, lengths)
#+END_SRC
随机不重复分割数据集； dataset：要被分割的数据集 lengths：长度列表，e.g. [7, 3]， 保证7+3=len(dataset),即将dataset划分为长度为7和长度为3的两个数据集
** torch.optim

~torch.optim~ 是一个实现了各种优化算法的库。大部分常用的方法得到支持，并且接口具备足够的通用性，使得未来能够集成更加复杂的方法。

为了使用torch.optim，你需要构建一个optimizer对象。这个对象能够保持当前参数状态并基于计算得到的梯度进行参数更新。
*** 构建
为了构建一个Optimizer，你需要给它一个包含了需要优化的参数（必须都是Variable对象）的iterable。然后，你可以设置optimizer的参 数选项，比如学习率，权重衰减，等等。
例子：
#+BEGIN_SRC python 
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr=0.0001)
#+END_SRC

#+BEGIN_QUOTE
如果您需要通过.cuda（）将模型移至GPU，请在为它构建优化器之前执行此操作。 .cuda（）之后的模型参数将与调用之前的对象不同。 通常，在构造和使用优化器时，应确保优化的参数位于一致的位置
#+END_QUOTE
*** 为每个参数单独设置选项
Optimizer也支持为每个参数单独设置选项。若想这么做，不要直接传入Variable的iterable，而是传入dict的iterable。每一个dict都分别定 义了一组参数，并且包含一个param键，这个键对应参数的列表。其他的键应该optimizer所接受的其他参数的关键字相匹配，并且会被用于对这组参数的 优化。

例如，当我们想指定每一层的学习率时，这是非常有用的：
#+BEGIN_SRC python 
optim.SGD([
                {'params': model.base.parameters()},
                {'params': model.classifier.parameters(), 'lr': 1e-3}
            ], lr=1e-2, momentum=0.9)
#+END_SRC
这意味着model.base的参数将会使用1e-2的学习率，model.classifier的参数将会使用1e-3的学习率，并且0.9的momentum将会被用于所 有的参数。
*** 进行单次优化
所有的optimizer都实现了step()方法，这个方法会更新所有的参数。它能按两种方式来使用： 
1. ~optimizer.step()~

这是大多数optimizer所支持的简化版本。一旦梯度被如backward()之类的函数计算好后，我们就可以调用这个函数。
#+BEGIN_SRC python 
for input, target in dataset:
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()
#+END_SRC
2. ~optimizer.step(closure)~

一些优化算法例如Conjugate Gradient和LBFGS需要重复多次计算函数，因此你需要传入一个闭包(closure)去允许它们重新计算你的模型。这个闭包(closure)应当清空梯度， 计算损失，然后返回。
#+BEGIN_SRC python 
for input, target in dataset:
    def closure():
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        return loss
    optimizer.step(closure)
#+END_SRC
*** 各种优化算法
*** torch.optim.lr_scheduler，调整学习率，
~torch.optim.lr_scheduler~ 提供了多种通过迭代次数调整学习率的方法。 ~torch.optim.lr_scheduler.ReduceLROnPlateau~ 允许基于某种衡量方法动态调整学习率。

学习率调整应该在优化更新后使用，例如，你应该用下列的方式写代码：
#+BEGIN_SRC bash
>>> scheduler = ...
>>> for epoch in range(100):
>>>     train(...)
>>>     validate(...)
>>>     scheduler.step()
#+END_SRC
**** torch.optim.lr_scheduler.ReduceLROnPlateau
当网络的评价指标不在提升的时候，可以通过降低网络的学习率来提高网络性能。所使用的类
#+BEGIN_SRC python
class torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10,
 verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)
#+END_SRC
- optimer指的是网络的优化器
- mode (str) ，可选择‘min’或者‘max’，min表示当监控量停止下降的时候，学习率将减小，max表示当监控量停止上升的时候，学习率将减小。默认值为‘min’
- factor 学习率每次降低多少，new_lr = old_lr * factor
- patience=10，容忍网路的性能不提升的次数，高于这个次数就降低学习率
- verbose（bool） - 如果为True，则为每次更新向stdout输出一条消息。 默认值：False
- threshold（float） - 测量新最佳值的阈值，仅关注重大变化。 默认值：1e-4
- cooldown： 减少lr后恢复正常操作之前要等待的时期数。 默认值：0。
- min_lr,学习率的下限
- eps ，适用于lr的最小衰减。 如果新旧lr之间的差异小于eps，则忽略更新。 默认值：1e-8。

使用的时候需要选择网络的度量指标，使用如下类的step方法实现，例子如下：
#+BEGIN_SRC python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
scheduler = ReduceLROnPlateau(optimizer, 'min',factor=0.5, patience=4, verbose=True)
.....
scheduler.step(train_loss)
# scheduler.step(val_loss)
#+END_SRC
** view的用法
相当于numpy中reshape（）的功能，但是用法可能不太一样。把原先tensor中的数据按照行优先的顺序排成一个一维的数据（这里应该是因为要求地址是连续存储的），然后按照参数组合成其他维度的tensor。比如说是不管你原先的数据是[[[1,2,3],[4,5,6]]]还是[1,2,3,4,5,6]，因为它们排成一维向量都是6个元素，所以只要view后面的参数一致，得到的结果都是一样的。比如，
#+BEGIN_SRC python
a=torch.Tensor([[[1,2,3],[4,5,6]]])
b=torch.Tensor([1,2,3,4,5,6])

print(a.view(1,6))
print(b.view(1,6))
#+END_SRC
得到的结果都是 ~tensor([[1., 2., 3., 4., 5., 6.])~

再看一个例子：
#+BEGIN_SRC python
a=torch.Tensor([[[1,2,3],[4,5,6]]])
print(a.view(3,2))
#+END_SRC
将会得到：
#+BEGIN_SRC python
tensor([[1., 2.],
        [3., 4.],
        [5., 6.]])
#+END_SRC
相当于就是从1，2，3，4，5，6顺序的拿数组来填充需要的形状。但是如果您想得到如下的结果：
#+BEGIN_SRC python
tensor([[1., 4.],
        [2., 5.],
        [3., 6.]])
#+END_SRC
就需要使用另一个函数了：permute（）
** permute的用法
~permute(dims)~  将tensor的维度换位。

参数：参数是一系列的整数，代表原来张量的维度。比如三维就有0，1，2这些dimension。

** 保存加载模型和参数
*** 保存加载模型基本用法
**** 保存加载整个模型
保存整个网络模型（网络结构+权重参数）。
#+BEGIN_SRC python
torch.save(model, 'net.pkl')
#+END_SRC
直接加载整个网络模型（可能比较耗时）。
#+BEGIN_SRC python
model = torch.load('net.pkl')
#+END_SRC
**** 只保存加载模型参数
只保存模型的权重参数（速度快，占内存少）。
#+BEGIN_SRC python
torch.save(model.state_dict(), 'net_params.pkl')
#+END_SRC
因为我们只保存了模型的参数，所以需要先定义一个网络对象，然后再加载模型参数。
#+BEGIN_SRC python
# 构建一个网络结构
model = ClassNet()
# 将模型参数加载到新模型中
state_dict = torch.load('net_params.pkl')
model.load_state_dict(state_dict)
#+END_SRC
*** 保存加载自定义模型
上面保存加载的 net.pkl 其实一个字典，通常包含如下内容：
1. 网络结构：输入尺寸、输出尺寸以及隐藏层信息，以便能够在加载时重建模型。
2. 模型的权重参数：包含各网络层训练后的可学习参数，可以在模型实例上调用 state_dict() 方法来获取，比如前面介绍只保存模型权重参数时用到的 model.state_dict()。
3. 优化器参数：有时保存模型的参数需要稍后接着训练，那么就必须保存优化器的状态和所其使用的超参数，也是在优化器实例上调用 state_dict() 方法来获取这些参数。
4. 其他信息：有时我们需要保存一些其他的信息，比如 epoch，batch_size 等超参数。

知道了这些，那么我们就可以自定义需要保存的内容，比如：
#+BEGIN_SRC python
# saving a checkpoint assuming the network class named ClassNet
checkpoint = {'model': ClassNet(),
              'model_state_dict': model.state_dict(),
              'optimizer_state_dict': optimizer.state_dict(),
              'epoch': epoch}
​
torch.save(checkpoint, 'checkpoint.pkl')
#+END_SRC
上面的 checkpoint 是个字典，里面有4个键值对，分别表示网络模型的不同信息。

然后我们要加载上面保存的自定义的模型：
#+BEGIN_SRC python
def load_checkpoint(filepath):
    checkpoint = torch.load(filepath)
    model = checkpoint['model']  # 提取网络结构
    model.load_state_dict(checkpoint['model_state_dict'])  # 加载网络权重参数
    optimizer = TheOptimizerClass()
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  # 加载优化器参数
    
    for parameter in model.parameters():
        parameter.requires_grad = False
    model.eval()
    
    return model
    
model = load_checkpoint('checkpoint.pkl')
#+END_SRC
state_dict() 也是一个Python字典对象，model.state_dict() 将每一层的可学习参数映射为参数矩阵，其中只包含具有可学习参数的层(卷积层、全连接层等)。

比如下面这个例子：
#+BEGIN_SRC python
# Define model
class TheModelClass(nn.Module):
    def __init__(self):
        super(TheModelClass, self).__init__()
        self.conv1 = nn.Conv2d(3, 8, 5)
        self.bn = nn.BatchNorm2d(8)
        self.conv2 = nn.Conv2d(8, 16, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 10)
​
    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.bn(x)
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
    
    # Initialize model
    model = TheModelClass()
​
    # Initialize optimizer
    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
​
    print("Model's state_dict:")
    for param_tensor in model.state_dict():
        print(param_tensor, "\t", model.state_dict()[param_tensor].size())
​
    print("Optimizer's state_dict:")
    for var_name in optimizer.state_dict():
        print(var_name, "\t", optimizer.state_dict()[var_name])
#+END_SRC
输出为：
#+BEGIN_SRC python
Model's state_dict:
conv1.weight            torch.Size([8, 3, 5, 5])
conv1.bias              torch.Size([8])
bn.weight               torch.Size([8])
bn.bias                 torch.Size([8])
bn.running_mean         torch.Size([8])
bn.running_var          torch.Size([8])
bn.num_batches_tracked  torch.Size([])
conv2.weight            torch.Size([16, 8, 5, 5])
conv2.bias              torch.Size([16])
fc1.weight              torch.Size([120, 400])
fc1.bias                torch.Size([120])
fc2.weight              torch.Size([10, 120])
fc2.bias                torch.Size([10])
Optimizer's state_dict:
state            {}
param_groups     [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [139805696932024, 139805483616008, 139805483616080, 139805483616152, 139805483616440, 139805483616512, 139805483616584, 139805483616656, 139805483616728, 139805483616800]}]
#+END_SRC
可以看到 model.state_dict() 保存了卷积层，BatchNorm层和最大池化层的信息；而 optimizer.state_dict() 则保存的优化器的状态和相关的超参数。
*** 跨设备保存加载模型
**** 在 CPU 上加载在 GPU 上训练并保存的模型（Save on GPU, Load on CPU）：
#+BEGIN_SRC python
device = torch.device('cpu')
model = TheModelClass()
# Load all tensors onto the CPU device
model.load_state_dict(torch.load('net_params.pkl', map_location=device))
#+END_SRC
map_location：a function, torch.device, string or a dict specifying how to remap storage locations

令 torch.load() 函数的 map_location 参数等于 torch.device('cpu') 即可。 这里令 map_location 参数等于 'cpu' 也同样可以。
**** 在 GPU 上加载在 GPU 上训练并保存的模型（Save on GPU, Load on GPU）：
#+BEGIN_SRC python
device = torch.device("cuda")
model = TheModelClass()
model.load_state_dict(torch.load('net_params.pkl'))
model.to(device)
#+END_SRC
在这里使用 map_location 参数不起作用，要使用 model.to(torch.device("cuda")) 将模型转换为CUDA优化的模型。

还需要对将要输入模型的数据调用 data = data.to(device)，即将数据从CPU转移到GPU。请注意，调用 my_tensor.to(device) 会返回一个 my_tensor 在 GPU 上的副本，它不会覆盖 my_tensor。因此需要手动覆盖张量：my_tensor = my_tensor.to(device)。
**** 在 GPU 上加载在 GPU 上训练并保存的模型（Save on CPU, Load on GPU）
#+BEGIN_SRC python
device = torch.device("cuda")
model = TheModelClass()
model.load_state_dict(torch.load('net_params.pkl', map_location="cuda:0"))
model.to(device)
#+END_SRC
当加载包含GPU tensors的模型时，这些tensors 会被默认加载到GPU上，不过是同一个GPU设备。

当有多个GPU设备时，可以通过将 map_location 设定为 cuda:device_id 来指定使用哪一个GPU设备，上面例子是指定编号为0的GPU设备。

其实也可以将 torch.device("cuda") 改为 torch.device("cuda:0") 来指定编号为0的GPU设备。

最后调用 model.to(torch.device('cuda')) 来将模型的tensors转换为 CUDA tensors。

*** pytorch预训练模型
1）加载预训练模型和参数
#+BEGIN_SRC python
resnet18 = models.resnet18(pretrained=True)
#+END_SRC
这里是直接调用pytorch中的常用模型
#+BEGIN_SRC python
# PyTorch中的torchvision里有很多常用的模型，可以直接调用：
import torchvision.models as models
 
resnet101 = models.resnet18()
alexnet = models.alexnet()
squeezenet = models.squeezenet1_0()
densenet = models.densenet_161()
#+END_SRC
2）只加载模型，不加载预训练参数
#+BEGIN_SRC python
# 导入模型结构
resnet18 = models.resnet18(pretrained=False)
# 加载预先下载好的预训练参数到resnet18
resnet18.load_state_dict(torch.load('resnet18-5c106cde.pth'))
#+END_SRC
3）加载部分预训练模型
#+BEGIN_SRC python
resnet152 = models.resnet152(pretrained=True)
pretrained_dict = resnet152.state_dict()
"""加载torchvision中的预训练模型和参数后通过state_dict()方法提取参数
   也可以直接从官方model_zoo下载：
   pretrained_dict = model_zoo.load_url(model_urls['resnet152'])"""
model_dict = model.state_dict()
# 将pretrained_dict里不属于model_dict的键剔除掉
pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}
# 更新现有的model_dict
model_dict.update(pretrained_dict)
# 加载我们真正需要的state_dict
model.load_state_dict(model_dict)
#+END_SRC
** torch.load
#+BEGIN_SRC python
torch.load(f, map_location=None, pickle_module=<module 'pickle' from '/scratch/rzou/pt/v1.6-env/lib/python3.8/pickle.py'>, **pickle_load_args)
#+END_SRC
Loads an object saved with torch.save() from a file.

torch.load() uses Python’s unpickling facilities but treats storages, which underlie tensors, specially. They are first deserialized on the CPU and are then moved to the device they were saved from. If this fails (e.g. because the run time system doesn’t have certain devices), an exception is raised. However, storages can be dynamically remapped to an alternative set of devices using the map_location argument.

If map_location is a callable, it will be called once for each serialized storage with two arguments: storage and location. The storage argument will be the initial deserialization of the storage, residing on the CPU. Each serialized storage has a location tag associated with it which identifies the device it was saved from, and this tag is the second argument passed to map_location. The builtin location tags are 'cpu' for CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors. map_location should return either None or a storage. If map_location returns a storage, it will be used as the final deserialized object, already moved to the right device. Otherwise, torch.load() will fall back to the default behavior, as if map_location wasn’t specified.

If map_location is a torch.device object or a string containing a device tag, it indicates the location where all tensors should be loaded.

Otherwise, if map_location is a dict, it will be used to remap location tags appearing in the file (keys), to ones that specify where to put the storages (values).

User extensions can register their own location tags and tagging and deserialization methods using torch.serialization.register_package().
*** 参数
- f – a file-like object (has to implement read(), :meth`readline`, :meth`tell`, and :meth`seek`), or a string or os.PathLike object containing a file name
- map_location – a function, torch.device, string or a dict specifying how to remap storage locations
- pickle_module – module used for unpickling metadata and objects (has to match the pickle_module used to serialize file)
- pickle_load_args – (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=....

#+BEGIN_EXAMPLE
When you call torch.load() on a file which contains GPU tensors, those tensors will be loaded to GPU by default. You can call torch.load(.., map_location='cpu') and then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint.
#+END_EXAMPLE
*** 官方例子
#+BEGIN_SRC python
>>> torch.load('tensors.pt')
# Load all tensors onto the CPU
>>> torch.load('tensors.pt', map_location=torch.device('cpu'))
# Load all tensors onto the CPU, using a function
>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage)
# Load all tensors onto GPU 1
>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))
# Map tensors from GPU 1 to GPU 0
>>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})
# Load tensor from io.BytesIO object
>>> with open('tensor.pt', 'rb') as f:
        buffer = io.BytesIO(f.read())
>>> torch.load(buffer)
# Load a module with 'ascii' encoding for unpickling
>>> torch.load('module.pt', encoding='ascii')
#+END_SRC

** torch.nn
*** torch.nn.L1Loss()
#+BEGIN_SRC python
class torch.nn.L1Loss(size_average=None, reduce=None) 
#+END_SRC
官方文档中仍有reduction='elementwise_mean'参数，但代码实现中已经删除该参数

功能： 计算output和target之差的绝对值，可选返回同维度的tensor或者是一个标量。

计算公式：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-08-14_09-18-29.png @ 2020-08-14 09:18:48
[[file:torch/2020-08-14_09-18-48_Snipaste_2020-08-14_09-18-29.png]]

参数： reduce(bool)- 返回值是否为标量，默认为True

size_average(bool)- 当reduce=True时有效。为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。
*** torch.nn.MSELoss()
#+BEGIN_SRC python
class torch.nn.MSELoss(size_average=None, reduce=None, reduction='elementwise_mean')
#+END_SRC
官方文档中仍有reduction='elementwise_mean'参数，但代码实现中已经删除该参数

功能： 计算output和target之差的平方，可选返回同维度的tensor或者是一个标量。

计算公式：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-08-14_09-19-52.png @ 2020-08-14 09:19:57
[[file:torch/2020-08-14_09-19-57_Snipaste_2020-08-14_09-19-52.png]]
参数： reduce(bool)- 返回值是否为标量，默认为True

size_average(bool)- 当reduce=True时有效。为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。
*** torch.nn.CrossEntropyLoss()
#+BEGIN_SRC python
class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='elementwise_mean')
#+END_SRC
功能： 将输入经过softmax激活函数之后，再计算其与target的交叉熵损失。即该方法将nn.LogSoftmax()和 nn.NLLLoss()进行了结合。严格意义上的交叉熵损失函数应该是nn.NLLLoss()。

计算公式：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-08-14_09-20-56.png @ 2020-08-14 09:21:00
[[file:torch/2020-08-14_09-21-00_Snipaste_2020-08-14_09-20-56.png]]
参数： weight(Tensor)- 为每个类别的loss设置权值，常用于类别不均衡问题。weight必须是float类型的tensor，其长度要于类别C一致，即每一个类别都要设置有weight。带weight的计算公式：

size_average(bool)- 当reduce=True时有效。为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。 reduce(bool)- 返回值是否为标量，默认为True ignore_index(int)- 忽略某一类别，不计算其loss，其loss会为0，并且，在采用size_average时，不会计算那一类的loss，除的时候的分母也不会统计那一类的样本。

补充： output不仅可以是向量，还可以是图片，即对图像进行像素点的分类
*** torch.nn.NLLLoss()
#+BEGIN_SRC python
class torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='elementwise_mean')
#+END_SRC
功能： 不好用言语描述其功能！请看计算公式：loss(input, class) = -input[class]。举个例，三分类任务，input=[-1.233, 2.657, 0.534]， 真实标签为2（class=2），则loss为-0.534。就是对应类别上的输出，取一个负号！感觉被NLLLoss的名字欺骗了。 实际应用： 常用于多分类任务，但是input在输入NLLLoss()之前，需要对input进行log_softmax函数激活，即将input转换成概率分布的形式，并且取对数。其实这些步骤在CrossEntropyLoss中就有，如果不想让网络的最后一层是log_softmax层的话，就可以采用CrossEntropyLoss完全代替此函数。

参数： weight(Tensor)- 为每个类别的loss设置权值，常用于类别不均衡问题。weight必须是float类型的tensor，其长度要于类别C一致，即每一个类别都要设置有weight。 size_average(bool)- 当reduce=True时有效。为True时，返回的loss为除以权重之和的平均值；为False时，返回的各样本的loss之和。

reduce(bool)- 返回值是否为标量，默认为True。

ignore_index(int)- 忽略某一类别，不计算其loss，其loss会为0，并且，在采用size_average时，不会计算那一类的loss，除的时候的分母也不会统计那一类的样本。

特别注意： 当带上权值，reduce = True, size_average = True, 其计算公式为：

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-08-14_09-22-39.png @ 2020-08-14 09:22:45
[[file:torch/2020-08-14_09-22-45_Snipaste_2020-08-14_09-22-39.png]]
例如当input为[[0.6, 0.2, 0.2], [0.4, 1.2, 0.4]]，target= [0, 1], weight = [0.6, 0.2, 0.2] l1 = - 0.60.6 = - 0.36 l2 = - 1.20.2 = - 0.24 loss = -0.36/(0.6+0.2) + -0.24/(0.6+0.2) = -0.75
*** torch.nn.PoissonNLLLoss()
#+BEGIN_SRC python
class torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='elementwise_mean')
#+END_SRC
功能： 用于target服从泊松分布的分类任务。

计算公式：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-08-14_09-24-03.png @ 2020-08-14 09:24:07
[[file:torch/2020-08-14_09-24-07_Snipaste_2020-08-14_09-24-03.png]]
参数：

log_input(bool)- 为True时，计算公式为：loss(input,target)=exp(input) - target * input; 为False时，loss(input,target)=input - target * log(input+eps)

full(bool)- 是否计算全部的loss。

例如，当采用斯特林公式近似阶乘项时，此为 target*log(target) - target+0.5∗log(2πtarget) eps(float)- 当log_input = False时，用来防止计算log(0)，而增加的一个修正项。即 loss(input,target)=input - target * log(input+eps)

size_average(bool)- 当reduce=True时有效。为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。

reduce(bool)- 返回值是否为标量，默认为True
*** torch.nn.KLDivLoss()
#+BEGIN_SRC python
class torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='elementwise_mean')
#+END_SRC
功能： 计算input和target之间的KL散度( Kullback–Leibler divergence) 。

计算公式：

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-08-14_09-25-53.png @ 2020-08-14 09:25:58
[[file:torch/2020-08-14_09-25-58_Snipaste_2020-08-14_09-25-53.png]]
补充：KL散度 KL散度( Kullback–Leibler divergence) 又称为相对熵(Relative Entropy)，用于描述两个概率分布之间的差异。计算公式(离散时)：

其中p表示真实分布，q表示p的拟合分布， D(P||Q)表示当用概率分布q来拟合真实分布p时，产生的信息损耗。这里的信息损耗，可以理解为损失，损失越低，拟合分布q越接近真实分布p。同时也可以从另外一个角度上观察这个公式，即计算的是 p 与 q 之间的对数差在 p 上的期望值。 特别注意，D(p||q) ≠ D(q||p)， 其不具有对称性，因此不能称为K-L距离。

信息熵 = 交叉熵 - 相对熵 从信息论角度观察三者，其关系为信息熵 = 交叉熵 - 相对熵。在机器学习中，当训练数据固定，最小化相对熵 D(p||q) 等价于最小化交叉熵 H(p,q) 。

参数：

size_average(bool)- 当reduce=True时有效。为True时，返回的loss为平均值，平均值为element-wise的，而不是针对样本的平均；为False时，返回是各样本各维度的loss之和。 reduce(bool)- 返回值是否为标量，默认为True。

使用注意事项： 要想获得真正的KL散度，需要如下操作：

1. reduce = True ；size_average=False

2. 计算得到的loss 要对batch进行求平均

*** torch.nn.BCELoss()
#+BEGIN_SRC python
class torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='elementwise_mean')
#+END_SRC
功能： 二分类任务时的交叉熵计算函数。此函数可以认为是nn.CrossEntropyLoss函数的特例。其分类限定为二分类，y必须是{0,1}。还需要注意的是，input应该为概率分布的形式，这样才符合交叉熵的应用。所以在BCELoss之前，input一般为sigmoid激活层的输出，官方例子也是这样给的。该损失函数在自编码器中常用。 

计算公式：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-08-14_09-27-04.png @ 2020-08-14 09:27:08
[[file:torch/2020-08-14_09-27-08_Snipaste_2020-08-14_09-27-04.png]]
参数：

weight(Tensor)- 为每个类别的loss设置权值，常用于类别不均衡问题。

size_average(bool)- 当reduce=True时有效。为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。

reduce(bool)- 返回值是否为标量，默认为True
*** torch.nn.BCEWithLogitsLoss()
#+BEGIN_SRC python
class torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='elementwise_mean', pos_weight=None)
#+END_SRC
功能： 将Sigmoid与BCELoss结合，类似于CrossEntropyLoss(将nn.LogSoftmax()和 nn.NLLLoss()进行结合）。即input会经过Sigmoid激活函数，将input变成概率分布的形式。 计算公式：

σ() 表示Sigmoid函数 特别地，当设置weight时：

参数：

weight(Tensor)- : 为batch中单个样本设置权值，If given, has to be a Tensor of size “nbatch”.

pos_weight-: 正样本的权重, 当p>1，提高召回率，当P<1，提高精确度。可达到权衡召回率(Recall)和精确度(Precision)的作用。 Must be a vector with length equal to the number of classes.

size_average(bool)- 当reduce=True时有效。为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。

reduce(bool)- 返回值是否为标量，默认为True
**** 实例解释
BCELoss

在图片多标签分类时，如果3张图片分3类，会输出一个3*3的矩阵。

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20181223124137625.png @ 2020-08-13 09:40:44
[[file:torch/2020-08-13_09-40-44_20181223124137625.png]]
先用Sigmoid给这些值都搞到0~1之间：

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20181223124238621.png @ 2020-08-13 09:47:40
[[file:torch/2020-08-13_09-47-40_20181223124238621.png]]

假设Target是：

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20181223132343472.png @ 2020-08-13 09:49:00
[[file:torch/2020-08-13_09-49-00_20181223132343472.png]]

BCELoss是
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/1811847o8o9gi5oi1igg9gu.png @ 2020-08-13 09:43:21
[[file:torch/2020-08-13_09-43-21_1811847o8o9gi5oi1igg9gu.png]]
所以对于第一行：
#+BEGIN_EXAMPLE
第一列0×ln0.3992+(1−0)×ln(1−0.3992)=−0.50950×ln0.3992+(1−0)×ln(1−0.3992)=−0.5095
第二列1×ln0.2232+(1−1)×ln(1−0.2232)=−1.49971×ln0.2232+(1−1)×ln(1−0.2232)=−1.4997
第三列1×ln0.6435+(1−1)×ln(1−0.6435)=−0.44081×ln0.6435+(1−1)×ln(1−0.6435)=−0.4408
第二行：
第一列0×ln0.3800+(1−0)×ln(1−0.3800)=−0.47800×ln0.3800+(1−0)×ln(1−0.3800)=−0.4780
第二列0×ln0.3044+(1−0)×ln(1−0.3044)=−0.36300×ln0.3044+(1−0)×ln(1−0.3044)=−0.3630
第三列1×ln0.3241+(1−1)×ln(1−0.3241)=−1.12671×ln0.3241+(1−1)×ln(1−0.3241)=−1.1267
第三行：
第一列1×ln0.6281+(1−1)×ln(1−0.6281)=−0.46511×ln0.6281+(1−1)×ln(1−0.6281)=−0.4651
第二列0×ln0.4689+(1−0)×ln(1−0.4689)=−0.63280×ln0.4689+(1−0)×ln(1−0.4689)=−0.6328
第三列1×ln0.3834+(1−1)×ln(1−0.3834)=−0.95871×ln0.3834+(1−1)×ln(1−0.3834)=−0.9587
#+END_EXAMPLE
去掉负号求个均值：
#+BEGIN_EXAMPLE
(0.5095+1.4997+0.4408)/3=0.8167
(0.4780+0.3630+1.1267)/3=0.6559
(0.4651+0.6328+0.9587)/3=0.6855
#+END_EXAMPLE
再取个平均：
(0.8167+0.6559+0.6855)/=0.7194

下面我们用BCELoss来验证一下Loss是不是0.7194！

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20181223132408533.png @ 2020-08-13 09:49:14
[[file:torch/2020-08-13_09-49-14_20181223132408533.png]]

BCEWithLogitsLoss就是把Sigmoid-BCELoss合成一步。我们直接用刚刚的input验证一下是不是0.7193：

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20181223132922363.png @ 2020-08-13 09:49:28
[[file:torch/2020-08-13_09-49-28_20181223132922363.png]]
*** torch.nn.MarginRankingLoss()
#+BEGIN_SRC python
class torch.nn.MarginRankingLoss(margin=0, size_average=None, reduce=None, reduction='elementwise_mean')
#+END_SRC
功能： 计算两个向量之间的相似度，当两个向量之间的距离大于margin，则loss为正，小于margin，loss为0。

计算公式：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-08-14_09-28-07.png @ 2020-08-14 09:28:12
[[file:torch/2020-08-14_09-28-12_Snipaste_2020-08-14_09-28-07.png]]
y == 1时，x1要比x2大，才不会有loss，反之，y == -1 时，x1要比x2小，才不会有loss。

参数： margin(float)- x1和x2之间的差异。

size_average(bool)- 当reduce=True时有效。为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。

reduce(bool)- 返回值是否为标量，默认为True。
*** torch.nn.utils
**** torch.nn.utils.clip_grad_norm_()
#+BEGIN_SRC python
# 根据loss计算梯度
loss.backward()

## 按范数裁剪
### 这里norm_type可以选择L1范数，L2范数和无穷范数，分别对应`1, 2, 'inf'`
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm, norm_type=2)

## 按值裁剪
### 指定clip_value之后，裁剪的范围就是[-clip_value, clip_value]
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value)
#+END_SRC
*** torch.nn.functional
**** torch.nn.functional.pad
#+BEGIN_SRC python
torch.nn.functional.pad(input, pad, mode='constant', value=0)
#+END_SRC
F.pad是pytorch内置的tensor扩充函数，便于对数据集图像或中间层特征进行维度扩充，下面是pytorch官方给出的函数定义。

函数变量说明：
- input,需要扩充的tensor，可以是图像数据，抑或是特征矩阵数据
- pad,扩充维度，用于预先定义出某维度上的扩充参数
- mode,扩充方法，’constant‘, ‘reflect’ or ‘replicate’三种模式，分别表示常量，反射，复制
- value,扩充时指定补充值，但是value只在mode='constant’有效，即使用value填充在扩充出的新维度位置，而在’reflect’和’replicate’模式下，value不可赋值

***** 实例解释
为了方便从可视角度上分析F.pad的实际效果，首先给出空值矩阵，并且为了能够让宁能复现效果，实际代码全部给出，并最小化解释复杂度。
#+BEGIN_SRC python
import torch
import torch.nn.functional as F

t4d = torch.empty(1, 3, 5, 3)
#+END_SRC
其中t4d中维度分别表示(batchsize, channel, height, width)

为了便于理解，只观察输入矩阵t4d的最后两维，即一个5行3列的矩阵， 如图1
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-08-17_13-48-44.png @ 2020-08-17 13:48:57
[[file:torch/2020-08-17_13-48-57_Snipaste_2020-08-17_13-48-44.png]]
如果F.pad中第二个参数pad只定义两个参数，表示只对输入矩阵的最后一个维度进行扩充，不会对前两个维度造成任何影响，所以此处直接忽略前两个维度。
#+BEGIN_SRC python 
p1d = (1, 2)
t1 = F.pad(t4d, p1d, 'constant', 1)
#+END_SRC
先输出看一下t1的维度变化：
#+BEGIN_SRC python
>>> print('原始矩阵大小为：', t4d.shape)
'''
原始矩阵大小为：torch.Size([1, 3, 5, 3])
'''
>>> print('t1矩阵大小为：', t1.shape)
'''
t1矩阵大小为：torch.Size([1, 3, 5, 6])
'''
#+END_SRC
接下来，从可视化的角度分析一下，原始矩阵全为0值，扩充维度全部用1值填充，这样易于理解。

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-08-17_13-49-46.png @ 2020-08-17 13:49:54
[[file:torch/2020-08-17_13-49-54_Snipaste_2020-08-17_13-49-46.png]]
从图2可以明显看出，左侧扩充了1列，右侧扩充了2列，即原始矩阵大小从5×3扩充到5×6，则p1d的参数设置意义为
#+BEGIN_SRC python
p1d = (1, 2)
# p1d = (左边填充数, 右边填充数)
#+END_SRC
此外，在实际项目中，为了保持代码的可扩展性，按下面定义，也可以获取同样的效果。
#+BEGIN_SRC python
p1d_ = (1, 2, 0, 0)
t1 = F.pad(t4d, p1d_, 'constant', 1)
#+END_SRC

两维扩充
#+BEGIN_SRC python
# p1d = (1, 2)	# 与p1d做对比
p2d = (1, 2, 3, 4)
t2 = F.pad(t4d, p2d, 'constant', 2)
#+END_SRC
同样的，先分析下原始矩阵的维度变化情况：
#+BEGIN_SRC python
>>> print('原始矩阵大小为：', t4d.shape)
'''
原始矩阵大小为：torch.Size([1, 3, 5, 3])
'''
>>> print('t2矩阵大小为：', t2.shape)
'''
t2矩阵大小为：torch.Size([1, 3, 12, 6])
'''
#+END_SRC
这里给出的是两维的扩充代码，为了便于理解，看一下实际的扩充效果，如图3
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-08-17_13-51-23.png @ 2020-08-17 13:51:28
[[file:torch/2020-08-17_13-51-28_Snipaste_2020-08-17_13-51-23.png]]
看图实际一目了然，对左侧扩充了1列，右侧扩充了2列，上边扩充了3行，下边扩充了4行。也就是说，前两个参数对最后一个维度有效，后两个参数对倒数第二维有效。接下来就可以看一下p2d参数的实际意义：
#+BEGIN_SRC python
p2d = (1, 2, 3, 4)
# p2d = (左边填充数， 右边填充数， 上边填充数， 下边填充数)
#+END_SRC

三维扩充
#+BEGIN_SRC python
# p1d = (1, 2)			# 与p1d做对比
# p2d = (1, 2, 3, 4)	# 与p2d做对比
p3d = (1, 2, 3, 4, 5, 6)
t3 = F.pad(t4d, p3d, 'constant', 3)
#+END_SRC
仍然先分析下原始矩阵的维度变化情况：
#+BEGIN_SRC python
>>> print('原始矩阵大小为：', t4d.shape)
'''
原始矩阵大小为：torch.Size([1, 3, 5, 3])
'''
>>> print('t3矩阵大小为：', t3.shape)
'''
t3矩阵大小为：torch.Size([1, 14, 12, 6])
'''
#+END_SRC
从可视化角度分析，如图4所示。
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-08-17_13-52-30.png @ 2020-08-17 13:52:35
[[file:torch/2020-08-17_13-52-35_Snipaste_2020-08-17_13-52-30.png]]

根据p3d = (1, 2, 3, 4, 5, 6)中，前4个参数完成了在高和宽维度上的扩张，后两个参数则完成了对通道维度上的扩充。接下来就可以看一下p3d参数的实际意义：
#+BEGIN_SRC python
p3d = (1, 2, 3, 4, 5, 6)
# p3d = (左边填充数， 右边填充数， 上边填充数， 下边填充数， 前边填充数，后边填充数)
#+END_SRC

** 张量类型转换
*** 使用独立的函数实现张量类型之间的转换
为了方便测试，我们构建一个新的张量，你要转变成不同的类型只需要根据自己的需求选择即可
#+BEGIN_SRC python
tensor = torch.Tensor(3, 5)
#torch.long() 将tensor投射为long类型
newtensor = tensor.long()
#torch.half()将tensor投射为半精度浮点类型
newtensor = tensor.half()
#torch.int()将该tensor投射为int类型
newtensor = tensor.int()
#torch.double()将该tensor投射为double类型
newtensor = tensor.double()
#torch.float()将该tensor投射为float类型
newtensor = tensor.float()
#torch.char()将该tensor投射为char类型
newtensor = tensor.char()
#torch.byte()将该tensor投射为byte类型
newtensor = tensor.byte()
#torch.short()将该tensor投射为short类型
newtensor = tensor.short()
#+END_SRC
*** 使用torch.type()函数
~type(new_type=None, async=False)~ 如果未提供new_type，则返回类型，否则将此对象转换为指定的类型。 如果已经是正确的类型，则不会执行且返回原对象。

用法如下：
#+BEGIN_SRC python
self = torch.LongTensor(3, 5)
# 转换为其他类型
print self.type(torch.FloatTensor)
#+END_SRC
*** 使用type_as(tesnor)将张量转换为给定类型的张量
如果张量已经是正确的类型，则不会执行操作。具体操作方法如下：
#+BEGIN_SRC python
self = torch.Tensor(3, 5)
tesnor = torch.IntTensor(2,3)
print self.type_as(tesnor)
#+END_SRC
** torch.mul() 和 torch.mm() 的区别
- torch.mul(a, b)是矩阵a和b对应位相乘，a和b的维度必须相等，比如a的维度是(1, 2)，b的维度是(1, 2)，返回的仍是(1, 2)的矩阵
- torch.mm(a, b)是矩阵a和b矩阵相乘，比如a的维度是(1, 2)，b的维度是(2, 3)，返回的就是(1, 3)的矩阵

#+BEGIN_SRC python
import torch

a = torch.rand(1, 2)
b = torch.rand(1, 2)
c = torch.rand(2, 3)

print(torch.mul(a, b))  # 返回 1*2 的tensor
print(torch.mm(a, c))   # 返回 1*3 的tensor
print(torch.mul(a, c))  # 由于a、b维度不同，报错
#+END_SRC
** 关于with torch.no_grad()
*** 关于with
with是python中上下文管理器，简单理解，当要进行固定的进入，返回操作时，可以将对应需要的操作，放在with所需要的语句中。比如文件的写入（需要打开关闭文件）等。

以下为一个文件写入使用with的例子。
#+BEGIN_SRC python
with open (filename,'w') as sh:    
  sh.write("#!/bin/bash\n")
  sh.write("#$ -N "+'IC'+altas+str(patientNumber)+altas+'\n')
  sh.write("#$ -o "+pathSh+altas+'log.log\n') 
  sh.write("#$ -e "+pathSh+altas+'err.log\n') 
  sh.write('source ~/.bashrc\n')          
  sh.write('. "/home/kjsun/anaconda3/etc/profile.d/conda.sh"\n')
  sh.write('conda activate python27\n')
  sh.write('echo "to python"\n')
  sh.write('echo "finish"\n')
  sh.close()
#+END_SRC
with后部分，可以将with后的语句运行，将其返回结果给到as后的变量（sh），之后的代码块对close进行操作。
*** 关于with torch.no_grad():
在使用pytorch时，并不是所有的操作都需要进行计算图的生成（计算过程的构建，以便梯度反向传播等操作）。而对于tensor的计算操作，默认是要进行计算图的构建的，在这种情况下，可以使用 with torch.no_grad():，强制之后的内容不进行计算图构建。

** torch.gather
~torch.gather(input, dim, index, *, sparse_grad=False, out=None) → Tensor~

沿给定轴dim，将输入索引张量index指定位置的值进行聚合。

Gathers values along an axis specified by dim.

If input is an n-dimensional tensor with size (x_0, x_1..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1}) and dim = i, then index must be an n-dimensional tensor with size (x_0, x_1, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1}) where y≥1 and out will have the same size as index.

*** Parameters
- input (Tensor) – the source tensor
- dim (int) – the axis along which to index
- index (LongTensor) – the indices of elements to gather
- sparse_grad (bool,optional) – If True, gradient w.r.t. input will be a sparse tensor.
- out (Tensor, optional) – the destination tensor
*** 例子
#+BEGIN_SRC python
>>> t = torch.tensor([[1,2],[3,4]])
>>> torch.gather(t, 1, torch.tensor([[0,0],[1,0]]))
tensor([[ 1,  1],
        [ 4,  3]])

>>> a = torch.Tensor([[1,2],[3,4]])
>>> a
 1  2
 3  4

>>> b = torch.gather(a,1,torch.LongTensor([[0,0],[1,0]]))
>>> b
 1  1
 4  3

>>> b = torch.gather(a,1,torch.LongTensor([[1,0],[1,0]]))
>>> b
 2  1
 4  3

>>> b = torch.gather(a,1,torch.LongTensor([[1,1],[1,0]]))
>>> b
 2  2
 4  3

#+END_SRC

** .cuda()方法和torch.cuda.set_device()
我们还可以使用.cuda()[包括model.cuda()/loss.cuda()/tensor.cuda()]方法和torch.cuda.set_device()来把模型和数据加载到对应的gpu上。

*** .cuda()

以model.cuda()为例，加载方法为：
#+begin_src python
model.cuda(gpu_id) # gpu_id为int类型变量，只能指定一张显卡
model.cuda('cuda:'+str(gpu_ids)) #输入参数为str类型，可指定多张显卡
model.cuda('cuda:1,2') #指定多张显卡的一个示例
#+END_SRC

*** torch.cuda.set_device()

使用torch.cuda.set_device()可以更方便地将模型和数据加载到对应GPU上, 直接定义模型之前加入一行代码即可
#+begin_src python
torch.cuda.set_device(gpu_id) #单卡
torch.cuda.set_device('cuda:'+str(gpu_ids)) #可指定多卡
#+END_SRC

但是这种写法的优先级低，如果model.cuda()中指定了参数，那么torch.cuda.set_device()会失效，而且pytorch的官方文档中明确说明，不建议用户使用该方法。

第1节和第2节所说的方法同时使用是并不会冲突，而是会叠加。比如在运行代码时使用
#+begin_src python
CUDA_VISIBLE_DEVICES=2,3,4,5 python3 train.py
#+END_SRC

而在代码内部又指定
#+begin_src python
model.cuda(1)
loss.cuda(1)
tensor.cuda(1)
#+END_SRC

那么代码会在GPU3上运行。原理是CUDA_VISIBLE_DEVICES使得只有GPU2,3,4,5可见，那么这4张显卡，程序就会把它们看成GPU0,1,2,3，.cuda(1)把模型/loss/数据都加载到了程序所以为的GPU1上，则实际使用的显卡是GPU3。

如果利用.cuda()或torch.cuda.set_device()把模型加载到多个显卡上，而实际上只使用一张显卡运行程序的话，那么程序会把模型加载到第一个显卡上，比如如果在代码中指定了
#+begin_src python
model.cuda('cuda:2,1')
#+END_SRC
在运行代码时使用
#+begin_src python
CUDA_VISIBLE_DEVICES=2,3,4,5 python3 train.py
#+END_SRC

这一指令，那么程序最终会在GPU4上运行。
** Sampler
*** Sampler
首先需要知道的是所有的采样器都继承自Sampler这个类，如下：

可以看到主要有三种方法：分别是：
- __init__: 这个很好理解，就是初始化
- __iter__: 这个是用来产生迭代索引值的，也就是指定每个step需要读取哪些数据
- __len__: 这个是用来返回每次迭代器的长度
#+begin_src python
class Sampler(object):
    r"""Base class for all Samplers.
    Every Sampler subclass has to provide an __iter__ method, providing a way
    to iterate over indices of dataset elements, and a __len__ method that
    returns the length of the returned iterators.
    """
    # 一个 迭代器 基类
    def __init__(self, data_source):
        pass

    def __iter__(self):
        raise NotImplementedError

    def __len__(self):
        raise NotImplementedError
#+END_SRC
*** 子类Sampler
介绍完父类后我们看看Pytorch给我们提供了哪些采样器

**** SequentialSampler
这个看名字就很好理解，其实就是按顺序对数据集采样。

其原理是首先在初始化的时候拿到数据集data_source，之后在__iter__方法中首先得到一个和data_source一样长度的range可迭代器。每次只会返回一个索引值。
#+begin_src python
class SequentialSampler(Sampler):
    r"""Samples elements sequentially, always in the same order.
    Arguments:
        data_source (Dataset): dataset to sample from
    """
   # 产生顺序 迭代器
    def __init__(self, data_source):
        self.data_source = data_source

    def __iter__(self):
        return iter(range(len(self.data_source)))

    def __len__(self):
        return len(self.data_source)
#+END_SRC

使用示例：
#+begin_src python
a = [1,5,78,9,68]
b = torch.utils.data.SequentialSampler(a)
for x in b:
    print(x)
    
>>> 0
    1
    2
    3
    4
#+END_SRC

**** RandomSampler
参数作用：
- data_source: 同上
- num_samples: 指定采样的数量，默认是所有。
- replacement: 

若为True，则表示可以重复采样，即同一个样本可以重复采样，这样可能导致有的样本采样不到。所以此时我们可以设置num_samples来增加采样数量使得每个样本都可能被采样到。
#+begin_src python
class RandomSampler(Sampler):
    r"""Samples elements randomly. If without replacement, then sample from a shuffled dataset.
    If with replacement, then user can specify ``num_samples`` to draw.
    Arguments:
        data_source (Dataset): dataset to sample from
        num_samples (int): number of samples to draw, default=len(dataset)
        replacement (bool): samples are drawn with replacement if ``True``, default=False
    """

    def __init__(self, data_source, replacement=False, num_samples=None):
        self.data_source = data_source
        self.replacement = replacement
        self.num_samples = num_samples

        if self.num_samples is not None and replacement is False:
            raise ValueError("With replacement=False, num_samples should not be specified, "
                             "since a random permute will be performed.")

        if self.num_samples is None:
            self.num_samples = len(self.data_source)

        if not isinstance(self.num_samples, int) or self.num_samples <= 0:
            raise ValueError("num_samples should be a positive integeral "
                             "value, but got num_samples={}".format(self.num_samples))
        if not isinstance(self.replacement, bool):
            raise ValueError("replacement should be a boolean value, but got "
                             "replacement={}".format(self.replacement))

    def __iter__(self):
        n = len(self.data_source)
        if self.replacement:
            return iter(torch.randint(high=n, size=(self.num_samples,), dtype=torch.int64).tolist())
        return iter(torch.randperm(n).tolist())

    def __len__(self):
        return len(self.data_source)
#+END_SRC

**** SubsetRandomSampler
#+begin_src python
class SubsetRandomSampler(Sampler):
    r"""Samples elements randomly from a given list of indices, without replacement.
    Arguments:
        indices (sequence): a sequence of indices
    """

    def __init__(self, indices):
        self.indices = indices

    def __iter__(self):
        return (self.indices[i] for i in torch.randperm(len(self.indices)))

    def __len__(self):
        return len(self.indices)
#+END_SRC
这个采样器常见的使用场景是将训练集划分成训练集和验证集，示例如下：
#+begin_src python
n_train = len(train_dataset)
split = n_train // 3
indices = random.shuffle(list(range(n_train)))
train_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[split:])
valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[:split])
train_loader = DataLoader(..., sampler=train_sampler, ...)
valid_loader = DataLoader(..., sampler=valid_sampler, ...)
#+END_SRC

**** WeightedRandomSampler
参数作用同上面的RandomSampler，不再赘述。
#+begin_src python
class WeightedRandomSampler(Sampler):
    r"""Samples elements from [0,..,len(weights)-1] with given probabilities (weights).
    Arguments:
        weights (sequence)   : a sequence of weights, not necessary summing up to one
        num_samples (int): number of samples to draw
        replacement (bool): if ``True``, samples are drawn with replacement.
            If not, they are drawn without replacement, which means that when a
            sample index is drawn for a row, it cannot be drawn again for that row.
    """

    def __init__(self, weights, num_samples, replacement=True):
        if not isinstance(num_samples, _int_classes) or isinstance(num_samples, bool) or \
                num_samples <= 0:
            raise ValueError("num_samples should be a positive integeral "
                             "value, but got num_samples={}".format(num_samples))
        if not isinstance(replacement, bool):
            raise ValueError("replacement should be a boolean value, but got "
                             "replacement={}".format(replacement))
        self.weights = torch.tensor(weights, dtype=torch.double)
        self.num_samples = num_samples
        self.replacement = replacement

    def __iter__(self):
        return iter(torch.multinomial(self.weights, self.num_samples, self.replacement).tolist())

    def __len__(self):
        return self.num_samples  ## 指的是一次一共采样的样本的数量
#+END_SRC

**** BatchSampler
前面的采样器每次都只返回一个索引，但是我们在训练时是对批量的数据进行训练，而这个工作就需要BatchSampler来做。也就是说BatchSampler的作用就是将前面的Sampler采样得到的索引值进行合并，当数量等于一个batch大小后就将这一批的索引值返回。
#+begin_src python
class BatchSampler(Sampler):
    r"""Wraps another sampler to yield a mini-batch of indices.
    Args:
        sampler (Sampler): Base sampler.
        batch_size (int): Size of mini-batch.
        drop_last (bool): If ``True``, the sampler will drop the last batch if
            its size would be less than ``batch_size``
    Example:
        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))
        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]
        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))
        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]
    """
# 批次采样
    def __init__(self, sampler, batch_size, drop_last):
        if not isinstance(sampler, Sampler):
            raise ValueError("sampler should be an instance of "
                             "torch.utils.data.Sampler, but got sampler={}"
                             .format(sampler))
        if not isinstance(batch_size, _int_classes) or isinstance(batch_size, bool) or \
                batch_size <= 0:
            raise ValueError("batch_size should be a positive integeral value, "
                             "but got batch_size={}".format(batch_size))
        if not isinstance(drop_last, bool):
            raise ValueError("drop_last should be a boolean value, but got "
                             "drop_last={}".format(drop_last))
        self.sampler = sampler
        self.batch_size = batch_size
        self.drop_last = drop_last

    def __iter__(self):
        batch = []
        for idx in self.sampler:
            batch.append(idx)
            if len(batch) == self.batch_size:
                yield batch
                batch = []
        if len(batch) > 0 and not self.drop_last:
            yield batch

    def __len__(self):
        if self.drop_last:
            return len(self.sampler) // self.batch_size
        else:
            return (len(self.sampler) + self.batch_size - 1) // self.batch_size
#+END_SRC

** PyTorch中的contiguous
Tensor多维数组底层实现是使用一块连续内存的1维数组（行优先顺序存储，下文描述），Tensor在元信息里保存了多维数组的形状，在访问元素时，通过多维度索引转化成1维数组相对于数组起始位置的偏移量即可找到对应的数据。某些Tensor操作（如transpose、permute、narrow、expand）与原Tensor是共享内存中的数据，不会改变底层数组的存储，但原来在语义上相邻、内存里也相邻的元素在执行这样的操作后，在语义上相邻，但在内存不相邻，即不连续了（is not contiguous）。

如果想要变得连续使用contiguous方法，如果Tensor不是连续的，则会重新开辟一块内存空间保证数据是在内存中是连续的，如果Tensor是连续的，则contiguous无操作。

transpose、permute 操作虽然没有修改底层一维数组，但是新建了一份Tensor元信息，并在新的元信息中的 重新指定 stride。torch.view 方法约定了不修改数组本身，只是使用新的形状查看数据。如果我们在 transpose、permute 操作后执行 view，Pytorch 会抛出以下错误：
#+BEGIN_EXAMPLE
invalid argument 2: view size is not compatible with input tensor's size and stride (at least one dimension 
spans across two contiguous subspaces). Call .contiguous() before .view(). 
at /Users/soumith/b101_2/2019_02_08/wheel_build_dirs/wheel_3.6/pytorch/aten/src/TH/generic/THTens
#+END_EXAMPLE

[[https://zhuanlan.zhihu.com/p/64551412][参考文章]]
* torchvision
torchvision包是服务于pytorch深度学习框架的,用来生成图片,视频数据集,和一些流行的模型类和预训练模型.
[[https://pytorch-cn.readthedocs.io/zh/latest/torchvision/torchvision/][中文文档]]、[[https://pytorch.org/docs/stable/torchvision/index.html][英文文档]]

torchvision由以下四个部分组成:
1. torchvision.datasets : Data loaders for popular vision datasets
2. torchvision.models : Definitions for popular model architectures, such as AlexNet, VGG, and ResNet and pre-trained models.
3. torchvision.transforms : Common image transformations such as random crop, rotations etc.
4. torchvision.utils : Useful stuff such as saving tensor (3 x H x W) as image to disk, given a mini-batch creating a grid of images, etc.
** torchvision.datasets
~torchvision.datasets~ 中包含了以下数据集

- MNIST
- Fashion-MNIST
- KMNIST
- EMNIST
- QMNIST
- FakeData
- COCO
  - Captions
  - Detection
- LSUN
- ImageFolder
- DatasetFolder
- ImageNet
- CIFAR
- STL10
- SVHN
- PhotoTour
- SBU
- Flickr
- VOC
- Cityscapes
- SBD
- USPS
- Kinetics-400
- HMDB51
- UCF101
- CelebA

由于以上Datasets都是  ~torch.utils.data.Dataset~ 的子类，所以，他们也可以通过 ~torch.utils.data.DataLoader~ 使用多线程（python的多进程）。例如：
#+BEGIN_SRC python
imagenet_data = torchvision.datasets.ImageNet('path/to/imagenet_root/')
data_loader = torch.utils.data.DataLoader(imagenet_data,
                                          batch_size=4,
                                          shuffle=True,
                                          num_workers=args.nThreads)
#+END_SRC

所有datasets都有相似的API。它们都有两个相同的参数： ~transform~ 和 ~target_transform~

- transform： 一个函数，原始图片作为输入，返回一个转换后的图片。
- target_transform - 一个函数，输入为target，输出对其的转换
*** MNIST
#+BEGIN_SRC python
torchvision.datasets.MNIST(root, train=True, transform=None, target_transform=None, download=False)
#+END_SRC
- root (string) – Root directory of dataset where MNIST/processed/training.pt and MNIST/processed/test.pt exist.
- train (bool, optional) – If True, creates dataset from training.pt, otherwise from test.pt.
- download (bool, optional) – If true, downloads the dataset from the internet and puts it in root directory. If - dataset is already downloaded, it is not downloaded again.
- transform (callable, optional) – A function/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop
- target_transform (callable, optional) – A function/transform that takes in the target and transforms it.
** torchvision.transforms
*** torchvision.transforms.Compose(transforms)
~torchvision.transforms.Compose(transforms)~
Composes several transforms together. This transform does not support torchscript.

Compose这个类是用来管理各个transform的，可以看到主要的__call__方法就是对输入图像img循环所有的transform操作
#+BEGIN_SRC python
class Compose(object):
    """Composes several transforms together.

    Args:
        transforms (list of ``Transform`` objects): list of transforms to compose.

    Example:
        >>> transforms.Compose([
        >>>     transforms.CenterCrop(10),
        >>>     transforms.ToTensor(),
        >>> ])
    """

    def __init__(self, transforms):
        self.transforms = transforms

    def __call__(self, img):
        for t in self.transforms:
            img = t(img)
        return img

    def __repr__(self):
        format_string = self.__class__.__name__ + '('
        for t in self.transforms:
            format_string += '\n'
            format_string += '    {0}'.format(t)
        format_string += '\n)'
        return format_string
#+END_SRC
**** Parameters
transforms (list of Transform objects) – list of transforms to compose.

**** Example
#+BEGIN_SRC python
>>> transforms.Compose([
>>>     transforms.CenterCrop(10),
>>>     transforms.ToTensor(),
>>> ])
#+END_SRC

*** torchvision.transforms.Normalize
~torchvision.transforms.Normalize(mean, std, inplace=False)~
Normalize a tensor image with mean and standard deviation. Given mean: (mean[1],...,mean[n]) and std: (std[1],..,std[n]) for n channels, this transform will normalize each channel of the input torch.*Tensor i.e., output[channel] = (input[channel] - mean[channel]) / std[channel]

Normalize类是做数据归一化的，一般都会对输入数据做这样的操作
**** Parameters
- mean (sequence) – Sequence of means for each channel.
- std (sequence) – Sequence of standard deviations for each channel.
- inplace (bool,optional) – Bool to make this operation in-place.

** torchvision.utils
~torch.utils.data.SubsetRandomSampler(indices: Sequence[int], generator=None)~
Samples elements randomly from a given list of indices, without replacement.
*** Parameters
- indices (sequence) – a sequence of indices
- generator (Generator) – Generator used in sampling.

* os库
=os= 是python标准库. =os= 顾名思义，就是与操作系统相关的标准库。如：文件，目录，执行系统命令等。

** path子模块
涉及与磁盘文件操作，最常使用的当属 path 模块了。path 是 os 的子模块，可以通过 from os import path 使用，也可以直接通过 os.path 属性的方式使用。本文，为了保持一致性，统一采用后者的书写形式。

*** exists(path)
检测文件或目录是否存在。存在返回 True , 不存在返回 False 。
#+BEGIN_SRC python
os.path.exists("dog.jpeg")
True
#+END_SRC

*** isfile(path)
判断是否为文件。是返回 True， 不是返回 False。也可以用来判断文件是否存在。
#+BEGIN_SRC python
os.path.isfile("dogs/")
False
#+END_SRC

*** isdir(path)
判断是否为目录。是返回 True， 不是返回 False。也可以用来判断目录是否存在。
#+BEGIN_SRC python
os.path.isdir("dogs/")
True
#+END_SRC

*** basename(path)
返回不包含所在目录的文件名（含扩展）。
#+BEGIN_SRC python
os.path.basename("dir1/dir2/file.ext")
'file.ext'
#+END_SRC

*** dirname(path)
返回文件所在目录。
#+BEGIN_SRC python
os.path.dirname("dir1/dir2/file.ext")
'dir1/dir2'
#+END_SRC

*** split(path)
返回一个元组。元组第一个元素为文件所在目录，第二个元素为文件名（含扩展）。等效于 (dirname(path), basename(path))。
#+BEGIN_SRC python
os.path.split("dir1/dir2/file.ext")
('dir1/dir2', 'file.ext')
#+END_SRC

*** splitext(path)
返回一个元组。元组第一个元素为文件所在目录和文件名（不含扩展），第二个元素为扩展名（包含 .）。常用来读取或更改文件扩展名。
#+BEGIN_SRC python
os.path.splitext("dir1/dir2/file.ext")
('dir1/dir2/file', '.ext')
#+END_SRC

*** join(path, *paths)
将路径不同部分拼接成一个完整的路径。等效于 os.sep.join([path, *paths]) 。
#+BEGIN_SRC python
os.path.join("dir1", "dir2", "file.ext")
'dir1/dir2/file.ext'
#+END_SRC

*** getsize(path)
返回文件大小。单位字节。
#+BEGIN_SRC python
os.path.getsize("dog.jpeg")
18335
#+END_SRC

*** getcwd()

#+begin_src python
print os.getcwd() #获取当前工作目录路径
#+END_SRC
*** abspath

#+begin_src python
print os.path.abspath('.') #获取当前工作目录路径
print os.path.abspath('test.txt') #获取当前目录文件下的工作目录路径
print os.path.abspath('..') #获取当前工作的父目录 ！注意是父目录路径
print os.path.abspath(os.curdir) #获取当前工作目录路径
#+END_SRC
** 目录操作
*** listdir(path='.')
返回一个列表。列表为给定目录下所有文件和子目录.列表是任意顺序的。它不包括特殊条目’.‘ 和’..‘，即使它们存在于目录中。
默认为当前目录。
#+BEGIN_SRC python
# Open a file
path = "d:\\tmp\\"
dirs = os.listdir( path )
 
# This would print all the files and directories
for file in dirs:
   print (file)

#+END_SRC
执行上面代码后，将得到以下结果 ，注意：文件是带有后缀的，而文件夹没有后缀名
#+BEGIN_SRC python
Applicationdocs.docx
test.java
book.zip
foo.txt
Java Multiple Inheritance.html
Java Multiple Inheritance_files
java.ppt
ParallelPortViewer
#+END_SRC

*** mkdir(path, mode=0o777)
创建名为 path 的目录。并以数字形式指定目录权限，默认权限为 777 。
#+BEGIN_SRC python
os.mkdir("newdir")
#+END_SRC
*** makedirs(name, mode=0o777, exist_ok=False)
递归方式创建路径为 path 的目录。并以数字形式指定目录权限，默认权限为 777 。可以看作功能更强大的 mkdir，它会自动创建叶子节点目录的所有上级目录，而 mkdir 必须在上级目录已经存在情况下，才能创建叶子节点的目录。

=, exist_ok=True= 是py3.2之后才有的写法,如果 exist_ok 为 False (默认值)，则如果目标目录已存在将引发 FileExistsError。

#+BEGIN_SRC python 
os.makedirs("parent/child/newdir")
#+END_SRC
*** rmdir(path)
删除目录。目录必须存在，并且只能删除空目录。不存在或不为空，都会异常。要想递归删除整个目录树，请使用 shutil.rmtree()。
#+BEGIN_SRC python
os.rmdir("newdir")
#+END_SRC
*** removedirs(path)
递归删除目录。目录必须存在，并且只能删除空目录。不存在或不为空，都会异常。与 rmdir 不同的是，在删除了叶子节点目录后，会逐次删除上级目录，直到遇到不为空的目录。
#+BEGIN_SRC python 
os.removedirs("parent/child/newdir")
#+END_SRC
*** remove(path)
删除文件。不能删除目录，给定路径必须为文件，否则会异常。

Warm Suggestion: 以下复制文件的操作，推荐使用 shutil.copyfile。
#+BEGIN_SRC python
# 复制文件
with open("dog.jpeg", "rb") as f:
    content = f.read()
    with open("dog.copy.jpeg", "wb") as f2:
        f2.write(content)

# 删除文件
os.remove("dog.copy.jpeg")
#+END_SRC
* sys库
** sys.path.append()

当我们导入一个模块时：import  xxx，默认情况下python解析器会搜索当前目录、已安装的内置模块和第三方模块，搜索路径存放在sys模块的path中：
#+begin_src python
>>> import sys
>>> sys.path
['', 'C:\\Python352\\Lib\\idlelib', 'C:\\Python352\\python35.zip', 'C:\\Python352\\DLLs', 'C:\\Python352\\lib', 'C:\\Python352', 'C:\\Python352\\lib\\site-packages', 'C:\\Python352\\lib\\site-packages\\setuptools-28.6.1-py3.5.egg', 'C:\\Python352\\lib\\site-packages\\pip-8.1.2-py3.5.egg', 'C:\\Python352\\lib\\site-packages\\requests-2.11.1-py3.5.egg', 'C:\\Python352\\lib\\site-packages\\xlutils-2.0.0-py3.5.egg', 'C:\\Python352\\lib\\site-packages\\xlwt-1.1.2-py3.5.egg', 'C:\\Python352\\lib\\site-packages\\pymongo-3.3.1-py3.5-win-amd64.egg', 'C:\\Python352\\lib\\site-packages\\pytz-2016.7-py3.5.egg', 'C:\\Python352\\lib\\site-packages\\zope.interface-4.3.3-py3.5-win-amd64.egg']
#+END_SRC

sys.path 返回的是一个列表！

该路径已经添加到系统的环境变量了，当我们要添加自己的搜索目录时，可以通过列表的append()方法；

对于模块和自己写的脚本不在同一个目录下，在脚本开头加sys.path.append('xxx')：
#+begin_src python
import sys
sys.path.append(’引用模块的地址')
#+END_SRC

这种方法是运行时修改，脚本运行后就会失效的。
** sys.stdout.encoding
查看系统环境编码
#+begin_src python
>>> import sys
>>> sys.stdout.encoding
'US-ASCII'
#+END_SRC

* time库
time库是Python中处理时间的标准库
** time()
time()获取当前时间戳，即计算机内部时间值，浮点数
#+BEGIN_SRC bash
>>>time.time()
1516939876.602228
#+END_SRC
** ctime()获取当前时间并以易读方式表示，返回字符串
#+BEGIN_SRC bash
>>>time.ctime()
'Fri Jan 26 12:11:16 2018'
#+END_SRC
** gmtime()获取当前时间，表示为计算机可处理的时间格式
#+BEGIN_SRC bash
>>>time.gmtime()
time.struct_time(tm_year=2018, tm_mon=1,
tm_mday=26, tm_hour=4, tm_min=11, tm_sec=16,
tm_wday=4, tm_yday=26, tm_isdst=0)
#+END_SRC
* namedtuple
namedtuple是继承自tuple的子类。namedtuple创建一个和tuple类似的对象，而且对象拥有可访问的属性。
#+BEGIN_SRC python
from collections import namedtuple

# 定义一个namedtuple类型User，并包含name，sex和age属性。
User = namedtuple('User', ['name', 'sex', 'age'])

# 创建一个User对象
user = User(name='kongxx', sex='male', age=21)

# 也可以通过一个list来创建一个User对象，这里注意需要使用"_make"方法
user = User._make(['kongxx', 'male', 21])

print user
# User(name='user1', sex='male', age=21)

# 获取用户的属性
print user.name
print user.sex
print user.age

# 修改对象属性，注意要使用"_replace"方法
user = user._replace(age=22)
print user
# User(name='user1', sex='male', age=21)

# 将User对象转换成字典，注意要使用"_asdict"
print user._asdict()
# OrderedDict([('name', 'kongxx'), ('sex', 'male'), ('age', 22)])

#+END_SRC

* collections库
** OrderedDict用法
Python中的字典对象可以以“键：值”的方式存取数据。OrderedDict是它的一个子类，实现了对字典对象中元素的排序。比如下面比较了两种方式的不同：
#+BEGIN_SRC python
import collections
 
print 'Regular dictionary:'
d={}
d['a']='A'
d['b']='B'
d['c']='C'
for k,v in d.items():
    print k,v
 
print '\nOrderedDict:'
d=collections.OrderedDict()
d['a']='A'
d['b']='B'
d['c']='C'
for k,v in d.items():
    print k,v
#+END_SRC
输出结果如下：
#+BEGIN_SRC bash
Regular dictionary:
a A
c C
b B
 
OrderedDict:
a A
b B
c C

#+END_SRC
可以看到，同样是保存了ABC三个元素，但是使用OrderedDict会根据放入元素的先后顺序进行排序。由于进行了排序，所以OrderedDict对象的字典对象，如果其顺序不同那么Python也会把他们当做是两个不同的对象，比如下面的代码：
#+BEGIN_SRC python
import collections
 
print 'Regular dictionary:'
d1={}
d1['a']='A'
d1['b']='B'
d1['c']='C'
 
d2={}
d2['c']='C'
d2['a']='A'
d2['b']='B'
 
print d1==d2
 
print '\nOrderedDict:'
d1=collections.OrderedDict()
d1['a']='A'
d1['b']='B'
d1['c']='C'
 
d2=collections.OrderedDict()
d2['c']='C'
d2['a']='A'
d2['b']='B'
 
print  d1==d2
#+END_SRC
其输出结果为：
#+BEGIN_SRC bash
Regular dictionary:
True
 
OrderedDict:
False
#+END_SRC
* isinstance() 函数
isinstance() 函数来判断一个对象是否是一个已知的类型，类似 type()。

isinstance() 与 type() 区别：
- type() 不会认为子类是一种父类类型，不考虑继承关系。
- isinstance() 会认为子类是一种父类类型，考虑继承关系。

如果要判断两个类型是否相同推荐使用 isinstance()。

#+BEGIN_SRC python
>a = 2
> isinstance (a,int)
True
> isinstance (a,str)
False
> isinstance (a,(str,int,list))    # 是元组中的一个返回 True
True
#+END_SRC
type() 与 isinstance() 区别：
#+BEGIN_SRC python
class A:
    pass
 
class B(A):
    pass
 
isinstance(A(), A)    # returns True
type(A()) == A        # returns True
isinstance(B(), A)    # returns True
type(B()) == A        # returns False
#+END_SRC
* format()用法
format():把传统的%替换为{}来实现格式化输出

其实就是format()后面的内容，填入大括号中（可以按位置，或者按变量）
#+BEGIN_SRC python
> '数字{1}{2}和{0}'.format("123",456,'789')
'数字456789和123'
#这里注意有两层大括号，输出的结果只有一层大括号
> '数字{{{1}{2}}}和{0}'.format("123",456,'789')
'数字{456789}和123'
#允许一个参数用两次
> '{1}{0}{1}岁'.format('jc',22) 
'22jc22岁'
#可以通过添加关键字参数
> '{name}{age}岁'.format(age=22,name='jc') 
'jc22岁'
#+END_SRC
* pandas库
pandas是常用的python数据处理包，把csv文件读入成dataframe各式
** read_csv()
Read a comma-separated values (csv) file into DataFrame.
#+BEGIN_SRC python
pandas.read_csv(filepath_or_buffer: Union[str, pathlib.Path, IO[~ AnyStr]], sep=',', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal: str = '.', lineterminator=None, quotechar='"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, dialect=None, error_bad_lines=True, warn_bad_lines=True, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None)
#+END_SRC
示例：
#+BEGIN_SRC python
data_train=pd.read_csv("./data/train.csv")
#+END_SRC
*** 参数
**** filepath_or_buffer：str, path object or file-like object
Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv.

If you want to pass in a path object, pandas accepts any os.PathLike.

By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO.
** pandas.get_dummies
get_dummies 是利用pandas实现one hot encode的方式。
#+BEGIN_SRC python
pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None) → ’DataFrame’
#+END_SRC
Convert categorical variable into dummy/indicator variables.

Returns:DataFrame,Dummy-coded data.
*** 参数
- data：array-like, Series, or DataFrame
Data of which to get dummy indicators.
- prefix：str, list of str, or dict of str, default None
String to append DataFrame column names. Pass a list with length equal to the number of columns when calling get_dummies on a DataFrame. Alternatively, prefix can be a dictionary mapping column names to prefixes.
- prefix_sep：str, default '_'
If appending prefix, separator/delimiter to use. Or pass a list or dictionary as with prefix.
- dummy_na:bool, default False
Add a column to indicate NaNs, if False NaNs are ignored.
- columns:list-like, default None
Column names in the DataFrame to be encoded. If columns is None then all the columns with object or category dtype will be converted.
- sparse:bool, default False
Whether the dummy-encoded columns should be backed by a SparseArray (True) or a regular NumPy array (False).
- drop_first:bool, default False
Whether to get k-1 dummies out of k categorical levels by removing the first level.
- dtype:dtype, default np.uint8
Data type for new columns. Only a single dtype is allowed.
New in version 0.23.0.
*** 示例
#+BEGIN_SRC python
> s = pd.Series(list('abca'))
> pd.get_dummies(s)
   a  b  c
0  1  0  0
1  0  1  0
2  0  0  1
3  1  0  0
> s1 = ['a', 'b', np.nan]
> pd.get_dummies(s1)
   a  b
0  1  0
1  0  1
2  0  0
> pd.get_dummies(s1, dummy_na=True)
   a  b  NaN
0  1  0    0
1  0  1    0
2  0  0    1
> df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'],
                   'C': [1, 2, 3]})
> pd.get_dummies(df, prefix=['col1', 'col2'])
   C  col1_a  col1_b  col2_a  col2_b  col2_c
0  1       1       0       0       1       0
1  2       0       1       1       0       0
2  3       1       0       0       0       1
> pd.get_dummies(pd.Series(list('abcaa')))
   a  b  c
0  1  0  0
1  0  1  0
2  0  0  1
3  1  0  0
4  1  0  0
> pd.get_dummies(pd.Series(list('abcaa')), drop_first=True)
   b  c
0  0  0
1  1  0
2  0  1
3  0  0
4  0  0
> pd.get_dummies(pd.Series(list('abc')), dtype=float)
     a    b    c
0  1.0  0.0  0.0
1  0.0  1.0  0.0
2  0.0  0.0  1.0
#+END_SRC
*** 示例2
#+BEGIN_SRC python
import pandas as pd
df = pd.DataFrame([  
            ['green' , 'A'],   
            ['red'   , 'B'],   
            ['blue'  , 'A']])  

df.columns = ['color',  'class'] 
pd.get_dummies(df) 
#+END_SRC
get_dummies 前：
|   | color | class |
| 0 | grean | A     |
| 1 | red   | B     |
| 2 | blue  | A     |
get_dummied 后：
|   | color_ble | color_grean | color_red | class_A | class_B |
| 0 |         0 |           1 |         0 |       1 |       0 |
| 1 |         0 |           0 |         1 |       0 |       1 |
| 2 |         1 |           0 |         0 |       1 | 0       |
可以对指定列进行get_dummies:
#+BEGIN_SRC python
pd.get_dummies(df.color)
#+END_SRC
|   | color_ble | color_grean | color_red | 
| 0 |         0 |           1 |         0 |    
| 1 |         0 |           0 |         1 |    
| 2 |         1 |           0 |         0 | 
  
** pandas.conca
#+BEGIN_SRC python
pandas.concat(objs: Union[Iterable[‘DataFrame’], Mapping[Optional[Hashable], ‘DataFrame’]], axis='0', join: str = "'outer'", ignore_index: bool = 'False', keys='None', levels='None', names='None', verify_integrity: bool = 'False', sort: bool = 'False', copy: bool = 'True') → ’DataFrame’
#+END_SRC
Concatenate pandas objects along a particular axis with optional set logic along the other axes.

Can also add a layer of hierarchical indexing on the concatenation axis, which may be useful if the labels are the same (or overlapping) on the passed axis number.

Returns:object, type of objs.
When concatenating all Series along the index (axis=0), a Series is returned. When objs contains at least one DataFrame, a DataFrame is returned. When concatenating along the columns (axis=1), a DataFrame is returned.
*** 参数
- objs：a sequence or mapping of Series or DataFrame objects
If a dict is passed, the sorted keys will be used as the keys argument, unless it is passed, in which case the values will be selected (see below). Any None objects will be dropped silently unless they are all None in which case a ValueError will be raised.
- axis：{0/’index’, 1/’columns’}, default 0
The axis to concatenate along.
- join:{'inner', 'outer'}, default ‘outer’
How to handle indexes on other axis (or axes).
- ignore_index:bool, default False
If True, do not use the index values along the concatenation axis. The resulting axis will be labeled 0, …, n - 1. This is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information. Note the index values on the other axes are still respected in the join.
- keys:sequence, default None
If multiple levels passed, should contain tuples. Construct hierarchical index using the passed keys as the outermost level.
- levels:list of sequences, default None
Specific levels (unique values) to use for constructing a MultiIndex. Otherwise they will be inferred from the keys.
- names:list, default None
Names for the levels in the resulting hierarchical index.
- verify_integrity:bool, default False
Check whether the new concatenated axis contains duplicates. This can be very expensive relative to the actual data concatenation.
- sort:bool, default False
Sort non-concatenation axis if it is not already aligned when join is ‘outer’. This has no effect when join='inner', which already preserves the order of the non-concatenation axis.
New in version 0.23.0.
Changed in version 1.0.0: Changed to not sort by default.
- copy:bool, default True
If False, do not copy data unnecessarily.
** pandas.DataFrame
*** pandas.DataFrame.plot
#+BEGIN_SRC python
DataFrame.plot(self, *args, **kwargs)
#+END_SRC
Make plots of Series or DataFrame.

Uses the backend specified by the option plotting.backend. By default, matplotlib is used.

Returns:matplotlib.axes.Axes or numpy.ndarray of them.

If the backend is not the default matplotlib one, the return value will be the object returned by the backend.
**** 参数
- data：Series or DataFrame
The object for which the method is called.

- x：label or position, default None
Only used if data is a DataFrame.

- y：label, position or list of label, positions, default None
Allows plotting of one column versus another. Only used if data is a DataFrame.

- kind：str
The kind of plot to produce:

'line' : line plot (default)

'bar' : vertical bar plot

'barh' : horizontal bar plot

'hist' : histogram

'box' : boxplot

'kde' : Kernel Density Estimation plot

'density' : same as 'kde'

'area' : area plot

'pie' : pie plot

'scatter' : scatter plot

'hexbin' : hexbin plot.

- figsize:a tuple (width, height) in inches

- use_index:bool, default True
Use index as ticks for x axis.

- title:str or list
Title to use for the plot. If a string is passed, print the string at the top of the figure. If a list is passed and subplots is True, print each item in the list above the corresponding subplot.

- grid:bool, default None (matlab style default)
Axis grid lines.

- legend:bool or {'reverse'}
Place legend on axis subplots.

- style:list or dict
The matplotlib line style per column.

- logx:bool or 'sym', default False
Use log scaling or symlog scaling on x axis. .. versionchanged:: 0.25.0

- logy:bool or 'sym' default False
Use log scaling or symlog scaling on y axis. .. versionchanged:: 0.25.0

- loglog:bool or 'sym', default False
Use log scaling or symlog scaling on both x and y axes. .. versionchanged:: 0.25.0

- xticks:sequence
Values to use for the xticks.

- yticks:sequence
Values to use for the yticks.

- xlim:2-tuple/list

- ylim:2-tuple/list

- rot:int, default None
Rotation for ticks (xticks for vertical, yticks for horizontal plots).

- fontsize:int, default None
Font size for xticks and yticks.

- colormap:str or matplotlib colormap object, default None
Colormap to select colors from. If string, load colormap with that name from matplotlib.

- colorbar:bool, optional
If True, plot colorbar (only relevant for 'scatter' and 'hexbin' plots).

- position:float
Specify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center).

- table:bool, Series or DataFrame, default False
If True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib's default layout. If a Series or DataFrame is passed, use passed data to draw a table.

- yerr:DataFrame, Series, array-like, dict and str
See Plotting with Error Bars for detail.

- xerr:DataFrame, Series, array-like, dict and str
Equivalent to yerr.

- mark_right:bool, default True
When using a secondary_y axis, automatically mark the column labels with “(right)” in the legend.

- include_bool:bool, default is False
If True, boolean values can be plotted.

- backend:str, default None
Backend to use instead of the backend specified in the option plotting.backend. For instance, 'matplotlib'. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.

New in version 1.0.0.

- **kwargs
Options to pass to matplotlib plotting method.
**** pandas.DataFrame.plot.kde
#+BEGIN_SRC python
DataFrame.plot.kde(self, bw_method=None, ind=None, **kwargs)
#+END_SRC
Generate Kernel Density Estimate plot using Gaussian kernels.

In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function (PDF) of a random variable. This function uses Gaussian kernels and includes automatic bandwidth determination.

Returns:matplotlib.axes.Axes or numpy.ndarray of them
***** 参数
- bw_method:str, scalar or callable, optional
The method used to calculate the estimator bandwidth. This can be 'scott', 'silverman', a scalar constant or a callable. If None (default), 'scott' is used. See scipy.stats.gaussian_kde for more information.

- ind:NumPy array or int, optional
Evaluation points for the estimated PDF. If None (default), 1000 equally spaced points are used. If ind is a NumPy array, the KDE is evaluated at the points passed. If ind is an integer, ind number of equally spaced points are used.

- **kwargs
Additional keyword arguments are documented in pandas.%(this-datatype)s.plot().
* Matplotlib
** 实例
Matplotlib 是 Python 的绘图库。 它可与 NumPy 一起使用，提供了一种有效的 MatLab 开源替代方案。 它也可以和图形工具包一起使用，如 PyQt 和 wxPython。
#+BEGIN_SRC python
import matplotlib.pyplot as plt
plt.plot([1, 2, 3, 4])
plt.ylabel('some numbers')
plt.show()
#+END_SRC
图形由 show() 函数显示。
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-07-23_10-23-39.png @ 2020-07-23 10:23:43
[[file:Matplotlib/2020-07-23_10-23-43_Snipaste_2020-07-23_10-23-39.png]]
You may be wondering why the x-axis ranges from 0-3 and the y-axis from 1-4. If you provide a single list or array to plot, matplotlib assumes it is a sequence of y values, and automatically generates the x values for you. Since python ranges start with 0, the default x vector has the same length as y but starts with 0. Hence the x data are [0, 1, 2, 3].

以下实例使用 matplotlib 生成折线图。
#+BEGIN_SRC python 
plt.plot([1, 2, 3, 4], [1, 4, 9, 16])  #前面的参数作为x，后面的列表作为y的取值
#+END_SRC
执行输出结果如下图：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-07-23_10-26-01.png @ 2020-07-23 10:26:13
[[file:Matplotlib/2020-07-23_10-26-13_Snipaste_2020-07-23_10-26-01.png]]q

** 格式化字符
作为线性图的替代，可以通过向 plot() 函数添加格式字符串来显示离散值。

要显示圆来代表点，而不是上面示例中的线，请使用 ob 作为 plot() 函数中的格式字符串。
#+BEGIN_SRC python
import numpy as np 
from matplotlib import pyplot as plt 
 
x = np.arange(1,11) 
y =  2  * x +  5 
plt.title("Matplotlib demo") 
plt.xlabel("x axis caption") 
plt.ylabel("y axis caption") 
plt.plot(x,y,"ob")  #这里ob的解释看下面的字符和颜色缩写
plt.show()
#+END_SRC
执行输出结果如下图：
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-07-23_10-11-18.png @ 2020-07-23 10:11:23
[[file:Matplotlib/2020-07-23_10-11-23_Snipaste_2020-07-23_10-11-18.png]]

*** 字符 
| 字符     | 描述         |
|----------+--------------|
| '-'      | 实线样式     |
| '--'     | 短横线样式   |
| '-.'     | 点划线样式   |
| ':'      | 虚线样式     |
| '.'      | 点标记       |
| ','      | 像素标记     |
| 'o'      | 圆标记       |
| 'v'      | 倒三角标记   |
| '^'      | 正三角标记   |
| '&lt;'   | 左三角标记   |
| '&gt;'   | 右三角标记   |
| '1'      | 下箭头标记   |
| '2'      | 上箭头标记   |
| '3'      | 左箭头标记   |
| '4'      | 右箭头标记   |
| 's'      | 正方形标记   |
| 'p'      | 五边形标记   |
| '*'      | 星形标记     |
| 'h'      | 六边形标记 1 |
| 'H'      | 六边形标记 2 |
| '+'      | 加号标记     |
| 'x'      | X 标记       |
| 'D'      | 菱形标记     |
| 'd'      | 窄菱形标记   |
| '&#124;' | 竖直线标记   |
| '_'      | 水平线标记   |

*** 颜色
以下是颜色的缩写：
| 字符 | 颜色   |
|------+--------|
| 'b'  | 蓝色   |
| 'g'  | 绿色   |
| 'r'  | 红色   |
| 'c'  | 青色   |
| 'm'  | 品红色 |
| 'y'  | 黄色   |
| 'k'  | 黑色   |
| 'w'  | 白色   |
** 图形中文显示
Matplotlib 默认情况不支持中文
#+BEGIN_SRC python
#解决中文显示问题
plt.rcParams['font.sans-serif']=['SimHei']
plt.rcParams['axes.unicode_minus'] = False
#+END_SRC

** pyplot
*** pyplot.scatter
scatter散点图
#+BEGIN_SRC python
matplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=<deprecated parameter>, edgecolors=None, *, plotnonfinite=False, data=None, **kwargs)[source]
#+END_SRC
A scatter plot of y vs. x with varying marker size and/or color.
**** 参数
- x, y：float or array-like, shape (n, )
The data positions.
- sfloat or array-like, shape (n, ), optional
The marker size in points**2. Default is rcParams['lines.markersize'] ** 2.
- c:array-like or list of colors or color, optional
The marker colors. Possible values:

A scalar or sequence of n numbers to be mapped to colors using cmap and norm.

A 2-D array in which the rows are RGB or RGBA.

A sequence of colors of length n.

A single color format string.

Note that c should not be a single numeric RGB or RGBA sequence because that is indistinguishable from an array of values to be colormapped. If you want to specify the same RGB or RGBA value for all points, use a 2-D array with a single row. Otherwise, value- matching will have precedence in case of a size matching with x and y.

If you wish to specify a single color for all points prefer the color keyword argument.

Defaults to None. In that case the marker color is determined by the value of color, facecolor or facecolors. In case those are not specified or None, the marker color is determined by the next color of the Axes' current "shape and fill" color cycle. This cycle defaults to rcParams["axes.prop_cycle"] (default: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])).
- marker:MarkerStyle, default: rcParams["scatter.marker"] (default: 'o')
The marker style. marker can be either an instance of the class or the text shorthand for a particular marker. See matplotlib.markers for more information about marker styles.
- cmap:str or Colormap, default: rcParams["image.cmap"] (default: 'viridis')
A Colormap instance or registered colormap name. cmap is only used if c is an array of floats.
- norm:Normalize, default: None
If c is an array of floats, norm is used to scale the color data, c, in the range 0 to 1, in order to map into the colormap cmap. If None, use the default colors.Normalize.
- vmin, vmax:float, default: None
nvmin and vmax are used in conjunction with the default norm to map the color array c to the colormap cmap. If None, the respective min and max of the color array is used. It is deprecated to use vmin/vmax when norm is given.
- alpha:float, default: None
The alpha blending value, between 0 (transparent) and 1 (opaque).
- linewidths:float or array-like, default: rcParams["lines.linewidth"] (default: 1.5)
The linewidth of the marker edges. Note: The default edgecolors is 'face'. You may want to change this as well.
- edgecolors:{'face', 'none', None} or color or sequence of color, default: rcParams["scatter.edgecolors"] (default: 'face')
The edge color of the marker. Possible values:

'face': The edge color will always be the same as the face color.

'none': No patch boundary will be drawn.

A color or sequence of colors.

For non-filled markers, the edgecolors kwarg is ignored and forced to 'face' internally.
- plotnonfinite:bool, default: False
Set to plot points with nonfinite c, in conjunction with set_bad.
- Other Parameters:	
**kwargs:Collection properties
*** pyplot.subplot()
 subplot() 函数允许你在同一图中绘制不同的东西。
*** pyplot.subplot2grid()
 subplot()这种子区函数只能绘制等分画布形式的图形样式，要想按照绘图区域的不同展示目的，进行非等分画布形式的图形展示，需要向画布多次使用子区函数subplot()完成非等分画布的展示任务，但是这么频繁地操作显得非常麻烦，而且在划分画布时易于出现疏漏和差错。因此，我们需要用高级的方法使用子区，需要定制化的网格区域，这个函数就是subplot2grid()，通过使用subplot2grid()函数的rowspan 和colspan 参数可以让子区跨越固定的网格布局的多个行和列，实现不同的子区布局。
 #+BEGIN_SRC python
 plt.subplot2grid(shape, loc, rowspan=1, colspan=1, fig=None, **kwargs)
 #+END_SRC
*** pyplot.grid
#+BEGIN_SRC python
matplotlib.pyplot.grid(b=None, which='major', axis='both', **kwargs)
#+END_SRC
Configure the grid lines.
**** 参数
- b:bool or None, optional
Whether to show the grid lines. If any kwargs are supplied, it is assumed you want the grid on and b will be set to True.

If b is None and there are no kwargs, this toggles the visibility of the lines.

- which{'major', 'minor', 'both'}, optional
The grid lines to apply the changes on.

- axis{'both', 'x', 'y'}, optional
The axis to apply the changes on.

- **kwargsLine2D properties
Define the line properties of the grid, e.g.:
#+BEGIN_SRC python
grid(color='r', linestyle='-', linewidth=2)
#+END_SRC
Valid keyword arguments are:
| Property                     | Description                                                                                           |
|------------------------------+-------------------------------------------------------------------------------------------------------|
| agg_filter                   | a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array |
| alpha                        | float or None                                                                                         |
| animated                     | bool                                                                                                  |
| antialiased or aa            | bool                                                                                                  |
| clip_box                     | Bbox                                                                                                  |
| clip_on                      | bool                                                                                                  |
| clip_path                    | Patch or (Path, Transform) or None                                                                    |
| color or c                   | color                                                                                                 |
| contains                     | unknown                                                                                               |
| dash_capstyle                | {'butt', 'round', 'projecting'}                                                                       |
| dash_joinstyle               | {'miter', 'round', 'bevel'}                                                                           |
| dashes                       | sequence of floats (on/off ink in points) or (None, None)                                             |
| data                         | (2, N) array or two 1D arrays                                                                         |
| drawstyle or ds              | {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'                      |
| figure                       | Figure                                                                                                |
| fillstyle                    | {'full', 'left', 'right', 'bottom', 'top', 'none'}                                                    |
| gid                          | str                                                                                                   |
| in_layout                    | bool                                                                                                  |
| label                        | object                                                                                                |
| linestyle or ls              | {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}                                                 |
| linewidth or lw              | float                                                                                                 |
| marker                       | marker style string, Path or MarkerStyle                                                              |
| markeredgecolor or mec       | color                                                                                                 |
| markeredgewidth or mew       | float                                                                                                 |
| markerfacecolor or mfc       | color                                                                                                 |
| markerfacecoloralt or mfcalt | color                                                                                                 |
| markersize or ms             | float                                                                                                 |
| markevery                    | None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]              |
| path_effects                 | AbstractPathEffect                                                                                    |
| picker                       | unknown                                                                                               |
| pickradius                   | float                                                                                                 |
| rasterized                   | bool or None                                                                                          |
| sketch_params                | (scale: float, length: float, randomness: float)                                                      |
| snap                         | bool or None                                                                                          |
| solid_capstyle               | {'butt', 'round', 'projecting'}                                                                       |
| solid_joinstyle              | {'miter', 'round', 'bevel'}                                                                           |
| transform                    | matplotlib.transforms.Transform                                                                       |
| url                          | str                                                                                                   |
| visible                      | bool                                                                                                  |
| xdata                        | 1D array                                                                                              |
| ydata                        | 1D array                                                                                              |
| zorder                       | float                                                                                                 |
*** add_subplot
#+BEGIN_SRC python
add_subplot(self, *args, **kwargs)[source]
#+END_SRC
Add an Axes to the figure as part of a subplot arrangement.

Returns:axes.SubplotBase, or another subclass of Axes.
The axes of the subplot. The returned axes base class depends on the projection used. It is Axes if rectilinear projection is used and projections.polar.PolarAxes if polar projection is used. The returned axes is then a subplot subclass of the base class.

注意，pyplot的方式中plt.subplot()参数和面向对象中的add_subplot()参数和含义都相同。

使用面向对象的方式:
#+BEGIN_SRC python
#!/usr/bin/python
#coding: utf-8
 
import numpy as np
import matplotlib.pyplot as plt
 
x = np.arange(0, 100)
 
fig = plt.figure()
 
ax1 = fig.add_subplot(221)
ax1.plot(x, x)
 
ax2 = fig.add_subplot(222)
ax2.plot(x, -x)
 
ax3 = fig.add_subplot(223)
ax3.plot(x, x ** 2)
 
ax4 = fig.add_subplot(224)
ax4.plot(x, np.log(x))
 
plt.show()
#+END_SRC
pyplot的方式:
#+BEGIN_SRC python
#!/usr/bin/python
#coding: utf-8
 
import numpy as np
import matplotlib.pyplot as plt
 
x = np.arange(0, 100)
 
plt.subplot(221)
plt.plot(x, x)
 
plt.subplot(222)
plt.plot(x, -x)
 
plt.subplot(223)
plt.plot(x, x ** 2)
 
plt.subplot(224)
plt.plot(x, np.log(x))
 
plt.show()
#+END_SRC
**** 参数
- *args:int, (int, int, index), or SubplotSpec, default: (1, 1, 1)
The position of the subplot described by one of

Three integers (nrows, ncols, index). The subplot will take the index position on a grid with nrows rows and ncols columns. index starts at 1 in the upper left corner and increases to the right. index can also be a two-tuple specifying the (first, last) indices (1-based, and including last) of the subplot, e.g., fig.add_subplot(3, 1, (1, 2)) makes a subplot that spans the upper 2/3 of the figure.

A 3-digit integer. The digits are interpreted as if given separately as three single-digit integers, i.e. fig.add_subplot(235) is the same as fig.add_subplot(2, 3, 5). Note that this can only be used if there are no more than 9 subplots.

A SubplotSpec.

In rare circumstances, add_subplot may be called with a single argument, a subplot axes instance already created in the present figure but not in the figure's list of axes.

- projection:{None, 'aitoff', 'hammer', 'lambert', 'mollweide', 'polar', 'rectilinear', str}, optional
The projection type of the subplot (Axes). str is the name of a custom projection, see projections. The default None results in a 'rectilinear' projection.

- polar:bool, default: False
If True, equivalent to projection='polar'.

- sharex, sharey:Axes, optional
Share the x or y axis with sharex and/or sharey. The axis will have the same limits, ticks, and scale as the axis of the shared axes.

- label:str
A label for the returned axes.
*** pyplot.legend
#+BEGIN_SRC python
matplotlib.pyplot.legend(*args, **kwargs)
#+END_SRC
Place a legend on the axes.该函数可以用以添加图例

* Sklearn 库
Scikit-learn(sklearn)是机器学习中常用的第三方模块，对常用的机器学习方法进行了封装，包括回归(Regression)、降维(Dimensionality Reduction)、分类(Classfication)、聚类(Clustering)等方法。
* glob库
glob是python自己带的一个文件操作相关模块，用它可以查找符合自己目的的文件，类似于Windows下的文件搜索，支持通配符操作，,?,[]这三个通配符，代表0个或多个字符，?代表一个字符，[]匹配指定范围内的字符，如[0-9]匹配数字。两个主要方法如下。
** glob方法
glob模块的主要方法就是glob,该方法返回所有匹配的文件路径列表（list）；该方法需要一个参数用来指定匹配的路径字符串（字符串可以为绝对路径也可以为相对路径），其返回的文件名只包括当前目录里的文件名，不包括子文件夹里的文件。
#+BEGIN_SRC python
glob(pathname, recursive=False)
#+END_SRC
第一个参数pathname为需要匹配的字符串。（该参数应尽量加上r前缀，以免发生不必要的错误）

第二个参数代表递归调用，与特殊通配符“**”一同使用，默认为False。

该函数返回一个符合条件的路径的字符串列表，如果使用的是Windows系统，路径上的“\”符号会自动加上转义符号变为“\\”（方便使用）。

在3.5版本之后，glob函数支持一个特殊的通配符“**”，该通配符可以匹配指定路径里所有文件和目录，包括子目录里的所有文件和目录。相当于递归地调用了这个函数。使用这个通配符必须加上recursive=True参数。（在有复杂目录结构的情况下使用该通配符可能会导致性能下降，拖累整个程序的运行，需谨慎使用！）

比如：
#+BEGIN_SRC python
glob.glob(r’c:*.txt’)
#+END_SRC
我这里就是获得C盘下的所有txt文件
#+BEGIN_SRC python
glob.glob(r’E:\pic**.jpg’)
#+END_SRC
获得指定目录下的所有jpg文件

使用相对路径：
#+BEGIN_SRC python
glob.glob(r’../*.py’)
#+END_SRC

需要注意的地方：

glob默认不匹配以点符号（.）开始的文件，如果有这类文件，则需要做特殊处理。

假如当前文件夹包含test.txt和.test.txt两个文件。
#+BEGIN_SRC python
>>> import glob
>>> glob.glob('*.txt')
['test.txt']
>>> glob.glob('.*.txt')
['.test.txt']
#+END_SRC
*** glob模块支持的通配符
| 通配符 | 功能                                                      |
|--------+-----------------------------------------------------------|
| *      | 匹配0或多个字符                                           |
| **     | 匹配所有文件、目录、子目录和子目录里的文件（3.5版本新增） |
| ?      | 匹配1个字符，与正则表达式里的?不同                        |
| [exp]  | 匹配指定范围内的字符，如：[1-9]匹配1至9范围内的字符       |
| [!exp] | 匹配不在指定范围内的字符                                  |
虽然glob模块可以很轻松地匹配特定文件和文件夹，但是仅仅支持少量的通配符，没办法像正则表达式一样匹配更复杂的字符串。使用的时候应当认真考虑使用场景，根据需求针对性地选择解决方案。
** iglob方法
获取一个迭代器（ iterator ）对象，使用它可以逐个获取匹配的文件路径名。与glob.glob()的区别是：glob.glob同时获取所有的匹配路径，而 glob.iglob一次只获取一个匹配路径。
#+BEGIN_SRC python
iglob(pathname, recursive=False)
#+END_SRC
参数与glob()一致。

返回一个迭代器，该迭代器不会同时保存所有匹配到的路径，遍历该迭代器的结果与使用相同参数调用glob()的返回结果一致。



下面是一个简单的例子：
#+BEGIN_SRC python
#父目录中所有的.py文件
f = glob.iglob(r'../*.py')
print f
<generator object iglob at 0x00B9FF80>

for py in f:
    print py

#+END_SRC
f是一个迭代器对象，通过遍历，可以输出所有满足条件的*.py文件
** escape(pathname)

这个函数是在3.4版本之后才有的，功能是忽略所有通配符。（可以用于测试某文件是否存在）
（3.5.1版本该函数不能正常运行，升级到3.5.2之后恢复正常）
* logging模块
** 日志相关概念
日志是一种可以追踪某些软件运行时所发生事件的方法。软件开发人员可以向他们的代码中调用日志记录相关的方法来表明发生了某些事情。一个事件可以用一个可包含可选变量数据的消息来描述。此外，事件也有重要性的概念，这个重要性也可以被称为严重性级别（level）。
*** 日志的作用
通过log的分析，可以方便用户了解系统或软件、应用的运行情况；如果你的应用log足够丰富，也可以分析以往用户的操作行为、类型喜好、地域分布或其他更多信息；如果一个应用的log同时也分了多个级别，那么可以很轻易地分析得到该应用的健康状况，及时发现问题并快速定位、解决问题，补救损失。

简单来讲就是，我们通过记录和分析日志可以了解一个系统或软件程序运行情况是否正常，也可以在应用程序出现故障时快速定位问题。比如，做运维的同学，在接收到报警或各种问题反馈后，进行问题排查时通常都会先去看各种日志，大部分问题都可以在日志中找到答案。再比如，做开发的同学，可以通过IDE控制台上输出的各种日志进行程序调试。对于运维老司机或者有经验的开发人员，可以快速的通过日志定位到问题的根源。可见，日志的重要性不可小觑。日志的作用可以简单总结为以下3点：
- 程序调试
- 了解软件程序运行情况，是否正常
- 软件程序运行故障分析与问题定位

如果应用的日志信息足够详细和丰富，还可以用来做用户行为分析，如：分析用户的操作行为、类型洗好、地域分布以及其它更多的信息，由此可以实现改进业务、提高商业利益。
*** 日志的等级
我们先来思考下下面的两个问题：
- 作为开发人员，在开发一个应用程序时需要什么日志信息？在应用程序正式上线后需要什么日志信息？
- 作为应用运维人员，在部署开发环境时需要什么日志信息？在部署生产环境时需要什么日志信息？

在软件开发阶段或部署开发环境时，为了尽可能详细的查看应用程序的运行状态来保证上线后的稳定性，我们可能需要把该应用程序所有的运行日志全部记录下来进行分析，这是非常耗费机器性能的。当应用程序正式发布或在生产环境部署应用程序时，我们通常只需要记录应用程序的异常信息、错误信息等，这样既可以减小服务器的I/O压力，也可以避免我们在排查故障时被淹没在日志的海洋里。那么，怎样才能在不改动应用程序代码的情况下实现在不同的环境记录不同详细程度的日志呢？这就是日志等级的作用了，我们通过配置文件指定我们需要的日志等级就可以了。

不同的应用程序所定义的日志等级可能会有所差别，分的详细点的会包含以下几个等级：
- DEBUG
- INFO
- NOTICE
- WARNING
- ERROR
- CRITICAL
- ALERT
- EMERGENCY

| 级别      | 何时使用                                                                           |
|-----------+------------------------------------------------------------------------------------|
| DEBUG     | 详细信息，典型地调试问题时会感兴趣。 详细的debug信息。                             |
| INFO      | 证明事情按预期工作。 关键事件。                                                    |
| WARNING   | 表明发生了一些意外，或者不久的将来会发生问题（如‘磁盘满了’）。软件还是在正常工作。 |
| ERROR     | 由于更严重的问题，软件已不能执行一些功能了。 一般错误消息。                        |
| CRITICAL  | 严重错误，表明软件已不能继续运行了。                                               |
| NOTICE    | 不是错误，但是可能需要处理。普通但是重要的事件。                                   |
| ALERT     | 需要立即修复，例如系统数据库损坏。                                                 |
| EMERGENCY | 紧急情况，系统不可用（例如系统崩溃），一般会通知所有用户。                         |
*** 日志字段信息与日志格式
一条日志信息对应的是一个事件的发生，而一个事件通常需要包括以下几个内容：
- 事件发生时间
- 事件发生位置
- 事件的严重程度--日志级别
- 事件内容

上面这些都是一条日志记录中可能包含的字段信息，当然还可以包括一些其他信息，如进程ID、进程名称、线程ID、线程名称等。日志格式就是用来定义一条日志记录中包含那些字段的，且日志格式通常都是可以自定义的。
*** 日志功能的实现
几乎所有开发语言都会内置日志相关功能，或者会有比较优秀的第三方库来提供日志操作功能，比如：log4j，log4php等。它们功能强大、使用简单。Python自身也提供了一个用于记录日志的标准库模块--logging。
** logging模块
logging模块是Python内置的标准模块，主要用于输出运行日志，可以设置输出日志的等级、日志保存路径、日志文件回滚等；相比print，具备如下优点：
- 可以通过设置不同的日志等级，在release版本中只输出重要信息，而不必显示大量的调试信息；
- print将所有信息都输出到标准输出中，严重影响开发者从标准输出中查看其它数据；logging则可以由开发者决定将信息输出到什么地方，以及怎么输出。
*** logging模块的日志级别
logging模块默认定义了以下几个日志等级，它允许开发人员自定义其他日志级别，但是这是不被推荐的，尤其是在开发供别人使用的库时，因为这会导致日志级别的混乱。
| 日志等级（level） | 描述                                                                                        |
|-------------------+---------------------------------------------------------------------------------------------|
| DEBUG             | 最详细的日志信息，典型应用场景是 问题诊断                                                   |
| INFO              | 信息详细程度仅次于DEBUG，通常只记录关键节点信息，用于确认一切都是按照我们预期的那样进行工作 |
| WARNING           | 当某些不期望的事情发生时记录的信息（如，磁盘可用空间较低），但是此时应用程序还是正常运行的  |
| ERROR             | 由于一个更严重的问题导致某些功能不能正常运行时记录的信息                                    |
| CRITICAL          | 当发生严重错误，导致应用程序不能继续运行时记录的信息                                        |
开发应用程序或部署开发环境时，可以使用DEBUG或INFO级别的日志获取尽可能详细的日志信息来进行开发或部署调试；

应用上线或部署生产环境时，应该使用WARNING或ERROR或CRITICAL级别的日志来降低机器的I/O压力和提高获取错误日志信息的效率。日志级别的指定通常都是在应用程序的配置文件中进行指定的。

上面列表中的日志等级是从上到下依次升高的，即：DEBUG < INFO < WARNING < ERROR < CRITICAL，而日志的信息量是依次减少的；

当为某个应用程序指定一个日志级别后，应用程序会记录所有日志级别大于或等于指定日志级别的日志信息，而不是仅仅记录指定级别的日志信息，nginx、php等应用程序以及这里的python的logging模块都是这样的。同样，logging模块也可以指定日志记录器的日志级别，只有级别大于或等于该指定日志级别的日志记录才会被输出，小于该等级的日志记录将会被丢弃。
*** 2、logging模块的使用方式介绍
logging模块提供了两种记录日志的方式：
- 第一种方式是使用logging提供的模块级别的函数
- 第二种方式是使用Logging日志系统的四大组件

其实，logging所提供的模块级别的日志记录函数也是对logging日志系统相关类的封装而已。

logging模块定义的模块级别的常用函数
| 函数                                   | 说明                                 |
|----------------------------------------+--------------------------------------|
| logging.debug(msg, *args, **kwargs)    | 创建一条严重级别为DEBUG的日志记录    |
| logging.info(msg, *args, **kwargs)     | 创建一条严重级别为INFO的日志记录     |
| logging.warning(msg, *args, **kwargs)  | 创建一条严重级别为WARNING的日志记录  |
| logging.error(msg, *args, **kwargs)    | 创建一条严重级别为ERROR的日志记录    |
| logging.critical(msg, *args, **kwargs) | 创建一条严重级别为CRITICAL的日志记录 |
| logging.log(level, *args, **kwargs)    | 创建一条严重级别为level的日志记录    |
| logging.basicConfig(**kwargs)          | 对root logger进行一次性配置          |
其中logging.basicConfig(**kwargs)函数用于指定“要记录的日志级别”、“日志格式”、“日志输出位置”、“日志文件的打开模式”等信息，
* PIL（Pillow）库
PIL：Python Imaging Library，已经是Python平台事实上的图像处理标准库了。PIL功能非常强大，但API却非常简单易用。

由于PIL仅支持到Python 2.7，加上年久失修，于是一群志愿者在PIL的基础上创建了兼容的版本，名字叫Pillow，支持最新Python 3.x，又加入了许多新特性，因此，我们可以直接安装使用Pillow。安装命令： pip3  install pillow
** PIL.Image
Image模块是在Python PIL图像处理中常见的模块，对图像进行基础操作的功能基本都包含于此模块内。如open、save、conver、show…等功能。
*** PIL.Image.open
#+BEGIN_SRC python
PIL.Image.open(fp, mode='r')[source]
#+END_SRC
Opens and identifies the given image file.This is a lazy operation; this function identifies the file, but the file remains open and the actual image data is not read from the file until you try to process the data (or call the load() method).
这个是一个懒操作；该函数只会读文件头，而真实的图像数据直到试图处理该数据才会从文件读取（调用load()方法将强行加载图像数据）。

Returns:An Image object.

要从文件加载图像，使用 open() 函数， 在 Image 模块：
#+BEGIN_SRC python
from PIL import Image             ##调用库
im = Image.open("E:\mywife.jpg")  ##文件存在的路径
im.show()                         
#+END_SRC
需要知道的是在win的环境下im.show的方式为win自带的图像显示应用。打开并确认给定的图像文件如果变量mode被设置，那必须是“r”。用户可以使用一个字符串（表示文件名称的字符串）或者文件对象作为变量file的值。文件对象必须实现read()，seek()和tell()方法，并且以二进制模式打开。
**** 参数
- fp – A filename (string), pathlib.Path object or a file object. The file object must implement read(), seek(), and tell() methods, and be opened in binary mode.
- mode – The mode. If given, this argument must be “r”.
*** PIL.Image.Save
#+BEGIN_EXAMPLE
im.save(outfile,options…)
im.save(outfile, format, options…)
#+END_EXAMPLE
** Image.fromarray的作用
简而言之，就是实现array到image的转换
1. PIL image转换成array
~img = np.asarray(image)~

需要注意的是，如果出现read-only错误，并不是转换的错误，一般是你读取的图片的时候，默认选择的是"r","rb"模式有关。

修正的办法:　手动修改图片的读取状态

~img.flags.writeable = True  # 将数组改为读写模式~

2. array转换成image
~Image.fromarray(np.uint8(img))~
* 关于梯度裁剪
神经网络的训练过程中，有时候会出现梯度爆炸的情况，具体表现为某些参数值变为nan，为了尽可能避免这种情况，需要对梯度进行裁剪，也就是把梯度限定在一定的范围内。

常见的两种clip方法：

按值裁剪：也就是将梯度限定在一定的值范围内，最大值max_value和最小值min_value是需要调节的超参
#+BEGIN_SRC python
if grad > max_value:
    grad = max_value
elif grad < min_value:
    grad = min_value
#+END_SRC
按范数裁剪：当梯度的范数超过指定值是进行裁剪，需要设置超参max_norm
#+BEGIN_SRC python 
if norm(grad) > max_norm:
    grad = grad * (max_norm / norm(grad))
#+END_SRC
* tensoflow
** 梯度裁剪
#+BEGIN_SRC python
[...]

optimizer = tf.train.AdamOptimizer(lr)
# 根据loss计算梯度
grads_and_vars = optimizer.compute_gradients(loss)

for idx, (grad, var) in enumerate(grads_and_vars):
    if grad is not None:
        ## 按范数裁剪
        grads_and_vars[idx] = (tf.clip_by_norm(grad, max_norm), var)

train_op = optimizer.apply_gradients(grads_and_vars, global_step)

## 按值裁剪
tf.clip_by_value(grad, min_value, max_value)

## 其他裁剪方法
tf.clip_by_average_norm(grad, clip_norm)
tf.clip_by_global_norm(grad, clip_norm)
#+END_SRC
* transformers
** HfArgumentParser的使用
HfArgumentParser是Transformer框架中的命令行解析工具，它是ArgumentParser的子类。用于从类对象中创建解析对象。
#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-11_21-53-58.png @ 2021-11-11 21:54:17
[[file:transformers/2021-11-11_21-54-17_Snipaste_2021-11-11_21-53-58.png]]
在python中，我们习惯于将有关联的一些参数放在一个类当中，HfArgumentParser可以将类对象中的实例属性转换成转换为解析参数。必须注意的是，这里的类对象必须是通过@dataclass()创建的类对象。并且通过HfArgumentParser创建的解析参数，都是可选参数。一个简单的例子如下：
#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-11_21-54-07.png @ 2021-11-11 21:54:39
[[file:transformers/2021-11-11_21-54-39_Snipaste_2021-11-11_21-54-07.png]]

终端输入：
#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-11_22-04-13.png @ 2021-11-11 22:04:17
[[file:transformers/2021-11-11_22-04-17_Snipaste_2021-11-11_22-04-13.png]]
#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-11_22-04-31.png @ 2021-11-11 22:04:36
[[file:transformers/2021-11-11_22-04-36_Snipaste_2021-11-11_22-04-31.png]]

* transformers使用
transformers 内置了 17 种以 transformer 结构为基础的神经网络：
- T5 model
- DistilBERT model
- ALBERT model
- CamemBERT model
- XLM-RoBERTa model
- Longformer model
- RoBERTa model
- Reformer model
- Bert model
- OpenAI GPT model
- OpenAI GPT-2 model
- Transformer-XL model
- XLNet model
- XLM model
- CTRL model
- Flaubert model
- ELECTRA model

这些模型的参数、用法大同小异。默认框架为 PyTorch，使用 TensorFlow 框架在类的前面加上 'TF" 即可。

每种模型都有至少一个预训练模型，限于篇幅，这里仅仅列举 Bert 的常用预训练模型：
| 模型                         | 模型细节                                                                                                                    |
|------------------------------+-----------------------------------------------------------------------------------------------------------------------------|
| bert-base-uncased            | 12-layer, 768-hidden, 12-heads, 110M parameters. Trained on lower-cased English text.                                       |
| bert-large-uncased           | 24-layer, 1024-hidden, 16-heads, 340M parameters. Trained on lower-cased English text.                                      |
| bert-base-cased              | 12-layer, 768-hidden, 12-heads, 110M parameters. Trained on cased English text.                                             |
| bert-large-cased             | 24-layer, 1024-hidden, 16-heads, 340M parameters. Trained on cased English text.                                            |
| bert-base-multilingual-cased | 12-layer, 768-hidden, 12-heads, 110M parameters. Trained on cased text in the top 104 languages with the largest Wikipedias |
| bert-base-chinese            | 12-layer, 768-hidden, 12-heads, 110M parameters. Trained on cased Chinese Simplified and Traditional text.                  |
完整的预训练模型列表可以在 transformers 官网上找到。

使用 transformers 库有三种方法：
- 使用 pipeline；
- 指定预训练模型；
- 使用 AutoModels 加载预训练模型。
** transformers.pipeline
这个管线函数包含三个部分：
- Tokenizer；
- 一个模型实例；
- 其它增强模型输出的功能。
它只有一个必需参数 task，接受如下变量之一：
- ”feature-extraction”
- ”sentiment-analysis”
- ”ner”
- ”question-answering”
- ”fill-mask”
- ”summarization”
- ”translation_xx_to_yy”
- ”text-generation”
这个函数还有其它可选参数

例子：
#+begin_src python
>>> from transformers import pipeline

>>> nlp = pipeline("sentiment-analysis")

>>> print(nlp("I hate you"))
[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]

>>> print(nlp("I love you"))
[{'label': 'POSITIVE', 'score': 0.9998656511306763}]
#+END_SRC
** 指定预训练模型
这里我们以 Bert 为例。

*** 配置 Bert 模型（可选，推荐不使用）transformers.BertConfig
transformers.BertConfig 可以自定义 Bert 模型的结构，以下参数都是可选的：
- vocab_size：词汇数，默认 30522；
- hidden_size：编码器内隐藏层神经元数量，默认 768；
- num_hidden_layers：编码器内隐藏层层数，默认 12；
- num_attention_heads：编码器内注意力头数，默认 12；
- intermediate_size：编码器内全连接层的输入维度，默认 3072；
- hidden_act：编码器内激活函数，默认 ‘gelu’，还可为 ‘relu’、‘swish’ 或 ‘gelu_new’
- hidden_dropout_prob：词嵌入层或编码器的 dropout，默认为 0.1；
- attention_probs_dropout_prob：注意力的 dropout，默认为 0.1；
- max_position_embeddings：模型使用的最大序列长度，默认为 512；
- type_vocab_size：词汇表类别，默认为 2；
- initializer_range：神经元权重的标准差，默认为 0.02；
- layer_norm_eps：layer normalization 的 epsilon 值，默认为 1e-12.

使用方法：
#+begin_src python
configuration = BertConfig() # 进行模型的配置，变量为空即使用默认参数

model = BertModel(configuration) # 使用自定义配置实例化 Bert 模型

configuration = model.config # 查看模型参数
#+END_SRC

*** 分词 transformers.BertTokenizer
所有的 tokenizer 都继承自 transformers.PreTrainedTokenizer 基类，因此有共同的参数和方法实例化的参数有：
- model_max_length：可选参数，最大输入长度，默认为 1e30；
- padding_side：可选参数，填充的方向，应为 ‘left’ 或 ‘right’；
- bos_token：可选参数，每句话的起始标记，默认为 ‘’；
- eos_token：可选参数，每句话的结束标记，默认为 ‘’；
- unk_token：可选参数，未知的标记，默认为 ‘’；
- sep_token：可选参数，分隔标记，默认为 ‘’；
- pad_token：可选参数，填充标记，默认为 ‘’；
- cls_token：可选参数，分类标记，默认为 ‘’；
- mask_token：可选参数，遮盖标记，默认为 ‘<MASK’。

为了演示，我们先实例化一个 BertTokenizer。
#+begin_src python
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
#+END_SRC
常用的方法有：
- from_pretrained(model)：载入预训练词汇表；
- tokenizer.tokenize(str)：分词；
#+begin_src python
>>> tokenizer.tokenize('Hello word!')
['Hello', 'word', '!']
#+END_SRC
encode(text, ...)：将文本分词后编码为包含对应 id 的列表；
#+begin_src python
>>> tokenizer.encode('Hello word!')
[101, 8667, 1937, 106, 102]
#+END_SRC
encode_plus(text, ...)：将文本分词后创建一个包含对应 id，token 类型及是否遮盖的词典；
#+begin_src python
tokenizer.encode_plus('Hello world!')
{'input_ids': [101, 8667, 1937, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}
#+END_SRC
convert_ids_to_tokens(ids, skip_special_tokens)：将 id 映射为 token；
#+begin_src python
>>> tokenizer.convert_ids_to_tokens(tokens)
['[CLS]', 'Hello', 'word', '!', '[SEP]']
#+END_SRC
decode(token_ids)：将 id 解码；
#+begin_src python
>>> tokenizer.decode(tokens)
'[CLS] Hello word! [SEP]'
#+END_SRC
convert_tokens_to_ids(tokens)：将 token 映射为 id。
#+begin_src python
>>> tokenizer.convert_tokens_to_ids(['[CLS]', 'Hello', 'word', '!', '[SEP]'])
[101, 8667, 1937, 106, 102]
#+END_SRC

*** 使用预训练模型
根据任务的需要，既可以选择没有为指定任务 finetune 的模型如 transformers.BertModel，也可以选择为指定任务 finetune 之后的模型如 transformers.BertForSequenceClassification。一共有 6 个指定的任务类型：
- transformers.BertForMaskedLM：语言模型；
- transformers.BertForNextSentencePrediction：判断下一句话是否与上一句有关；
- transformers.BertForSequenceClassification：序列分类如 GLUE；
- transformers.BertForMultipleChoice：文本分类；
- transformers.BertForTokenClassification：token 分类如 NER，
- transformers.BertForQuestionAnswering；问答。
** 使用 AutoModels
使用 AutoModels 与上面的指定模型进行预训练大同小异，只不过是另一种方式加载模型而已。

*** 加载自动配置 transformers.AutoConfig
使用类方法 from_pretrained 加载模型配置，参数既可以为模型名称，也可以为具体文件。
#+begin_src python
config = AutoConfig.from_pretrained('bert-base-uncased')
# 或者直接加载模型文件
config = AutoConfig.from_pretrained('./test/bert_saved_model/')
#+END_SRC

*** 加载分词器 transformers.AutoTokenizer
与上面的 BertTokenizer 非常相似，也是使用 from_pretrained 类方法加载预训练模型。
#+begin_src python
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
# 或者直接加载模型文件
tokenizer = AutoTokenizer.from_pretrained('./test/bert_saved_model/')
#+END_SRC

*** 加载模型 transformers.AutoModel
可以使用 from_pretrained 加载预训练模型：
#+begin_src python
model = AutoModel.from_pretrained('bert-base-uncased')
# 或者直接加载模型文件
model = AutoModel.from_pretrained('./test/bert_model/') 
#+END_SRC
选好了预训练模型以后，只需要给模型接一个全连接层，这个神经网络就搭好了（当然可以根据需要添加更复杂的结构）。
* Keras
** tf.layers
*** tf.keras.layers.Embedding
#+BEGIN_SRC python
keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)
#+END_SRC
将正整数（索引值）转换为固定尺寸的稠密向量。 例如： [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]

该层只能用作模型中的第一层。

输入尺寸为 (batch_size, sequence_length) 的 2D 张量。

输出尺寸为 (batch_size, sequence_length, output_dim) 的 3D 张量。
**** 例子
#+BEGIN_SRC python
model = Sequential()
model.add(Embedding(1000, 64, input_length=10))
# 模型将输入一个大小为 (batch, input_length) 的整数矩阵。
# 输入中最大的整数（即词索引）不应该大于 999 （词汇表大小）
# 现在 model.output_shape == (None, 10, 64)，其中 None 是 batch 的维度。

input_array = np.random.randint(1000, size=(32, 10))

model.compile('rmsprop', 'mse')
output_array = model.predict(input_array)
assert output_array.shape == (32, 10, 64)
#+END_SRC
**** 参数
- input_dim: int > 0。词汇表大小， 即，最大整数 index + 1。
- output_dim: int >= 0。词向量的维度。
- embeddings_initializer: embeddings 矩阵的初始化方法 (详见 initializers)。
- embeddings_regularizer: embeddings matrix 的正则化方法 (详见 regularizer)。
- embeddings_constraint: embeddings matrix 的约束函数 (详见 constraints)。
- mask_zero: 是否把 0 看作为一个应该被遮蔽的特殊的 "padding" 值。 这对于可变长的 循环神经网络层 十分有用。 如果设定为 True，那么接下来的所有层都必须支持 masking，否则就会抛出异常。 如果 mask_zero 为 True，作为结果，索引 0 就不能被用于词汇表中 （input_dim 应该与 vocabulary + 1 大小相同）。
- input_length: 输入序列的长度，当它是固定的时。 如果你需要连接 Flatten 和 Dense 层，则这个参数是必须的 （没有它，dense 层的输出尺寸就无法计算）。

*** tf.keras.layers.GRU
#+BEGIN_SRC python
tf.keras.layers.GRU(
    units,
    activation="tanh",
    recurrent_activation="sigmoid",
    use_bias=True,
    kernel_initializer="glorot_uniform",
    recurrent_initializer="orthogonal",
    bias_initializer="zeros",
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    unroll=False,
    time_major=False,
    reset_after=True,
    **kwargs
)
#+END_SRC
门限循环单元网络（Gated Recurrent Unit） - Cho et al. 2014.

Based on available runtime hardware and constraints, this layer will choose different implementations (cuDNN-based or pure-TensorFlow) to maximize the performance. If a GPU is available and all the arguments to the layer meet the requirement of the CuDNN kernel (see below for details), the layer will use a fast cuDNN implementation.

The requirements to use the cuDNN implementation are:
1. activation == tanh
2. recurrent_activation == sigmoid
3. recurrent_dropout == 0
4. unroll is False
5. use_bias is True
6. reset_after is True
7. Inputs, if use masking, are strictly right-padded.
8. Eager execution is enabled in the outermost context.

There are two variants of the GRU implementation. The default one is based on v3 and has reset gate applied to hidden state before matrix multiplication. The other one is based on original and has the order reversed.

The second variant is compatible with CuDNNGRU (GPU-only) and allows inference on CPU. Thus it has separate biases for kernel and recurrent_kernel. To use this variant, set reset_after=True and recurrent_activation=sigmoid.

**** 参数
- units: 正整数，输出空间的维度。
- activation: 要使用的激活函数 (详见 activations)。 默认：双曲正切 (tanh)。 如果传入 None，则不使用激活函数 (即 线性激活：a(x) = x)。
- recurrent_activation: 用于循环时间步的激活函数 (详见 activations)。 默认：分段线性近似 sigmoid (hard_sigmoid)。 如果传入 None，则不使用激活函数 (即 线性激活：a(x) = x)。
- use_bias: 布尔值，该层是否使用偏置向量。
- kernel_initializer: kernel 权值矩阵的初始化器， 用于输入的线性转换 (详见 initializers)。
- recurrent_initializer: recurrent_kernel 权值矩阵 的初始化器，用于循环层状态的线性转换 (详见 initializers)。
- bias_initializer:偏置向量的初始化器 (详见initializers).
- kernel_regularizer: 运用到 kernel 权值矩阵的正则化函数 (详见 regularizer)。
- recurrent_regularizer: 运用到 recurrent_kernel 权值矩阵的正则化函数 (详见 regularizer)。
- bias_regularizer: 运用到偏置向量的正则化函数 (详见 regularizer)。
- activity_regularizer: 运用到层输出（它的激活值）的正则化函数 (详见 regularizer)。
- kernel_constraint: 运用到 kernel 权值矩阵的约束函数 (详见 constraints)。
- recurrent_constraint: 运用到 recurrent_kernel 权值矩阵的约束函数 (详见 constraints)。
- bias_constraint: 运用到偏置向量的约束函数 (详见 constraints)。
- dropout: 在 0 和 1 之间的浮点数。 单元的丢弃比例，用于输入的线性转换。
- recurrent_dropout: 在 0 和 1 之间的浮点数。 单元的丢弃比例，用于循环层状态的线性转换。
- implementation: 实现模式，1 或 2。 模式 1 将把它的操作结构化为更多的小的点积和加法操作， 而模式 2 将把它们分批到更少，更大的操作中。 这些模式在不同的硬件和不同的应用中具有不同的性能配置文件。
- return_sequences: 布尔值。是返回输出序列中的最后一个输出，还是全部序列。
- return_state: 布尔值。除了输出之外是否返回最后一个状态。
- go_backwards: 布尔值 (默认 False)。 如果为 True，则向后处理输入序列并返回相反的序列。
- stateful: 布尔值 (默认 False)。 如果为 True，则批次中索引 i 处的每个样品的最后状态 将用作下一批次中索引 i 样品的初始状态。
- unroll: 布尔值 (默认 False)。 如果为 True，则网络将展开，否则将使用符号循环。 展开可以加速 RNN，但它往往会占用更多的内存。 展开只适用于短序列。
- reset_after:
- GRU 公约 (是否在矩阵乘法之前或者之后使用重置门)。 False =「之前」(默认)，Ture =「之后」( CuDNN 兼容)。
* pickle库
pickle是python语言的一个标准模块，安装python后已包含pickle库，不需要单独再安装。

#+BEGIN_EXAMPLE
pickle是Python序列化的一个工具。
序列化，通俗来讲，就是把一个Python对象转成一串字节。如果你想把Python对象保存到一个文件里，你是需要Python虚拟机支持的，不能够直接把Python对象保存进文件，更不能进行网络传输，所以序列化这个东西就产生了。
pickle可以把Python对象变成一串字符，让它可以保存到一个文件里，可以让Python对象进行网络传输，也可以从一串字符中恢复为Python对象，这样就可以把Python中虚拟的对象转化为实际的一串字符了。

如果你想存取一个对象到文件当中就可以用这个库啊。就像你在java当中如果要传输一个对象的时候，或者你想存取一个一个对象到文件当中的时候，你发现直接不像基本数据类型int，char，byte这种可以直接写，而还要实现Seriable活着时Paracle接口啥的。而现在在python当中直接用pickle.dump(obj,file,[protocol])就可以了，所以说python有的时候是很好用的，可以快速的解决一些问题，好多库可以用，大牛们都帮你实现好了，等着你去用就好了。
#+END_EXAMPLE

可以参考[[https://docs.python.org/3/library/pickle.html][官方文档]]

** dumps
To serialize an object hierarchy, you simply call the dumps() function. 

~pickle.dump(obj, file, protocol=None, *, fix_imports=True, buffer_callback=None)~

Write the pickled representation of the object obj to the open file object file. This is equivalent to Pickler(file, protocol).dump(obj).

Arguments file, protocol, fix_imports and buffer_callback have the same meaning as in the [[https://docs.python.org/3/library/pickle.html#pickle.Pickler][Pickler]] constructor.

Changed in version 3.8: The buffer_callback argument was added.
** pickle.load
~pickle.load(file, *, fix_imports=True, encoding="ASCII", errors="strict", buffers=None)~

Read the pickled representation of an object from the open file object file and return the reconstituted object hierarchy specified therein. This is equivalent to Unpickler(file).load().

The protocol version of the pickle is detected automatically, so no protocol argument is needed. Bytes past the pickled representation of the object are ignored.

Arguments file, fix_imports, encoding, errors, strict and buffers have the same meaning as in the [[https://docs.python.org/3/library/pickle.html#pickle.Unpickler][Unpickler]] constructor.

Changed in version 3.8: The buffers argument was added.

* PyAutoGUI库
** PyAutoGUI简介
pywin32直接包装了几乎所有的Windows API，可以方便地从Python直接调用，把Windows API按照功能分了一些大类，每一个大类作为一个模块，常见如win32api、win32gui、win32com等，其中win32com使用微软独门的COM接口技术进行进程间通信，可以实现控制GUI程序。但前提是，这些程序得支持COM接口。win32api则更加原始，完全通过Win32 API调用获得/生成窗口句柄、发送消息事件，十分繁琐。

PyAutoGUI的思路与此完全不同，它是接管了鼠标、键盘使用权，基本上完全照搬人的操作，底层不必套牢在Windows系统，没错，它是跨平台的。官网地址：https://github.com/asweigart/pyautogui。原本，这类GUI自动化工具的初衷是给GUI程序自动化测试用，产生点击鼠标、敲击键盘的行为，在日志中记录下消息事件和GUI程序的响应结果，事后分析GUI程序可能存在的bug。不过，既然能产生点击鼠标、敲击键盘的行为，我们就可以用来控制GUI程序批量完成文件编辑、保存工作。

按照官方的说法，PyAutoGUI给人类用的GUI自动化神器，简单高效、函数分类清晰，它被awesome-python、awesome-python-cn收录。

PyAutoGUI设计简洁，相关符号经过内部import之后，被封装在pyautogui单个模块中，因此Python程序中只要import pyautogui之后便可通过.符号访问pyautogui中的函数、变量。

pyautogui中函数大致分为通用功能、鼠标控制、键盘控制、消息窗口、截图5大类。
** 通用功能
#+begin_src python
In [1]: import pyautogui

In [2]: pyautogui.size()  # 获取屏幕尺寸（分辨率×分辨率）
Out[2]: Size(width=1920, height=1080)

In [3]: pyautogui.position() # 获取鼠标当前位置
Out[3]: Point(x=846, y=437)

In [4]: pyautogui.onScreen(100,200) # 判断坐标是否在屏幕范围内
Out[4]: True

In [5]: pyautogui.onScreen(100,2000) # 判断坐标是否在屏幕范围内
Out[5]: False
#+END_SRC
坐标体系至关重要，后续鼠标位置、图片大小都根据这套体系定义。PyAutoGUI沿用了传统的坐标体系，并未重新定义，如下图所示。x的取值范围是[0, 宽度分辨率-1]，y的取值范围是[0, 高度分辨率-1]。


#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-08_22-51-31.png @ 2021-11-08 22:51:39
[[file:PyAutoGUI%E5%BA%93/2021-11-08_22-51-39_Snipaste_2021-11-08_22-51-31.png]]
** 鼠标控制
鼠标移动，包括绝对位置移动和相对位置移动。

#+begin_src python
In [7]: sizex,sizey=pyautogui.size() # 保存屏幕尺寸

# 绝对位置移动，移动至屏幕正中心，鼠标移动过渡时间duration设为1秒
In [9]: pyautogui.moveTo(sizex/2,sizey/2,duration=1)

# 相对位置移动，向右100、向上200，鼠标移动过渡时间duration设为0.5秒
In [10]: pyautogui.moveRel(100, -200, duration=0.5)
#+END_SRC
鼠标点击，一个click()函数把点鼠标的活包干，过程也可分解为mouseDown()、mouseUp()；另有在click()之上封装的rightClick()、middleClick()、doubleClick()、tripleClick()等函数。鼠标点击之前允许指定要移动的目标位置，若目标位置不在运行当前Python程序的终端/IDE范围内，则可能对其他GUI程序触发鼠标点击事件，从而引起其响应，即焦点移动至其他GUI程序。
#+begin_src python
# 移动至屏幕中心点击一下左键，过渡时间0.5秒
In [16]: pyautogui.click(sizex/2,sizey/2, duration=0.5)

# 不指定x、y，在当前位置点击一下右键
In [17]: pyautogui.click(button='right')

# 移动至(100,100)点击3次左键，点击间隔0.1s，鼠标移动过渡时间0.5秒
In [18]: pyautogui.click(100,100, clicks=3,interval=0.1,duration=0.5)

# 移动至(100,100)点击2次右键，点击间隔0.5s，鼠标移动过渡时间0.2秒
In [19]: pyautogui.click(100,100, clicks=2,interval=0.5,button='right',duration=0.2)
#+END_SRC
滚动鼠标滚轮。
#+begin_src python
# 鼠标位置不动，向上回滚2个单位，项目文档对滚动量参数说明不详
In [22]: pyautogui.scroll(2)

# 鼠标移动至(1000,700)，前下滚动10个单位
# 运行发现鼠标并没有动
In [26]: pyautogui.scroll(-10,1000,700)
#+END_SRC
鼠标拖曳，指从当前位置按下鼠标，移动至目标位置再释放的过程，指定目标位置同样有绝对位置和相对位置两种方式，和移动鼠标函数很像。另外，试用下来，未发现drag()函数和dragRel()的差异。
#+begin_src python
# 将鼠标从当前位置拖至屏幕中心，默认左键
In [32]: pyautogui.dragTo(sizex/2,sizey/2)

# 将鼠标从当前位置向左100像素、向右200像素拖动，过渡时间0.5秒，指定右键
In [33]: pyautogui.dragRel(-100,200,duration=0.5,button='right')
#+END_SRC
** 键盘控制

控制按键，也是一个press()函数基本把活包干，按键动作往细分解包含keyDown()和keyUp()两个过程；在此基础上封装，有typewrite()和hotkey()两个高阶一点的函数，分别用于输入字符串和按快捷键。
#+begin_src python
# 键名用字符串表示，支持的所有键名，存在pyautogui.KEYBOARD_KEYS变量中，包括26个字母、数字、符号、F1~F20、方向等等所有按键
In [4]: pyautogui.press('a') # 按字母A键，字母支持大小写

# 程序向终端输入了字符a，若程序运行时输入法为中文状态，由于没有继续输入空格或回车，输入法仅列出候选字，并不会输入到终端
In [5]: a 

# 传入键名列表（按键p、按键y、空格），按键之间间隔0.1秒（默认0）
In [6]: pyautogui.press(['p','y','space'], interval=0.1)

# 运行前将输入法切换到中文状态，往终端直接输入了“培养”
In [7]: 培养

# typewrite方式一：传入字符串，不支持中文字符，因为函数无法知道输入法需要什么按键才能得到中文字符
In [9]: pyautogui.typewrite('hello, PyAutoGUI!\n')

# 程序把字符串"'hello, PyAutoGUI!"和换行符输入到了终端
In [10]: hello, PyAutoGUI!
    ...:

# typewrite方式二：传入键名列表，按键之间间隔0.1秒（默认0）
In [11]: pyautogui.typewrite(['s','r','f','space'], interval=0.1)

# 运行前将输入法切换到中文状态，往终端直接输入了“输入法”3个字
In [12]: 输入法

# 大小写字母是自动支持的，仍然尝试一次切换到大写
In [13]: pyautogui.typewrite(['capslock','p','y'])

# CapsLock按键灯被点亮，程序往终端输入了"PY"
In [14]: PY

# hotkey屏蔽了需要反复keyDown、keyUp的细节，参数是任意个键名，而非列表
In [18]: pyautogui.hotkey('ctrl', 'shift', 'esc') #调出任务管理器

In [19]:pyautogui.hotkey('alt','ctrl','delete') # 并未调出重启界面
#+END_SRC
** 消息窗口

PyAutoGUI利用pymsgbox的功能，以JavaScript风格函数提供消息框功能，包括alert()、confirm()、prompt() 、password()，连参数都是一致的，熟悉JavaScript的朋友不会陌生。
#+begin_src python
In [24]: pyautogui.alert(text='警告',title='PyAutoGUI消息框',button='OK')
Out[24]: 'OK' # 点击的按键被返回

In [28]: pyautogui.confirm(text='请选择',title='PyAutoGUI消息框',buttons=['1','2'
    ...: ,'3'])
Out[28]: '2' # 点击的按键被返回

In [30]: pyautogui.prompt(text='请输入',title='PyAutoGUI消息框',default='请输入')
Out[30]: 'input by 伪码人' # 点OK按钮后返回输入内容

In [32]: pyautogui.password(text='输入密码',title='PyAutoGUI消息框',default='',mask='*')
Out[32]: 'We_Coder' # 点OK按钮后返回输入内容
#+END_SRC

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-08_22-53-51.png @ 2021-11-08 22:53:56
[[file:PyAutoGUI%E5%BA%93/2021-11-08_22-53-56_Snipaste_2021-11-08_22-53-51.png]]
** 截图相关

PyAutoGUI提供了screenshot()函数进行屏幕截图，返回是Image对象，这是在Pillow库中定义的，因此需要安装Pillow库才能正常工作。
#+begin_src python
# imageFilename参数，截图要保存的文件全路径名，默认`None`，不保存；
# region参数，截图区域，由左上角坐标、宽度、高度4个值确定，如果指定区域超出了屏幕范围，超出部分会被黑色填充，默认`None`,截全屏
In [41]: pyautogui.screenshot('shot.png',region=(1000,600,600,400))
Out[41]: <PIL.Image.Image image mode=RGB size=600x400 at 0x20C87497390>
#+END_SRC

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-08_22-54-30.png @ 2021-11-08 22:54:35
[[file:PyAutoGUI%E5%BA%93/2021-11-08_22-54-35_Snipaste_2021-11-08_22-54-30.png]]

PyAutoGUI还有个图片匹配功能，它是在屏幕按像素匹配，定位图片在屏幕上的坐标位置，locateOnScreen()函数返回region对象,即左上角坐标、宽度、高度4个值组成的元组，再用center()函数计算出中心坐标，locateCenterOnScreen()函数则一步到位，返回中心坐标。如果把需要点击的菜单、按钮事先保存成图片，可以用来自动查找菜单、按钮位置，再交由click()函数控制鼠标去点击。
#+begin_src python
loc = pyautogui.locateCenterOnScreen("icon_xx.png", region=(0, 0,sizex/2, sizey/10) ) # region参数限制查找范围，加快查找速度
pyautogui.moveTo(*loc, duration=0.5) # 移动鼠标
pyautogui.click(clicks=1) #点击
#+END_SRC
** 参考文档
[[https://zhuanlan.zhihu.com/p/302592540][Python自动操作GUI神器PyAutoGUI]]
* selenium 库
** 下载浏览器驱动
Firefox浏览器驱动：[[https://github.com/mozilla/geckodriver/releases][geckodriver]]

Chrome浏览器驱动：[[https://sites.google.com/a/chromium.org/chromedriver/home][chromedriver]] ,[[https://link.zhihu.com/?target=https%253A//npm.taobao.org/mirrors/chromedriver][taobao备用地址]]

IE浏览器驱动：[[https://link.zhihu.com/?target=http%253A//selenium-release.storage.googleapis.com/index.html][IEDriverServer]]

Edge浏览器驱动：[[https://link.zhihu.com/?target=https%253A//developer.microsoft.com/en-us/microsoft-edge/tools/webdriver][MicrosoftWebDriver]]

Opera浏览器驱动：[[https://link.zhihu.com/?target=https%253A//github.com/operasoftware/operachromiumdriver/releases][operadriver]]

PhantomJS浏览器驱动：[[https://link.zhihu.com/?target=http%253A//phantomjs.org/][phantomjs]]

需要把浏览器驱动放入系统路径中，或者直接告知selenuim的驱动路径

另外启动浏览器，可以设置一些参数，比如无界面之类的，详细参考：[[https://zhuanlan.zhihu.com/p/60852696][selenium启动Chrome配置参数问题]]
补充上面一个参数，添加代理：options.add_argument("--proxy-server=http://XXXXX.com:80")
可以测试是否正常使用，以下代码：
#+begin_src python
from selenium import webdriver


driver = webdriver.Firefox()   # Firefox浏览器
# driver = webdriver.Firefox("驱动路径")

driver = webdriver.Chrome()    # Chrome浏览器

driver = webdriver.Ie()        # Internet Explorer浏览器

driver = webdriver.Edge()      # Edge浏览器

driver = webdriver.Opera()     # Opera浏览器

driver = webdriver.PhantomJS()   # PhantomJS

# 打开网页
driver.get(url) # 打开url网页 比如 driver.get("http://www.baidu.com")
#+END_SRC
** 用selenium 登录新版的edge方法

首先，安装pip install msedge-selenium-tools

接下来，使用以下代码：
#+begin_src python
from selenium import webdriver
from msedge.selenium_tools import Edge, EdgeOptions
options = EdgeOptions()
options.use_chromium = True
options.binary_location = r"C:\xx\Microsoft\EdgeCore\93.0.926.0\msedge.exe" # 浏览器的位置
driver = Edge(options=options, executable_path=r"D:\xx\Desktop\edgedriver_win64\msedgedriver.exe") # 相应的浏览器的驱动位置
​
driver.get("www.baidu.com")
#+END_SRC
在element变成elements就是找所有满足的条件，返回数组。

一般我都自己采用 xpath 获取元素的，复制即可

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-05_15-01-12.png @ 2021-11-05 15:01:51
[[file:selenium_%E5%BA%93/2021-11-05_15-01-51_Snipaste_2021-11-05_15-01-12.png]]
关于获取对象的api查找：[[https://www.selenium.dev/selenium/docs/api/py/webdriver_remote/selenium.webdriver.remote.webelement.html][selenium.webdriver.remote.webelement]]

** 导入Selenium库
# 导入Selenium驱动
from selenium import webdriver
** 创建浏览器对象
即：打开一个浏览器。
#+begin_src python
# 语法：driver = webdriver.xxx()
driver = webdriver.Chrome()
# 使用dir(driver)查看浏览器对象的操作
print(dir(driver))
#+END_SRC
** 浏览器窗口大小设置
#+begin_src python
# 1.设置浏览器尺寸
# 宽480、高800(最好根据显示器分别调整)
driver.set_window_size(480, 800)
# 2.获取浏览器尺寸
driver.get_window_size()		
# 3.浏览器窗口最大化（很常用）
driver.maximize_window()
#+END_SRC

示例：

1.学习目标
    掌握selenium中控制浏览器窗口大小的方法
2.操作步骤（方法）
    2.1设置浏览器窗口大小，宽度，高度
        driver.set_window_size（宽，高）
    2.2 获取浏览器窗口大小
        driver.get_window_size()
    2.3将浏览器窗口最大化
        driver.maximize_window）
3.需求
    使用selenium实现对浏览器窗口大小的设置
#+begin_src python
# 1.导入seleniun
from selenium import webdriver
from time import sleep
# 2.打开谷歌浏览器（获取浏览器操作对象）
driver = webdriver.Chrome()
# 3.设置浏览器窗口大小
# 3.1 将窗口设置为宽100，高200
# (windowHandle参数为窗口句柄，以后再说)
driver.set_window_size(100, 200)
sleep(3)
# 3.2 获取浏览器窗口大小
window_size = driver.get_window_size()
print(window_size)
# 3.3 窗口最大化
driver.maximize_window()
# 4.关闭浏览器
driver.quit()

#+END_SRC
输出结果：
#+BEGIN_EXAMPLE
{'width': 516, 'height': 200}
#+END_EXAMPLE
** 浏览器位置设置
#+begin_src python
# 1.获取浏览器位置
driver.get_window_position()		
# 2.设置浏览器位置
driver.set_window_position(x,y)		
#+END_SRC
注：显示器以左上角为(0,0)，所有的位置操作都是相对于显示器左上角展开的位移操作，单位是像素。

示例：

1.学习目标
    掌握selenium中控制浏览器窗口位置的方法
2.操作步骤（方法）
    2.1 设置浏览器窗口位置（横纵坐标）
        set_window_position(横坐标，纵坐标)
    2.2 获取浏览器窗口位置
        driver.get_window_position()
3.需求
    使用selenium实现对浏览器窗口位置的设置
#+begin_src python
# 1.导入seleniun
from selenium import webdriver
from time import sleep
# 2.打开谷歌浏览器（获取浏览器操作对象）
driver = webdriver.Chrome()
# 3.设置浏览器位置
# 3.1 将窗口的位置设置为100，300
driver.set_window_position(100, 300)
sleep(2)
# 3.2 获取浏览器窗口位置
window_position = driver.get_window_position()
print(window_position)
# 4.关闭浏览器
driver.quit()
#+END_SRC
输出结果：
#+BEGIN_EXAMPLE
{'x': 100, 'y': 300}
#+END_EXAMPLE
** 请求访问网址
#+begin_src python
# 请求某个url
# 语法：driver.get(url)	
# 工作中写法
url = "http://www.baidu.com"
driver.get(url)
#+END_SRC

示例：

1.学习目标
    掌握selenium中控制浏览器访问指定网站的操作
2.操作步骤（方法）
    请求某个url
    语法：driver.get(url)
3.需求
    使用selenium实现对浏览器访问指定网站的操作
#+begin_src python
# 1.导入seleniun
from selenium import webdriver
from time import sleep
# 2.打开谷歌浏览器（获取浏览器操作对象）
driver = webdriver.Chrome()
# 3.访问网站
url = "http://www.baidu.com"
driver.get(url)
sleep(2)
# 4.关闭浏览器
driver.quit()
#+END_SRC
** 浏览器页面前进、后退和刷新
#+begin_src python
# 1.页面前进
driver.forward()
# 2.页面后退
driver.back()
# 3.页面刷新
driver.refresh()
#+END_SRC

示例：

1.学习目标
    掌握selenium控制浏览器的前进，后退，刷新
2.操作步骤（语法）
    2.1前进
        driver.forward（）
    2.2后退
        driver.back（）
    2.3刷新
        driver.refresh（）
3.需求
    使用谷歌浏览器分别打开百度，京东，淘宝，使用前进，后退，刷新方法
#+begin_src python
# 1.导入selenium
from selenium import webdriver
from time import sleep
# 2.打开浏览器---谷歌浏览器
driver = webdriver.Chrome()
# 3.窗口最大化
driver.maximize_window()
sleep(2)
# 4.输入网址百度，京东，淘宝
driver.get("http://www.baidu.com")
sleep(2)
driver.get("http://www.jd.com")
sleep(2)
driver.get("http://www.taobao.com")
sleep(2)
# 5.使用前进，后退，刷新命令
# 前进
driver.back()  # 后退到京东
sleep(2)
driver.back()  # 后退到百度
sleep(2)
# 后退
driver.forward()  # 前进到京东
sleep(2)
driver.forward()  # 前进到淘宝
sleep(2)
# 刷新
driver.refresh()  # 保持在淘宝页面
sleep(2)
# 6.关闭浏览器
driver.quit()
#+END_SRC
** 元素定位
#+BEGIN_EXAMPLE
find_element_by_id()
find_element_by_name()
find_element_by_class_name()
find_element_by_tag_name()
find_element_by_link_text()
find_element_by_partial_link_text()
find_element_by_xpath()
find_element_by_css_selector()
#+END_EXAMPLE
** Webelement常用方法
点击和输入
#+begin_src python
driver.find_element_by_id("kw").clear() # 清楚文本 driver.find_element_by_id("kw").send_keys("selenium") # 模拟按键输入 driver.find_element_by_id("su").click() # 单机元素
#+END_SRC

提交

可以在搜索框模拟回车操作
#+begin_src python
search_text = driver.find_element_by_id('kw') search_text.send_keys('selenium') search_text.submit()
#+END_SRC

其他

size： 返回元素的尺寸。

text： 获取元素的文本。

get_attribute(name)： 获得属性值。

is_displayed()： 设置该元素是否用户可见。
** 鼠标操作
在 WebDriver 中， 将这些关于鼠标操作的方法封装在 ActionChains 类提供。

ActionChains 类提供了鼠标操作的常用方法：
- perform()： 执行所有 ActionChains 中存储的行为；
- context_click()： 右击；
- double_click()： 双击；
- drag_and_drop()： 拖动；
- move_to_element()： 鼠标悬停。

举个例子：
#+begin_src python
from selenium import webdriver
# 引入 ActionChains 类
from selenium.webdriver.common.action_chains import ActionChains

driver = webdriver.Chrome()
driver.get("https://www.baidu.cn")

# 定位到要悬停的元素
above = driver.find_element_by_link_text("设置")
# 对定位到的元素执行鼠标悬停操作
ActionChains(driver).move_to_element(above).perform()
#+END_SRC
** 键盘事件
以下为常用的键盘操作：
- send_keys(Keys.BACK_SPACE) 删除键（BackSpace）
- send_keys(Keys.SPACE) 空格键(Space)
- send_keys(Keys.TAB) 制表键(Tab)
- send_keys(Keys.ESCAPE) 回退键（Esc）
- send_keys(Keys.ENTER) 回车键（Enter）
- send_keys(Keys.CONTROL,'a') 全选（Ctrl+A）
- send_keys(Keys.CONTROL,'c') 复制（Ctrl+C）
- send_keys(Keys.CONTROL,'x') 剪切（Ctrl+X）
- send_keys(Keys.CONTROL,'v') 粘贴（Ctrl+V）
- send_keys(Keys.F1) 键盘 F1
- ……
- send_keys(Keys.F12) 键盘 F12
#+begin_src python
# 输入框输入内容
driver.find_element_by_id("kw").send_keys("seleniumm")

# 删除多输入的一个 m
driver.find_element_by_id("kw").send_keys(Keys.BACK_SPACE)
#+END_SRC
** 获取断言信息
#+begin_src python
title = driver.title # 打印当前页面title
now_url = driver.current_url # 打印当前页面URL
user = driver.find_element_by_class_name('nums').text # # 获取结果数目
#+END_SRC
** 等待页面加载完成
*** 显示等待
显式等待使WebdDriver等待某个条件成立时继续执行，否则在达到最大时长时抛出超时异常（TimeoutException）。
#+begin_src python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

driver = webdriver.Firefox()
driver.get("http://www.baidu.com")

element = WebDriverWait(driver, 5, 0.5).until(
                      EC.presence_of_element_located((By.ID, "kw"))
                      )
element.send_keys('selenium')
driver.quit()
#+END_SRC
WebDriverWait类是由WebDirver 提供的等待方法。在设置时间内，默认每隔一段时间检测一次当前页面元素是否存在，如果超过设置时间检测不到则抛出异常。具体格式如下：

WebDriverWait(driver, timeout, poll_frequency=0.5, ignored_exceptions=None)

- driver ：浏览器驱动。
- timeout ：最长超时时间，默认以秒为单位。
- poll_frequency ：检测的间隔（步长）时间，默认为0.5S。
- ignored_exceptions ：超时后的异常信息，默认情况下抛NoSuchElementException异常。
- WebDriverWait()一般由until()或until_not()方法配合使用，下面是until()和until_not()方法的说明。
- until(method, message=‘’) 调用该方法提供的驱动程序作为一个参数，直到返回值为True。
- until_not(method, message=‘’) 调用该方法提供的驱动程序作为一个参数，直到返回值为False。

在本例中，通过as关键字将expected_conditions 重命名为EC，并调用presence_of_element_located()方法判断元素是否存在。
*** 隐式等待
如果某些元素不是立即可用的，隐式等待是告诉WebDriver去等待一定的时间后去查找元素。 默认等待时间是0秒，一旦设置该值，隐式等待是设置该WebDriver的实例的生命周期。
#+begin_src python
from selenium import webdriver
driver = webdriver.Firefox()    
driver.implicitly_wait(10) # seconds    
driver.get("http://somedomain/url_that_delays_loading")    
myDynamicElement = driver.find_element_by_id("myDynamicElement") 
#+END_SRC
** 在不同的窗口和框架之间移动
#+begin_src python
driver.switch_to_window("windowName")
driver.switch_to_frame("frameName")
#+END_SRC

以直接取表单的id 或name属性。如果iframe没有可用的id和name属性，则可以通过下面的方式进行定位。
#+begin_src python
#先通过xpth定位到iframe
xf = driver.find_element_by_xpath('//*[@id="x-URS-iframe"]')

#再将定位对象传给switch_to_frame()方法
driver.switch_to_frame(xf)
#+END_SRC
一旦我们完成了frame中的工作，我们可以这样返回父frame:
#+begin_src python
driver.switch_to_default_content()
#+END_SRC
** 警告框处理
#+begin_src python
alert = driver.switch_to_alert()
text：返回 alert/confirm/prompt 中的文字信息。
accept()：接受现有警告框。
dismiss()：解散现有警告框。
send_keys(keysToSend)：发送文本至警告框。keysToSend：将文本发送至警告框。
#+END_SRC
** 下拉框选择
#+begin_src python
from selenium import webdriver
from selenium.webdriver.support.select import Select
from time import sleep

driver = webdriver.Chrome()
driver.implicitly_wait(10)
driver.get('http://www.baidu.com')
sel = driver.find_element_by_xpath("//select[@id='nr']")
Select(sel).select_by_value('50')  # 显示50条
#+END_SRC
** 文件上传
#+begin_src python
driver.find_element_by_name("file").send_keys('D:\\upload_file.txt')  
# 定位上传按钮，添加本地文件
#+END_SRC
** cookie操作
WebDriver操作cookie的方法：
#+begin_src python
get_cookies()： 获得所有cookie信息。
get_cookie(name)： 返回字典的key为“name”的cookie信息。
add_cookie(cookie_dict) ： 添加cookie。“cookie_dict”指字典对象，必须有name 和value 值。
delete_cookie(name,optionsString)：删除cookie信息。“name”是要删除的cookie的名称，“optionsString”是该cookie的选项，目前支持的选项包括“路径”，“域”。
delete_all_cookies()： 删除所有cookie信息
#+END_SRC
** 调用JavaScript代码
#+begin_src python
js="window.scrollTo(100,450);"
driver.execute_script(js) # 通过javascript设置浏览器窗口的滚动条位置
#+END_SRC
通过execute_script()方法执行JavaScripts代码来移动滚动条的位置。
** 窗口截图
#+begin_src python
driver.get_screenshot_as_file("D:\\baidu_img.jpg") 
# 截取当前窗口，并指定截图图片的保存位置
#+END_SRC
** 直接用cookie登录方法
链接：https://www.jianshu.com/p/773c58406bdb

1. 手动获取网页的cookie，将其序列化并存储在本地
2. 写入代码
#+begin_src python
for item in cookies:
    driver.add_cookie(item)
#+END_SRC

** 关闭浏览器
（1）关闭当前窗口
#+begin_src python
# 只关闭当前浏览器窗口
driver.close()
#+END_SRC

（2）退出驱动并关闭所有关联的窗口
#+begin_src python
# 即关闭浏览器窗口，同时关闭浏览器驱动
driver.quit()
#+END_SRC

* imageio库
IMAIO是一个Python库，它提供了一个简单的接口来读取和写入大量的图像数据，包括动画图像、体积数据和科学格式。它是跨平台的，运行在Python 2 .x和3。x上，并且易于安装。

** imageio.imread
~imageio.imread(uri, format=None, **kwargs)~
Reads an image from the specified file. Returns a numpy array, which comes with a dict of meta data at its ‘meta’ attribute.

Note that the image data is returned as-is, and may not always have a dtype of uint8 (and thus may differ from what e.g. PIL returns).

从指定的文件读取图像。返回一个NUMPY数组，该数组带有元数据的元属性。注意，图像数据按原样返回，并且可能不总是具有uTI8的dType（因此可能不同于例如PIL返回）。
*** Parameters
- uri:{str, pathlib.Path, bytes, file}
The resource to load the image from, e.g. a filename, pathlib.Path, http address or file object, see the docs for more info.

- format:str
The format to use to read the file. By default imageio selects the appropriate for you based on the filename and its contents.

- kwargs:…
Further keyword arguments are passed to the reader. See help() to see what arguments are available for a particular format.

* urlparse 模块
urlparse模块主要是用于解析url中的参数对url按照一定格式进行拆分或拼接 

urlparse.urlparse

将url分为6个部分，返回一个包含6个字符串项目的元组：协议、位置、路径、参数、查询、片段。
#+BEGIN_SRC python
import urlparse
url_change = urlparse.urlparse('https://i.cnblogs.com/EditPosts.aspx?opt=1')
print url_change
#+END_SRC
输出结果为：
#+BEGIN_EXAMPLE
ParseResult(scheme='https', netloc='i.cnblogs.com', path='/EditPosts.aspx', params='', query='opt=1', fragment='')
#+END_EXAMPLE
其中 scheme 是协议  netloc 是域名服务器  path 相对路径  params是参数，query是查询的条件

urlparse.parse_qs(urlparse.urlparse(url).query)

这个是获取urlparse分割后元祖中的某一项  urlparse.urlparse(url).query   获取查询条件

parse_qs 有几种实现

urlparse.parse_qs 返回字典
urlparse.parse_qsl 返回列表

* json
** json.dumps()
json.dumps将一个Python数据结构转换为JSON
#+BEGIN_SRC python
import json
data = {
    'name' : 'myname',
    'age' : 100,
}
json_str = json.dumps(data)
#+END_SRC
** json库的一些用法
| 方法                            | 作用                                                         |
|---------------------------------+--------------------------------------------------------------|
| json.dumps()                    | 将python对象编码成Json字符串                                 |
| json.loads()                    | 将Json字符串解码成python对象                                 |
| json.dump()                     | 将python中的对象转化成json储存到文件中                       |
| json.load()                     | 将文件中的json的格式转化成python对象提取出来                 |
| json.dump()和json.dumps()的区别 |                                                              |
| json.dumps()                    | 是把python对象转换成json对象的一个过程，生成的是字符串。     |
| json.dump()                     | 是把python对象转换成json对象生成一个fp的文件流，和文件相关。 |
** json参数
#+BEGIN_SRC 
json.dumps(obj, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, encoding="utf-8", default=None, sort_keys=False, **kw)
#+END_SRC
- obj:转化成json的对象。
- sort_keys =True:是告诉编码器按照字典排序(a到z)输出。如果是字典类型的python对象，就把关键字按照字典排序。
- indent:参数根据数据格式缩进显示，读起来更加清晰。
- separators:是分隔符的意思，参数意思分别为不同dict项之间的分隔符和dict项内key和value之间的分隔符，把：和，后面的空格都除去了。

#+BEGIN_SRC python
import json

x = {'name':'你猜','age':19,'city':'四川'}
#用dumps将python编码成json字符串
y = json.dumps(x)
print(y)
i = json.dumps(x,separators=(',',':'))
print(i)
# 输出结果
{"name": "\u4f60\u731c", "age": 19, "city": "\u56db\u5ddd"}
{"name":"\u4f60\u731c","age":19,"city":"\u56db\u5ddd"}
#+END_SRC
- skipkeys：默认值是False，如果dict的keys内的数据不是python的基本类型(str,unicode,int,long,float,bool,None)，设置为False时，就会报TypeError的错误。此时设置成True，则会跳过这类key 。
- ensure_ascii=True：默认输出ASCLL码，如果把这个该成False,就可以输出中文。
- check_circular：如果check_circular为false，则跳过对容器类型的循环引用检查，循环引用将导致溢出错误(或更糟的情况)。
- allow_nan：如果allow_nan为假，则ValueError将序列化超出范围的浮点值(nan、inf、-inf)，严格遵守JSON规范，而不是使用JavaScript等价值(nan、Infinity、-Infinity)。
- default：default(obj)是一个函数，它应该返回一个可序列化的obj版本或引发类型错误。默认值只会引发类型错误。

* unicodedata 
分类小分队
unicodedata.category(chr)
unicodedata.bidirectional(chr)
unicodedata.east_asian_width(chr)

在 unicode 中，有多种分类标准，字符在各个分类标准中都有自己的的分类定位。以上三个函数是查询字符分别在三种分类标准下的分类名称。

参数:
    chr: str，字符
返回值:
    str，分类名称

unicodedata.category(chr) 可以获取指定字符在常规分类中所属的分类名称，unicodedata.bidirectional(chr) 可以获取指定字符的双向字符类型名称，unicodedata.east_asian_width(chr)可以获取指定字符的字符宽度分类名称。
* 问题集锦
** python3 报错TypeError: 'range' object does not support item assignment
尝试使用range() 创建整数列表（导致“TypeError: ‘range’ object does not support item assignment”）有时你想要得到一个有序的整数列表，所以range() 看上去是生成此列表的不错方式。然而，你需要记住range() 返回的是“range object”，而不是实际的list 值。

~nums = range(5)改为nums = list(range(5))~
** Python读取中文文件：解决: 'ascii' codec can't decode byte 0xe6 in position 2: ordinal not in range(128)
利用iO的open函数读取中文文件，在read或readline这一步直接会报’ascii’ codec can’t decode byte 0xe6 in position 2: ordinal not in range(128)错误
所以Python3 利用IO的open读取中文文件如果卡在这一步，可以用如下方法

第一种：open函数指名encoding
#+begin_src python
open('stopwords.txt', encoding = 'utf-8')
#+END_SRC


第二种：codecs调用open 指名 encoding
#+begin_src python
f = codecs.open('文件路径','r+',encoding=编码格式)
text = f.readline()
f.close()
#+END_SRC

** 怎么把整个网页爬取下来并保存为pdf
*** 准备工作
**** 软件安装
由于我们是要把html转为pdf，所以需要手动wkhtmltopdf 。Windows平台直接在 http://wkhtmltopdf.org/downloads.html 下载稳定版的 wkhtmltopdf 进行安装，安装完成之后把该程序的执行路径加入到系统环境 $PATH 变量中，否则 pdfkit 找不到 wkhtmltopdf 就出现错误 “No wkhtmltopdf executable found”。Ubuntu 和 CentOS 可以直接用命令行进行安装
#+begin_src bash
$ sudo apt-get install wkhtmltopdf  # ubuntu
$ sudo yum intsall wkhtmltopdf      # centos
#+END_SRC
**** 库安装
#+begin_src bash
pip install requests # 用于网络请求
pip install beautifulsoup4 # 用于操作html
pip install pdfkit # wkhtmltopdf 的Python封装包
pip install PyPDF2 # 用于合并pdf
#+END_SRC
*** 爬取内容
本文的目标网址为：http://python3-cookbook.readthedocs.io/zh_CN/latest/ 。
**** 获取教程名称
页面的左边一栏为目录，按F12调出开发者工具并按以下步骤定位到目录元素：
1. 点击开发者工具左上角"选取页面元素"按钮；
2. 用鼠标点击左上角教程名称处。

通过以上步骤即可定位到目录元素，用图说明：

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-23_16-46-51.png @ 2021-11-23 16:46:57
[[file:%E9%97%AE%E9%A2%98%E9%9B%86%E9%94%A6/2021-11-23_16-46-57_Snipaste_2021-11-23_16-46-51.png]]

从图看到我们需要的教程名称包含在<div class="wy-side-nav-search"></div>之间的a标签里。假设我们已经获取到了网页内容为html，可以使用以下代码获取该内容：
#+begin_src python
book_name = soup.find('div', class_='wy-side-nav-search').a.text
#+END_SRC
**** 获取目录及对应网址
使用相同的步骤来获取：

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-23_16-47-38.png @ 2021-11-23 16:47:43
[[file:%E9%97%AE%E9%A2%98%E9%9B%86%E9%94%A6/2021-11-23_16-47-43_Snipaste_2021-11-23_16-47-38.png]]

从图看到我们需要的目录包含在<div class="section"></div>之间，<li class="toctree-l1"></li>标签里为一级目录及网址；<li class="toctree-l2"></li>标签里为二级目录及网址。当然这个url是相对的url，前面还要拼接http://python3-cookbook.readthedocs.io/zh_CN/latest/。

使用BeautifulSoup进行数据的提取：
#+begin_src python
# 全局变量
base_url = 'http://python3-cookbook.readthedocs.io/zh_CN/latest/'
book_name = ''
chapter_info = []

def parse_title_and_url(html):
    """
    解析全部章节的标题和url
    :param html: 需要解析的网页内容
    :return None
    """
    soup = BeautifulSoup(html, 'html.parser')

    # 获取书名
    book_name = soup.find('div', class_='wy-side-nav-search').a.text
    menu = soup.find_all('div', class_='section')
    chapters = menu[0].div.ul.find_all('li', class_='toctree-l1')
    for chapter in chapters:
        info = {
    }
        # 获取一级标题和url
        # 标题中含有'/'和'*'会保存失败
        info['title'] = chapter.a.text.replace('/', '').replace('*', '')
        info['url'] = base_url + chapter.a.get('href')
        info['child_chapters'] = []

        # 获取二级标题和url
        if chapter.ul is not None:
            child_chapters = chapter.ul.find_all('li')
            for child in child_chapters:
                url = child.a.get('href')
                # 如果在url中存在'#'，则此url为页面内链接，不会跳转到其他页面
                # 所以不需要保存
                if '#' not in url:
                    info['child_chapters'].append({
    
                        'title': child.a.text.replace('/', '').replace('*', ''),
                        'url': base_url + child.a.get('href'),
                    })

        chapter_info.append(info)
#+END_SRC
代码中定义了两个全局变量来保存信息。章节内容保存在chapter_info列表里，里面包含了层级结构，大致结构为：
#+BEGIN_EXAMPLE
[
    {
    
        'title': 'first_level_chapter',
        'url': 'www.xxxxxx.com',
        'child_chapters': [
            {
    
                'title': 'second_level_chapter',
            	'url': 'www.xxxxxx.com',
            }
            ...            
        ]
    }
    ...
]
#+END_EXAMPLE
**** 获取章节内容
还是同样的方法定位章节内容：

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-23_16-48-55.png @ 2021-11-23 16:56:34
[[file:%E9%97%AE%E9%A2%98%E9%9B%86%E9%94%A6/2021-11-23_16-56-34_Snipaste_2021-11-23_16-48-55.png]]

代码中我们通过itemprop这个属性来定位，好在一级目录内容的元素位置和二级目录内容的元素位置相同，省去了不少麻烦。
#+begin_src python
html_template = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
</head>
<body>
{content}
</body>
</html>
"""

def get_content(url):
    """
    解析URL，获取需要的html内容
    :param url: 目标网址
    :return: html
    """
    html = get_one_page(url)
    soup = BeautifulSoup(html, 'html.parser')
    content = soup.find('div', attrs={
    'itemprop': 'articleBody'})
    html = html_template.format(content=content)
    return html
#+END_SRC
**** 保存pdf
#+begin_src python
def save_pdf(html, filename):
    """
    把所有html文件保存到pdf文件
    :param html:  html内容
    :param file_name: pdf文件名
    :return:
    """
    options = {
    
        'page-size': 'Letter',
        'margin-top': '0.75in',
        'margin-right': '0.75in',
        'margin-bottom': '0.75in',
        'margin-left': '0.75in',
        'encoding': "UTF-8",
        'custom-header': [
            ('Accept-Encoding', 'gzip')
        ],
        'cookie': [
            ('cookie-name1', 'cookie-value1'),
            ('cookie-name2', 'cookie-value2'),
        ],
        'outline-depth': 10,
    }

    pdfkit.from_string(html, filename, options=options)

def parse_html_to_pdf():
    """
    解析URL，获取html，保存成pdf文件
    :return: None
    """
    try:
        for chapter in chapter_info:
            ctitle = chapter['title']
            url = chapter['url']
            # 文件夹不存在则创建（多级目录）
            dir_name = os.path.join(os.path.dirname(__file__), 'gen', ctitle)
            if not os.path.exists(dir_name):
                os.makedirs(dir_name)

            html = get_content(url)

            padf_path = os.path.join(dir_name, ctitle + '.pdf')
            save_pdf(html, os.path.join(dir_name, ctitle + '.pdf'))

            children = chapter['child_chapters']
            if children:
                for child in children:
                    html = get_content(child['url'])
                    pdf_path = os.path.join(dir_name, child['title'] + '.pdf')
                    save_pdf(html, pdf_path)

    except Exception as e:
        print(e)
#+END_SRC
**** 合并pdf
经过上一步，所有章节的pdf都保存下来了，最后我们希望留一个pdf，就需要合并所有pdf并删除单个章节pdf。
#+begin_src python
from PyPDF2 import PdfFileReader, PdfFileWriter

def merge_pdf(infnList, outfn):
    """
    合并pdf
    :param infnList: 要合并的PDF文件路径列表
    :param outfn: 保存的PDF文件名
    :return: None
    """
    pagenum = 0
    pdf_output = PdfFileWriter()

    for pdf in infnList:
        # 先合并一级目录的内容
        first_level_title = pdf['title']
        dir_name = os.path.join(os.path.dirname(
            __file__), 'gen', first_level_title)
        padf_path = os.path.join(dir_name, first_level_title + '.pdf')

        pdf_input = PdfFileReader(open(padf_path, 'rb'))
        # 获取 pdf 共用多少页
        page_count = pdf_input.getNumPages()
        for i in range(page_count):
            pdf_output.addPage(pdf_input.getPage(i))

        # 添加书签
        parent_bookmark = pdf_output.addBookmark(
            first_level_title, pagenum=pagenum)

        # 页数增加
        pagenum += page_count

        # 存在子章节
        if pdf['child_chapters']:
            for child in pdf['child_chapters']:
                second_level_title = child['title']
                padf_path = os.path.join(dir_name, second_level_title + '.pdf')

                pdf_input = PdfFileReader(open(padf_path, 'rb'))
                # 获取 pdf 共用多少页
                page_count = pdf_input.getNumPages()
                for i in range(page_count):
                    pdf_output.addPage(pdf_input.getPage(i))

                # 添加书签
                pdf_output.addBookmark(
                    second_level_title, pagenum=pagenum, parent=parent_bookmark)
                # 增加页数
                pagenum += page_count

    # 合并
    pdf_output.write(open(outfn, 'wb'))
    # 删除所有章节文件
    shutil.rmtree(os.path.join(os.path.dirname(__file__), 'gen'))
#+END_SRC
本来PyPDF2库中有一个类PdfFileMerger专门用来合并pdf，但是在合并过程中会抛出异常，网上有人也遇到同样的问题，解决办法是修改库源码，本着“不动库源码”的理念，毅然选择了上面这种比较笨的办法，代码还是比较好理解的。

经过以上几个步骤，我们想要的pdf文件已经生成.

*** 完整代码

#+begin_src python
# -*- coding: utf-8 -*-

import os
import shutil
import requests
from requests.exceptions import RequestException
from bs4 import BeautifulSoup
import pdfkit
from PyPDF2 import PdfFileReader, PdfFileWriter

html_template = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
</head>
<body>
{content}
</body>
</html>
"""

base_url = 'http://python3-cookbook.readthedocs.io/zh_CN/latest/'
book_name = ''
chapter_info = []

def get_one_page(url):
    """
    获取网页html内容并返回
    :param url: 目标网址
    :return html
    """
    # headers可用可不用
    # headers = {
    #     'User-Agent': """Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36(KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36"""
    # }

    try:
        # 获取网页内容，返回html格式数据
        # response = requests.get(url, headers=headers) #需要headers时用到
        response = requests.get(url) #不需要headers时用到
        # 通过状态码判断是否获取成功
        if response.status_code == 200:
            # 指定编码，否则中文出现乱码
            response.encoding = 'utf-8'
            return response.text
        return None
    except RequestException as e:
        return None
# 这个get_one_page代码比较少，也是可以用的        
# def get_one_page(url):
#     return requests.get(url).content


def parse_title_and_url(html):
    """
    解析全部章节的标题和url
    :param html: 需要解析的网页内容
    :return None
    """
    soup = BeautifulSoup(html, 'html.parser')

    # 获取书名
    global book_name
    book_name = soup.find('div', class_='wy-side-nav-search').a.text.strip()
    menu = soup.find_all('div', class_='section')
    chapters = menu[0].div.ul.find_all('li', class_='toctree-l1')
    for chapter in chapters:
        info = {
    }
        # 获取一级标题和url
        # 标题中含有'/'和'*'会保存失败
        info['title'] = chapter.a.text.replace('/', '').replace('*', '')
        info['url'] = base_url + chapter.a.get('href')
        info['child_chapters'] = []

        # 获取二级标题和url
        if chapter.ul is not None:
            child_chapters = chapter.ul.find_all('li')
            for child in child_chapters:
                url = child.a.get('href')
                # 如果在url中存在'#'，则此url为页面内链接，不会跳转到其他页面
                # 所以不需要保存
                if '#' not in url:
                    info['child_chapters'].append({
    
                        'title': child.a.text.replace('/', '').replace('*', ''),
                        'url': base_url + child.a.get('href'),
                    })

        chapter_info.append(info)


def save_pdf(html, filename):
    """
    把所有html文件保存到pdf文件
    :param html:  html内容
    :param file_name: pdf文件名
    :return:
    """
    options = {
    
        'page-size': 'Letter',
        'margin-top': '0.75in',
        'margin-right': '0.75in',
        'margin-bottom': '0.75in',
        'margin-left': '0.75in',
        'encoding': "UTF-8",
        'custom-header': [
            ('Accept-Encoding', 'gzip')
        ],
        'cookie': [
            ('cookie-name1', 'cookie-value1'),
            ('cookie-name2', 'cookie-value2'),
        ],
        'outline-depth': 10,
    }

    pdfkit.from_string(html, filename, options=options)


def get_content(url):
    """
    解析URL，获取需要的html内容
    :param url: 目标网址
    :return: html
    """
    html = get_one_page(url)
    soup = BeautifulSoup(html, 'html.parser')
    content = soup.find('div', attrs={
    'itemprop': 'articleBody'})
    html = html_template.format(content=content)
    return html


def parse_html_to_pdf():
    """
    解析URL，获取html，保存成pdf文件
    :return: None
    """
    try:
        for chapter in chapter_info:
            ctitle = chapter['title']
            url = chapter['url']
            # 文件夹不存在则创建（多级目录）
            dir_name = os.path.join(os.path.dirname(__file__), 'gen', ctitle)
            if not os.path.exists(dir_name):
                os.makedirs(dir_name)

            html = get_content(url)

            print('保存章节：', ctitle)
            save_pdf(html, os.path.join(dir_name, ctitle + '.pdf'))

            children = chapter['child_chapters']
            if children:
                for child in children:
                    html = get_content(child['url'])
                    pdf_path = os.path.join(dir_name, child['title'] + '.pdf')
                    save_pdf(html, pdf_path)
        print('====== 本章保存完毕')
    except Exception as e:
        print(e)


def merge_pdf(infnList, outfn):
    """
    合并pdf
    :param infnList: 要合并的PDF文件路径列表
    :param outfn: 保存的PDF文件名
    :return: None
    """
    pagenum = 0
    pdf_output = PdfFileWriter()

    for pdf in infnList:
        # 先合并一级目录的内容
        first_level_title = pdf['title']
        dir_name = os.path.join(os.path.dirname(
            __file__), 'gen', first_level_title)
        padf_path = os.path.join(dir_name, first_level_title + '.pdf')

        pdf_input = PdfFileReader(open(padf_path, 'rb'))
        # 获取 pdf 共用多少页
        page_count = pdf_input.getNumPages()
        for i in range(page_count):
            pdf_output.addPage(pdf_input.getPage(i))

        # 添加书签
        parent_bookmark = pdf_output.addBookmark(
            first_level_title, pagenum=pagenum)

        # 页数增加
        pagenum += page_count

        # 存在子章节
        if pdf['child_chapters']:
            for child in pdf['child_chapters']:
                second_level_title = child['title']
                padf_path = os.path.join(dir_name, second_level_title + '.pdf')

                pdf_input = PdfFileReader(open(padf_path, 'rb'))
                # 获取 pdf 共用多少页
                page_count = pdf_input.getNumPages()
                for i in range(page_count):
                    pdf_output.addPage(pdf_input.getPage(i))

                # 添加书签
                pdf_output.addBookmark(
                    second_level_title, pagenum=pagenum, parent=parent_bookmark)
                # 增加页数
                pagenum += page_count

    # 合并
    pdf_output.write(open(outfn, 'wb'))
    # 删除所有章节文件.这里会报错，我不知道为什么，所以注释掉了
    # shutil.rmtree(os.path.join(os.path.dirname(__file__), 'gen'))

def main():
    html = get_one_page(base_url)
    parse_title_and_url(html)
    parse_html_to_pdf()
    merge_pdf(chapter_info, os.path.join(
        os.path.dirname(__file__), book_name + '.pdf'))

if __name__ == '__main__':
    main()
#+END_SRC
*** 参考文章
[[https://www.cxyzjd.com/article/yaoyefengchen/79949284][Python爬虫：爬取在线教程生成pdf_C与Python实战-程序员宅基地]]
[[https://lhchen74.github.io/2020/11/01/python-html2pdf/][Python HTML2PDF]]

