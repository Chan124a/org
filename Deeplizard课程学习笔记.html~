<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2020-07-13 周一 10:52 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org0bc9ebc">1. 第2课</a></li>
<li><a href="#org85b7750">2. 第4课</a></li>
<li><a href="#orgb9bcd4b">3. 第5课</a></li>
<li><a href="#org08a0bf7">4. 第6课</a></li>
<li><a href="#org5fb47e4">5. 第8集</a>
<ul>
<li><a href="#orgb86fb29">5.1. 使用数据创建张量的方式</a></li>
<li><a href="#org3aa1059">5.2. 特殊张量的创建</a></li>
</ul>
</li>
<li><a href="#org9124500">6. 第9课</a></li>
<li><a href="#org1d20283">7. 第10课</a>
<ul>
<li><a href="#org435285e">7.1. Reshaping operations</a></li>
<li><a href="#org391aa56">7.2. 张量拼接</a></li>
</ul>
</li>
<li><a href="#org898c300">8. 第11课</a></li>
<li><a href="#org3064804">9. 第12课</a>
<ul>
<li><a href="#org04a34d4">9.1. 张量广播</a></li>
<li><a href="#org72b8172">9.2. 比较运算</a></li>
<li><a href="#orgeaf8edb">9.3. Element-Wise Operations Using Functions</a></li>
</ul>
</li>
<li><a href="#org1da1be7">10. 第13课</a>
<ul>
<li><a href="#orgac3462c">10.1. Reduction Operation Example</a></li>
<li><a href="#orgbdbc9a0">10.2. Accessing Elements Inside Tensors</a></li>
</ul>
</li>
<li><a href="#org86396bf">11. 第14课</a>
<ul>
<li><a href="#org6c674c1">11.1. MNIST Dataset?</a></li>
<li><a href="#org51a2ab6">11.2. Fashion-MNIST</a></li>
</ul>
</li>
<li><a href="#orgfeb3e55">12. 第15课</a>
<ul>
<li><a href="#orge5b364a">12.1. Preparing Our Data Using PyTorch</a></li>
<li><a href="#org8aa71f0">12.2. PyTorch Torchvision Package</a></li>
</ul>
</li>
<li><a href="#org1d9b686">13. 第16课</a>
<ul>
<li><a href="#orgcb0c0bc">13.1. Exploring The Data</a></li>
<li><a href="#org75af151">13.2. Accessing Data In The Training Set</a></li>
<li><a href="#org7d69d91">13.3. How To Plot Images Using PyTorch DataLoader</a></li>
</ul>
</li>
<li><a href="#org650007b">14. 第17课</a>
<ul>
<li><a href="#orgf2e77d5">14.1. Building A Neural Network In PyTorch</a></li>
<li><a href="#orgfc42785">14.2. Define The Network’s Layers As Class Attributes</a></li>
</ul>
</li>
<li><a href="#org6ffbb9a">15. 第18课</a>
<ul>
<li><a href="#org13c1a3c">15.1. Parameter Vs Argument</a></li>
<li><a href="#orged588f3">15.2. Two Types Of Parameters</a></li>
</ul>
</li>
<li><a href="#org2da9793">16. 第19课</a></li>
<li><a href="#orgf54a6a9">17. 第20课</a></li>
<li><a href="#orgf0109c7">18. 第21课</a>
<ul>
<li><a href="#orgfa232d3">18.1. Implementing The <code>forward()</code> Method</a></li>
</ul>
</li>
<li><a href="#org08396f3">19. 第22课</a>
<ul>
<li><a href="#orgd149efe">19.1. Passing A Single Image To The Network</a></li>
</ul>
</li>
<li><a href="#org4c65120">20. 第23课</a></li>
<li><a href="#org50914f4">21. 第24集</a></li>
<li><a href="#org63d6905">22. 第25课</a></li>
<li><a href="#org01b07d5">23. 第26课</a>
<ul>
<li><a href="#org62f1819">23.1. Training With All Batches (Single Epoch)</a></li>
<li><a href="#org2c11bc7">23.2. Training With Multiple Epochs</a></li>
</ul>
</li>
<li><a href="#org97c6479">24. 第27课</a>
<ul>
<li><a href="#org508cfc2">24.1. Getting prediction for the entire training set<sub>grad</sub><sub>enabled</sub></a></li>
<li><a href="#orgf8adb35">24.2. Building The Confusion Matrix</a></li>
<li><a href="#org1050d91">24.3. Plotting The Confusion Matrix</a></li>
</ul>
</li>
<li><a href="#org84577c7">25. 第28集</a>
<ul>
<li><a href="#org21f3762">25.1. How To Add Or Insert An Axis Into A Tensor</a></li>
<li><a href="#orgacdd2fe">25.2. Stack Vs Cat In PyTorch</a></li>
<li><a href="#orge0efa58">25.3. Stack Vs Concat In TensorFlow</a></li>
<li><a href="#org469847c">25.4. Stack Vs Concatenate In NumPy</a></li>
</ul>
</li>
<li><a href="#org2973e4d">26. 第29集</a>
<ul>
<li><a href="#org667606c">26.1. TensorBoard: TensorFlow's Visualization Toolkit</a></li>
<li><a href="#orgc268825">26.2. Getting Started With TensorBoard For PyTorch</a>
<ul>
<li><a href="#org7600d59">26.2.1. Network Graph And Training Set Image</a></li>
</ul>
</li>
<li><a href="#org7bb6b0b">26.3. Running TensorBoard</a></li>
<li><a href="#org7864468">26.4. TensorBoard Histograms And Scalars</a></li>
</ul>
</li>
<li><a href="#orga4e273f">27. 第30集</a>
<ul>
<li><a href="#orgb1549ce">27.1. Naming The Training Runs For TensorBoard</a>
<ul>
<li><a href="#orgdd1462e">27.1.1. Choosing A Name For The Run</a></li>
</ul>
</li>
<li><a href="#org461a76a">27.2. Creating Variables For Our Hyperparameters</a></li>
<li><a href="#orge29ece5">27.3. Calculate Loss With Different Batch Sizes</a></li>
<li><a href="#org744b500">27.4. Experimenting With Hyperparameter Values</a>
<ul>
<li><a href="#org1f57a8c">27.4.1. Batch Size Vs Training Set Size</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org0bc9ebc" class="outline-2">
<h2 id="org0bc9ebc"><span class="section-number-2">1</span> 第2课</h2>
<div class="outline-text-2" id="text-1">
<p>
pytorch内置支持GPU功能，可以方便地将张量移动到GPU上。 
</p>

<p>
pytorch于2016年10月发布，在发布之前已经有torch的框架存在。torch是基于lua编程语言的机器学习框架。pytorch是用python对torch进行重写。
</p>

<p>
pytorch使用了动态计算的计算图，这意味着图形是在操作发生时动态生成的。
</p>

<div class="org-src-container">
<pre class="src src-python">&gt; torch.cuda.is_available()
<span style="color: #AE81FF;">True</span>

&gt; torch.version.cuda
<span style="color: #E6DB74;">'10.0'</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org85b7750" class="outline-2">
<h2 id="org85b7750"><span class="section-number-2">2</span> 第4课</h2>
<div class="outline-text-2" id="text-2">
<p>
GPU适用于并行计算。 CPU通常有4个或16个核，而GPU有成千上万个核。
</p>

<p>
最适合GPU的是可以并行完成的计算，我们可以用并行编程方法和GPU完成计算。
</p>

<p>
使用并行编程方法和GPU可以加速卷积运算。
</p>

<p>
Cuda为开发者提供了使用英伟达GPU的API
</p>

<p>
pytorch自带Cudnn。我们可以用pytorch驱动cuda，不需要知道如何使用cuda的API。
</p>

<p>
下面创建张量的方式默认是在CPU上面运行的
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">t</span> = torch.tensor([1,2,3])
&gt; t
tensor([1, 2, 3])
</pre>
</div>
<p>
下面的代码可以把张量移到GPU上
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">t</span> = t.cuda()
&gt; t
tensor([1, 2, 3], device=<span style="color: #E6DB74;">'cuda:0'</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-orgb9bcd4b" class="outline-2">
<h2 id="orgb9bcd4b"><span class="section-number-2">3</span> 第5课</h2>
<div class="outline-text-2" id="text-3">
<p>
张量是多维数组。张量维数没有告诉我们张量中有多少个分量。
</p>
</div>
</div>
<div id="outline-container-org08a0bf7" class="outline-2">
<h2 id="org08a0bf7"><span class="section-number-2">4</span> 第6课</h2>
<div class="outline-text-2" id="text-4">
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">dd</span> = [
[1,2,3],
[4,5,6],
[7,8,9]
]
&gt; dd[0]
[1, 2, 3]

&gt; dd[1]
[4, 5, 6]

&gt; dd[2]
[7, 8, 9]

&gt; dd[0][0]
1

&gt; dd[1][0]
4
</pre>
</div>

<p>
张量的形状（shape）告诉我们每个轴的长度，即每个轴上有多少个索引。
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">dd</span> = [
[1,2,3],
[4,5,6],
[7,8,9]
]

&gt; <span style="color: #FD971F;">t</span> = torch.tensor(dd)
&gt; t
tensor([
[1, 2, 3],
[4, 5, 6],
[7, 8, 9]
])

&gt; <span style="color: #F92672;">type</span>(t)
torch.Tensor

&gt; t.shape
torch.Size([3,3])
</pre>
</div>
<p>
在pytorch中，一个张量的大小（size）和形状（shape）是一样的。
</p>

<p>
可以用 <code>reshape</code> 改变张量形状
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">t</span> = torch.tensor(dd)
&gt; t
tensor([
[1, 2, 3],
[4, 5, 6],
[7, 8, 9]
])

&gt; t.reshape(1,9)
tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])

&gt; t.reshape(1,9).shape
torch.Size([1, 9])
</pre>
</div>
</div>
</div>
<div id="outline-container-org5fb47e4" class="outline-2">
<h2 id="org5fb47e4"><span class="section-number-2">5</span> 第8集</h2>
<div class="outline-text-2" id="text-5">
<p>
pytorch的张量是类 <code>torch.Tensor</code> 的实例。可以用下面的方法创建一个空的张量。
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">t</span> = torch.Tensor()
&gt; <span style="color: #F92672;">type</span>(t)
torch.Tensor
</pre>
</div>
<p>
每个 <code>torch.Tensor</code> 都有三个属性：
</p>
<ul class="org-ul">
<li>torch.dtype</li>
<li>torch.device</li>
<li>torch.layout</li>
</ul>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #F92672;">print</span>(t.dtype)
&gt; <span style="color: #F92672;">print</span>(t.device)
&gt; <span style="color: #F92672;">print</span>(t.layout)
torch.float32
cpu
torch.strided
</pre>
</div>

<p>
下面是数据类型列表，只有相同数据类型的张量才能进行运算。
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Data type</th>
<th scope="col" class="org-left">dtype</th>
<th scope="col" class="org-left">CPU tensor</th>
<th scope="col" class="org-left">GPU tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">32-bit floating point</td>
<td class="org-left">torch.float32</td>
<td class="org-left">torch.FloatTensor</td>
<td class="org-left">torch.cuda.FloatTensor</td>
</tr>

<tr>
<td class="org-left">64-bit floating point</td>
<td class="org-left">torch.float64</td>
<td class="org-left">torch.DoubleTensor</td>
<td class="org-left">torch.cuda.DoubleTensor</td>
</tr>

<tr>
<td class="org-left">16-bit floating point</td>
<td class="org-left">torch.float16</td>
<td class="org-left">torch.HalfTensor</td>
<td class="org-left">torch.cuda.HalfTensor</td>
</tr>

<tr>
<td class="org-left">8-bit integer (unsigned)</td>
<td class="org-left">torch.uint8</td>
<td class="org-left">torch.ByteTensor</td>
<td class="org-left">torch.cuda.ByteTensor</td>
</tr>

<tr>
<td class="org-left">8-bit integer (signed)</td>
<td class="org-left">torch.int8</td>
<td class="org-left">torch.CharTensor</td>
<td class="org-left">torch.cuda.CharTensor</td>
</tr>

<tr>
<td class="org-left">16-bit integer (signed)</td>
<td class="org-left">torch.int16</td>
<td class="org-left">torch.ShortTensor</td>
<td class="org-left">torch.cuda.ShortTensor</td>
</tr>

<tr>
<td class="org-left">32-bit integer (signed)</td>
<td class="org-left">torch.int32</td>
<td class="org-left">torch.IntTensor</td>
<td class="org-left">torch.cuda.IntTensor</td>
</tr>

<tr>
<td class="org-left">64-bit integer (signed)</td>
<td class="org-left">torch.int64</td>
<td class="org-left">torch.LongTensor</td>
<td class="org-left">torch.cuda.LongTensor</td>
</tr>
</tbody>
</table>

<p>
我们可以用下面的方法创建一个设备
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">device</span> = torch.device(<span style="color: #E6DB74;">'cuda:0'</span>)
&gt; device
device(<span style="color: #F92672;">type</span>=<span style="color: #E6DB74;">'cuda'</span>, index=0)
</pre>
</div>
<p>
<code>cuda</code> 表示GPU的意思。如果我们将索引设置为3或4，而我们系统中并没有3到4个gpu时，当我们给该设备分配张量时，将会报错。
使用多设备时，张量之间的操作必须与存在于同一设备上的张量发生。
</p>

<p>
布局表示张量在内存中的存储方式。
</p>
</div>
<div id="outline-container-orgb86fb29" class="outline-3">
<h3 id="orgb86fb29"><span class="section-number-3">5.1</span> 使用数据创建张量的方式</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>torch.Tensor(data)</li>
<li>torch.tensor(data)</li>
<li>torch.as<sub>tensor</sub>(data)</li>
<li>torch.from<sub>numpy</sub>(data)</li>
</ul>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">data</span> = np.array([1,2,3])
&gt; <span style="color: #F92672;">type</span>(data)
numpy.ndarray

&gt; <span style="color: #FD971F;">o1</span> = torch.Tensor(data)  //&#36825;&#20010;&#26159;&#31867;&#26500;&#36896;&#20989;&#25968;&#65292;&#27880;&#24847;&#23427;&#30340;&#36755;&#20986;&#23558;&#25972;&#25968;&#21464;&#25104;&#20102;&#28014;&#28857;&#25968;
&gt; <span style="color: #FD971F;">o2</span> = torch.tensor(data)  //&#36825;&#20010;&#21644;&#19979;&#38754;&#20004;&#20010;&#37117;&#26159;&#25152;&#35859;&#30340;&#24037;&#21378;&#20989;&#25968;&#65292;&#24037;&#21378;&#20989;&#25968;&#25509;&#21463;&#21442;&#25968;&#36755;&#20837;&#24182;&#36820;&#22238;&#29305;&#23450;&#31867;&#22411;&#23545;&#35937;
&gt; <span style="color: #FD971F;">o3</span> = torch.as_tensor(data)
&gt; <span style="color: #FD971F;">o4</span> = torch.from_numpy(data)

&gt; <span style="color: #F92672;">print</span>(o1)
&gt; <span style="color: #F92672;">print</span>(o2)
&gt; <span style="color: #F92672;">print</span>(o3)
&gt; <span style="color: #F92672;">print</span>(o4)
tensor([1., 2., 3.])
tensor([1, 2, 3], dtype=torch.int32)
tensor([1, 2, 3], dtype=torch.int32)
tensor([1, 2, 3], dtype=torch.int32)
</pre>
</div>
</div>
</div>
<div id="outline-container-org3aa1059" class="outline-3">
<h3 id="org3aa1059"><span class="section-number-3">5.2</span> 特殊张量的创建</h3>
<div class="outline-text-3" id="text-5-2">
<div class="org-src-container">
<pre class="src src-bash">&gt;torch.eye(2)
<span style="color: #A6E22E;">tensor</span>([
    [1., 0.],
    [0., 1.]
])
&gt;torch.zeros([2,2])
<span style="color: #A6E22E;">tensor</span>([
    [0., 0.],
    [0., 0.]
])
&gt;torch.ones([2,2])
<span style="color: #A6E22E;">tensor</span>([
    [1., 1.],
    [1., 1.]
])
&gt;torch.rand([2,2])
<span style="color: #A6E22E;">tensor</span>([
    [0.0465, 0.4557],
    [0.6596, 0.0941]
])
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org9124500" class="outline-2">
<h2 id="org9124500"><span class="section-number-2">6</span> 第9课</h2>
<div class="outline-text-2" id="text-6">
<p>
所有的工厂函数都有更好的文档和更多的配置参数。
</p>

<p>
构造函数在构造一个张量时使用全局缺省值，而工厂函数则根据输入推断数据类型。
</p>

<p>
我们可以用 <code>torch.get_default_dtype()</code> 参看 <code>dtype</code> 的缺省值
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; torch.get_default_dtype()
torch.float32
</pre>
</div>

<p>
我们也可以为工厂函数指定数据类型
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt;torch.tensor(np.array([1,2,3]),<span style="color: #FD971F;">dtype</span>=torch.float64)
<span style="color: #A6E22E;">tensor</span>([1.,2.,3.],<span style="color: #FD971F;">dtype</span>=torch.float64)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">data</span> = np.array([1,2,3])
&gt; data
array([1,2,3])

&gt; <span style="color: #FD971F;">t1</span> = torch.Tensor(data) 
&gt; <span style="color: #FD971F;">t2</span> = torch.tensor(data) 
&gt; <span style="color: #FD971F;">t3</span> = torch.as_tensor(data)
&gt; <span style="color: #FD971F;">t4</span> = torch.from_numpy(data)

&gt;<span style="color: #FD971F;">data</span>[0]=0
&gt;<span style="color: #FD971F;">data</span>[1]=0
&gt;<span style="color: #FD971F;">data</span>[2]=0

&gt; <span style="color: #F92672;">print</span>(t1)
tensor([1., 2., 3.])
&gt; <span style="color: #F92672;">print</span>(t2)
tensor([1, 2, 3], dtype=torch.int32)

//t3&#21644;t4&#20250;&#38543;&#30528;data&#30340;&#25913;&#21464;&#32780;&#25913;&#21464;
&gt; <span style="color: #F92672;">print</span>(t3)
tensor([0, 0, 0], dtype=torch.int32)
&gt; <span style="color: #F92672;">print</span>(t4)
tensor([0, 0, 0], dtype=torch.int32)
</pre>
</div>

<p>
copy vs share，共享数据比复制数据更有效，使用更少的内存
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Share Data</th>
<th scope="col" class="org-left">Copy Data</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">torch.as<sub>tensor</sub>()</td>
<td class="org-left">torch.tensor()</td>
</tr>

<tr>
<td class="org-left">torch.from<sub>numpy</sub>()</td>
<td class="org-left">torch.Tensor()</td>
</tr>
</tbody>
</table>

<p>
<code>torch.tensor()</code> 是最常使用的，当需要对性能优化时，可以使用数据共享的方法。 <code>torch.as_tensor()</code> 比 <code>torch.from_numpy()</code> 更常用，因为 <code>torch.as_tensor()</code> 可以接受任何像python这样的数组，而 <code>torch.from_numpy()</code> 只接受numpy数组。
</p>

<blockquote>
<p>
Some things to keep in mind about memory sharing (it works where it can):
</p>
<ol class="org-ol">
<li>Since <code>numpy.ndarray</code> objects are allocated on the CPU, the <code>as_tensor()</code> function must copy the data from the CPU to the GPU when a GPU is being used.</li>
<li>The memory sharing of <code>as_tensor()</code> doesn’t work with built-in Python data structures like lists.</li>
<li>The <code>as_tensor()</code> call requires developer knowledge of the sharing feature. This is necessary so we don’t inadvertently make an unwanted change in the underlying data without realizing the change impacts multiple objects.</li>
<li>The <code>as_tensor()</code> performance improvement will be greater if there are a lot of back and forth operations between <code>numpy.ndarray</code> objects and tensor objects. However, if there is just a single load operation, there shouldn’t be much impact from a performance perspective.</li>
</ol>
</blockquote>
</div>
</div>
<div id="outline-container-org1d20283" class="outline-2">
<h2 id="org1d20283"><span class="section-number-2">7</span> 第10课</h2>
<div class="outline-text-2" id="text-7">
<p>
Reshaping operations
Element-wise operations
Reduction operations
Access operations
</p>
</div>

<div id="outline-container-org435285e" class="outline-3">
<h3 id="org435285e"><span class="section-number-3">7.1</span> Reshaping operations</h3>
<div class="outline-text-3" id="text-7-1">
<div class="org-src-container">
<pre class="src src-bash">&gt; t = torch.tensor([
    [1,1,1,1],
    [2,2,2,2],
    [3,3,3,3]
], <span style="color: #FD971F;">dtype</span>=torch.float32)

&gt; t.size()
<span style="color: #A6E22E;">torch.Size</span>([3, 4])

&gt; t.shape
<span style="color: #A6E22E;">torch.Size</span>([3, 4])

//&#33719;&#21462;&#31209;
&gt; len(t.shape)  
2
</pre>
</div>
<p>
下面两种方法可以得到张量的元素总个数
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; torch.tensor(t.shape).prod()
<span style="color: #A6E22E;">tensor</span>(12)
&gt; t.numel()
12
</pre>
</div>
<p>
在使用 <code>reshape</code> 时必须保证元素总个数相等
</p>

<p>
<code>squeeze</code> 可以移除（压缩）所有长度为1的轴，而 <code>unsqueeze</code> 可以增加一个长度为1的维度
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; print(t.reshape([1,12]))
&gt; print(t.reshape([1,12]).shape)
<span style="color: #A6E22E;">tensor</span>([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])
<span style="color: #A6E22E;">torch.Size</span>([1, 12])

&gt; print(t.reshape([1,12]).squeeze())
&gt; print(t.reshape([1,12]).squeeze().shape)
<span style="color: #A6E22E;">tensor</span>([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])
<span style="color: #A6E22E;">torch.Size</span>([12])

&gt; print(t.reshape([1,12]).squeeze().unsqueeze(<span style="color: #FD971F;">dim</span>=0))
&gt; print(t.reshape([1,12]).squeeze().unsqueeze(<span style="color: #FD971F;">dim</span>=0).shape)
<span style="color: #A6E22E;">tensor</span>([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])
<span style="color: #A6E22E;">torch.Size</span>([1, 12])
</pre>
</div>

<p>
我们可以写一个展平（flatten）函数将一个张量变成一维数组
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #F92672;">def</span> <span style="color: #A6E22E;">flatten</span>(t):
    <span style="color: #FD971F;">t</span> = t.reshape(1, -1)
    <span style="color: #FD971F;">t</span> = t.squeeze()
    <span style="color: #F92672;">return</span> t
</pre>
</div>
<div class="org-src-container">
<pre class="src src-bash">&gt; t = torch.ones(4, 3)
&gt; t
<span style="color: #A6E22E;">tensor</span>([[1., 1., 1.],
    [1., 1., 1.],
    [1., 1., 1.],
    [1., 1., 1.]])

&gt; flatten(t)
<span style="color: #A6E22E;">tensor</span>([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
</pre>
</div>
</div>
</div>

<div id="outline-container-org391aa56" class="outline-3">
<h3 id="org391aa56"><span class="section-number-3">7.2</span> 张量拼接</h3>
<div class="outline-text-3" id="text-7-2">
<p>
使用 <code>cat</code> 并指定在哪个维度进行拼接即可
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; t1 = torch.tensor([
    [1,2],
    [3,4]
])
&gt; t2 = torch.tensor([
    [5,6],
    [7,8]
])

&gt; torch.cat((t1, t2), <span style="color: #FD971F;">dim</span>=0)
<span style="color: #A6E22E;">tensor</span>([[1, 2],
        [3, 4],
        [5, 6],
        [7, 8]])

&gt; torch.cat((t1, t2), <span style="color: #FD971F;">dim</span>=1)
<span style="color: #A6E22E;">tensor</span>([[1, 2, 5, 6],
        [3, 4, 7, 8]])

&gt; torch.cat((t1, t2), <span style="color: #FD971F;">dim</span>=0).shape
<span style="color: #A6E22E;">torch.Size</span>([4, 2])

&gt; torch.cat((t1, t2), <span style="color: #FD971F;">dim</span>=1).shape
<span style="color: #A6E22E;">torch.Size</span>([2, 4])
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org898c300" class="outline-2">
<h2 id="org898c300"><span class="section-number-2">8</span> 第11课</h2>
<div class="outline-text-2" id="text-8">
<p>
<code>stack</code> 可以创建训练批（batch）
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FD971F;">t1</span> = torch.tensor([
    [1,1,1,1],
    [1,1,1,1],
    [1,1,1,1],
    [1,1,1,1]
])

<span style="color: #FD971F;">t2</span> = torch.tensor([
    [2,2,2,2],
    [2,2,2,2],
    [2,2,2,2],
    [2,2,2,2]
])

<span style="color: #FD971F;">t3</span> = torch.tensor([
    [3,3,3,3],
    [3,3,3,3],
    [3,3,3,3],
    [3,3,3,3]
])

&gt; <span style="color: #FD971F;">t</span> = torch.stack((t1, t2, t3))
&gt; t.shape
torch.Size([3, 4, 4])
&gt; t
tensor([[[1, 1, 1, 1],
         [1, 1, 1, 1],
         [1, 1, 1, 1],
         [1, 1, 1, 1]],

        [[2, 2, 2, 2],
         [2, 2, 2, 2],
         [2, 2, 2, 2],
         [2, 2, 2, 2]],

        [[3, 3, 3, 3],
         [3, 3, 3, 3],
         [3, 3, 3, 3],
         [3, 3, 3, 3]]])
</pre>
</div>

<p>
由于cnn需要的输入需要3个rgb维度，所以我们可以用 <code>reshape</code> 将张量变成4个维度的。
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">t</span> = t.reshape(3,1,4,4)
&gt; t
tensor([[[[1, 1, 1, 1],
          [1, 1, 1, 1],
          [1, 1, 1, 1],
          [1, 1, 1, 1]]],
        [[[2, 2, 2, 2],
          [2, 2, 2, 2],
          [2, 2, 2, 2],
          [2, 2, 2, 2]]],
        [[[3, 3, 3, 3],
          [3, 3, 3, 3],
          [3, 3, 3, 3],
          [3, 3, 3, 3]]]])
</pre>
</div>

<p>
下面几种方法都可以将张量展开（faltten）
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; t.reshape(1,-1)[0] <span style="color: #75715E;"># </span><span style="color: #75715E;">Thank you Mick!</span>
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
    2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])

&gt; t.reshape(-1) <span style="color: #75715E;"># </span><span style="color: #75715E;">Thank you Aamir!</span>
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
    2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])

&gt; t.view(t.numel()) <span style="color: #75715E;"># </span><span style="color: #75715E;">Thank you Ulm!</span>
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
    2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])

&gt; t.flatten() <span style="color: #75715E;"># </span><span style="color: #75715E;">Thank you PyTorch!</span>
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
    2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])
</pre>
</div>

<p>
我们可以用 <code>flatten</code> 对某一个特定的轴进行展开. <code>start_dim</code> 指定从哪个轴开始进行展开
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; t.flatten(start_dim=1).shape
torch.Size([3, 16])

&gt; t.flatten(start_dim=1)
tensor(
[
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
    [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
]
)
</pre>
</div>
<p>
<code>t.reshape(-1,16)</code> 也可以达到同样的效果
</p>
</div>
</div>
<div id="outline-container-org3064804" class="outline-2">
<h2 id="org3064804"><span class="section-number-2">9</span> 第12课</h2>
<div class="outline-text-2" id="text-9">
</div>
<div id="outline-container-org04a34d4" class="outline-3">
<h3 id="org04a34d4"><span class="section-number-3">9.1</span> 张量广播</h3>
<div class="outline-text-3" id="text-9-1">
<p>
进行操作的两个张量必须有相同的形状（shape），即轴的数量相等并且轴的长度相等。
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">t1</span> = torch.tensor([
    [1,2],
    [3,4]
], dtype=torch.float32)

&gt; <span style="color: #FD971F;">t2</span> = torch.tensor([
    [9,8],
    [7,6]
], dtype=torch.float32)
&gt; t1 + t2
tensor([[10., 10.],
        [10., 10.]])
</pre>
</div>

<p>
张量广播定义了在元素操作过程中如何处理不同形状的张量。
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #F92672;">print</span>(t + 2)
tensor([[3., 4.],
        [5., 6.]])

&gt; <span style="color: #F92672;">print</span>(t - 2)
tensor([[-1.,  0.],
        [ 1.,  2.]])

&gt; <span style="color: #F92672;">print</span>(t * 2)
tensor([[2., 4.],
        [6., 8.]])

&gt; <span style="color: #F92672;">print</span>(t / 2)
tensor([[0.5000, 1.0000],
        [1.5000, 2.0000]])

<span style="color: #75715E;">## </span><span style="color: #75715E;">or equivalently, (2) these built-in tensor object methods:</span>
&gt; <span style="color: #F92672;">print</span>(t1.add(2))
tensor([[3., 4.],
        [5., 6.]])

&gt; <span style="color: #F92672;">print</span>(t1.sub(2))
tensor([[-1.,  0.],
        [ 1.,  2.]])

&gt; <span style="color: #F92672;">print</span>(t1.mul(2))
tensor([[2., 4.],
        [6., 8.]])

&gt; <span style="color: #F92672;">print</span>(t1.div(2))
tensor([[0.5000, 1.0000],
        [1.5000, 2.0000]])
</pre>
</div>

<p>
我们可以用 <code>broadcast_to()</code> 来检查广播后的张量形状
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; np.broadcast_to(2, t1.shape)
array([[2, 2],
        [2, 2]])
&gt; t1 + 2
tensor([[3., 4.],
        [5., 6.]])
</pre>
</div>

<p>
当两个不同形状的张量进行操作时，会应用张量广播将两个张量转化为相同的形状。同样的，我们也可以用 <code>broadcast_to()</code> 检查广播后的转换结果
</p>
<div class="org-src-container">
<pre class="src src-python">&gt;<span style="color: #FD971F;">t1</span> = torch.tensor([
    [1,1],
    [1,1]
], dtype=torch.float32)

&gt;<span style="color: #FD971F;">t2</span> = torch.tensor([2,4], dtype=torch.float32)

&gt; t1.shape
torch.Size([2, 2])

&gt; t2.shape
torch.Size([2])

&gt; np.broadcast_to(t2.numpy(), t1.shape)
array([[2., 4.],
        [2., 4.]], dtype=float32)

&gt; t1 + t2
tensor([[3., 5.],
        [3., 5.]])
</pre>
</div>
</div>
</div>
<div id="outline-container-org72b8172" class="outline-3">
<h3 id="org72b8172"><span class="section-number-3">9.2</span> 比较运算</h3>
<div class="outline-text-3" id="text-9-2">
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">t</span> = torch.tensor([
    [0,5,0],
    [6,0,7],
    [0,8,0]
], dtype=torch.float32)

&gt; t.eq(0)
tensor([[<span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">False</span>, <span style="color: #AE81FF;">True</span>],
        [<span style="color: #AE81FF;">False</span>, <span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">False</span>],
        [<span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">False</span>, <span style="color: #AE81FF;">True</span>]])


&gt; t.ge(0)
tensor([[<span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">True</span>],
        [<span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">True</span>],
        [<span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">True</span>]])


&gt; t.gt(0)
tensor([[<span style="color: #AE81FF;">False</span>, <span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">False</span>],
        [<span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">False</span>, <span style="color: #AE81FF;">True</span>],
        [<span style="color: #AE81FF;">False</span>, <span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">False</span>]])


&gt; t.lt(0)
tensor([[<span style="color: #AE81FF;">False</span>, <span style="color: #AE81FF;">False</span>, <span style="color: #AE81FF;">False</span>],
        [<span style="color: #AE81FF;">False</span>, <span style="color: #AE81FF;">False</span>, <span style="color: #AE81FF;">False</span>],
        [<span style="color: #AE81FF;">False</span>, <span style="color: #AE81FF;">False</span>, <span style="color: #AE81FF;">False</span>]])

&gt; t.le(7)
tensor([[<span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">True</span>],
        [<span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">True</span>],
        [<span style="color: #AE81FF;">True</span>, <span style="color: #AE81FF;">False</span>, <span style="color: #AE81FF;">True</span>]])
</pre>
</div>
</div>
</div>
<div id="outline-container-orgeaf8edb" class="outline-3">
<h3 id="orgeaf8edb"><span class="section-number-3">9.3</span> Element-Wise Operations Using Functions</h3>
<div class="outline-text-3" id="text-9-3">
<div class="org-src-container">
<pre class="src src-python">&gt; t.<span style="color: #F92672;">abs</span>() 
tensor([[0., 5., 0.],
        [6., 0., 7.],
        [0., 8., 0.]])


&gt; t.sqrt()
tensor([[0.0000, 2.2361, 0.0000],
        [2.4495, 0.0000, 2.6458],
        [0.0000, 2.8284, 0.0000]])

&gt; t.neg()
tensor([[-0., -5., -0.],
        [-6., -0., -7.],
        [-0., -8., -0.]])

&gt; t.neg().<span style="color: #F92672;">abs</span>()
tensor([[0., 5., 0.],
        [6., 0., 7.],
        [0., 8., 0.]])
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org1da1be7" class="outline-2">
<h2 id="org1da1be7"><span class="section-number-2">10</span> 第13课</h2>
<div class="outline-text-2" id="text-10">
</div>
<div id="outline-container-orgac3462c" class="outline-3">
<h3 id="orgac3462c"><span class="section-number-3">10.1</span> Reduction Operation Example</h3>
<div class="outline-text-3" id="text-10-1">
<p>
下面的方法在所有张量元素上操作，将张量减少为单个元素标量张量
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">t</span> = torch.tensor([
    [0,1,0],
    [2,0,2],
    [0,3,0]
], dtype=torch.float32)

&gt; t.<span style="color: #F92672;">sum</span>()
tensor(8.)
&gt; t.prod()
tensor(0.)
&gt; t.mean()
tensor(.8889)
&gt; t.std()
tensor(1.1667)
</pre>
</div>

<p>
我们可以指定要压缩的维度
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">t</span> = torch.tensor([
    [1,1,1,1],
    [2,2,2,2],
    [3,3,3,3]
], dtype=torch.float32)

&gt; t.<span style="color: #F92672;">sum</span>(dim=0)
tensor([6., 6., 6., 6.])

&gt; t.<span style="color: #F92672;">sum</span>(dim=1)
tensor([ 4.,  8., 12.])
</pre>
</div>

<p>
<code>argmax</code> 返回最大值的索引
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">t</span> = torch.tensor([
    [1,0,0,2],
    [0,3,3,0],
    [4,0,0,5]
], dtype=torch.float32)
&gt; t.<span style="color: #F92672;">max</span>()
tensor(5.)
&gt; t.argmax()
tensor(11)

&gt; t.flatten()
tensor([1., 0., 0., 2., 0., 3., 3., 0., 4., 0., 0., 5.])
</pre>
</div>
<p>
可以指定轴进行操作，注意 <code>max</code> 返回最大值和索引：（最大值，索引值）
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; t.<span style="color: #F92672;">max</span>(dim=0)  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#22240;&#20026;&#25105;&#20204;&#26159;&#23545;&#31532;0&#32500;&#24230;&#36827;&#34892;&#27714;&#26368;&#22823;&#20540;&#65292;&#25152;&#20197;&#32034;&#24341;&#20540;&#21578;&#35785;&#25105;&#20204;&#30340;&#26159;&#31532;0&#32500;&#24230;&#30340;&#32034;&#24341;&#65292;&#21363;&#20998;&#21035;&#26159;&#22312;&#31532;2&#65292;1&#65292;1&#65292;2&#20010;&#25968;&#32452;&#21462;&#21040;&#26368;&#22823;&#20540;</span>
(tensor([4., 3., 3., 5.]), tensor([2, 1, 1, 2]))

&gt; t.argmax(dim=0)
tensor([2, 1, 1, 2])

&gt; t.<span style="color: #F92672;">max</span>(dim=1)
(tensor([2., 3., 5.]), tensor([3, 1, 3]))

&gt; t.argmax(dim=1)
tensor([3, 1, 3])
</pre>
</div>
</div>
</div>
<div id="outline-container-orgbdbc9a0" class="outline-3">
<h3 id="orgbdbc9a0"><span class="section-number-3">10.2</span> Accessing Elements Inside Tensors</h3>
<div class="outline-text-3" id="text-10-2">
<p>
注意 <code>item</code> 方法只能作用于标量型张量
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">t</span> = torch.tensor([
    [1,2,3],
    [4,5,6],
    [7,8,9]
], dtype=torch.float32)

&gt; t.mean()
tensor(5.)

&gt; t.mean().item()
5.0
</pre>
</div>
<p>
可以指定维度进行操作
</p>
<div class="org-src-container">
<pre class="src src-python">&gt; <span style="color: #FD971F;">t</span> = torch.tensor([
    [1,2,3],
    [4,5,6],
    [7,8,9]
], dtype=torch.float32)
&gt; t.mean(dim=0)
tensor([4., 5., 6.])
&gt; t.mean(dim=0).tolist()
[4.0, 5.0, 6.0]

&gt; t.mean(dim=0).numpy()
array([4., 5., 6.], dtype=float32)
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org86396bf" class="outline-2">
<h2 id="org86396bf"><span class="section-number-2">11</span> 第14课</h2>
<div class="outline-text-2" id="text-11">
</div>
<div id="outline-container-org6c674c1" class="outline-3">
<h3 id="org6c674c1"><span class="section-number-3">11.1</span> MNIST Dataset?</h3>
<div class="outline-text-3" id="text-11-1">
<p>
What Is The MNIST Dataset?NIST stands for National Institute of Standards and Technology.The M in MNIST stands for modified, and this is because there was an original NIST dataset of digits that was modified to give us MNIST.
</p>

<p>
The dataset consists of 70,000 images of hand written digits with the following split:
</p>
<ul class="org-ul">
<li>60,000 training images</li>
<li>10,000 testing images</li>
</ul>
</div>
</div>

<div id="outline-container-org51a2ab6" class="outline-3">
<h3 id="org51a2ab6"><span class="section-number-3">11.2</span> Fashion-MNIST</h3>
<div class="outline-text-3" id="text-11-2">
<p>
Fashion-MNIST as the name suggests is a dataset of fashion items. Specifically, the dataset has the following ten classes of fashion items:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">Index</th>
<th scope="col" class="org-left">Label</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-left">T-shirt/top</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-left">Trouser</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-left">Pullover</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-left">Dress</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-left">Coat</td>
</tr>

<tr>
<td class="org-right">5</td>
<td class="org-left">Sandal</td>
</tr>

<tr>
<td class="org-right">6</td>
<td class="org-left">Shirt</td>
</tr>

<tr>
<td class="org-right">7</td>
<td class="org-left">Sneaker</td>
</tr>

<tr>
<td class="org-right">8</td>
<td class="org-left">Bag</td>
</tr>

<tr>
<td class="org-right">9</td>
<td class="org-left">Ankle boot</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>

<div id="outline-container-orgfeb3e55" class="outline-2">
<h2 id="orgfeb3e55"><span class="section-number-2">12</span> 第15课</h2>
<div class="outline-text-2" id="text-12">
</div>
<div id="outline-container-orge5b364a" class="outline-3">
<h3 id="orge5b364a"><span class="section-number-3">12.1</span> Preparing Our Data Using PyTorch</h3>
<div class="outline-text-3" id="text-12-1">
<p>
Our ultimate goal when preparing our data is to do the following (ETL):
</p>

<ul class="org-ul">
<li>Extract – Get the Fashion-MNIST image data from the source.</li>
<li>Transform – Put our data into tensor form.</li>
<li>Load – Put our data into an object to make it easily accessible.</li>
</ul>

<p>
For these purposes, PyTorch provides us with two classes:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Class</th>
<th scope="col" class="org-left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">torch.utils.data.Dataset</td>
<td class="org-left">An abstract class for representing a dataset.</td>
</tr>

<tr>
<td class="org-left">torch.utils.data.DataLoader</td>
<td class="org-left">Wraps a dataset and provides access to the underlying data.</td>
</tr>
</tbody>
</table>

<pre class="example">
All subclasses of the Dataset class must override __len__, that provides the size of the dataset, and __getitem__, supporting integer indexing in range from 0 to len(self) exclusive.
</pre>
</div>
</div>
<div id="outline-container-org8aa71f0" class="outline-3">
<h3 id="org8aa71f0"><span class="section-number-3">12.2</span> PyTorch Torchvision Package</h3>
<div class="outline-text-3" id="text-12-2">
<p>
The <code>torchvision</code> package, gives us access to the following resources:
</p>

<ul class="org-ul">
<li>Datasets (like MNIST and Fashion-MNIST)</li>
<li>Models (like VGG16)</li>
<li>Transforms</li>
<li>Utils</li>
</ul>

<p>
The PyTorch FashionMNIST dataset simply extends the MNIST dataset and overrides the urls.
</p>

<p>
创建Fashion-MNIST数据集实例
</p>

<p>
To get an instance of the FashionMNIST dataset using torchvision, we just create one like so:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #F92672;">import</span> torch
<span style="color: #F92672;">import</span> torchvision
<span style="color: #F92672;">import</span> torchvision.transforms <span style="color: #F92672;">as</span> transforms

<span style="color: #FD971F;">train_set</span> = torchvision.datasets.FashionMNIST(
    root=<span style="color: #E6DB74;">'./data'</span>  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#34920;&#31034;&#25968;&#25454;&#38598;&#19979;&#36733;&#20301;&#32622;</span>
    ,train=<span style="color: #AE81FF;">True</span>    <span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#20010;&#34920;&#31034;&#25105;&#20204;&#24076;&#26395;&#25968;&#25454;&#38598;&#26159;&#29992;&#20110;&#35757;&#32451;&#38598;&#30340;&#65292;FashionMNIST&#25968;&#25454;&#38598;&#20013;&#26377;6&#19975;&#24352;&#29992;&#20316;&#35757;&#32451;&#25968;&#25454;&#65292;1&#19975;&#24352;&#29992;&#20110;&#27979;&#35797;&#25968;&#25454;</span>
    ,download=<span style="color: #AE81FF;">True</span> <span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#20010;&#34920;&#31034;&#22914;&#26524;&#19979;&#36733;&#20301;&#32622;&#27809;&#26377;&#25968;&#25454;&#38598;&#65292;&#21017;&#19979;&#36733;&#25968;&#25454;&#38598;</span>
    ,transform=transforms.Compose([  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#20010;&#20351;&#29992;&#20102;&#20869;&#32622;&#30340; ToTensor &#34920;&#25442;&#26469;&#36716;&#25442;&#22270;&#20687;</span>
        transforms.ToTensor()
    ])
)
</pre>
</div>

<p>
将数据集打包或加载到数据加载器中：
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FD971F;">train_loader</span> = torch.utils.data.DataLoader(train_set
    ,batch_size=1000
    ,shuffle=<span style="color: #AE81FF;">True</span>
)
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org1d9b686" class="outline-2">
<h2 id="org1d9b686"><span class="section-number-2">13</span> 第16课</h2>
<div class="outline-text-2" id="text-13">
</div>
<div id="outline-container-orgcb0c0bc" class="outline-3">
<h3 id="orgcb0c0bc"><span class="section-number-3">13.1</span> Exploring The Data</h3>
<div class="outline-text-3" id="text-13-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #F92672;">import</span> torch
<span style="color: #F92672;">import</span> torch.nn <span style="color: #F92672;">as</span> nn
<span style="color: #F92672;">import</span> torch.optim <span style="color: #F92672;">as</span> optim
<span style="color: #F92672;">import</span> torch.nn.functional <span style="color: #F92672;">as</span> F

<span style="color: #F92672;">import</span> torchvision
<span style="color: #F92672;">import</span> torchvision.transforms <span style="color: #F92672;">as</span> transforms

<span style="color: #F92672;">import</span> numpy <span style="color: #F92672;">as</span> np
<span style="color: #F92672;">import</span> pandas <span style="color: #F92672;">as</span> pd
<span style="color: #F92672;">import</span> matplotlib.pyplot <span style="color: #F92672;">as</span> plt

<span style="color: #F92672;">from</span> sklearn.metrics <span style="color: #F92672;">import</span> confusion_matrix
<span style="color: #75715E;">#</span><span style="color: #75715E;">from plotcm import plot_confusion_matrix</span>

<span style="color: #F92672;">import</span> pdb

<span style="color: #FD971F;">train_set</span> = torchvision.datasets.FashionMNIST(
    root=<span style="color: #E6DB74;">'./data'</span>  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#34920;&#31034;&#25968;&#25454;&#38598;&#19979;&#36733;&#20301;&#32622;</span>
    ,train=<span style="color: #AE81FF;">True</span>    <span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#20010;&#34920;&#31034;&#25105;&#20204;&#24076;&#26395;&#20351;&#29992;&#35757;&#32451;&#35757;&#32451;&#38598;&#30340;&#25968;&#25454;&#65292;FashionMNIST&#25968;&#25454;&#38598;&#20013;&#26377;6&#19975;&#24352;&#29992;&#20316;&#35757;&#32451;&#25968;&#25454;&#65292;1&#19975;&#24352;&#29992;&#20110;&#27979;&#35797;&#25968;&#25454;</span>
    ,download=<span style="color: #AE81FF;">True</span> <span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#20010;&#34920;&#31034;&#22914;&#26524;&#19979;&#36733;&#20301;&#32622;&#27809;&#26377;&#25968;&#25454;&#38598;&#65292;&#21017;&#19979;&#36733;&#25968;&#25454;&#38598;</span>
    ,transform=transforms.Compose([  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#20010;&#20351;&#29992;&#20102;&#20869;&#32622;&#30340; ToTensor &#34920;&#25442;&#26469;&#36716;&#25442;&#22270;&#20687;</span>
        transforms.ToTensor()
    ])
)
<span style="color: #FD971F;">train_loader</span> = torch.utils.data.DataLoader(train_set
    ,batch_size=10
)

torch.set_printoptions(linewidth=120) <span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#20010;&#26159;&#20026;&#20102;&#35774;&#32622;&#25171;&#21360;&#34892;&#23485;</span>
</pre>
</div>
<div class="org-src-container">
<pre class="src src-bash">&gt; len(train_set)  <span style="color: #75715E;">#</span><span style="color: #75715E;">fashionmnist&#25968;&#25454;&#38598;&#20013;&#26377;6&#19975;&#24352;&#29992;&#20316;&#35757;&#32451;&#25968;&#25454;</span>
60000
&gt; train_set.train_labels <span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#20010;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#24352;&#37327;</span>
<span style="color: #A6E22E;">tensor</span>([9, 0, 0, ..., 3, 0, 5])
<span style="color: #75715E;"># </span><span style="color: #75715E;">Starting with torchvision 0.2.2</span>
&gt; train_set.targets
<span style="color: #A6E22E;">tensor</span>([9, 0, 0, ..., 3, 0, 5])
</pre>
</div>

<p>
If we want to see how many of each label exists in the dataset, we can use the PyTorch bincount() function like so:
</p>
<div class="org-src-container">
<pre class="src src-bash"><span style="color: #75715E;"># </span><span style="color: #75715E;">Before torchvision 0.2.2</span>
&gt; train_set.train_labels.bincount()
<span style="color: #A6E22E;">tensor</span>([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])
<span style="color: #75715E;"># </span><span style="color: #75715E;">Before torchvision 0.2.2</span>
&gt; train_set.targets.bincount()
<span style="color: #A6E22E;">tensor</span>([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])
</pre>
</div>
</div>
</div>

<div id="outline-container-org75af151" class="outline-3">
<h3 id="org75af151"><span class="section-number-3">13.2</span> Accessing Data In The Training Set</h3>
<div class="outline-text-3" id="text-13-2">
<p>
To access an individual element from the training set, we first pass the <code>train_set</code> object to Python’s <code>iter()</code> built-in function, which returns an object representing a stream of data.With the stream of data, we can use Python built-in <code>next()</code> function to get the next data element in the stream of data. 
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; sample = next(iter(train_set))
&gt; len(sample)  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#35757;&#32451;&#38598;&#20013;&#30340;&#27599;&#20010;&#26679;&#26412;&#37117;&#21253;&#21547;&#22270;&#20687;&#25968;&#25454;&#24352;&#37327;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#26631;&#31614;&#24352;&#37327;&#65292;sample[0]&#20026;&#22270;&#20687;&#65292;sample[1]&#20026;&#26631;&#31614;</span>
2

&gt; type(image)
torch.Tensor

<span style="color: #75715E;"># </span><span style="color: #75715E;">Before torchvision 0.2.2</span>
&gt; type(label)
torch.Tensor
<span style="color: #75715E;"># </span><span style="color: #75715E;">Starting at torchvision 0.2.2</span>
&gt; type(label)
int

&gt;image,<span style="color: #FD971F;">label</span>=sample <span style="color: #75715E;">#</span><span style="color: #75715E;">&#21487;&#20197;&#29992;&#36825;&#31181;&#26041;&#24335;&#20998;&#37197;&#22270;&#20687;&#21644;&#26631;&#31614;</span>

&gt; image.shape
<span style="color: #A6E22E;">torch.Size</span>([1, 28, 28]) 

&gt; torch.tensor(label).shape
<span style="color: #A6E22E;">torch.Size</span>([])

&gt; image.squeeze().shape
<span style="color: #A6E22E;">torch.Size</span>([28, 28])

&gt; plt.imshow(image.squeeze(), <span style="color: #FD971F;">cmap</span>=<span style="color: #E6DB74;">"gray"</span>)
&gt; torch.tensor(label)
<span style="color: #A6E22E;">tensor</span>(9)

</pre>
</div>
<p>
下面是进行一批图像的展示：
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; batch = next(iter(train_loader))

&gt; print(<span style="color: #E6DB74;">'len:'</span>, len(batch))
len: 2

&gt; images, labels = batch

&gt; print(<span style="color: #E6DB74;">'types:'</span>, type(images), type(labels))
&gt; print(<span style="color: #E6DB74;">'shapes:'</span>, images.shape, labels.shape)
types: &lt;class <span style="color: #E6DB74;">'torch.Tensor'</span>&gt; &lt;class <span style="color: #E6DB74;">'torch.Tensor'</span>&gt;
shapes: torch.Size([10, 1, 28, 28]) torch.Size([10])

<span style="color: #75715E;">#</span><span style="color: #75715E;">&#29992;&#19979;&#38754;&#30340;&#26041;&#27861;&#21487;&#20197;&#30011;&#20986;&#19968;&#25209;&#22270;&#20687;</span>
&gt; grid = torchvision.utils.make_grid(images, <span style="color: #FD971F;">nrow</span>=10)  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#21019;&#24314;&#19968;&#20010;&#32593;&#26684;&#65292;nrow &#25351;&#23450;&#27599;&#34892;&#30340;&#22270;&#20687;&#25968;&#37327;</span>

&gt; plt.figure(<span style="color: #FD971F;">figsize</span>=(15,15))
&gt; plt.imshow(np.transpose(grid, (1,2,0)))

&gt; print(<span style="color: #E6DB74;">'labels:'</span>, labels)
labels: tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])
</pre>
</div>
</div>
</div>

<div id="outline-container-org7d69d91" class="outline-3">
<h3 id="org7d69d91"><span class="section-number-3">13.3</span> How To Plot Images Using PyTorch DataLoader</h3>
<div class="outline-text-3" id="text-13-3">
<p>
Here is another was to plot the images using the PyTorch DataLoader. 
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FD971F;">how_many_to_plot</span> = 20

<span style="color: #FD971F;">train_loader</span> = torch.utils.data.DataLoader(
    train_set, batch_size=1, shuffle=<span style="color: #AE81FF;">True</span>
)

plt.figure(figsize=(50,50))
<span style="color: #F92672;">for</span> i, batch <span style="color: #F92672;">in</span> <span style="color: #F92672;">enumerate</span>(train_loader, start=1):
    <span style="color: #FD971F;">image</span>, <span style="color: #FD971F;">label</span> = batch
    plt.subplot(10,10,i)
    plt.imshow(image.reshape(28,28), cmap=<span style="color: #E6DB74;">'gray'</span>)
    plt.axis(<span style="color: #E6DB74;">'off'</span>)
    plt.title(train_set.classes[label.item()], fontsize=28)
    <span style="color: #F92672;">if</span> (i &gt;= how_many_to_plot): <span style="color: #F92672;">break</span>
plt.show()
</pre>
</div>


<div class="figure">
<p><img src="%E7%AC%AC16%E8%AF%BE/2020-07-04_22-18-24_fashion%2520mnist%2520grid%2520sample%25202.png" alt="2020-07-04_22-18-24_fashion%2520mnist%2520grid%2520sample%25202.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org650007b" class="outline-2">
<h2 id="org650007b"><span class="section-number-2">14</span> 第17课</h2>
<div class="outline-text-2" id="text-14">
</div>
<div id="outline-container-orgf2e77d5" class="outline-3">
<h3 id="orgf2e77d5"><span class="section-number-3">14.1</span> Building A Neural Network In PyTorch</h3>
<div class="outline-text-3" id="text-14-1">
<p>
We now have enough information to provide an outline for building neural networks in PyTorch. The steps are as follows:
</p>
<ol class="org-ol">
<li>Create a neural network class that extends the <code>nn.Module</code> base class.</li>
<li>In the class constructor, define the network’s layers as class attributes using pre-built layers from <code>torch.nn</code>.</li>
<li>Use the network’s layer attributes as well as operations from the <code>nn.functional</code> API to define the network’s forward pass.</li>
</ol>

<p>
let’s create a simple class to represent a neural network:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #F92672;">class</span> <span style="color: #66D9EF;">Network</span>:
    <span style="color: #F92672;">def</span> <span style="color: #A6E22E;">__init__</span>(<span style="color: #F92672;">self</span>):
        <span style="color: #F92672;">self</span>.layer = <span style="color: #AE81FF;">None</span>

    <span style="color: #F92672;">def</span> <span style="color: #A6E22E;">forward</span>(<span style="color: #F92672;">self</span>, t):
        <span style="color: #FD971F;">t</span> = <span style="color: #F92672;">self</span>.layer(t)
        <span style="color: #F92672;">return</span> t
</pre>
</div>
<p>
This is a good start, but the class hasn’t yet extended the <code>nn.Module</code> class. To make our <code>Network</code> class extend <code>nn.Module</code>, we must do two additional things:
</p>
<ol class="org-ol">
<li>Specify the <code>nn.Module</code> class in parentheses on line 1.</li>
<li>Insert a call to the super class constructor on line 3 inside the constructor.</li>
</ol>

<p>
This gives us:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #F92672;">class</span> <span style="color: #66D9EF;">Network</span>(nn.Module): <span style="color: #75715E;"># </span><span style="color: #75715E;">line 1</span>
    <span style="color: #F92672;">def</span> <span style="color: #A6E22E;">__init__</span>(<span style="color: #F92672;">self</span>):
        <span style="color: #75715E;">#</span><span style="color: #75715E;">super(Network,self).__init__ &#36825;&#26159;python2&#30340;&#20889;&#27861;</span>
        <span style="color: #F92672;">super</span>().__init__() <span style="color: #75715E;"># </span><span style="color: #75715E;">line 3</span>
        <span style="color: #F92672;">self</span>.layer = <span style="color: #AE81FF;">None</span>

    <span style="color: #F92672;">def</span> <span style="color: #A6E22E;">forward</span>(<span style="color: #F92672;">self</span>, t):
        <span style="color: #FD971F;">t</span> = <span style="color: #F92672;">self</span>.layer(t)
        <span style="color: #F92672;">return</span> t
</pre>
</div>
<p>
These changes transform our simple neural network into a PyTorch neural network because we are now extending PyTorch's <code>nn.Module</code> base class.
</p>

<p>
With this, we are done! Now we have a Network class that has all of the functionality of the PyTorch <code>nn.Module</code> class.
</p>
</div>
</div>
<div id="outline-container-orgfc42785" class="outline-3">
<h3 id="orgfc42785"><span class="section-number-3">14.2</span> Define The Network’s Layers As Class Attributes</h3>
<div class="outline-text-3" id="text-14-2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #F92672;">class</span> <span style="color: #66D9EF;">Network</span>(nn.Module):
    <span style="color: #F92672;">def</span> <span style="color: #A6E22E;">__init__</span>(<span style="color: #F92672;">self</span>):
        <span style="color: #F92672;">super</span>().__init__()
        <span style="color: #F92672;">self</span>.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
        <span style="color: #F92672;">self</span>.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)

        <span style="color: #F92672;">self</span>.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)
        <span style="color: #F92672;">self</span>.fc2 = nn.Linear(in_features=120, out_features=60)
        <span style="color: #F92672;">self</span>.out = nn.Linear(in_features=60, out_features=10)

    <span style="color: #F92672;">def</span> <span style="color: #A6E22E;">forward</span>(<span style="color: #F92672;">self</span>, t):
        <span style="color: #75715E;"># </span><span style="color: #75715E;">implement the forward pass</span>
        <span style="color: #F92672;">return</span> t
</pre>
</div>
<div class="org-src-container">
<pre class="src src-bash">&gt;<span style="color: #FD971F;">network</span>=Network()
&gt;network
<span style="color: #A6E22E;">Network</span>(
  (conv1): Conv2d(1, 6, <span style="color: #FD971F;">kernel_size</span>=(5, 5), <span style="color: #FD971F;">stride</span>=(1, 1))
  (conv2): Conv2d(6, 12, <span style="color: #FD971F;">kernel_size</span>=(5, 5), <span style="color: #FD971F;">stride</span>=(1, 1))
  (fc1): Linear(<span style="color: #FD971F;">in_features</span>=192, <span style="color: #FD971F;">out_features</span>=120, <span style="color: #FD971F;">bias</span>=True)
  (fc2): Linear(<span style="color: #FD971F;">in_features</span>=120, <span style="color: #FD971F;">out_features</span>=60, <span style="color: #FD971F;">bias</span>=True)
  (out): Linear(<span style="color: #FD971F;">in_features</span>=60, <span style="color: #FD971F;">out_features</span>=10, <span style="color: #FD971F;">bias</span>=True)
)
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org6ffbb9a" class="outline-2">
<h2 id="org6ffbb9a"><span class="section-number-2">15</span> 第18课</h2>
<div class="outline-text-2" id="text-15">
</div>
<div id="outline-container-org13c1a3c" class="outline-3">
<h3 id="org13c1a3c"><span class="section-number-3">15.1</span> Parameter Vs Argument</h3>
<div class="outline-text-3" id="text-15-1">
<p>
We often hear the words parameter and argument, but what's the difference between these two?
</p>

<p>
We'll parameters are used in function definitions as place-holders while arguments are the actual values that are passed to the function. The parameters can be thought of as local variables that live inside a function.
</p>

<p>
parameter 在函数定义中使用，可以把parameter看做占位符。argument是函数被调用时传递给函数的实际值。
</p>
</div>
</div>
<div id="outline-container-orged588f3" class="outline-3">
<h3 id="orged588f3"><span class="section-number-3">15.2</span> Two Types Of Parameters</h3>
<div class="outline-text-3" id="text-15-2">
<p>
To better understand the argument values for these parameters, let's consider two categories or types of parameters that we used when constructing our layers.
</p>

<ul class="org-ul">
<li>Hyperparameters</li>
<li>Data dependent hyperparameters</li>
</ul>

<p>
In general, hyperparameters are parameters whose values are chosen manually and arbitrarily.
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Parameter</th>
<th scope="col" class="org-left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">kernel<sub>size</sub></td>
<td class="org-left">Sets the filter size. The words kernel and filter are interchangeable.</td>
</tr>

<tr>
<td class="org-left">out<sub>channels</sub></td>
<td class="org-left">Sets the number of filters. One filter produces one output channel.</td>
</tr>

<tr>
<td class="org-left">out<sub>features</sub></td>
<td class="org-left">Sets the size of the output tensor.</td>
</tr>
</tbody>
</table>

<p>
Data dependent hyperparameters are parameters whose values are dependent on data. The first two data dependent hyperparameters that stick out are the <code>in_channels</code> of the first convolutional layer, and the <code>out_features</code> of the output layer.
</p>

<p>
You see, the <code>in_channels</code> of the first convolutional layer depend on the number of color channels present inside the images that make up the training set. Since we are dealing with grayscale images, we know that this value should be a 1.
</p>

<p>
The <code>out_features</code> for the output layer depend on the number of classes that are present inside our training set. Since we have 10 classes of clothing inside the Fashion-MNIST dataset, we know that we need 10 output features.
</p>
</div>
</div>
</div>
<div id="outline-container-org2da9793" class="outline-2">
<h2 id="org2da9793"><span class="section-number-2">16</span> 第19课</h2>
<div class="outline-text-2" id="text-16">
<p>
<code>Model</code> 类中的 <code>__repr__(self)</code> 可以重写python的默认字符串表示，即调用 <code>print</code> 时要输出的内容。
</p>

<p>
我们也可以自定义重写 <code>__repr__(self)</code> 
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #F92672;">class</span> <span style="color: #66D9EF;">Network</span>(nn.Module):
    <span style="color: #F92672;">def</span> <span style="color: #A6E22E;">__init__</span>(<span style="color: #F92672;">self</span>):
        <span style="color: #F92672;">super</span>().__init__()
        <span style="color: #F92672;">self</span>.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
        <span style="color: #F92672;">self</span>.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)

        <span style="color: #F92672;">self</span>.fc1 = nn.Linear(in_features=12*4*4, out_features=120)
        <span style="color: #F92672;">self</span>.fc2 = nn.Linear(in_features=120, out_features=60)
        <span style="color: #F92672;">self</span>.out = nn.Linear(in_features=60, out_features=10)

    <span style="color: #F92672;">def</span> <span style="color: #A6E22E;">forward</span>(<span style="color: #F92672;">self</span>, t):
        <span style="color: #75715E;"># </span><span style="color: #75715E;">implement the forward pass</span>
        <span style="color: #F92672;">return</span> t

    <span style="color: #F92672;">def</span> <span style="color: #A6E22E;">__repr__</span>(<span style="color: #F92672;">self</span>):
    <span style="color: #F92672;">return</span> <span style="color: #E6DB74;">"lizardnet"</span>
</pre>
</div>
<div class="org-src-container">
<pre class="src src-bash">&gt;<span style="color: #FD971F;">network</span>=Network()
&gt;print(network)
lizardnet
</pre>
</div>

<p>
在python中，所有特殊的oop方法通常都有前双划线和后双划线。
</p>

<p>
我们用点符号访问对象中的属性和方法
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; network.conv1
<span style="color: #A6E22E;">Conv2d</span>(1, 6, <span style="color: #FD971F;">kernel_size</span>=(5, 5), <span style="color: #FD971F;">stride</span>=(1, 1))

&gt; network.conv2
<span style="color: #A6E22E;">Conv2d</span>(6, 12, <span style="color: #FD971F;">kernel_size</span>=(5, 5), <span style="color: #FD971F;">stride</span>=(1, 1))

&gt; network.fc1
<span style="color: #A6E22E;">Linear</span>(<span style="color: #FD971F;">in_features</span>=192, <span style="color: #FD971F;">out_features</span>=120, <span style="color: #FD971F;">bias</span>=True)

&gt; network.fc2                                    
<span style="color: #A6E22E;">Linear</span>(<span style="color: #FD971F;">in_features</span>=120, <span style="color: #FD971F;">out_features</span>=60, <span style="color: #FD971F;">bias</span>=True)

&gt; network.out
<span style="color: #A6E22E;">Linear</span>(<span style="color: #FD971F;">in_features</span>=60, <span style="color: #FD971F;">out_features</span>=10, <span style="color: #FD971F;">bias</span>=True)
</pre>
</div>

<p>
可以用下面的方式访问层中的权重：
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; network.conv1.weight
Parameter containing:
<span style="color: #A6E22E;">tensor</span>([[[[ 0.0692,  0.1029, -0.1793,  0.0495,  0.0619],
            [ 0.1860,  0.0503, -0.1270, -0.1240, -0.0872],
            [-0.1924, -0.0684, -0.0028,  0.1031, -0.1053],
            [-0.0607,  0.1332,  0.0191,  0.1069, -0.0977],
            [ 0.0095, -0.1570,  0.1730,  0.0674, -0.1589]]],

        [[[-0.1392,  0.1141, -0.0658,  0.1015,  0.0060],
            [-0.0519,  0.0341,  0.1161,  0.1492, -0.0370],
            [ 0.1077,  0.1146,  0.0707,  0.0927,  0.0192],
            [-0.0656,  0.0929, -0.1735,  0.1019, -0.0546],
            [ 0.0647, -0.0521, -0.0687,  0.1053, -0.0613]]],

        [[[-0.1066, -0.0885,  0.1483, -0.0563,  0.0517],
            [ 0.0266,  0.0752, -0.1901, -0.0931, -0.0657],
            [ 0.0502, -0.0652,  0.0523, -0.0789, -0.0471],
            [-0.0800,  0.1297, -0.0205,  0.0450, -0.1029],
            [-0.1542,  0.1634, -0.0448,  0.0998, -0.1385]]],

        [[[-0.0943,  0.0256,  0.1632, -0.0361, -0.0557],
            [ 0.1083, -0.1647,  0.0846, -0.0163,  0.0068],
            [-0.1241,  0.1761,  0.1914,  0.1492,  0.1270],
            [ 0.1583,  0.0905,  0.1406,  0.1439,  0.1804],
            [-0.1651,  0.1374,  0.0018,  0.0846, -0.1203]]],

        [[[ 0.1786, -0.0800, -0.0995,  0.1690, -0.0529],
            [ 0.0685,  0.1399,  0.0270,  0.1684,  0.1544],
            [ 0.1581, -0.0099, -0.0796,  0.0823, -0.1598],
            [ 0.1534, -0.1373, -0.0740, -0.0897,  0.1325],
            [ 0.1487, -0.0583, -0.0900,  0.1606,  0.0140]]],

        [[[ 0.0919,  0.0575,  0.0830, -0.1042, -0.1347],
            [-0.1615,  0.0451,  0.1563, -0.0577, -0.1096],
            [-0.0667, -0.1979,  0.0458,  0.1971, -0.1380],
            [-0.1279,  0.1753, -0.1063,  0.1230, -0.0475],
            [-0.0608, -0.0046, -0.0043, -0.1543,  0.1919]]]], 
            <span style="color: #FD971F;">requires_grad</span>=True
)
</pre>
</div>

<p>
为了跟踪网络中的所有权重张量，pytorch 有一个名为 <code>parameter</code> 的特殊类。 <code>parameter</code> 类扩张了张量类。每一层的权重就是 <code>parameter</code> 类的实例。
</p>

<p>
下面的两种方法可以同时获得网络的权重参数形状：
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt;for param<span style="color: #F92672;"> in</span> network.parameters():
    print(param.shape)

<span style="color: #A6E22E;">torch.Size</span>([6, 1, 5, 5])
<span style="color: #A6E22E;">torch.Size</span>([6])
<span style="color: #A6E22E;">torch.Size</span>([12, 6, 5, 5])
<span style="color: #A6E22E;">torch.Size</span>([12])
<span style="color: #A6E22E;">torch.Size</span>([120, 192])
<span style="color: #A6E22E;">torch.Size</span>([120])
<span style="color: #A6E22E;">torch.Size</span>([60, 120])
<span style="color: #A6E22E;">torch.Size</span>([60])
<span style="color: #A6E22E;">torch.Size</span>([10, 60])
<span style="color: #A6E22E;">torch.Size</span>([10])

&gt;for name, param<span style="color: #F92672;"> in</span> network.named_parameters():
    print(name, <span style="color: #E6DB74;">'\t\t'</span>, param.shape)

conv1.weight             torch.Size([6, 1, 5, 5])
conv1.bias               torch.Size([6])
conv2.weight             torch.Size([12, 6, 5, 5])
conv2.bias               torch.Size([12])
fc1.weight               torch.Size([120, 192])
fc1.bias                 torch.Size([120])
fc2.weight               torch.Size([60, 120])
fc2.bias                 torch.Size([60])
out.weight               torch.Size([10, 60])
out.bias                 torch.Size([10])
</pre>
</div>
</div>
</div>
<div id="outline-container-orgf54a6a9" class="outline-2">
<h2 id="orgf54a6a9"><span class="section-number-2">17</span> 第20课</h2>
<div class="outline-text-2" id="text-17">
<div class="org-src-container">
<pre class="src src-bash">&gt;in_features = torch.tensor([1,2,3,4], <span style="color: #FD971F;">dtype</span>=torch.float32)

&gt;weight_matrix = torch.tensor([
    [1,2,3,4],
    [2,3,4,5],
    [3,4,5,6]
], <span style="color: #FD971F;">dtype</span>=torch.float32)

&gt; weight_matrix.matmul(in_features)
<span style="color: #A6E22E;">tensor</span>([30., 40., 50.])

&gt;fc = nn.Linear(<span style="color: #FD971F;">in_features</span>=4, <span style="color: #FD971F;">out_features</span>=3, <span style="color: #FD971F;">bias</span>=False) <span style="color: #75715E;">#</span><span style="color: #75715E;">pytorch &#23558;&#25968;&#20540;4&#21644;3&#20256;&#36882;&#32473;&#26500;&#36896;&#20989;&#25968;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;3x4&#30340;&#26435;&#37325;&#30697;&#38453;</span>
</pre>
</div>

<p>
Let's see how we can call our layer now by passing the in<sub>features</sub> tensor.
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; fc(in_features)
<span style="color: #A6E22E;">tensor</span>([-0.8877,  1.4250,  0.8370], <span style="color: #FD971F;">grad_fn</span>=&lt;SqueezeBackward3&gt;) <span style="color: #75715E;">#</span><span style="color: #75715E;">&#26435;&#37325;&#30697;&#38453;&#26159;&#38543;&#26426;&#21021;&#35797;&#21270;&#30340;</span>
</pre>
</div>
<p>
We can call the object instance like this because PyTorch neural network modules are <code>callable Python objects</code>. 
</p>

<p>
我们也可以自己制定权重矩阵：
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; fc.weight = nn.Parameter(weight_matrix)  
&gt; fc(in_features)
<span style="color: #A6E22E;">tensor</span>([30.0261, 40.1404, 49.7643], <span style="color: #FD971F;">grad_fn</span>=&lt;AddBackward0&gt;) <span style="color: #75715E;">#</span><span style="color: #75715E;">&#21487;&#20197;&#30475;&#21040;&#36825;&#37324;&#30340;&#32467;&#26524;&#25509;&#36817;&#20110;&#19978;&#38754;&#30340;&#20363;&#23376;&#65292;&#30001;&#20110;&#32447;&#24615;&#23618;&#33258;&#21160;&#28155;&#21152;&#20102;&#20559;&#32622;&#65292;&#25152;&#20197;&#20250;&#26377;&#24046;&#24322;</span>
</pre>
</div>
<p>
当我们去掉偏置时，结果就一样了：
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; fc = nn.Linear(<span style="color: #FD971F;">in_features</span>=4, <span style="color: #FD971F;">out_features</span>=3, <span style="color: #FD971F;">bias</span>=False)
&gt; fc.weight = nn.Parameter(weight_matrix)
&gt; fc(in_features)  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#25105;&#20204;&#24182;&#19981;&#30452;&#25509;&#35843;&#29992;forward&#26041;&#27861;&#65292;&#32780;&#26159;&#36890;&#36807;pytorch&#20869;&#32622;&#30340; __call__ &#21435;&#35843;&#29992;forward</span>
<span style="color: #A6E22E;">tensor</span>([30., 40., 50.], <span style="color: #FD971F;">grad_fn</span>=&lt;SqueezeBackward3&gt;)
</pre>
</div>
</div>
</div>
<div id="outline-container-orgf0109c7" class="outline-2">
<h2 id="orgf0109c7"><span class="section-number-2">18</span> 第21课</h2>
<div class="outline-text-2" id="text-18">
</div>
<div id="outline-container-orgfa232d3" class="outline-3">
<h3 id="orgfa232d3"><span class="section-number-3">18.1</span> Implementing The <code>forward()</code> Method</h3>
<div class="outline-text-3" id="text-18-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #F92672;">class</span> <span style="color: #66D9EF;">Network</span>(nn.Module):
    <span style="color: #F92672;">def</span> <span style="color: #A6E22E;">__init__</span>(<span style="color: #F92672;">self</span>):
        <span style="color: #F92672;">super</span>().__init__()
        <span style="color: #F92672;">self</span>.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
        <span style="color: #F92672;">self</span>.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)

        <span style="color: #F92672;">self</span>.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)
        <span style="color: #F92672;">self</span>.fc2 = nn.Linear(in_features=120, out_features=60)
        <span style="color: #F92672;">self</span>.out = nn.Linear(in_features=60, out_features=10)

    <span style="color: #F92672;">def</span> <span style="color: #A6E22E;">forward</span>(<span style="color: #F92672;">self</span>, t):
        <span style="color: #75715E;"># </span><span style="color: #75715E;">(1) input layer</span>
        <span style="color: #FD971F;">t</span> = t

        <span style="color: #75715E;"># </span><span style="color: #75715E;">(2) hidden conv layer</span>
        <span style="color: #FD971F;">t</span> = <span style="color: #F92672;">self</span>.conv1(t)
        <span style="color: #FD971F;">t</span> = F.relu(t)
        <span style="color: #FD971F;">t</span> = F.max_pool2d(t, kernel_size=2, stride=2)

        <span style="color: #75715E;"># </span><span style="color: #75715E;">(3) hidden conv layer</span>
        <span style="color: #FD971F;">t</span> = <span style="color: #F92672;">self</span>.conv2(t)
        <span style="color: #FD971F;">t</span> = F.relu(t)
        <span style="color: #FD971F;">t</span> = F.max_pool2d(t, kernel_size=2, stride=2)

        <span style="color: #75715E;"># </span><span style="color: #75715E;">(4) hidden linear layer</span>
        <span style="color: #FD971F;">t</span> = t.reshape(-1, 12 * 4 * 4)
        <span style="color: #FD971F;">t</span> = <span style="color: #F92672;">self</span>.fc1(t)
        <span style="color: #FD971F;">t</span> = F.relu(t)

        <span style="color: #75715E;"># </span><span style="color: #75715E;">(5) hidden linear layer</span>
        <span style="color: #FD971F;">t</span> = <span style="color: #F92672;">self</span>.fc2(t)
        <span style="color: #FD971F;">t</span> = F.relu(t)

        <span style="color: #75715E;"># </span><span style="color: #75715E;">(6) output layer</span>
        <span style="color: #FD971F;">t</span> = <span style="color: #F92672;">self</span>.out(t)
        <span style="color: #75715E;">#</span><span style="color: #75715E;">t = F.softmax(t, dim=1)   #However, in our case, we won't use softmax() because the loss function that we'll use, F.cross_entropy(), implicitly performs the softmax() operation on its input, so we'll just return the result of the last linear transformation.</span>

        <span style="color: #F92672;">return</span> t

</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org08396f3" class="outline-2">
<h2 id="org08396f3"><span class="section-number-2">19</span> 第22课</h2>
<div class="outline-text-2" id="text-19">
<p>
pytorch是动态计算的，我们可以关闭这个功能来减少内存消耗（此时这个图没有存储在内存中）：
</p>
<div class="org-src-container">
<pre class="src src-python">torch.set_grad_enabled(<span style="color: #AE81FF;">False</span>)
</pre>
</div>
</div>
<div id="outline-container-orgd149efe" class="outline-3">
<h3 id="orgd149efe"><span class="section-number-3">19.1</span> Passing A Single Image To The Network</h3>
<div class="outline-text-3" id="text-19-1">
<div class="org-src-container">
<pre class="src src-bash">&gt; network = Network()
&gt; sample = next(iter(train_set)) 
&gt; image, label = sample 
&gt; image.shape 
<span style="color: #A6E22E;">torch.Size</span>([1, 28, 28])
&gt; image.unsqueeze(0).shape
<span style="color: #A6E22E;">torch.Size</span>([1, 1, 28, 28])

&gt; pred = network(image.unsqueeze(0)) <span style="color: #75715E;"># </span><span style="color: #75715E;">image shape needs to be (batch_size &#215; in_channels &#215; H &#215; W)</span>
&gt; pred
<span style="color: #A6E22E;">tensor</span>([[0.0991, 0.0916, 0.0907, 0.0949, 0.1013, 0.0922, 0.0990, 0.1130, 0.1107, 0.1074]])
&gt; pred.shape
<span style="color: #A6E22E;">torch.Size</span>([1, 10])
&gt; label
9
&gt; pred.argmax(<span style="color: #FD971F;">dim</span>=1)  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#37324;&#30001;&#20110;&#21482;&#26159;&#21069;&#21521;&#20256;&#25773;&#65292;&#27809;&#26377;&#35757;&#32451;&#32593;&#32476;&#65292;&#25152;&#20197;&#39044;&#27979;&#20102;7&#65292;&#20294;&#23454;&#38469;&#20540;&#24212;&#35813;&#20026;9&#65292;&#25152;&#20197;&#32593;&#32476;&#36825;&#37324;&#39044;&#27979;&#38169;&#35823;</span>
<span style="color: #A6E22E;">tensor</span>([7])

&gt; F.softmax(pred, <span style="color: #FD971F;">dim</span>=1)  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#20063;&#21487;&#20197;&#29992;softmax&#26469;&#23545;&#25152;&#26377;&#31867;&#30340;&#27010;&#29575;&#36827;&#34892;&#24402;&#19968;&#21270;</span>
<span style="color: #A6E22E;">tensor</span>([[0.1096, 0.1018, 0.0867, 0.0936, 0.1102, 0.0929, 0.1083, 0.0998, 0.0943, 0.1030]])
&gt; F.softmax(pred, <span style="color: #FD971F;">dim</span>=1).sum()
<span style="color: #A6E22E;">tensor</span>(1.)
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org4c65120" class="outline-2">
<h2 id="org4c65120"><span class="section-number-2">20</span> 第23课</h2>
<div class="outline-text-2" id="text-20">
<p>
下面是进行批处理的代码：
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; <span style="color: #FD971F;">network</span>=Network()
&gt; <span style="color: #FD971F;">data_loader</span>=torch.utils.data.DataLoader(train_set,<span style="color: #FD971F;">batch_size</span>=10) <span style="color: #75715E;">#</span><span style="color: #75715E;">&#19982;&#21333;&#20010;&#22270;&#20687;&#30340;&#19981;&#21516;&#23601;&#22312;&#20110;&#22810;&#20102;&#36825;&#19968;&#34892;&#20195;&#30721;</span>
&gt; <span style="color: #FD971F;">batch</span>=next(iter(data_loader))
&gt; images,<span style="color: #FD971F;">labels</span>=batch
&gt; images.shape
<span style="color: #A6E22E;">torch.Size</span>([10, 1, 28, 28])
&gt; labes.shape
<span style="color: #A6E22E;">torch.Size</span>([10])
&gt; <span style="color: #FD971F;">preds</span>=network(images)
&gt; preds.shape
<span style="color: #A6E22E;">torch.Size</span>([10, 10])
&gt; preds
<span style="color: #A6E22E;">tensor</span>([[-0.0939, -0.0701, -0.1039, -0.0429, -0.0485,  0.1127, -0.0217,  0.1144,
         -0.0583,  0.0962],
        [-0.0903, -0.0716, -0.0986, -0.0350, -0.0625,  0.1070, -0.0222,  0.1121,
         -0.0665,  0.0947],
        [-0.0969, -0.0732, -0.0954, -0.0223, -0.0632,  0.1114, -0.0306,  0.1066,
         -0.0685,  0.0969],
        [-0.0964, -0.0757, -0.0994, -0.0219, -0.0633,  0.1086, -0.0302,  0.1073,
         -0.0679,  0.0994],
        [-0.0965, -0.0741, -0.0973, -0.0243, -0.0649,  0.1091, -0.0300,  0.0996,
         -0.0664,  0.0946],
        [-0.0909, -0.0734, -0.0997, -0.0330, -0.0605,  0.1038, -0.0228,  0.1105,
         -0.0672,  0.0962],
        [-0.0942, -0.0715, -0.1013, -0.0376, -0.0533,  0.1124, -0.0292,  0.1054,
         -0.0637,  0.0948],
        [-0.0922, -0.0765, -0.1023, -0.0333, -0.0606,  0.1053, -0.0168,  0.1156,
         -0.0627,  0.1014],
        [-0.0993, -0.0709, -0.0952, -0.0297, -0.0588,  0.1225, -0.0284,  0.1175,
         -0.0641,  0.1043],
        [-0.0931, -0.0636, -0.0966, -0.0426, -0.0502,  0.1173, -0.0184,  0.1162,
         -0.0677,  0.0980]])
&gt; preds.argmax(<span style="color: #FD971F;">dim</span>=1)
<span style="color: #A6E22E;">tensor</span>([7, 7, 5, 5, 5, 7, 5, 7, 5, 5])
&gt; labels
<span style="color: #A6E22E;">tensor</span>([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])
&gt; preds.argmax(<span style="color: #FD971F;">dim</span>=1).eq(labels)
<span style="color: #A6E22E;">tensor</span>([0, 0, 0, 0, 0, 0, 0, 0, 1, 1], <span style="color: #FD971F;">dtype</span>=torch.uint8)
</pre>
</div>
</div>
</div>
<div id="outline-container-org50914f4" class="outline-2">
<h2 id="org50914f4"><span class="section-number-2">21</span> 第24集</h2>
<div class="outline-text-2" id="text-21">
<ul class="org-ul">
<li>Suppose we have an nxn input.</li>
<li>Suppose we have an fxf filter.</li>
<li>Suppose we have a padding of p and a stride of s.</li>
</ul>

<p>
卷积层输出大小等于：（n-f+2p）/s +1
</p>
</div>
</div>
<div id="outline-container-org63d6905" class="outline-2">
<h2 id="org63d6905"><span class="section-number-2">22</span> 第25课</h2>
<div class="outline-text-2" id="text-22">
<p>
我们可以选择打开或关闭梯度追踪功能
</p>
<pre class="example">
&gt; torch.set_grad_enabled(True)  #打开梯度追踪，默认是打开的
&lt;torch.autograd.grad_mode.set_grad_enabled at 0x15b22d012b0&gt;
</pre>

<div class="org-src-container">
<pre class="src src-bash">&gt; <span style="color: #FD971F;">network</span>=Network()

&gt; <span style="color: #FD971F;">train_loader</span>=torch.utils.data.DataLoader(train_set,<span style="color: #FD971F;">batch_size</span>=100)
&gt; <span style="color: #FD971F;">batch</span>=next(iter(train_loader))
&gt; images,<span style="color: #FD971F;">labels</span>=batch

&gt; <span style="color: #FD971F;">preds</span>=network(images)
&gt; <span style="color: #FD971F;">loss</span>=F.cross_entropy(preds,labels)
&gt; loss.item()
2.312843084335327

&gt; network.conv1.weight.grad  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#21487;&#20197;&#30475;&#21040;&#29616;&#22312;&#36824;&#27809;&#26377;&#26799;&#24230;</span>
None 
&gt; loss.backward() <span style="color: #75715E;"># </span><span style="color: #75715E;">Calculating the gradients</span>
&gt; network.conv1.weight.grad.shape <span style="color: #75715E;">#</span><span style="color: #75715E;">&#29616;&#22312;&#26377;&#26799;&#24230;&#20102;</span>
<span style="color: #A6E22E;">torch.Size</span>([6, 1, 5, 5])
</pre>
</div>

<p>
Updating The Weights:To the Adam class constructor, we pass the network parameters (this is how the optimizer is able to access the gradients), and we pass the learning rate .
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; optimizer = optim.Adam(network.parameters(), <span style="color: #FD971F;">lr</span>=0.01)
&gt; optimizer.step() <span style="color: #75715E;"># </span><span style="color: #75715E;">Updating the weights</span>

&gt; preds = network(images)
&gt; loss.item()

&gt; loss = F.cross_entropy(preds, labels)
2.262690782546997   <span style="color: #75715E;">#</span><span style="color: #75715E;">&#21487;&#20197;&#30475;&#21040;&#25439;&#22833;&#20943;&#23567;&#20102;</span>
</pre>
</div>

<p>
下面是单批次训练的代码总结：
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FD971F;">network</span> = Network()

<span style="color: #FD971F;">train_loader</span> = torch.utils.data.DataLoader(train_set, batch_size=100)
<span style="color: #FD971F;">optimizer</span> = optim.Adam(network.parameters(), lr=0.01)

<span style="color: #FD971F;">batch</span> = <span style="color: #F92672;">next</span>(<span style="color: #F92672;">iter</span>(train_loader)) <span style="color: #75715E;"># </span><span style="color: #75715E;">Get Batch</span>
<span style="color: #FD971F;">images</span>, <span style="color: #FD971F;">labels</span> = batch

<span style="color: #FD971F;">preds</span> = network(images) <span style="color: #75715E;"># </span><span style="color: #75715E;">Pass Batch</span>
<span style="color: #FD971F;">loss</span> = F.cross_entropy(preds, labels) <span style="color: #75715E;"># </span><span style="color: #75715E;">Calculate Loss</span>

loss.backward() <span style="color: #75715E;"># </span><span style="color: #75715E;">Calculate Gradients</span>
optimizer.step() <span style="color: #75715E;"># </span><span style="color: #75715E;">Update Weights</span>

<span style="color: #F92672;">print</span>(<span style="color: #E6DB74;">'loss1:'</span>, loss.item())
<span style="color: #FD971F;">preds</span> = network(images)
<span style="color: #FD971F;">loss</span> = F.cross_entropy(preds, labels)
<span style="color: #F92672;">print</span>(<span style="color: #E6DB74;">'loss2:'</span>, loss.item())
</pre>
</div>
</div>
</div>
<div id="outline-container-org01b07d5" class="outline-2">
<h2 id="org01b07d5"><span class="section-number-2">23</span> 第26课</h2>
<div class="outline-text-2" id="text-23">
</div>
<div id="outline-container-org62f1819" class="outline-3">
<h3 id="org62f1819"><span class="section-number-3">23.1</span> Training With All Batches (Single Epoch)</h3>
<div class="outline-text-3" id="text-23-1">
<p>
下面是对所有批次进行训练的代码:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FD971F;">network</span> = Network()

<span style="color: #FD971F;">train_loader</span> = torch.utils.data.DataLoader(train_set, batch_size=100)
<span style="color: #FD971F;">optimizer</span> = optim.Adam(network.parameters(), lr=0.01)

<span style="color: #FD971F;">total_loss</span> = 0
<span style="color: #FD971F;">total_correct</span> = 0

<span style="color: #F92672;">for</span> batch <span style="color: #F92672;">in</span> train_loader: <span style="color: #75715E;"># </span><span style="color: #75715E;">Get Batch</span>
    <span style="color: #FD971F;">images</span>, <span style="color: #FD971F;">labels</span> = batch 

    <span style="color: #FD971F;">preds</span> = network(images) <span style="color: #75715E;"># </span><span style="color: #75715E;">Pass Batch</span>
    <span style="color: #FD971F;">loss</span> = F.cross_entropy(preds, labels) <span style="color: #75715E;"># </span><span style="color: #75715E;">Calculate Loss</span>

    optimizer.zero_grad()  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#27880;&#24847;&#36825;&#37324;&#35201;&#23545;&#26799;&#24230;&#28165;&#38646;&#65292;&#22240;&#20026;pytorch&#20250;&#31215;&#32047;&#65288;&#32047;&#21152;&#65289;&#26799;&#24230;</span>
    loss.backward() <span style="color: #75715E;"># </span><span style="color: #75715E;">Calculate Gradients</span>
    optimizer.step() <span style="color: #75715E;"># </span><span style="color: #75715E;">Update Weights</span>

    <span style="color: #FD971F;">total_loss</span> += loss.item()
    <span style="color: #FD971F;">total_correct</span> += get_num_correct(preds, labels)

<span style="color: #F92672;">print</span>( 
    <span style="color: #E6DB74;">"epoch:"</span>, 0, 
    <span style="color: #E6DB74;">"total_correct:"</span>, total_correct, 
    <span style="color: #E6DB74;">"loss:"</span>, total_loss
) <span style="color: #75715E;"># </span><span style="color: #75715E;">epoch: 0 total_correct: 42104 loss: 476.6809593439102</span>
</pre>
</div>

<p>
We get the results, and we can see that the total number correct out of 60,000 was 42,104.
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; total_correct / len(train_set)
0.7017333333333333
</pre>
</div>
</div>
</div>
<div id="outline-container-org2c11bc7" class="outline-3">
<h3 id="org2c11bc7"><span class="section-number-3">23.2</span> Training With Multiple Epochs</h3>
<div class="outline-text-3" id="text-23-2">
<p>
To do multiple epochs, all we have to do is put this code into a for loop. We'll also add the epoch number to the print statement.
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FD971F;">network</span> = Network()

<span style="color: #FD971F;">train_loader</span> = torch.utils.data.DataLoader(train_set, batch_size=100)
<span style="color: #FD971F;">optimizer</span> = optim.Adam(network.parameters(), lr=0.01)

<span style="color: #F92672;">for</span> epoch <span style="color: #F92672;">in</span> <span style="color: #F92672;">range</span>(10):

    <span style="color: #FD971F;">total_loss</span> = 0
    <span style="color: #FD971F;">total_correct</span> = 0

    <span style="color: #F92672;">for</span> batch <span style="color: #F92672;">in</span> train_loader: <span style="color: #75715E;"># </span><span style="color: #75715E;">Get Batch</span>
        <span style="color: #FD971F;">images</span>, <span style="color: #FD971F;">labels</span> = batch 

        <span style="color: #FD971F;">preds</span> = network(images) <span style="color: #75715E;"># </span><span style="color: #75715E;">Pass Batch</span>
        <span style="color: #FD971F;">loss</span> = F.cross_entropy(preds, labels) <span style="color: #75715E;"># </span><span style="color: #75715E;">Calculate Loss</span>

        optimizer.zero_grad()
        loss.backward() <span style="color: #75715E;"># </span><span style="color: #75715E;">Calculate Gradients</span>
        optimizer.step() <span style="color: #75715E;"># </span><span style="color: #75715E;">Update Weights</span>

        <span style="color: #FD971F;">total_loss</span> += loss.item()
        <span style="color: #FD971F;">total_correct</span> += get_num_correct(preds, labels)

    <span style="color: #F92672;">print</span>(
        <span style="color: #E6DB74;">"epoch"</span>, epoch, 
        <span style="color: #E6DB74;">"total_correct:"</span>, total_correct, 
        <span style="color: #E6DB74;">"loss:"</span>, total_loss
    )
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org97c6479" class="outline-2">
<h2 id="org97c6479"><span class="section-number-2">24</span> 第27课</h2>
<div class="outline-text-2" id="text-24">
<div class="org-src-container">
<pre class="src src-bash">&gt; len(train_set)
60000
&gt; len(train_set.targets)
60000
</pre>
</div>
</div>
<div id="outline-container-org508cfc2" class="outline-3">
<h3 id="org508cfc2"><span class="section-number-3">24.1</span> Getting prediction for the entire training set<sub>grad</sub><sub>enabled</sub></h3>
<div class="outline-text-3" id="text-24-1">
<p>
在训练过程中，我们不会得到任何张量的梯度值，直到我们对张量进行方向调用。
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; def get_all_preds(model, loader):
    all_preds = torch.tensor([])
    <span style="color: #F92672;">for</span> batch<span style="color: #F92672;"> in</span> loader:
        images, labels = batch

        preds = model(images)
        all_preds = torch.cat(
            (all_preds, preds)
            ,<span style="color: #FD971F;">dim</span>=0
        )
    <span style="color: #F92672;">return</span> all_preds
&gt; <span style="color: #FD971F;">prediction_loader</span>=torch.utils.data.DataLoader(train_set,batch_size =1000)
&gt; <span style="color: #FD971F;">train_preds</span>=get_all_preds(network,prediction_loader)
&gt; train_preds.shape
<span style="color: #A6E22E;">torch.Size</span>([60000,10])
&gt; print(train_preds.requires_grad)
True
&gt; train_preds.grad <span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#37324;&#19981;&#20250;&#26174;&#31034;&#20219;&#20309;&#19996;&#35199;&#65292;&#22240;&#20026;&#25105;&#20204;&#27809;&#26377;&#20570;&#21453;&#21521;&#20256;&#25773;</span>
&gt; train_preds.grad_fn 

<span style="color: #75715E;">#</span><span style="color: #75715E;">&#19979;&#38754;&#36825;&#20010;&#34920;&#31034;&#25152;&#26377;&#35745;&#31639;&#37117;&#19981;&#29992;&#36861;&#36394;&#26799;&#24230;&#65292;&#36825;&#26159;&#19968;&#31181;&#23616;&#37096;&#20851;&#38381;&#26799;&#24230;&#36861;&#36394;&#30340;&#26041;&#27861;</span>
&gt; with torch.no_grad():
     prediction_loader = torch.utils.data.DataLoader(train_set, <span style="color: #FD971F;">batch_size</span>=10000)
     train_preds = get_all_preds(network, prediction_loader)

&gt; print(train_preds.requires_grad) <span style="color: #75715E;">#</span><span style="color: #75715E;">false,&#22240;&#20026;&#21019;&#24314;&#36825;&#20010;&#24352;&#37327;&#26102;&#20851;&#38381;&#20102;&#26799;&#24230;&#36319;&#36394;</span>

&gt; train_preds.grad <span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#37324;&#19981;&#20250;&#26174;&#31034;&#20219;&#20309;&#19996;&#35199;&#65292;&#22240;&#20026;&#25105;&#20204;&#27809;&#26377;&#20570;&#21453;&#21521;&#20256;&#25773;</span>
&gt; train_preds.grad_fn 

&gt; preds_correct = get_num_correct(train_preds, train_set.targets)
&gt; print(<span style="color: #E6DB74;">'total correct:'</span>, preds_correct)
&gt; print(<span style="color: #E6DB74;">'accuracy:'</span>, preds_correct / len(train_set))
total correct: 53578
accuracy: 0.8929666666666667

</pre>
</div>

<p>
关闭梯度跟踪的另一个方法就是用下面的修饰 <code>torch.no_grad</code> 来注释函数.这意味着无论何时调用这个函数，它的梯度跟踪在该函数的上下门中被局部调用
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #66D9EF;">@torch.no_grad</span>()
<span style="color: #F92672;">def</span> <span style="color: #A6E22E;">get_all_preds</span>(model, loader):
    <span style="color: #FD971F;">all_preds</span> = torch.tensor([])
    <span style="color: #F92672;">for</span> batch <span style="color: #F92672;">in</span> loader:
        <span style="color: #FD971F;">images</span>, <span style="color: #FD971F;">labels</span> = batch

        <span style="color: #FD971F;">preds</span> = model(images)
        <span style="color: #FD971F;">all_preds</span> = torch.cat(
            (all_preds, preds)
            ,dim=0
        )
    <span style="color: #F92672;">return</span> all_preds
</pre>
</div>
<pre class="example">
Note at the top, we have annotated the function using the @torch.no_grad() PyTorch decoration. This is because we want this functions execution to omit gradient tracking.

This is because gradient tracking uses memory, and during inference (getting predictions while not training) there is no need to keep track of the computational graph. The decoration is one way of locally turning off the gradient tracking feature while executing specific functions.
</pre>
</div>
</div>

<div id="outline-container-orgf8adb35" class="outline-3">
<h3 id="orgf8adb35"><span class="section-number-3">24.2</span> Building The Confusion Matrix</h3>
<div class="outline-text-3" id="text-24-2">
<p>
To do this, we need to have the targets tensor and the predicted label from the train<sub>preds</sub> tensor.
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; train_set.targets
<span style="color: #A6E22E;">tensor</span>([9, 0, 0,  ..., 3, 0, 5])

&gt; train_preds.argmax(<span style="color: #FD971F;">dim</span>=1)
<span style="color: #A6E22E;">tensor</span>([9, 0, 0,  ..., 3, 0, 5])

&gt; stacked = torch.stack(
    (
        train_set.targets
        ,train_preds.argmax(<span style="color: #FD971F;">dim</span>=1)
    )
    ,<span style="color: #FD971F;">dim</span>=1
)

&gt; stacked.shape
<span style="color: #A6E22E;">torch.Size</span>([60000, 2])

&gt; stacked
<span style="color: #A6E22E;">tensor</span>([
    [9, 9],
    [0, 0],
    [0, 0],
    ...,
    [3, 3],
    [0, 0],
    [5, 5]
])

&gt; stacked[0].tolist()
[9, 9]

&gt; cmt = torch.zeros(10,10, <span style="color: #FD971F;">dtype</span>=torch.int64)
&gt; cmt
<span style="color: #A6E22E;">tensor</span>([
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
])
&gt; for p<span style="color: #F92672;"> in</span> stacked:
     tl, pl = p.tolist()
     cmt[tl, pl] = cmt[tl, pl] + 1

&gt; cmt
<span style="color: #A6E22E;">tensor</span>([
    [5637,    3,   96,   75,   20,   10,   86,    0,   73,    0],
    [  40, 5843,    3,   75,   16,    8,    5,    0,   10,    0],
    [  87,    4, 4500,   70, 1069,    8,  156,    0,  106,    0],
    [ 339,   61,   19, 5269,  203,   10,   72,    2,   25,    0],
    [  23,    9,  263,  209, 5217,    2,  238,    0,   39,    0],
    [   0,    0,    0,    1,    0, 5604,    0,  333,   13,   49],
    [1827,    7,  716,  104,  792,    3, 2370,    0,  181,    0],
    [   0,    0,    0,    0,    0,   22,    0, 5867,    4,  107],
    [  32,    1,   13,   15,   19,    5,   17,   11, 5887,    0],
    [   0,    0,    0,    0,    0,   28,    0,  234,    6, 5732]
])
</pre>
</div>
</div>
</div>

<div id="outline-container-org1050d91" class="outline-3">
<h3 id="org1050d91"><span class="section-number-3">24.3</span> Plotting The Confusion Matrix</h3>
<div class="outline-text-3" id="text-24-3">
<p>
To generate the actual confusion matrix as a <code>numpy.ndarray</code>, we use the <code>confusion_matrix()</code> function from the <code>sklearn.metrics</code> library. Let's get this imported along with our other needed imports.
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; import matplotlib.pyplot as plt

&gt; from sklearn.metrics import confusion_matrix
&gt; from resources.plotcm import plot_confusion_matrix

&gt; cm = confusion_matrix(train_set.targets, train_preds.argmax(<span style="color: #FD971F;">dim</span>=1))
&gt; print(<span style="color: #F92672;">type</span>(cm))
&gt; cm
&lt;class <span style="color: #E6DB74;">'numpy.ndarray'</span>&gt;
Out[74]:
<span style="color: #A6E22E;">array</span>([[5431,   14,   88,  145,   26,    7,  241,    0,   48,    0],
        [   4, 5896,    6,   75,    8,    0,    8,    0,    3,    0],
        [  92,    6, 5002,   76,  565,    1,  232,    1,   25,    0],
        [ 191,   49,   23, 5504,  162,    1,   61,    0,    7,    2],
        [  15,   12,  267,  213, 5305,    1,  168,    0,   19,    0],
        [   0,    0,    0,    0,    0, 5847,    0,  112,    3,   38],
        [1159,   16,  523,  189,  676,    0, 3396,    0,   41,    0],
        [   0,    0,    0,    0,    0,   99,    0, 5540,    0,  361],
        [  28,    6,   29,   15,   32,   23,   26,   14, 5827,    0],
        [   0,    0,    0,    0,    1,   61,    0,  107,    1, 5830]],
        <span style="color: #FD971F;">dtype</span>=int64)

&gt; plt.figure(<span style="color: #FD971F;">figsize</span>=(10,10))
&gt; plot_confusion_matrix(cm, train_set.classes)

Confusion matrix, without normalization
[[5431   14   88  145   26    7  241    0   48    0]
[   4 5896    6   75    8    0    8    0    3    0]
[  92    6 5002   76  565    1  232    1   25    0]
[ 191   49   23 5504  162    1   61    0    7    2]
[  15   12  267  213 5305    1  168    0   19    0]
[   0    0    0    0    0 5847    0  112    3   38]
[1159   16  523  189  676    0 3396    0   41    0]
[   0    0    0    0    0   99    0 5540    0  361]
[  28    6   29   15   32   23   26   14 5827    0]
[   0    0    0    0    1   61    0  107    1 5830]]
</pre>
</div>

<p>
下面是 <code>plotcm.py</code> 的内容,可以自己安装 <code>resources</code> 库，或者把下面用到的函数直接复制使用即可:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #F92672;">import</span> itertools
<span style="color: #F92672;">import</span> numpy <span style="color: #F92672;">as</span> np
<span style="color: #F92672;">import</span> matplotlib.pyplot <span style="color: #F92672;">as</span> plt

<span style="color: #F92672;">def</span> <span style="color: #A6E22E;">plot_confusion_matrix</span>(cm, classes, normalize=<span style="color: #AE81FF;">False</span>, title=<span style="color: #E6DB74;">'Confusion matrix'</span>, cmap=plt.cm.Blues):
    <span style="color: #F92672;">if</span> normalize:
        <span style="color: #FD971F;">cm</span> = cm.astype(<span style="color: #E6DB74;">'float'</span>) / cm.<span style="color: #F92672;">sum</span>(axis=1)[:, np.newaxis]
        <span style="color: #F92672;">print</span>(<span style="color: #E6DB74;">"Normalized confusion matrix"</span>)
    <span style="color: #F92672;">else</span>:
        <span style="color: #F92672;">print</span>(<span style="color: #E6DB74;">'Confusion matrix, without normalization'</span>)

    <span style="color: #F92672;">print</span>(cm)
    plt.imshow(cm, interpolation=<span style="color: #E6DB74;">'nearest'</span>, cmap=cmap)
    plt.title(title)
    plt.colorbar()
    <span style="color: #FD971F;">tick_marks</span> = np.arange(<span style="color: #F92672;">len</span>(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    <span style="color: #FD971F;">fmt</span> = <span style="color: #E6DB74;">'.2f'</span> <span style="color: #F92672;">if</span> normalize <span style="color: #F92672;">else</span> <span style="color: #E6DB74;">'d'</span>
    <span style="color: #FD971F;">thresh</span> = cm.<span style="color: #F92672;">max</span>() / 2.
    <span style="color: #F92672;">for</span> i, j <span style="color: #F92672;">in</span> itertools.product(<span style="color: #F92672;">range</span>(cm.shape[0]), <span style="color: #F92672;">range</span>(cm.shape[1])):
        plt.text(j, i, <span style="color: #F92672;">format</span>(cm[i, j], fmt), horizontalalignment=<span style="color: #E6DB74;">"center"</span>, color=<span style="color: #E6DB74;">"white"</span> <span style="color: #F92672;">if</span> cm[i, j] &gt; thresh <span style="color: #F92672;">else</span> <span style="color: #E6DB74;">"black"</span>)

    plt.tight_layout()
    plt.ylabel(<span style="color: #E6DB74;">'True label'</span>)
    plt.xlabel(<span style="color: #E6DB74;">'Predicted label'</span>)
</pre>
</div>


<p>
下面是完整的代码：
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #F92672;">import</span> matplotlib.pyplot <span style="color: #F92672;">as</span> plt
<span style="color: #F92672;">import</span> itertools
<span style="color: #F92672;">import</span> numpy <span style="color: #F92672;">as</span> np
<span style="color: #F92672;">import</span> matplotlib.pyplot <span style="color: #F92672;">as</span> plt
<span style="color: #F92672;">from</span> sklearn.metrics <span style="color: #F92672;">import</span> confusion_matrix
<span style="color: #F92672;">from</span> resources.plotcm <span style="color: #F92672;">import</span> plot_confusion_matrix

<span style="color: #F92672;">def</span> <span style="color: #A6E22E;">get_all_preds</span>(model, loader):
    <span style="color: #FD971F;">all_preds</span> = torch.tensor([])
    <span style="color: #F92672;">for</span> batch <span style="color: #F92672;">in</span> loader:
        <span style="color: #FD971F;">images</span>, <span style="color: #FD971F;">labels</span> = batch

        <span style="color: #FD971F;">preds</span> = model(images)
        <span style="color: #FD971F;">all_preds</span> = torch.cat(
            (all_preds, preds)
            ,dim=0
        )
    <span style="color: #F92672;">return</span> all_preds

<span style="color: #F92672;">with</span> torch.no_grad():
     <span style="color: #FD971F;">prediction_loader</span> = torch.utils.data.DataLoader(train_set, batch_size=10000)
     <span style="color: #FD971F;">train_preds</span> = get_all_preds(network, prediction_loader)

<span style="color: #FD971F;">cm</span> = confusion_matrix(train_set.targets, train_preds.argmax(dim=1))
plt.figure(figsize=(10,10))
plot_confusion_matrix(cm, train_set.classes)
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org84577c7" class="outline-2">
<h2 id="org84577c7"><span class="section-number-2">25</span> 第28集</h2>
<div class="outline-text-2" id="text-25">
<p>
The difference between stacking and concatenating tensors can be described in a single sentence, so here goes.
</p>
<pre class="example">
Concatenating joins a sequence of tensors along an existing axis, and stacking joins a sequence of tensors along a new axis.
</pre>

<p>
下表是拼接操作在不同库里的名称，而堆叠的名称都是一样的，用 <code>stack</code> 。
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Library</th>
<th scope="col" class="org-left">Function Name</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">PyTorch</td>
<td class="org-left">cat()</td>
</tr>

<tr>
<td class="org-left">TensorFlow</td>
<td class="org-left">concat()</td>
</tr>

<tr>
<td class="org-left">NumPy</td>
<td class="org-left">concatenate()</td>
</tr>
</tbody>
</table>

<p>
使用 <code>??toech.stack</code> 可以查看帮助
</p>
</div>

<div id="outline-container-org21f3762" class="outline-3">
<h3 id="org21f3762"><span class="section-number-3">25.1</span> How To Add Or Insert An Axis Into A Tensor</h3>
<div class="outline-text-3" id="text-25-1">
<div class="org-src-container">
<pre class="src src-bash">&gt; import torch
&gt; t1 = torch.tensor([1,1,1])
&gt; t1.unsqueeze(<span style="color: #FD971F;">dim</span>=0) <span style="color: #75715E;">#</span><span style="color: #75715E;">&#30456;&#24403;&#20110;t1.unsqueeze(0)</span>
<span style="color: #A6E22E;">tensor</span>([[1, 1, 1]])
&gt; t1.unsqueeze(<span style="color: #FD971F;">dim</span>=1) <span style="color: #75715E;">#</span><span style="color: #75715E;">&#30456;&#24403;&#20110;t1.unsqueeze(1)</span>
<span style="color: #A6E22E;">tensor</span>([[1],
        [1],
        [1]])
&gt; print(t1.shape)
&gt; print(t1.unsqueeze(<span style="color: #FD971F;">dim</span>=0).shape)
&gt; print(t1.unsqueeze(<span style="color: #FD971F;">dim</span>=1).shape)
<span style="color: #A6E22E;">torch.Size</span>([3])
<span style="color: #A6E22E;">torch.Size</span>([1, 3])
<span style="color: #A6E22E;">torch.Size</span>([3, 1])
</pre>
</div>
</div>
</div>
<div id="outline-container-orgacdd2fe" class="outline-3">
<h3 id="orgacdd2fe"><span class="section-number-3">25.2</span> Stack Vs Cat In PyTorch</h3>
<div class="outline-text-3" id="text-25-2">
<div class="org-src-container">
<pre class="src src-bash">import torch

&gt; t1 = torch.tensor([1,1,1])
&gt; t2 = torch.tensor([2,2,2])
&gt; t3 = torch.tensor([3,3,3])

&gt; torch.cat(
    (t1,t2,t3)
    ,<span style="color: #FD971F;">dim</span>=0
  )
<span style="color: #A6E22E;">tensor</span>([1, 1, 1, 2, 2, 2, 3, 3, 3])

&gt; torch.stack(
    (t1,t2,t3)
    ,<span style="color: #FD971F;">dim</span>=0
  )
<span style="color: #A6E22E;">tensor</span>([[1, 1, 1],
        [2, 2, 2],
        [3, 3, 3]])

&gt; torch.cat((t1.unsqueeze(0),t2.unsqueeze(0),t3.unsqueeze(0)),<span style="color: #FD971F;">dim</span>=0)    <span style="color: #75715E;">#</span><span style="color: #75715E;">&#27880;&#24847;&#36825;&#31181;&#26041;&#24335;&#21644;stack&#26159;&#19968;&#26679;&#30340;&#65292;&#22240;&#20026;&#36825;&#37324;&#29992;unsqueeze(0)&#26032;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#32500;&#24230;&#20102;</span>
<span style="color: #A6E22E;">tensor</span>([[1, 1, 1],
        [2, 2, 2],
        [3, 3, 3]])
</pre>
</div>
<p>
由于 <code>t1,t2,t3</code> 没有第2个维度，所以是没有办法对第二个维度进行 <code>cat</code> 操作的，所以我们只能用stack
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; torch.stack(   
    (t1,t2,t3)
    ,<span style="color: #FD971F;">dim</span>=1
)
<span style="color: #A6E22E;">tensor</span>([[1, 2, 3],
        [1, 2, 3],
        [1, 2, 3]])
&gt; t1.unsqueeze(1)  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#27880;&#24847;&#32473;t1&#28155;&#21152;&#31532;2&#20010;&#32500;&#24230;&#30340;&#32467;&#26524;&#26159;&#36825;&#26679;&#30340;</span>
<span style="color: #A6E22E;">tensor</span>([[1],
        [1],
        [1]])
&gt; torch.cat(
    (
         t1.unsqueeze(1)
        ,t2.unsqueeze(1)
        ,t3.unsqueeze(1)
    )
    ,<span style="color: #FD971F;">dim</span>=1
)
<span style="color: #A6E22E;">tensor</span>([[1, 2, 3],
        [1, 2, 3],
        [1, 2, 3]])
</pre>
</div>
<p>
从上面的例子可以看出： <code>stack</code> 就是相当于用 <code>unsqueeze(dim)</code> 创建了新的维度，然后用 <code>cat</code> 的方式拼接起来。
</p>

<p>
当需要把很多3维图像转化成果一批图像时，可以用stack；当需要把两批图像结合成一批图像时，可以用cat.
</p>
</div>
</div>
<div id="outline-container-orge0efa58" class="outline-3">
<h3 id="orge0efa58"><span class="section-number-3">25.3</span> Stack Vs Concat In TensorFlow</h3>
<div class="outline-text-3" id="text-25-3">
<div class="org-src-container">
<pre class="src src-bash">&gt; import tensorflow as tf

&gt; t1 = tf.constant([1,1,1])
&gt; t2 = tf.constant([2,2,2])
&gt; t3 = tf.constant([3,3,3])

&gt; tf.concat(
    (t1,t2,t3)
    ,<span style="color: #FD971F;">axis</span>=0
)
tf.Tensor: <span style="color: #FD971F;">id</span>=4, <span style="color: #FD971F;">shape</span>=(9,), <span style="color: #FD971F;">dtype</span>=int32, <span style="color: #FD971F;">numpy</span>=array([1, 1, 1, 2, 2, 2, 3, 3, 3])

&gt; tf.stack(
    (t1,t2,t3)
    ,<span style="color: #FD971F;">axis</span>=0
)
tf.Tensor: <span style="color: #FD971F;">id</span>=6, <span style="color: #FD971F;">shape</span>=(3, 3), <span style="color: #FD971F;">dtype</span>=int32, <span style="color: #FD971F;">numpy</span>=
<span style="color: #A6E22E;">array</span>([[1, 1, 1],
       [2, 2, 2],
       [3, 3, 3]])

&gt; tf.concat(
    (
         tf.expand_dims(t1, 0)  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#30456;&#24403;&#20110;unsqueeze</span>
        ,tf.expand_dims(t2, 0)
        ,tf.expand_dims(t3, 0)
    )    
    ,<span style="color: #FD971F;">axis</span>=0
)
tf.Tensor: <span style="color: #FD971F;">id</span>=15, <span style="color: #FD971F;">shape</span>=(3, 3), <span style="color: #FD971F;">dtype</span>=int32, <span style="color: #FD971F;">numpy</span>=
<span style="color: #A6E22E;">array</span>([[1, 1, 1],
       [2, 2, 2],
       [3, 3, 3]])
</pre>
</div>
<p>
同样的， <code>concat</code> 只能对已经有的维度使用，当维度不存在时，只能用 <code>stack</code>
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; tf.stack(
    (t1,t2,t3)
    ,<span style="color: #FD971F;">axis</span>=1
)
tf.Tensor: <span style="color: #FD971F;">id</span>=17, <span style="color: #FD971F;">shape</span>=(3, 3), <span style="color: #FD971F;">dtype</span>=int32, <span style="color: #FD971F;">numpy</span>=
<span style="color: #A6E22E;">array</span>([[1, 2, 3],
       [1, 2, 3],
       [1, 2, 3]])

&gt; tf.concat(
    (
         tf.expand_dims(t1, 1)
        ,tf.expand_dims(t2, 1)
        ,tf.expand_dims(t3, 1)
    )
    ,<span style="color: #FD971F;">axis</span>=1
)
tf.Tensor: <span style="color: #FD971F;">id</span>=26, <span style="color: #FD971F;">shape</span>=(3, 3), <span style="color: #FD971F;">dtype</span>=int32, <span style="color: #FD971F;">numpy</span>=
<span style="color: #A6E22E;">array</span>([[1, 2, 3],
       [1, 2, 3],
       [1, 2, 3]])

</pre>
</div>
</div>
</div>
<div id="outline-container-org469847c" class="outline-3">
<h3 id="org469847c"><span class="section-number-3">25.4</span> Stack Vs Concatenate In NumPy</h3>
<div class="outline-text-3" id="text-25-4">
<div class="org-src-container">
<pre class="src src-bash">&gt; import numpy as np

&gt; t1 = np.array([1,1,1])
&gt; t2 = np.array([2,2,2])
&gt; t3 = np.array([3,3,3])

&gt; np.concatenate(
    (t1,t2,t3)
    ,<span style="color: #FD971F;">axis</span>=0
  )
<span style="color: #A6E22E;">array</span>([1, 1, 1, 2, 2, 2, 3, 3, 3])
&gt; np.stack(
    (t1,t2,t3)
    ,<span style="color: #FD971F;">axis</span>=0
  )
<span style="color: #A6E22E;">array</span>([[1, 1, 1],
       [2, 2, 2],
       [3, 3, 3]])

&gt; np.concatenate(
    (
         np.expand_dims(t1, 0)
        ,np.expand_dims(t2, 0)
        ,np.expand_dims(t3, 0)
    )
    ,<span style="color: #FD971F;">axis</span>=0
)
<span style="color: #A6E22E;">array</span>([[1, 1, 1],
       [2, 2, 2],
       [3, 3, 3]])
</pre>
</div>
<p>
同样的，只能对已经有的维度使用 <code>stack</code>
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; np.stack(  
    (t1,t2,t3)
    ,<span style="color: #FD971F;">axis</span>=1
)  
<span style="color: #A6E22E;">array</span>([[1, 2, 3],
       [1, 2, 3],
       [1, 2, 3]])

&gt; np.concatenate(
    (
         np.expand_dims(t1, 1)
        ,np.expand_dims(t2, 1)
        ,np.expand_dims(t3, 1)
    )
    ,<span style="color: #FD971F;">axis</span>=1
)
<span style="color: #A6E22E;">array</span>([[1, 2, 3],
       [1, 2, 3],
       [1, 2, 3]])
</pre>
</div>

<p>
可以用下面的方式查看 <code>stack</code> 的函数定义
</p>
<div class="org-src-container">
<pre class="src src-bash">&gt; import numpy
&gt; ??numpy.stack
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org2973e4d" class="outline-2">
<h2 id="org2973e4d"><span class="section-number-2">26</span> 第29集</h2>
<div class="outline-text-2" id="text-26">
</div>
<div id="outline-container-org667606c" class="outline-3">
<h3 id="org667606c"><span class="section-number-3">26.1</span> TensorBoard: TensorFlow's Visualization Toolkit</h3>
<div class="outline-text-3" id="text-26-1">
<p>
<code>TensorBoard</code> provides the visualization and tooling needed for machine learning experimentation:
</p>

<ul class="org-ul">
<li>Tracking and visualizing metrics such as loss and accuracy</li>
<li>Visualizing the model graph (ops and layers)</li>
<li>Viewing histograms of weights, biases, or other tensors as they change over time</li>
<li>Projecting embeddings to a lower dimensional space</li>
<li>Displaying images, text, and audio data</li>
<li>Profiling TensorFlow programs</li>
<li>And much more</li>
</ul>

<p>
As of PyTorch version <code>1.1.0</code>, PyTorch has added a <code>tensorboard</code> utility package that enables us to use TensorBoard with PyTorch.
</p>

<p>
<code>tensorboard</code> 必须另外安装才能使用。
</p>
</div>
</div>
<div id="outline-container-orgc268825" class="outline-3">
<h3 id="orgc268825"><span class="section-number-3">26.2</span> Getting Started With TensorBoard For PyTorch</h3>
<div class="outline-text-3" id="text-26-2">
<p>
To make this easy for us, PyTorch has created a utility class called <code>SummaryWriter</code>. To get access to this class we use the following import:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #F92672;">from</span> torch.utils.tensorboard <span style="color: #F92672;">import</span> SummaryWriter
</pre>
</div>
</div>
<div id="outline-container-org7600d59" class="outline-4">
<h4 id="org7600d59"><span class="section-number-4">26.2.1</span> Network Graph And Training Set Image</h4>
<div class="outline-text-4" id="text-26-2-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #F92672;">import</span> torch
<span style="color: #F92672;">import</span> torch.nn <span style="color: #F92672;">as</span> nn
<span style="color: #F92672;">import</span> torch.nn.functional <span style="color: #F92672;">as</span> F
<span style="color: #F92672;">import</span> torch.optim <span style="color: #F92672;">as</span> optim

<span style="color: #F92672;">import</span> torchvision
<span style="color: #F92672;">import</span> torchvision.transforms <span style="color: #F92672;">as</span> transforms

torch.set_printoptions(linewidth=120)

<span style="color: #F92672;">from</span> torch.utils.tensorboard <span style="color: #F92672;">import</span> SummaryWriter

<span style="color: #F92672;">def</span> <span style="color: #A6E22E;">get_num_correct</span>(preds,labels):
    <span style="color: #F92672;">return</span> preds.argmax(dim=1).eq(labels).<span style="color: #F92672;">sum</span>().item()

<span style="color: #F92672;">class</span> <span style="color: #66D9EF;">Network</span>(nn.Module):
    <span style="color: #F92672;">def</span> <span style="color: #A6E22E;">__init__</span>(<span style="color: #F92672;">self</span>):
        <span style="color: #F92672;">super</span>().__init__()
        <span style="color: #F92672;">self</span>.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
        <span style="color: #F92672;">self</span>.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)

        <span style="color: #F92672;">self</span>.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)
        <span style="color: #F92672;">self</span>.fc2 = nn.Linear(in_features=120, out_features=60)
        <span style="color: #F92672;">self</span>.out = nn.Linear(in_features=60, out_features=10)

    <span style="color: #F92672;">def</span> <span style="color: #A6E22E;">forward</span>(<span style="color: #F92672;">self</span>, t):
        <span style="color: #75715E;"># </span><span style="color: #75715E;">(1) input layer</span>
        <span style="color: #FD971F;">t</span> = t

        <span style="color: #75715E;"># </span><span style="color: #75715E;">(2) hidden conv layer</span>
        <span style="color: #FD971F;">t</span> = <span style="color: #F92672;">self</span>.conv1(t)
        <span style="color: #FD971F;">t</span> = F.relu(t)
        <span style="color: #FD971F;">t</span> = F.max_pool2d(t, kernel_size=2, stride=2)

        <span style="color: #75715E;"># </span><span style="color: #75715E;">(3) hidden conv layer</span>
        <span style="color: #FD971F;">t</span> = <span style="color: #F92672;">self</span>.conv2(t)
        <span style="color: #FD971F;">t</span> = F.relu(t)
        <span style="color: #FD971F;">t</span> = F.max_pool2d(t, kernel_size=2, stride=2)

        <span style="color: #75715E;"># </span><span style="color: #75715E;">(4) hidden linear layer</span>
        <span style="color: #FD971F;">t</span> = t.reshape(-1, 12 * 4 * 4)
        <span style="color: #FD971F;">t</span> = <span style="color: #F92672;">self</span>.fc1(t)
        <span style="color: #FD971F;">t</span> = F.relu(t)

        <span style="color: #75715E;"># </span><span style="color: #75715E;">(5) hidden linear layer</span>
        <span style="color: #FD971F;">t</span> = <span style="color: #F92672;">self</span>.fc2(t)
        <span style="color: #FD971F;">t</span> = F.relu(t)

        <span style="color: #75715E;"># </span><span style="color: #75715E;">(6) output layer</span>
        <span style="color: #FD971F;">t</span> = <span style="color: #F92672;">self</span>.out(t)
        <span style="color: #75715E;">#</span><span style="color: #75715E;">t = F.softmax(t, dim=1)   </span>
        <span style="color: #F92672;">return</span> t

<span style="color: #FD971F;">train_set</span> = torchvision.datasets.FashionMNIST(
    root=<span style="color: #E6DB74;">'./data'</span>  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#34920;&#31034;&#25968;&#25454;&#38598;&#19979;&#36733;&#20301;&#32622;</span>
    ,train=<span style="color: #AE81FF;">True</span>    <span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#20010;&#34920;&#31034;&#25105;&#20204;&#24076;&#26395;&#20351;&#29992;&#35757;&#32451;&#35757;&#32451;&#38598;&#30340;&#25968;&#25454;&#65292;FashionMNIST&#25968;&#25454;&#38598;&#20013;&#26377;6&#19975;&#24352;&#29992;&#20316;&#35757;&#32451;&#25968;&#25454;&#65292;1&#19975;&#24352;&#29992;&#20110;&#27979;&#35797;&#25968;&#25454;</span>
    ,download=<span style="color: #AE81FF;">True</span> <span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#20010;&#34920;&#31034;&#22914;&#26524;&#19979;&#36733;&#20301;&#32622;&#27809;&#26377;&#25968;&#25454;&#38598;&#65292;&#21017;&#19979;&#36733;&#25968;&#25454;&#38598;</span>
    ,transform=transforms.Compose([  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#20010;&#20351;&#29992;&#20102;&#20869;&#32622;&#30340; ToTensor &#34920;&#25442;&#26469;&#36716;&#25442;&#22270;&#20687;</span>
        transforms.ToTensor()
    ])
)

<span style="color: #FD971F;">train_loader</span>=torch.utils.data.DataLoader(train_set,batch_size=100,shuffle=<span style="color: #AE81FF;">True</span>)
</pre>
</div>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FD971F;">tb</span> = SummaryWriter()

<span style="color: #FD971F;">network</span> = Network()
<span style="color: #FD971F;">images</span>, <span style="color: #FD971F;">labels</span> = <span style="color: #F92672;">next</span>(<span style="color: #F92672;">iter</span>(train_loader))
<span style="color: #FD971F;">grid</span> = torchvision.utils.make_grid(images)  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#20010;&#26159;&#21046;&#20316;&#32593;&#26684;&#30340;&#20989;&#25968;&#65292;&#36825;&#20010;&#21019;&#36896;&#20986;&#19968;&#32452;&#22270;&#20687;&#32593;&#26684;</span>

tb.add_image(<span style="color: #E6DB74;">'images'</span>, grid) <span style="color: #75715E;">#</span><span style="color: #75715E;">&#20256;&#36882;&#26631;&#31614;&#21644;&#25968;&#25454;</span>
tb.add_graph(network, images)<span style="color: #75715E;">#</span><span style="color: #75715E;">&#36825;&#20010;&#30340;&#32467;&#26524;&#26159;&#33021;&#22815;&#30475;&#21040;&#19968;&#20010;&#22270;&#24418;&#25110;&#32773;&#19968;&#20010;&#32593;&#32476;&#30340;&#21487;&#35270;&#21270;&#22312;tensorboard&#20013;</span>
tb.close()  <span style="color: #75715E;">#</span><span style="color: #75715E;">&#20851;&#38381;summarywriter</span>
</pre>
</div>
<p>
运行之后会在当前工作目录下生成名为 <code>runs</code> 的文件夹，里面保存着tensorboard的显示数据
</p>
</div>
</div>
</div>
<div id="outline-container-org7bb6b0b" class="outline-3">
<h3 id="org7bb6b0b"><span class="section-number-3">26.3</span> Running TensorBoard</h3>
<div class="outline-text-3" id="text-26-3">
<p>
To launch TensorBoard, we need to run the tensorboard command at our terminal. This will launch a local server that will serve the TensorBoard UI and the the data our SummaryWriter wrote to disk.
</p>

<p>
By default, the PyTorch SummaryWriter object writes the data to disk in a directory called <code>./runs</code> that is created in the current working directory.
</p>

<p>
When we run the tensorboard command, we pass an argument that tells tensorboard where the data is. So it's like this:
（在终端输入命令） 
</p>
<div class="org-src-container">
<pre class="src src-bash">tensorboard --logdir=runs  <span style="color: #75715E;">#</span><span style="color: #75715E;">TensorBoard 2.2.1 at http://localhost:6006/ (Press CTRL+C to quit)</span>
</pre>
</div>
<p>
The TensorBoard server will launch and be listening for <code>http</code> requests on port <code>6006</code>. These details will be displayed in the console.
</p>

<p>
Access the TensorBoard UI by browsing to:
</p>
<div class="org-src-container">
<pre class="src src-bash">http://localhost:6006 <span style="color: #75715E;">#</span><span style="color: #75715E;">&#22312;&#27983;&#35272;&#22120;&#36755;&#20837;&#35813;&#22320;&#22336;&#23601;&#21487;&#20197;&#24320;&#21551;tensorboard</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org7864468" class="outline-3">
<h3 id="org7864468"><span class="section-number-3">26.4</span> TensorBoard Histograms And Scalars</h3>
<div class="outline-text-3" id="text-26-4">
<p>
To add scalars and histograms we use the corresponding methods provided by the PyTorch <code>SummaryWriter</code> class.
</p>

<p>
Here is an example of the calls:
</p>
<div class="org-src-container">
<pre class="src src-python">tb.add_scalar(<span style="color: #E6DB74;">'Loss'</span>, total_loss, epoch)
tb.add_scalar(<span style="color: #E6DB74;">'Number Correct'</span>, total_correct, epoch)
tb.add_scalar(<span style="color: #E6DB74;">'Accuracy'</span>, total_correct / <span style="color: #F92672;">len</span>(train_set), epoch)

tb.add_histogram(<span style="color: #E6DB74;">'conv1.bias'</span>, network.conv1.bias, epoch)
tb.add_histogram(<span style="color: #E6DB74;">'conv1.weight'</span>, network.conv1.weight, epoch)
tb.add_histogram(<span style="color: #E6DB74;">'conv1.weight.grad'</span>, network.conv1.weight.grad, epoch)
</pre>
</div>
<p>
And here is an example of where we would place these calls inside our training loop:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FD971F;">network</span> = Network()
<span style="color: #FD971F;">train_loader</span> = torch.utils.data.DataLoader(train_set, batch_size=100)
<span style="color: #FD971F;">optimizer</span> = optim.Adam(network.parameters(), lr=0.01)

<span style="color: #FD971F;">images</span>, <span style="color: #FD971F;">labels</span> = <span style="color: #F92672;">next</span>(<span style="color: #F92672;">iter</span>(train_loader))
<span style="color: #FD971F;">grid</span> = torchvision.utils.make_grid(images)

<span style="color: #FD971F;">tb</span> = SummaryWriter()
tb.add_image(<span style="color: #E6DB74;">'images'</span>, grid)
tb.add_graph(network, images)

<span style="color: #F92672;">for</span> epoch <span style="color: #F92672;">in</span> <span style="color: #F92672;">range</span>(10):

    <span style="color: #FD971F;">total_loss</span> = 0
    <span style="color: #FD971F;">total_correct</span> = 0

    <span style="color: #F92672;">for</span> batch <span style="color: #F92672;">in</span> train_loader: <span style="color: #75715E;"># </span><span style="color: #75715E;">Get Batch</span>
        <span style="color: #FD971F;">images</span>, <span style="color: #FD971F;">labels</span> = batch 

        <span style="color: #FD971F;">preds</span> = network(images) <span style="color: #75715E;"># </span><span style="color: #75715E;">Pass Batch</span>
        <span style="color: #FD971F;">loss</span> = F.cross_entropy(preds, labels) <span style="color: #75715E;"># </span><span style="color: #75715E;">Calculate Loss</span>

        optimizer.zero_grad()
        loss.backward() <span style="color: #75715E;"># </span><span style="color: #75715E;">Calculate Gradients</span>
        optimizer.step() <span style="color: #75715E;"># </span><span style="color: #75715E;">Update Weights</span>

        <span style="color: #FD971F;">total_loss</span> += loss.item()
        <span style="color: #FD971F;">total_correct</span> += get_num_correct(preds, labels)

    tb.add_scalar(<span style="color: #E6DB74;">'Loss'</span>, total_loss, epoch)
    tb.add_scalar(<span style="color: #E6DB74;">'Number Correct'</span>, total_correct, epoch)
    tb.add_scalar(<span style="color: #E6DB74;">'Accuracy'</span>, total_correct / <span style="color: #F92672;">len</span>(train_set), epoch)

    tb.add_histogram(<span style="color: #E6DB74;">'conv1.bias'</span>, network.conv1.bias, epoch)
    tb.add_histogram(<span style="color: #E6DB74;">'conv1.weight'</span>, network.conv1.weight, epoch)
    tb.add_histogram(<span style="color: #E6DB74;">'conv1.weight.grad'</span>,network.conv1.weight.grad,epoch)

    <span style="color: #F92672;">print</span>(
        <span style="color: #E6DB74;">"epoch"</span>, epoch, 
        <span style="color: #E6DB74;">"total_correct:"</span>, total_correct, 
        <span style="color: #E6DB74;">"loss:"</span>, total_loss
    )

tb.close()
</pre>
</div>
<p>
频率分布图会随着直方图一起出现。
</p>
</div>
</div>
</div>
<div id="outline-container-orga4e273f" class="outline-2">
<h2 id="orga4e273f"><span class="section-number-2">27</span> 第30集</h2>
<div class="outline-text-2" id="text-27">
</div>
<div id="outline-container-orgb1549ce" class="outline-3">
<h3 id="orgb1549ce"><span class="section-number-3">27.1</span> Naming The Training Runs For TensorBoard</h3>
<div class="outline-text-3" id="text-27-1">
<p>
To take advantage of TensorBoard comparison capabilities, we need to do multiple runs and name each run in such a way that we can identify it uniquely.
</p>

<p>
With PyTorch's <code>SummaryWriter</code>, a run starts when the writer object instance is created and ends when the writer instance is closed or goes out of scope.
</p>

<p>
To uniquely identify each run, we can either set the file name of the run directly, or pass a comment string to the constructor that will be appended to the auto-generated file name.
</p>

<p>
At the time of the creation of this post, the name of the run is contained inside the <code>SummaryWriter</code> in an attribute called <code>log_dir</code>. It is created like this:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #75715E;"># </span><span style="color: #75715E;">PyTorch version 1.1.0 SummaryWriter class</span>
<span style="color: #F92672;">if</span> <span style="color: #F92672;">not</span> log_dir:
    <span style="color: #F92672;">import</span> socket
    <span style="color: #F92672;">from</span> datetime <span style="color: #F92672;">import</span> datetime
    <span style="color: #FD971F;">current_time</span> = datetime.now().strftime(<span style="color: #E6DB74;">'%b%d_%H-%M-%S'</span>)
    <span style="color: #FD971F;">log_dir</span> = os.path.join(
        <span style="color: #E6DB74;">'runs'</span>, 
        current_time + <span style="color: #E6DB74;">'_'</span> + socket.gethostname() + comment
    )
<span style="color: #F92672;">self</span>.log_dir = log_dir
</pre>
</div>
<p>
Here, we can see that the <code>log_dir</code> attribute, which corresponds to the location on disk and the name of the run, is set to <code>runs + time + host + comment</code>. This is of course assuming that the <code>log_dir</code> parameter doesn't have a value that was passed in. Hence, this is the default behavior.
</p>
</div>
<div id="outline-container-orgdd1462e" class="outline-4">
<h4 id="orgdd1462e"><span class="section-number-4">27.1.1</span> Choosing A Name For The Run</h4>
<div class="outline-text-4" id="text-27-1-1">
<p>
One way to name the run is to add the parameter names and values as a comment for the run. This will allow us to see how each parameter value stacks up with the others later when we are reviewing the runs inside TensorBoard.
</p>

<p>
We'll see that this is how we set the comment up later:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FD971F;">tb</span> = SummaryWriter(comment=f<span style="color: #E6DB74;">' batch_size={batch_size} lr={lr}'</span>)
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org461a76a" class="outline-3">
<h3 id="org461a76a"><span class="section-number-3">27.2</span> Creating Variables For Our Hyperparameters</h3>
<div class="outline-text-3" id="text-27-2">
<p>
To make the experimentation easy, we will pull out our hard-coded values and turn them into variables.
</p>

<p>
This is the hard-coded way:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FD971F;">network</span> = Network()
<span style="color: #FD971F;">train_loader</span> = torch.utils.data.DataLoader(
    train_set, batch_size=100
)
<span style="color: #FD971F;">optimizer</span> = optim.Adam(
    network.parameters(), lr=0.01
)
</pre>
</div>
<p>
Notice how the <code>batch_size</code> and <code>lr</code> parameter values are hard-coded.
</p>

<p>
This is what we change it to (now our values are set using variables):
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FD971F;">batch_size</span> = 100
<span style="color: #FD971F;">lr</span> = 0.01

<span style="color: #FD971F;">network</span> = Network()
<span style="color: #FD971F;">train_loader</span> = torch.utils.data.DataLoader(
    train_set, batch_size=batch_size
)
<span style="color: #FD971F;">optimizer</span> = optim.Adam(
    network.parameters(), lr=lr
)
</pre>
</div>
<p>
This will allow us to change the values in a single place and have them propagate through our code.
</p>

<p>
Now, we will create the value for our comment parameter using the variables like so:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FD971F;">tb</span> = SummaryWriter(comment=f<span style="color: #E6DB74;">' batch_size={batch_size} lr={lr}'</span>)
</pre>
</div>
<p>
With this setup, we can change the value of our hyperparameters and our runs will be automatically tracked and identifiable in TensorBoard.
</p>

<div class="org-src-container">
<pre class="src src-bash">&gt; for name, weight<span style="color: #F92672;"> in</span> network.named_parameters():
     print(name,weight.size())
conv1.weight torch.Size([6, 1, 5, 5])
conv1.bias torch.Size([6])
conv2.weight torch.Size([12, 6, 5, 5])
conv2.bias torch.Size([12])
fc1.weight torch.Size([120, 192])
fc1.bias torch.Size([120])
fc2.weight torch.Size([60, 120])
fc2.bias torch.Size([60])
out.weight torch.Size([10, 60])
out.bias torch.Size([10])

&gt; for name, weight<span style="color: #F92672;"> in</span> network.named_parameters():
    print(f<span style="color: #E6DB74;">'{name}.grad'</span>,weight.grad.size())   <span style="color: #75715E;">#</span><span style="color: #75715E;">f&#30340;&#20316;&#29992;&#20854;&#23454;&#30456;&#24403;&#20110;&#23383;&#31526;&#20018;&#38142;&#25509;</span>
conv1.weight.grad torch.Size([6, 1, 5, 5])
conv1.bias.grad torch.Size([6])
conv2.weight.grad torch.Size([12, 6, 5, 5])
conv2.bias.grad torch.Size([12])
fc1.weight.grad torch.Size([120, 192])
fc1.bias.grad torch.Size([120])
fc2.weight.grad torch.Size([60, 120])
fc2.bias.grad torch.Size([60])
out.weight.grad torch.Size([10, 60])
out.bias.grad torch.Size([10])
</pre>
</div>
</div>
</div>
<div id="outline-container-orge29ece5" class="outline-3">
<h3 id="orge29ece5"><span class="section-number-3">27.3</span> Calculate Loss With Different Batch Sizes</h3>
<div class="outline-text-3" id="text-27-3">
<p>
Since we'll be varying our batch sizes now, we'll need to make a change to the way we are calculating and accumulating the loss. Instead of just summing the loss returned by the loss function. We'll adjust it to account for the batch size.
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FD971F;">total_loss</span> += loss.item() * batch_size
</pre>
</div>
<p>
Why do this? We'll the <code>cross_entropy</code> loss function averages the loss values that are produced by the batch and then returns this average loss. This is why we need to account for the batch size.
</p>

<p>
There is a parameter that the <code>cross_entropy</code> function accepts called <code>reduction</code> that we could also use.
</p>

<p>
The reduction parameter optionally accepts a string as an argument. This parameter specifies the reduction to apply to the output of the loss function.
</p>
<ol class="org-ol">
<li><code>'none'</code> - no reduction will be applied.</li>
<li><code>'mean'</code> - the sum of the output will be divided by the number of elements in the output.</li>
<li><code>'sum'</code> - the output will be summed.</li>
</ol>
<p>
Note that the default is <code>'mean'</code>. This is why <code>loss.item() * batch_size</code> works.
</p>
</div>
</div>
<div id="outline-container-org744b500" class="outline-3">
<h3 id="org744b500"><span class="section-number-3">27.4</span> Experimenting With Hyperparameter Values</h3>
<div class="outline-text-3" id="text-27-4">
<p>
Now that we have this setup, we can do more!
</p>

<p>
All we need to do is create some lists and some loops, and we can run the code and sit back and wait for all the combinations to run.
</p>

<p>
Here is an example of what we mean:
</p>

<p>
Parameter Lists
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FD971F;">batch_size_list</span> = [100, 1000, 10000]
<span style="color: #FD971F;">lr_list</span> = [.01, .001, .0001, .00001]
</pre>
</div>
<p>
Nested Iteration
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #F92672;">for</span> batch_size <span style="color: #F92672;">in</span> batch_size_list:
    <span style="color: #F92672;">for</span> lr <span style="color: #F92672;">in</span> lr_list:
        <span style="color: #FD971F;">network</span> = Network()

        <span style="color: #FD971F;">train_loader</span> = torch.utils.data.DataLoader(
            train_set, batch_size=batch_size
        )
        <span style="color: #FD971F;">optimizer</span> = optim.Adam(
            network.parameters(), lr=lr
        )

        <span style="color: #FD971F;">images</span>, <span style="color: #FD971F;">labels</span> = <span style="color: #F92672;">next</span>(<span style="color: #F92672;">iter</span>(train_loader))
        <span style="color: #FD971F;">grid</span> = torchvision.utils.make_grid(images)

        <span style="color: #FD971F;">comment</span>=f<span style="color: #E6DB74;">' batch_size={batch_size} lr={lr}'</span>
        <span style="color: #FD971F;">tb</span> = SummaryWriter(comment=comment)
        tb.add_image(<span style="color: #E6DB74;">'images'</span>, grid)
        tb.add_graph(network, images)

        <span style="color: #F92672;">for</span> epoch <span style="color: #F92672;">in</span> <span style="color: #F92672;">range</span>(5):
            <span style="color: #FD971F;">total_loss</span> = 0
            <span style="color: #FD971F;">total_correct</span> = 0
            <span style="color: #F92672;">for</span> batch <span style="color: #F92672;">in</span> train_loader:
                <span style="color: #FD971F;">images</span>, <span style="color: #FD971F;">labels</span> = batch <span style="color: #75715E;"># </span><span style="color: #75715E;">Get Batch</span>
                <span style="color: #FD971F;">preds</span> = network(images) <span style="color: #75715E;"># </span><span style="color: #75715E;">Pass Batch</span>
                <span style="color: #FD971F;">loss</span> = F.cross_entropy(preds, labels) <span style="color: #75715E;"># </span><span style="color: #75715E;">Calculate Loss</span>
                optimizer.zero_grad() <span style="color: #75715E;"># </span><span style="color: #75715E;">Zero Gradients</span>
                loss.backward() <span style="color: #75715E;"># </span><span style="color: #75715E;">Calculate Gradients</span>
                optimizer.step() <span style="color: #75715E;"># </span><span style="color: #75715E;">Update Weights</span>

                <span style="color: #FD971F;">total_loss</span> += loss.item() * batch_size
                <span style="color: #FD971F;">total_correct</span> += get_num_correct(preds, labels)

            tb.add_scalar(
                <span style="color: #E6DB74;">'Loss'</span>, total_loss, epoch
            )
            tb.add_scalar(
                <span style="color: #E6DB74;">'Number Correct'</span>, total_correct, epoch
            )
            tb.add_scalar(
                <span style="color: #E6DB74;">'Accuracy'</span>, total_correct / <span style="color: #F92672;">len</span>(train_set), epoch
            )

            <span style="color: #F92672;">for</span> name, param <span style="color: #F92672;">in</span> network.named_parameters():
                tb.add_histogram(name, param, epoch)
                tb.add_histogram(f<span style="color: #E6DB74;">'{name}.grad'</span>, param.grad, epoch)

            <span style="color: #F92672;">print</span>(
                <span style="color: #E6DB74;">"epoch"</span>, epoch
                ,<span style="color: #E6DB74;">"total_correct:"</span>, total_correct
                ,<span style="color: #E6DB74;">"loss:"</span>, total_loss
            )  
        tb.close()
</pre>
</div>
<p>
Once this code completes we run TensorBoard and all the runs will be displayed graphically and easily comparable.
</p>
<div class="org-src-container">
<pre class="src src-bash">tensorboard --logdir runs
</pre>
</div>
</div>
<div id="outline-container-org1f57a8c" class="outline-4">
<h4 id="org1f57a8c"><span class="section-number-4">27.4.1</span> Batch Size Vs Training Set Size</h4>
<div class="outline-text-4" id="text-27-4-1">
<p>
When the training set size is not divisible by the batch size, the last batch of data will contain fewer samples than the other batches.
</p>

<p>
One simple way to deal with this discrepancy is to drop the last batch. The PyTorch <code>DataLoader</code> class gives us the ability to do this by setting <code>drop_last=True</code>. By default the <code>drop_last</code> parameter value is set to <code>False</code>.</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2020-07-13 周一 10:52</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
