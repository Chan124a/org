* 简介
Pandas是基于Numpy的专业数据分析工具，可以灵活高效的处理各种数据集，也是我们后期分析案例的神器。它提供了两种类型的数据结构，分别是DataFrame和Series，我们可以简单粗暴的把DataFrame理解为Excel里面的一张表，而Series就是表中的某一列，后面学习和用到的所有Pandas骚操作，都是基于这些表和列进行的操作
* 创建、读取和存储
** 创建
在Pandas中我们想要构造下面这一张表应该如何操作呢？
|      | 备注   | 工资 | 绩效分 |
| 老王 | 不及格 | 5000 |     60 |
| 小刘 | 良好   | 7000 |     84 |
| 小赵 | 最佳   | 9000 |     98 |
| 老龚 | 优秀   | 8500 |     91 |

第一步一定是先导入我们的库——import pandas as pd

构造DataFrame最常用的方式是字典+列表，语句很简单，先是字典外括，然后依次打出每一列标题及其对应的列值（此处一定要用列表），这里列的顺序并不重要：
#+BEGIN_SRC python
>>>df1=pd.DataFrame({'工资':[5000,7000,9000,8500],'绩效分':[60,84,98,91],'备注':['不及格','良好','最佳','优秀']},index=['小王','小刘','小赵','老龚']) #注意这里面用单引号表示字符串，实际上用双引号也是可以的
>>>df1
#+END_SRC
结果为：
|      | 工资 | 绩效分 | 备注   |
| 小王 | 5000 |     60 | 不及格 |
| 小刘 | 7000 |     84 | 良好   |
| 小赵 | 9000 |     98 | 最佳   |
| 老龚 | 8500 |     91 | 优秀   |

注意:如果我们在创建时不指定index，系统会自动生成从0开始的索引。
** 读取
更多时候，我们是把相关文件数据直接读进PANDAS中进行操作，这里介绍两种非常接近的读取方式，一种是CSV格式的文件，一种是EXCEL格式（.xlsx和xls后缀）的文件。
#+BEGIN_SRC python
>>> df2=pd.read_csv("csv文件路径",engine='python')  #engine是使用的分析引擎，读取csv文件一般指定python避免中文和编码造成的报错,如果数据全是英文数据则不必加engine参数
>>> df2.head()

>>> df3=pd.read_excel('xls文件路径')
>>> df3.head()
#+END_SRC
** 存储
#+BEGIN_SRC python
df2.to_csv('xxx.csv')
df3.to_excel('xxx.xls')
#+END_SRC
* 快速认识数据
** 查看头部和尾部
用df.head()函数直接可以查看默认的前5行

df.tail()就可以查看数据尾部的5行数据

这两个参数内可以传入一个数值来控制查看的行数，例如df.head(10)表示查看前10行数据。
** 格式查看
df.info()帮助我们一步摸清各列数据的类型，以及缺失情况：
#+BEGIN_SRC python
>>> df1=pd.DataFrame({"工资":[5000,7000,9000,8500],'绩效分':[60,84,98,91],'备注':['不及格','良好','最佳','优秀']},index=['小王','小刘','小赵','老龚'])
>>> df1.info()
<class 'pandas.core.frame.DataFrame'>
Index: 4 entries, 小王 to 老龚
Data columns (total 3 columns):
 #   Column  Non-Null Count  Dtype 
---  ------  --------------  ----- 
 0   工资      4 non-null      int64 
 1   绩效分     4 non-null      int64 
 2   备注      4 non-null      object
dtypes: int64(2), object(1)
memory usage: 128.0+ bytes
#+END_SRC
从上面直接可以知道数据集的行列数，数据集的大小，每一列的数据类型，以及有多少条非空数据。
** 统计信息概览
快速计算数值型数据的关键统计指标，像平均数、中位数、标准差等等。
#+BEGIN_SRC python
>>> df1.describe()

工资	绩效分
count	4.000000	4.00000
mean	7375.000000	83.25000
std	1796.988221	16.52019
min	5000.000000	60.00000
25%	6500.000000	78.00000
50%	7750.000000	87.50000
75%	8625.000000	92.75000
max	9000.000000	98.00000
#+END_SRC
这个操作只针对数值型的列进行计算。其中count是统计每一列的有多少个非空数值，mean、std、min、max对应的分别是该列的均值、标准差、最小值和最大值，25%、50%、75%对应的则是分位数。
* 列的基本处理方式
温馨提示：使用Pandas时，尽量避免用行或者EXCEL操作单元格的思维来处理数据，要逐渐养成一种列向思维，每一列是同宗同源，处理起来是嗖嗖的快。
** 增加一列
增加一列，用df['新列名'] = 新列值的形式，在原数据基础上赋值即可：
#+BEGIN_SRC python
>>> df2=pd.DataFrame({"流量来源":['一级','一级','一级','一级','一级'],"来源明细":['A','B','C','D','E'],"访客数":[222,333,444,555,666],"支付转化率":["9.98%","33%","44%","22%","4%"],"客单价":[54,22,33,55,22]},index=[0,1,2,3,4])
>>> df2
       流量来源	来源明细	访客数	支付转化率	客单价
0	一级	A	222	9.98%	         54
1	一级	B	333	33%	         22
2	一级	C	444	44%	         33
3	一级	D	555	22%	         55
4	一级	E	666	4%	         22

>>> df2['新增的列']=range(1,len(df2)+1)

       流量来源	来源明细	访客数	支付转化率	客单价	新增的列
0	一级	A	222	9.98%	54	1
1	一级	B	333	33%	22	2
2	一级	C	444	44%	33	3
3	一级	D	555	22%	55	4
4	一级	E	666	4%	22	5
#+END_SRC
** 删除一列
我们用drop函数制定删除对应的列，axis = 1表示针对列的操作，inplace为True，则直接在源数据上进行修改，否则源数据会保持原样。
#+BEGIN_SRC python
>>>df2.drop("新增的列",axis=1)     #不加inplace,执行后直接返回结果

       流量来源	来源明细	访客数	支付转化率	客单价
0	一级	A	222	9.98%	54
1	一级	B	333	33%	22
2	一级	C	444	44%	33
3	一级	D	555	22%	55
4	一级	E	666	4%	22
>>>df2.drop("新增的列",axis=1,inplace=True)  #加inplace是对源数据进行修改
>>>df2
流量来源	来源明细	访客数	支付转化率	客单价
0	一级	A	222	9.98%	54
1	一级	B	333	33%	22
2	一级	C	444	44%	33
3	一级	D	555	22%	55
4	一级	E	666	4%	22
#+END_SRC
** 选取一列
想要选取某一列怎么办？df['列名']即可：
#+BEGIN_SRC python
>>>df2['来源明细']
0    A
1    B
2    C
3    D
4    E
Name: 来源明细, dtype: object
#+END_SRC
选取多列呢？需要用列表来传递：
#+BEGIN_SRC python
>>>df2[['流量来源','访客数','客单价']]   #
流量来源	访客数	客单价
0	一级	222	54
1	一级	333	22
2	一级	444	33
3	一级	555	55
4	一级	666	22
#+END_SRC
** 改
这里只讲一下最简单的更改：df['旧列名'] =  某个值或者某列值，就完成了对原列数值的修改。
* 常用数据类型及操作
** 字符串
字符串类型是最常用的格式之一了，Pandas中字符串的操作和原生字符串操作几乎一模一样，唯一不同的是需要在操作前加上".str"。

小Z温馨提示：我们最初用df2.info()查看数据类型时，非数值型的列都返回的是object格式，和str类型深层机制上的区别就不展开了，在常规实际应用中，我们可以先理解为object对应的就是str格式，int64对应的就是int格式，float64对应的就是float格式即可。

在案例数据中，我们发现来源明细那一列，可能是系统导出的历史遗留问题，每一个字符串前面都有一个“-”符号，又丑又无用，所以把他给拿掉：
#+BEGIN_SRC python
>>> df2['来源明细'].str.replace('-','')
#+END_SRC
一般来说清洗之后的列是要替换掉原来列的：
#+BEGIN_SRC python
>>> df2['来源明细']=df2['来源明细'].str.replace('-','')
#+END_SRC
** 数值型
数值型数据，常见的操作是计算，分为与单个值的运算，长度相等列的运算。

以案例数据为例，源数据访客数我们是知道的，现在想把所有渠道的访客都加上10000，怎么操作呢？
#+BEGIN_SRC python
>>>df2['访客数']
0    222
1    333
2    444
3    555
4    666
Name: 访客数, dtype: int64
>>>df2['访客数']+1000
0    1222
1    1333
2    1444
3    1555
4    1666
Name: 访客数, dtype: int64
#+END_SRC
只需要选中访客数所在列，然后加上1000即可，pandas自动将10000和每一行数值相加，针对单个值的其他运算（减乘除）也是如此。

列之间的运算语句也非常简洁。源数据是包含了访客数、转化率和客单价，而实际工作中我们对每个渠道贡献的销售额更感兴趣。（销售额 = 访客数 X 转化率 X 客单价）

对应操作语句：df['销售额'] = df['访客数']  * df['转化率']  * df['客单价']
#+BEGIN_SRC python
>>>df2['支付转化率']=df2['支付转化率'].str.replace('%','').astype(float)  #先拿掉百分号，再将支付转化率这一列转化为浮点型数据
>>>df2
	流量来源	来源明细	访客数	支付转化率	客单价
0	一级	A	222	9.98	54
1	一级	B	333	33.00	22
2	一级	C	444	44.00	33
3	一级	D	555	22.00	55
4	一级	E	666	4.00	22

>>> df2['支付转化率']=df2['支付转化率']/100  #要注意的是，这样操作，把9.98%变成了9.98，所以我们还需要让支付转化率除以100，来还原百分数的真实数值：
>>> df2
流量来源	来源明细	访客数	支付转化率	客单价
0	一级	A	222	0.0998	54
1	一级	B	333	0.3300	22
2	一级	C	444	0.4400	33
3	一级	D	555	0.2200	55
4	一级	E	666	0.0400	22

>>>df2['销售额']=df2['访客数']*df2['支付转化率']*df2['客单价']  #然后，再用三个指标相乘计算销售额
>>>df2 

      流量来源	来源明细	访客数	支付转化率	客单价	销售额
0	一级	A	222	0.0998	54	1196.4024
1	一级	B	333	0.3300	22	2417.5800
2	一级	C	444	0.4400	33	6446.8800
3	一级	D	555	0.2200	55	6715.5000
4	一级	E	666	0.0400	22	586.0800
#+END_SRC
** 时间类型
PANDAS中时间序列相关的水非常深，这里只对日常中最基础的时间格式进行讲解，对时间序列感兴趣的同学可以自行查阅相关资料，深入了解。

以案例数据为例，我们这些渠道数据，是在2019年8月2日提取的，后面可能涉及到其他日期的渠道数据，所以需要加一列时间予以区分，在EXCEL中常用的时间格式是'2019-8-3'或者'2019/8/3'，我们用PANDAS来实现一下：
#+BEGIN_SRC python
>>> df2['日期']='2019-8-3'
>>> df2

     流量来源	来源明细	访客数	支付转化率	客单价	销售额	日期
0	一级	A	222	0.0998	54	1196.4024	2019-8-3
1	一级	B	333	0.3300	22	2417.5800	2019-8-3
2	一级	C	444	0.4400	33	6446.8800	2019-8-3
3	一级	D	555	0.2200	55	6715.5000	2019-8-3
4	一级	E	666	0.0400	22	586.0800	2019-8-3
>>> df2['日期']
0    2019-8-3
1    2019-8-3
2    2019-8-3
3    2019-8-3
4    2019-8-3
Name: 日期, dtype: object
#+END_SRC
在实际业务中，一些时候PANDAS会把文件中日期格式的字段读取为字符串格式，这里我们先把字符串'2019-8-3'赋值给新增的日期列，然后用to_datetime()函数将字符串类型转换成时间格式：
#+BEGIN_SRC python
>>>df2['日期']=pd.to_datetime(df2['日期'])
>>>df2['日期']
0   2019-08-03
1   2019-08-03
2   2019-08-03
3   2019-08-03
4   2019-08-03
Name: 日期, dtype: datetime64[ns]
#+END_SRC
转换成时间格式（这里是datetime64）之后，我们可以用处理时间的思路高效处理这些数据，比如，我现在想知道提取数据这一天离年末还有多少天（'2019-12-31'），直接做减法（该函数接受时间格式的字符串序列，也接受单个字符串）：
#+BEGIN_SRC python
>>>pd.to_datetime('2019-12-31')-df2['日期']
0   150 days
1   150 days
2   150 days
3   150 days
4   150 days
Name: 日期, dtype: timedelta64[ns]
#+END_SRC

* 基于位置(数字)的索引
下表是用到的数据集:
|    | 流量来源 | 	来源明细 | 	      访客数 |   	支付转化率|     	   客单价 |
|  0 | 一级     | -A               |          35188 |             0.0998 |          54.30 |
|  1 | 一级     | -B               |          28467 |             0.1127 |          99.93 |
|  2 | 一级     | -C               |          13747 |             0.0254 |           0.08 |
|  3 | 一级     | -D               |           5183 |             0.0247 |          37.15 |
|  4 | 一级     | -E               |           4361 |             0.0431 |          91.73 |
|  5 | 一级     | -F               |           4063 |             0.1157 |          65.09 |
|  6 | 一级     | -G               |           2122 |             0.1027 |          86.45 |
|  7 | 一级     | -H               |           2041 |             0.0706 |          44.07 |
|  8 | 一级     | -I               |           1991 |             0.1652 |         104.57 |
|  9 | 一级     | -J               |           1981 |             0.0575 |          75.93 |
| 10 | 一级     | -K               |           1958 |             0.1471 |          85.03 |
| 11 | 一级     | -L               |           1780 |             0.1315 |          98.87 |
| 12 | 一级     | -M               |           1447 |             0.0104 |          80.07 |
| 13 | 二级     | -A               |          39048 |             0.1160 |          91.91 |
| 14 | 二级     | -B               |           3316 |             0.0709 |          66.28 |
| 15 | 二级     | -C               |           2043 |             0.0504 |          41.91 |
| 16 | 三级     | -A               |          23140 |             0.0969 |          83.75 |
| 17 | 三级     | -B               |          14813 |             0.2014 |          82.97 |
| 18 | 四级     | -A               |            216 |             0.0185 |          94.25 |
| 19 | 四级     | -B               |             31 |             0.0000 |            NaN |
| 20 | 四级     | -C               |             17 |             0.0000 |            NaN |
| 21 | 四级     | -D               |              3 |             0.0000 |            NaN |
#+BEGIN_SRC python
>>>df=pd.read_excel("C:/Users/123/Desktop/流量练习数据.xls")
>>>df

流量来源	来源明细	访客数	支付转化率	客单价
0	一级	-A	35188	0.0998	54.30
1	一级	-B	28467	0.1127	99.93
2	一级	-C	13747	0.0254	0.08
3	一级	-D	5183	0.0247	37.15
4	一级	-E	4361	0.0431	91.73
5	一级	-F	4063	0.1157	65.09
6	一级	-G	2122	0.1027	86.45
7	一级	-H	2041	0.0706	44.07
8	一级	-I	1991	0.1652	104.57
9	一级	-J	1981	0.0575	75.93
10	一级	-K	1958	0.1471	85.03
11	一级	-L	1780	0.1315	98.87
12	一级	-M	1447	0.0104	80.07
13	二级	-A	39048	0.1160	91.91
14	二级	-B	3316	0.0709	66.28
15	二级	-C	2043	0.0504	41.91
16	三级	-A	23140	0.0969	83.75
17	三级	-B	14813	0.2014	82.97
18	四级	-A	216	0.0185	94.25
19	四级	-B	31	0.0000	NaN
20	四级	-C	17	0.0000	NaN
21	四级	-D	3	0.0000	NaN
#+END_SRC
先看一下索引的操作方式：df.iloc[行索引,列索引]

第一个位置是行索引,输入我们想要选取那几行的位置参数

第二个位置是列索引,输入我们想要选取那几列的位置参数

** 场景一（行选取）
目标：选择“流量来源”等于“一级”的所有行。

思路：一级的渠道，是从第1行到第13行，对应行索引是0-12，但Python切片默认是含首不含尾的，要想选取0-12的索引行，我们得输入“0:13”，列想要全部选取，则输入冒号“：”即可。
#+BEGIN_SRC python
>>>df.iloc[:13,:]
	流量来源	来源明细	访客数	支付转化率	客单价
0	一级	-A	35188	0.0998	54.30
1	一级	-B	28467	0.1127	99.93
2	一级	-C	13747	0.0254	0.08
3	一级	-D	5183	0.0247	37.15
4	一级	-E	4361	0.0431	91.73
5	一级	-F	4063	0.1157	65.09
6	一级	-G	2122	0.1027	86.45
7	一级	-H	2041	0.0706	44.07
8	一级	-I	1991	0.1652	104.57
9	一级	-J	1981	0.0575	75.93
10	一级	-K	1958	0.1471	85.03
11	一级	-L	1780	0.1315	98.87
12	一级	-M	1447	0.0104	80.07
#+END_SRC
** 场景二（列选取）

目标：我们想要把所有渠道的流量来源和客单价单拎出来看一看。

思路：所有流量渠道，也就是所有行，在第一个行参数的位置我们输入“：”；再看列，流量来源是第1列，客单价是第5列，对应的列索引分别是0和4：
#+BEGIN_SRC python
>>>df.iloc[:,[0,4]]
     流量来源	客单价
0	一级	54.30
1	一级	99.93
2	一级	0.08
3	一级	37.15
4	一级	91.73
5	一级	65.09
6	一级	86.45
7	一级	44.07
8	一级	104.57
9	一级	75.93
10	一级	85.03
11	一级	98.87
12	一级	80.07
13	二级	91.91
14	二级	66.28
15	二级	41.91
16	三级	83.75
17	三级	82.97
18	四级	94.25
19	四级	NaN
20	四级	NaN
21	四级	NaN
#+END_SRC
值得注意的是，如果我们要跨列选取，得先把位置参数构造成列表形式，这里就是[0,4]，如果是连续选取，则无需构造成列表，直接输入0:5（选取索引为0的列到索引为4的列）就好。
** 场景三（行列交叉选取）

目标：我们想要看一看二级、三级流量来源、来源明细对应的访客和支付转化率

思路：先看行，二级三级渠道对应行索引是13:17，再次强调索引含首不含尾的原则，我们传入的行参数是13:18；列的话我们需要流量来源、来源明细、访客和转化，也就是前4列，传入参数0:4。
#+BEGIN_SRC python
>>>df.iloc[13:18,0:4]
	流量来源	来源明细	访客数	支付转化率
13	二级	-A	39048	0.1160
14	二级	-B	3316	0.0709
15	二级	-C	2043	0.0504
16	三级	-A	23140	0.0969
17	三级	-B	14813	0.2014
#+END_SRC
* 基于名称（标签）的索引
** 场景一：选择一级渠道的所有行。
思路：这次我们不用一个个数位置了，要筛选流量渠道为"一级"的所有行，只需做一个判断，判断流量来源这一列，哪些值等于"一级"。
#+BEGIN_SRC python
>>>df['流量来源']=='一级'
0      True
1      True
2      True
3      True
4      True
5      True
6      True
7      True
8      True
9      True
10     True
11     True
12     True
13    False
14    False
15    False
16    False
17    False
18    False
19    False
20    False
21    False
Name: 流量来源, dtype: bool
#+END_SRC
返回的结果由True和False（布尔型）构成，在这个例子中分别代表结果等于一级和非一级。在loc方法中，我们可以把这一列判断得到的值传入行参数位置，Pandas会默认返回结果为True的行（这里是索引从0到12的行），而丢掉结果为False的行，直接上例子：
#+BEGIN_SRC python
>>>df.loc[df['流量来源']=='一级',:]

     流量来源	来源明细	访客数	支付转化率	客单价
0	一级	-A	35188	0.0998	54.30
1	一级	-B	28467	0.1127	99.93
2	一级	-C	13747	0.0254	0.08
3	一级	-D	5183	0.0247	37.15
4	一级	-E	4361	0.0431	91.73
5	一级	-F	4063	0.1157	65.09
6	一级	-G	2122	0.1027	86.45
7	一级	-H	2041	0.0706	44.07
8	一级	-I	1991	0.1652	104.57
9	一级	-J	1981	0.0575	75.93
10	一级	-K	1958	0.1471	85.03
11	一级	-L	1780	0.1315	98.87
12	一级	-M	1447	0.0104	80.07
#+END_SRC
** 场景二：我们想要把所有渠道的流量来源和客单价单拎出来看一看。
思路：所有渠道等于所有行，我们在行参数位置直接输入“:”，要提取流量来源和客单价列，直接输入名称到列参数位置，由于这里涉及到两列，所以得用列表包起来：
#+BEGIN_SRC python
>>>df.loc[:,['流量来源','客单价']]
      流量来源	客单价
0	一级	54.30
1	一级	99.93
2	一级	0.08
3	一级	37.15
4	一级	91.73
5	一级	65.09
6	一级	86.45
7	一级	44.07
8	一级	104.57
9	一级	75.93
10	一级	85.03
11	一级	98.87
12	一级	80.07
13	二级	91.91
14	二级	66.28
15	二级	41.91
16	三级	83.75
17	三级	82.97
18	四级	94.25
19	四级	NaN
20	四级	NaN
21	四级	NaN
#+END_SRC
** 场景三：我们想要提取二级、三级流量来源、来源明细对应的访客和支付转化率。
思路：行提取用判断，列提取输入具体名称参数。
#+BEGIN_SRC python
>>>df.loc[df['流量来源'].isin(['二级','三级']),['流量来源','来源明细','访客数','支付转化率']]

      流量来源	来源明细	访客数	支付转化率
13	二级	-A	39048	0.1160
14	二级	-B	3316	0.0709
15	二级	-C	2043	0.0504
16	三级	-A	23140	0.0969
17	三级	-B	14813	0.2014
#+END_SRC
isin函数,这个函数能够帮助我们快速判断源数据中某一列（Series）的值是否等于列表中的值。

拿案例来说，df['流量来源'].isin(['二级','三级'])，判断的是流量来源这一列的值，是否等于“二级”或者“三级”，如果等于（等于任意一个）就返回True，否则返回False。我们再把这个布尔型判断结果传入行参数，就能够很容易的得到流量来源等于二级或者三级的渠道。

Pandas中列（Series）向求值的用法，具体操作如下：
#+BEGIN_SRC python
df['访客数'].mead()  #求均值
df['访客数'].std()   #计算标准差
df['访客数'].median()#计算中位数
df['访客数'].max()   #计算最大值
df['访客数'].min()   #计算最小值
#+END_SRC
** 场景四：对于流量渠道数据，我们真正应该关注的是优质渠道，假如这里我们定义访客数、转化率、客单价都高于平均值渠道是优质渠道，那怎么找到这些渠道呢？
思路：优质渠道，得同时满足访客、转化、客单高于平均值这三个条件，这是解题的关键。

要三个条件同时满足，他们之间是一个“且”的关系（同时满足），在pandas中，要表示同时满足，各条件之间要用"&"符号连接，条件内部最好用括号区分；如果是“或”的关系（满足一个即可），则用“|”符号连接：
#+BEGIN_SRC python
>>>df.loc[(df['访客数']>df['访客数'].mean())&(df['支付转化率']>df['支付转化率'].mean())&(df['客单价']>df['客单价'].mean()),:]
	流量来源	来源明细	访客数	支付转化率	客单价
1	一级	-B	28467	0.1127	99.93
13	二级	-A	39048	0.1160	91.91
16	三级	-A	23140	0.0969	83.75
17	三级	-B	14813	0.2014	82.97

>>>df.loc[(df['访客数']>df['访客数'].mean())&(df['支付转化率']>df['支付转化率'].mean())&(df['客单价']>df['客单价'].mean()),'来源明细']
1     -B
13    -A
16    -A
17    -B
Name: 来源明细, dtype: object
#+END_SRC


* 清洗常用4板斧
首先，导入案例数据集。因为案例数据存放在同一个Excel表的不同Sheet下，我们需要指定sheetname分别读取：
#+BEGIN_SRC python
>>>d1=pd.read_excel('C:/Users/123/Desktop/清洗数据集.xlsx',sheet_name='一级流量')
>>>d1.head(2)
       流量级别	投放地区	访客数	支付转化率	客单价	支付金额
0	一级	A区	44300	0.1178	58.79	306887.83
1	一级	B区	30612	0.1385	86.64	367338.10
>>>d2=pd.read_excel('C:/Users/123/Desktop/清洗数据集.xlsx',sheet_name='二级流量')
>>>d2.head(2)
	流量级别	投放地区	访客数	支付转化率	客单价	支付金额
0	二级	A区	29111	0.1066	87.40	271189.23
1	二级	B区	17165	0.2271	91.22	355662.39
>>>d3=pd.read_excel('C:/Users/123/Desktop/清洗数据集.xlsx',sheet_name='三级流量')
>>>d3.head(2)
	流量级别	投放地区	访客数	支付转化率	客单价	支付金额
0	三级	A区	45059	0.1366	90.11	554561.22
1	三级	B区	2133	0.1083	74.48	17204.50
#+END_SRC
下面开始清洗的正餐。
** 01 增——拓展数据维度
这三个sheet的数据，维度完全一致（每列数据都是一样），纵向合并起来分析十分方便。

pd.concat([表1，表2，表3])，对于列字段统一的数据，我们只需把表依次传入参数：
#+BEGIN_SRC python
>>>df=pd.concat([d1,d2,d3])
>>>df
      流量级别	投放地区	访客数	支付转化率	客单价	支付金额
0	一级	A区	44300	0.1178	58.79	306887.83
1	一级	B区	30612	0.1385	86.64	367338.10
2	一级	C区	18389	0.0250	0.28	129.58
3	一级	D区	4509	0.1073	64.12	31035.14
4	一级	E区	3769	0.0573	92.91	20068.20
5	一级	F区	2424	0.2207	89.33	47791.60
6	一级	G区	2412	0.0821	56.04	11096.42
0	二级	A区	29111	0.1066	87.40	271189.23
1	二级	B区	17165	0.2271	91.22	355662.39
2	二级	C区	8870	0.0078	44.52	3072.00
0	三级	A区	45059	0.1366	90.11	554561.22
1	三级	B区	2133	0.1083	74.48	17204.50
2	三级	C区	899	0.0990	92.99	8276.50
3	三级	D区	31	0.0000	NaN	NaN
4	三级	E区	17	0.0000	NaN	NaN
#+END_SRC
concat大佬继续说到：“其实把我参数axis设置成1就可以横向合并.."说时迟那时快，我一个箭步冲上去捂住他的嘴巴“牛逼的人做好一件事就够了，横向的就交给merge吧~”

温馨提示：pandas中很多函数功能十分强大，能够实现多种功能，但对于萌新来说，过多甚至交叉的功能往往会造成懵B的状态，所以这里一种功能先只用一种方式来实现。
** 横向合并
横向合并涉及到连接问题，为方便理解，我们构造一些更有代表性的数据集练手：
#+BEGIN_SRC python 
>>>h1=pd.DataFrame({'语文':[93,80,85,76,58],'数学':[81,22,33,44,55],'英语':[66,77,88,99,33]},index=['大狗','东2','dong3','dong4','dong5'])
>>>h1
	语文	数学	英语
大狗	93	81	66
东2	80	22	77
dong3	85	33	88
dong4	76	44	99
dong5	58	55	33

>>>h2=pd.DataFrame({'篮球':[44,55,66,77],'舞蹈':[55,78,99,56]},index=['大狗','二狗子','卤蛋','煎饼'])
>>>h2
        篮球	舞蹈
大狗	44	55
二狗子	55	78
卤蛋	66	99
煎饼	77	56
#+END_SRC
两个DataFrame是两张成绩表，h1是5位同学的数学、英语、语文成绩，h2是4位同学的篮球和舞蹈成绩，现在想找到并合并两张表同时出现的同学及其成绩，可以用merge方法：
#+BEGIN_SRC python
>>>pd.merge(left=h1,right=h2,left_index=True,right_index=True,how='inner')M

       语文	数学	英语	篮球	舞蹈
大狗	93	81	66	44	55
#+END_SRC

