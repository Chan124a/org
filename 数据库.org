* 数据密集型应用的基本设计原则
** 可靠性
当出现意外情况如硬件\\软件故障\\人为失误等,系统应可以继续正常运转.

故障与失效的意义不同.故障意味着组件偏离其正常规格,而失效意味系统作为一个整体停止,无法向用户提供所需的服务.

由于不太可能将故障概率降低至零,通常设计容错机制来避免从故障引发系统失效.
*** 可以被消除的故障类型
**** 硬件故障
硬盘崩溃,内存故障,电网停电,误拔网线等等.

研究证明硬盘的平均无故障时间(MTTF)约为10~50年.在一个拥有10000个磁盘的存储集群中,预期平均每天有一个磁盘发生故障.

可以通过为硬件添加冗余来减少系统故障率.例如对磁盘添加RAID,服务器配置双电源,热拔插CPU.
**** 软件错误
导致软件故障的bug通常会长时间处于引而不发的状态,知道碰到特定的触发条件.
**** 人为失误
例如运维人员的配置错误.
** 可扩展性
随着规模的增长,例如数据量\\流量或复杂性,系统应以合理的方式来匹配这种增长.

可扩展性是用来描述系统应对负载增加能力的术语.

*** 负载
负载可以用负载参数进行描述.参数的最佳选择取决于系统的体系结构,可以是web服务器的每秒请求处理次数,数据库中写入的比例,缓存命中率等.

*** 性能
可以用吞吐量\\服务响应时间作为性能指标.

延迟和响应时间表达的意义不同.延迟指的是请求花费在处理上的时间.响应=延迟+网络延迟+排队延迟.

中位数指标(也称为50百分位数)非常适合描述多少用户需要等待多长时间:一半的用户请求的服务时间少于中位数响应时间,另一半则多于中位数的时间.

*** 应对负载增加的方法
垂直扩展:升级到更强大的机器.
水平扩展:将负载分布到多个更小的机器.
** 可维护性
随着时间的推移,许多新的人员参与到系统开发和运维,以维护现有功能或适配新场景等,系统都应该高效运转.

提升可维护性的设计原则:
- 可运维性:方便运营团队保持系统平稳运行.自动化工具,监测工具
- 简单性:简化系统复杂度,使新工程师能够轻松理解系统..使用抽象的设计.
- 可演化性:后续工程师能够轻松对系统进行改进,并根据需求变化将其适配到非典型场景.
* 数据模型与查询语言
数据模型的基本设计思想:每层都通过一个简洁的数据模型来隐藏下层的复杂性.
** 关系模型
数据被组织层关系,在sql中称为表,其中每个关系都是元组的无序集合(在sql中称为行)
** 文档模型
像简历这样一对多(一个人可能有多份工作经历和多个教育经历)的数据结构,主要是一个自包含的文档,因此用JSON表示非常合适.

面向文档的数据库(如MongoDB,RethinkDB,CouchDB,Espresso)都支持该数据模型.

一对多得关系形成了树状结构.
** 多对一的关系
使用ID来保存内容的好处是它对人类没有任何意义,所以永远不需要直接改变:即使ID标识的信息发生了变化,它也可以保持不变.
任何对人类有意义的东西都可能在将来某个时刻发生改变.
如果这些信息被复制,那么所有的冗余副本也都需要更新.这会导致更多的写入开销,并且存在数据不一致的风险.
消除这种重复正是数据库规范化的核心思想.

这种数据规范化需要表达多对一的关系.

文档模型不适合表达多对一的关系.
而对于关系数据库,由于支持联结操作,可以方便地通过ID来引用其它表中的行,所以适合表示多对一的关系.

如果数据库本身不支持联结,则必须在应用程序代码中,通过对数据库进行多次查询来模拟联结.
** 数据查询语言
数据查询语言分为声明式和命令式.
- 声明式:SQL
- 命令式:IMS,CODASYL

命令式语言告诉计算机以特定顺序执行某些操作.
声明式语言只需指定所需的数据模式,结果需要满足什么条件,以及如何转换数据,而不需要指明如何实现这一目标.查询优化器会决定执行顺序.

*** MapReduce查询
MapReduce是介于声明式和命令式之间的查询语言:查询的逻辑用代码片段表示,基于函数式编程中的map和reduce函数实现.

*** 聚合管道查询语言

** 图状数据模型
图状数据模型适合处理多对多的关系.

图由两种对象组成:顶点和边.
*** 图模型
每个顶点包括:
- 唯一的标识符
- 出边的集合
- 入边的集合
- 属性的集合

每个边包括:
- 唯一的标识符
- 边开始的顶点
- 边结束的顶点
- 描述两个顶点之间关系类型的标签
- 属性的集合(键-值对)

图模型的特性:
- 任何顶点都可以连接到其他顶点.
- 给定某个顶点,可以快速得到所有入边和出边.
- 通过使用不同类型的标签,可以在图中存储多种不同类型的信息,同时仍然保持整洁的数据模型.
**** Cypher查询语言
*** 三元存储模型
* 存储引擎
** 日志结构的存储引擎
日志是一个仅支持追加式更新的数据文件.

索引是一种可以高效查找数据库中特定键的值的数据结构.

由于每次写数据时都要更新索引,任何类型的索引通常都会降低写的速度.

索引可以加快读取查询,但是每个索引都会引入减慢写的速度,这需要作出权衡的设计.
*** 哈希索引
哈希索引指的是键-值类型的索引.

**** 内存哈希表
用内存中的hash map来反映写入数据的偏移量.
key的个数受到内存大小的限制,而值的数据量则可以超过内存大小.
这种索引方式适合每个键的值频繁更新的场景.

文件格式:最好的文件格式是二进制格式,以字节为单位记录字符串长度,然后记录原始字符串.
删除记录:删除键值时,必须添加一个删除记录,用于合并时可以丢弃这个键的所有值.
崩溃恢复:崩溃后,内存中的hash map将丢失,可以通过将每个段的hash map存到磁盘上,加快恢复速度.
数据一致性:可通过校验值发现损坏部分数据.
并发控制:由于写入按先后顺序追加到文件中,通常只有一个写线程,但是读线程可以有多个.

局限性:
- 哈希表必须全部放入内存.
- 区间查询效率不高
**** SSTables(排序字符串表)和LSM-tree
将key-value对的顺序按键排序.

优点:
- 合并段更加高效
- 在文件中查找特定的键时,只需查找排序前后键的值,所以不需要在内存中保存所有键的索引.
- 读请求通常需要扫描某个范围内的多个key-value对,可以将这些key-value对记录保存到一个块中并在写入磁盘前进行压缩.然后将索引指向压缩块的开头.

工作流程:
1. 写入时,添加到内存中的平衡树数据结构中(例如红黑树)
2. 当内存表大于某个阈值,将其作为SSTable文件写入磁盘.
3. 处理读请求时,先在内存表中查找键,然后查找最新的磁盘段文件.
4. 后台周期对段执行合并与压缩过程.

可以用日志文件处理数据库崩溃的问题.

基于合并和压缩排序文件原理的存储引擎通常都被称为LSM存储引擎(Log-Structured Merge-Tree,LSM-Tree).
LSM-tree的基本思想:保存在后台合并的一系列SSTable.
由于数据按顺序存储,可以高效执行区间查询,由于磁盘是顺序写入,可以支持非常高的写入吞吐量.

性能优化:
1. 当查找某个不存在的键时,必须先检查内存表,然后将段一直回溯到最旧的段文件.为了优化这种访问,可以使用布隆过滤器这种内存高效的数据结构.
2. 不同的策略会影响SSTables压缩和合并的顺序和时机.最常见的方式是大小分级和分层压缩.

**** B-trees
B-tree将数据库分解成固定大小的块或页.页是内部读写的最小单位.

B-tree中一个页所包含的子页引用数量称为分支因子.

B-tree底层的基本写操作是使用新数据覆盖磁盘上的旧页.

可以使用预写日志(write-head log,WAL)解决数据库崩溃的问题.

需要用锁解决B-tree的并发访问.

***** 优化
使用写时复制解决中途崩溃的问题:修改的页被写入不同的位置,树中父页的新版本被创建,并指向新的位置.

保存键的缩略信息,而不是完整的键,可节省页空间.

对树进行布局,使得相邻叶子页可以按顺序保存在磁盘上,加快B-teee的查询速度.

添加额外的指针到树中,用于访问同级的兄弟页.

使用B-tree的变体如分形树.


**** 对比B-tree和LSM-treee

***** LSM-tree的优点
B-tree的索引至少写两次数据:一次写入预写日志,一次写入树的页(可能有分裂).
即使页中只有几个字节的修改,也必须修改整个页.


** 面向页的存储引擎
* raft算法
** 概述
不同于Paxos算法直接从分布式一致性问题出发推导出来，Raft算法则是从多副本状态机的角度提出，用于管理多副本状态机的日志复制。
Raft实现了和Paxos相同的功能，它将一致性分解为多个子问题：Leader选举（Leader election）、日志同步（Log replication）、安全性（Safety）、日志压缩（Log compaction）、成员变更（Membership change）等。
同时，Raft算法使用了更强的假设来减少了需要考虑的状态，使之变的易于理解和实现。

Raft将系统中的角色分为领导者（Leader）、跟从者（Follower）和候选人（Candidate）：
- Leader：接受客户端请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志。
- Follower：接受并持久化Leader同步的日志，在Leader告之日志可以提交之后，提交日志。
- Candidate：Leader选举过程中的临时角色。


#+DOWNLOADED: screenshot @ 2023-07-22 23:15:32
[[file:images/数据库/raft算法/2023-07-22_23-15-32_screenshot.png]]
Raft要求系统在任意时刻最多只有一个Leader，正常工作期间只有Leader和Followers。

Raft算法角色状态转换如下：

#+DOWNLOADED: screenshot @ 2023-07-24 21:09:38
[[file:images/数据库/raft算法/2023-07-24_21-09-38_screenshot.png]]
Follower只响应其他服务器的请求。如果Follower超时没有收到Leader的消息，它会成为一个Candidate并且开始一次Leader选举。
收到大多数服务器投票的Candidate会成为新的Leader。Leader在宕机之前会一直保持Leader的状态。

#+DOWNLOADED: screenshot @ 2023-07-24 21:10:12
[[file:images/数据库/raft算法/2023-07-24_21-10-12_screenshot.png]]
Raft算法将时间分为一个个的任期（term），每一个term的开始都是Leader选举。在成功选举Leader之后，Leader会在整个term内管理整个集群。如果Leader选举失败，该term就会因为没有Leader而结束。

** Leader选举
Raft 使用心跳（heartbeat）触发Leader选举。
当服务器启动时，初始化为Follower。Leader向所有Followers周期性发送heartbeat。
如果Follower在选举超时时间内没有收到Leader的heartbeat，就会等待一段随机的时间后发起一次Leader选举。

Follower将其当前term加一然后转换为Candidate。它首先给自己投票并且给集群中的其他服务器发送 RequestVote RPC ,结果有以下三种情况：
- 赢得了多数的选票，成功选举为Leader；
- 收到了Leader的消息，表示有其它服务器已经抢先当选了Leader；
- 没有服务器赢得多数的选票，Leader选举失败，等待选举时间超时后发起下一次选举。


#+DOWNLOADED: screenshot @ 2023-07-24 21:12:04
[[file:images/数据库/raft算法/2023-07-24_21-12-04_screenshot.png]]
选举出Leader后，Leader通过定期向所有Followers发送心跳信息维持其统治。若Follower一段时间未收到Leader的心跳则认为Leader可能已经挂了，再次发起Leader选举过程。

Raft保证选举出的Leader上一定具有最新的已提交的日志.

** 日志同步
Leader选出后，就开始接收客户端的请求。Leader把请求作为日志条目（Log entries）加入到它的日志中，然后并行的向其他服务器发起 AppendEntries RPC （RPC细节参见八、Raft算法总结）复制日志条目。当这条日志被复制到大多数服务器上，Leader将这条日志应用到它的状态机并向客户端返回执行结果。

#+DOWNLOADED: screenshot @ 2023-07-24 21:17:14
[[file:images/数据库/raft算法/2023-07-24_21-17-14_screenshot.png]]
某些Followers可能没有成功的复制日志，Leader会无限的重试 AppendEntries RPC直到所有的Followers最终存储了所有的日志条目。

日志由有序编号（log index）的日志条目组成。每个日志条目包含它被创建时的任期号（term），和用于状态机执行的命令。如果一个日志条目被复制到大多数服务器上，就被认为可以提交（commit）了。


#+DOWNLOADED: screenshot @ 2023-07-24 21:18:16
[[file:images/数据库/raft算法/2023-07-24_21-18-16_screenshot.png]]
Raft日志同步保证如下两点：
- 如果不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的。
- 如果不同日志中的两个条目有着相同的索引和任期号，则它们之前的所有条目都是完全一样的。

第一条特性源于Leader在一个term内在给定的一个log index最多创建一条日志条目，同时该条目在日志中的位置也从来不会改变。

第二条特性源于 AppendEntries 的一个简单的一致性检查。当发送一个 AppendEntries RPC 时，Leader会把新日志条目紧接着之前的条目的log index和term都包含在里面。如果Follower没有在它的日志中找到log index和term都相同的日志，它就会拒绝新的日志条目。

一般情况下，Leader和Followers的日志保持一致，因此 AppendEntries 一致性检查通常不会失败。然而，Leader崩溃可能会导致日志不一致：旧的Leader可能没有完全复制完日志中的所有条目。

#+DOWNLOADED: screenshot @ 2023-07-24 21:24:20
[[file:images/数据库/raft算法/2023-07-24_21-24-20_screenshot.png]]
上图阐述了一些Followers可能和新的Leader日志不同的情况。一个Follower可能会丢失掉Leader上的一些条目，也有可能包含一些Leader没有的条目，也有可能两者都会发生。丢失的或者多出来的条目可能会持续多个任期。

Leader通过强制Followers复制它的日志来处理日志的不一致，Followers上的不一致的日志会被Leader的日志覆盖。

Leader为了使Followers的日志同自己的一致，Leader需要找到Followers同它的日志一致的地方，然后覆盖Followers在该位置之后的条目。

Leader会从后往前试，每次AppendEntries失败后尝试前一个日志条目，直到成功找到每个Follower的日志一致位点，然后向后逐条覆盖Followers在该位置之后的条目。

** 安全性
Raft增加了如下两条限制以保证安全性：
(1)拥有最新的已提交的log entry的Follower才有资格成为Leader。
这个保证是在RequestVote RPC中做的，Candidate在发送RequestVote RPC时，要带上自己的最后一条日志的term和log index，其他节点收到消息时，如果发现自己的日志比请求中携带的更新，则拒绝投票。日志比较的原则是，如果本地的最后一条log entry的term更大，则term大的更新，如果term一样大，则log index更大的更新。

(2)Leader只能推进commit index来提交当前term的已经复制到大多数服务器上的日志，旧term日志的提交要等到提交当前term的日志来间接提交（log index 小于 commit index的日志被间接提交）。
之所以要这样，是因为可能会出现已提交的日志又被覆盖的情况：
* 术语
** 扇出
在事务处理系统中,扇出用来描述为了服务一个输入请求而需要做的请求总数.

#+DOWNLOADED: screenshot @ 2023-07-24 21:40:54
[[file:images/数据库/术语/2023-07-24_21-40-54_screenshot.png]]
在阶段a，term为2，S1是Leader，且S1写入日志（term, index）为(2, 2)，并且日志被同步写入了S2；

在阶段b，S1离线，触发一次新的选主，此时S5被选为新的Leader，此时系统term为3，且写入了日志（term, index）为（3， 2）;

S5尚未将日志推送到Followers就离线了，进而触发了一次新的选主，而之前离线的S1经过重新上线后被选中变成Leader，此时系统term为4，此时S1会将自己的日志同步到Followers，按照上图就是将日志（2， 2）同步到了S3，而此时由于该日志已经被同步到了多数节点（S1, S2, S3），因此，此时日志（2，2）可以被提交了。；

在阶段d，S1又下线了，触发一次选主，而S5有可能被选为新的Leader（这是因为S5可以满足作为主的一切条件：1. term = 5 > 4，2. 最新的日志为（3，2），比大多数节点（如S2/S3/S4的日志都新），然后S5会将自己的日志更新到Followers，于是S2、S3中已经被提交的日志（2，2）被截断了。

增加上述限制后，即使日志（2，2）已经被大多数节点（S1、S2、S3）确认了，但是它不能被提交，因为它是来自之前term（2）的日志，直到S1在当前term（4）产生的日志（4， 4）被大多数Followers确认，S1方可提交日志（4，4）这条日志，当然，根据Raft定义，（4，4）之前的所有日志也会被提交。此时即使S1再下线，重新选主时S5不可能成为Leader，因为它没有包含大多数节点已经拥有的日志（4，4）。
** 日志压缩
在实际的系统中，不能让日志无限增长，否则系统重启时需要花很长的时间进行回放，从而影响可用性。Raft采用对整个系统进行snapshot来解决，snapshot之前的日志都可以丢弃。

每个副本独立的对自己的系统状态进行snapshot，并且只能对已经提交的日志记录进行snapshot。

Snapshot中包含以下内容：
- 日志元数据。最后一条已提交的 log entry的 log index和term。这两个值在snapshot之后的第一条log entry的AppendEntries RPC的完整性检查的时候会被用上。
- 系统当前状态。

当Leader要发给某个日志落后太多的Follower的log entry被丢弃，Leader会将snapshot发给Follower。或者当新加进一台机器时，也会发送snapshot给它。发送snapshot使用InstalledSnapshot RPC（RPC细节参见八、Raft算法总结）。

做snapshot既不要做的太频繁，否则消耗磁盘带宽， 也不要做的太不频繁，否则一旦节点重启需要回放大量日志，影响可用性。推荐当日志达到某个固定的大小做一次snapshot。

做一次snapshot可能耗时过长，会影响正常日志同步。可以通过使用copy-on-write技术避免snapshot过程影响正常日志同步。
** 成员变更
成员变更是在集群运行过程中副本发生变化，如增加/减少副本数、节点替换等。

成员变更也是一个分布式一致性问题，既所有服务器对新成员达成一致。但是成员变更又有其特殊性，因为在成员变更的一致性达成的过程中，参与投票的进程会发生变化。

如果将成员变更当成一般的一致性问题，直接向Leader发送成员变更请求，Leader复制成员变更日志，达成多数派之后提交，各服务器提交成员变更日志后从旧成员配置（Cold）切换到新成员配置（Cnew）。

因为各个服务器提交成员变更日志的时刻可能不同，造成各个服务器从旧成员配置（Cold）切换到新成员配置（Cnew）的时刻不同。

成员变更不能影响服务的可用性，但是成员变更过程的某一时刻，可能出现在Cold和Cnew中同时存在两个不相交的多数派，进而可能选出两个Leader，形成不同的决议，破坏安全性。

#+DOWNLOADED: screenshot @ 2023-07-24 21:42:10
[[file:images/数据库/术语/2023-07-24_21-42-10_screenshot.png]]
由于成员变更的这一特殊性，成员变更不能当成一般的一致性问题去解决。

为了解决这一问题，Raft提出了两阶段的成员变更方法。集群先从旧成员配置Cold切换到一个过渡成员配置，称为共同一致（joint consensus），共同一致是旧成员配置Cold和新成员配置Cnew的组合Cold U Cnew，一旦共同一致Cold U Cnew被提交，系统再切换到新成员配置Cnew。

#+DOWNLOADED: screenshot @ 2023-07-24 21:42:27
[[file:images/数据库/术语/2023-07-24_21-42-27_screenshot.png]]
Raft两阶段成员变更过程如下：
1. Leader收到成员变更请求从Cold切成Cold,new；
2. Leader在本地生成一个新的log entry，其内容是Cold∪Cnew，代表当前时刻新旧成员配置共存，写入本地日志，同时将该log entry复制至Cold∪Cnew中的所有副本。在此之后新的日志同步需要保证得到Cold和Cnew两个多数派的确认；
3. Follower收到Cold∪Cnew的log entry后更新本地日志，并且此时就以该配置作为自己的成员配置；
4. 如果Cold和Cnew中的两个多数派确认了Cold U Cnew这条日志，Leader就提交这条log entry并切换到Cnew；
5. 接下来Leader生成一条新的log entry，其内容是新成员配置Cnew，同样将该log entry写入本地日志，同时复制到Follower上；
6. Follower收到新成员配置Cnew后，将其写入日志，并且从此刻起，就以该配置作为自己的成员配置，并且如果发现自己不在Cnew这个成员配置中会自动退出；
7 Leader收到Cnew的多数派确认后，表示成员变更成功，后续的日志只要得到Cnew多数派确认即可。Leader给客户端回复成员变更执行成功。

异常分析：
- 如果Leader的Cold U Cnew尚未推送到Follower，Leader就挂了，此后选出的新Leader并不包含这条日志，此时新Leader依然使用Cold作为自己的成员配置。
- 如果Leader的Cold U Cnew推送到大部分的Follower后就挂了，此后选出的新Leader可能是Cold也可能是Cnew中的某个Follower。
- 如果Leader在推送Cnew配置的过程中挂了，那么同样，新选出来的Leader可能是Cold也可能是Cnew中的某一个，此后客户端继续执行一次改变配置的命令即可。
- 如果大多数的Follower确认了Cnew这个消息后，那么接下来即使Leader挂了，新选出来的Leader肯定位于Cnew中。

两阶段成员变更比较通用且容易理解，但是实现比较复杂，同时两阶段的变更协议也会在一定程度上影响变更过程中的服务可用性，因此我们期望增强成员变更的限制，以简化操作流程。

两阶段成员变更，之所以分为两个阶段，是因为对Cold与Cnew的关系没有做任何假设，为了避免Cold和Cnew各自形成不相交的多数派选出两个Leader，才引入了两阶段方案。

如果增强成员变更的限制，假设Cold与Cnew任意的多数派交集不为空，这两个成员配置就无法各自形成多数派，那么成员变更方案就可能简化为一阶段。

那么如何限制Cold与Cnew，使之任意的多数派交集不为空呢？方法就是每次成员变更只允许增加或删除一个成员。

可从数学上严格证明，只要每次只允许增加或删除一个成员，Cold与Cnew不可能形成两个不相交的多数派。

一阶段成员变更：
- 成员变更限制每次只能增加或删除一个成员（如果要变更多个成员，连续变更多次）。
- 成员变更由Leader发起，Cnew得到多数派确认后，返回客户端成员变更成功。
- 一次成员变更成功前不允许开始下一次成员变更，因此新任Leader在开始提供服务前要将自己本地保存的最新成员配置重新投票形成多数派确认。
- Leader只要开始同步新成员配置，即可开始使用新的成员配置进行日志同步。
** Raft与Multi-Paxos的异同
Raft与Multi-Paxos都是基于领导者的一致性算法，乍一看有很多地方相同，下面总结一下Raft与Multi-Paxos的异同。

Raft与Multi-Paxos中相似的概念：
#+DOWNLOADED: screenshot @ 2023-07-24 21:45:29
[[file:images/数据库/术语/2023-07-24_21-45-29_screenshot.png]]
Raft与Multi-Paxos的不同：


#+DOWNLOADED: screenshot @ 2023-07-24 21:45:40
[[file:images/数据库/术语/2023-07-24_21-45-40_screenshot.png]]
** 参考文章
[[https://zhuanlan.zhihu.com/p/32052223][Raft算法详解]]
