* pip的用法
#+BEGIN_SRC bash
sudo pip3 --no-cache-dir --default-timeout=100 install -i https://pypi.tuna.tsinghua.edu.cn/simple torch
#+END_SRC
--no-cache-dir：pip下载库时会占用缓存，当文件过大会导致错误，这个选项可以避免该错误
#+BEGIN_SRC bash
pip install -r requirements.txt
#+END_SRC
-r表示从requirements.txt读取安装列表
浙大内部镜像：http://mirrors.aliyun.com/
** socket.timeout: The read operation timed out解决方案
解决方法：pip --default-timeout=100 install -U tensorflow
* 远程主机
** 互传文件
scp 本地用户名@IP地址:文件名1 远程用户名@IP地址:文件名2 

#[本地用户名@IP地址:]可以不输入

scp -r file username@ip:filepath #参数-r用于传文件夹

1. 从服务器复制文件到本地：

scp root@×××.×××.×××.×××:/data/test.txt /home/myfile/

root@×××.×××.×××.×××   root是目标服务器（有你需要拷贝文件的服务器）的用户名，×××.×××.×××.×××是IP地址，如192.168.1.100，后面紧跟的：不要忘记，/data/test.txt是目标服务器中你要拷贝文件的地址，接一个空格，后面的/home/myfile/是本地接收文件的地址。

2. 从服务器复制文件夹到本地：

scp -r root@×××.×××.×××.×××:/data/ /home/myfile/

只需在前面加-r即可，就可以拷贝整个文件夹。

3. 从本地复制文件到服务器：

scp /home/myfile/test.txt root@192.168.1.100:/data/

4. 从本地复制文件夹到服务器：

scp -r /home/myfile/ root@192.168.1.100:/data/
* apt-get
*** 修改源
1、原文件备份
 
sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak
 
2、编辑源列表文件
 
sudo vim /etc/apt/sources.list
 
3、将原来的列表删除，添加如下内容

阿里云源
#+BEGIN_EXAMPLE
deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse

#+END_EXAMPLE
清华源
#+BEGIN_EXAMPLE
Q# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释
# 若出现no longer has a Release file错误，把https该成http即可
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse

#+END_EXAMPLE
4、更新源
 
更新软件包列表
sudo apt-get update
*** no longer has a Release file错误的解决方法
把软件源里的https改成http就行了

今天我sudo apt-get update遇到了一个
#+BEGIN_SRC bash
Hit:1 http://packages.microsoft.com/repos/code stable InRelease
Ign:2 http://mirrors.tuna.tsinghua.edu.cn/ros/ubuntu bionic InRelease          
Ign:3 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic InRelease             
Hit:4 http://mirrors.tuna.tsinghua.edu.cn/ros/ubuntu bionic Release            
Hit:5 http://dl.google.com/linux/chrome/deb stable InRelease                   
Ign:6 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-updates InRelease     
Ign:8 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-backports InRelease   
Ign:9 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-security InRelease    
Err:10 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic Release              
  Certificate verification failed: The certificate is NOT trusted. The certificate chain uses expired certificate.  Could not handshake: Error in the certificate verification. [IP: 101.6.15.130 443]
Err:11 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-updates Release  
  Certificate verification failed: The certificate is NOT trusted. The certificate chain uses expired certificate.  Could not handshake: Error in the certificate verification. [IP: 101.6.15.130 443]
Hit:12 http://archive.ubuntu.com/ubuntu bionic InRelease
Hit:13 http://ppa.launchpad.net/videolan/master-daily/ubuntu bionic InRelease  
Err:14 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-backports Release    
  Certificate verification failed: The certificate is NOT trusted. The certificate chain uses expired certificate.  Could not handshake: Error in the certificate verification. [IP: 101.6.15.130 443]
Err:15 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-security Release     
  Certificate verification failed: The certificate is NOT trusted. The certificate chain uses expired certificate.  Could not handshake: Error in the certificate verification. [IP: 101.6.15.130 443]
Reading package lists... Done                                
E: The repository 'https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic Release' no longer has a Release file.
N: Updating from such a repository can't be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.
E: The repository 'https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-updates Release' no longer has a Release file.
N: Updating from such a repository can't be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.
E: The repository 'https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-backports Release' no longer has a Release file.
N: Updating from such a repository can't be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.
E: The repository 'https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-security Release' no longer has a Release file.
N: Updating from such a repository can't be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.
#+END_SRC

* Vmware虚拟机
NAT模式下，多个虚拟机都是连接到同一个网关地址
** ubuntu终端放大缩小
<Ctrl> + <->减小字号
<Ctrl> + <Shift> + <+>增大字号
终端设置
1、打开终端；
2、Edit→Profile Preferences
3、在General里最后一行设置默认大小

Ubuntu快速安装模式中，root用户默认不设置密码，可执行下面的命令为root设置密码
#+BEGIN_SRC bash
sudo passwd
#+END_SRC
** 硬盘扩容
1.扩展虚拟机硬盘大小（关机状态才能扩容）

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20171225160128418.png @ 2020-06-02 19:14:00
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-06-02_19-14-00_20171225160128418.png]]

2.安装修改文件大小的软件，此软件和Window上的DiskGenius用法相似。
#+BEGIN_SRC bash
sudo apt-get install gparted
#+END_SRC

打开gparted
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20171225160133078.png @ 2020-06-02 19:15:12
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-06-02_19-15-12_20171225160133078.png]]

3.和DiskGenius相同，只有相邻的空间时没有被分配的才能扩展空间大小，所以我们先删除/dev/sda2，保存修改（点击变成绿色的对号）。

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20171225160136902.png @ 2020-06-02 19:17:46
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-06-02_19-17-46_20171225160136902.png]]

4./dev/sda1之后的空间都是未分配的空间，我们可以把鼠标放在/dev/sda1，右键
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20171225160140891.png @ 2020-06-02 19:18:34
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-06-02_19-18-34_20171225160140891.png]]

鼠标拖动改变大小，或者直接在New size对应的文本框输入大小。预留部分空间给我们在第三步删除的交换分区。
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20171225160144569.png @ 2020-06-02 19:27:49
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-06-02_19-27-49_20171225160144569.png]]

5.鼠标放在剩余的未分配的空间，创建交换分区，保存修改。
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20171225160148123.png @ 2020-06-02 19:28:52
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-06-02_19-28-52_20171225160148123.png]]

6.sudo fdisk -l，/dev/sda1空间从40G扩展到了58G
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20171225160152672.png @ 2020-06-02 19:39:22
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-06-02_19-39-22_20171225160152672.png]]
** 解决VM Workstation安装VMware Tools显示灰色的办法
解决办法如下：
1.关闭虚拟机；

2.在虚拟机设置分别设置CD/DVD、CD/DVD2和软盘为自动检测三个步骤；

3.再重启虚拟机，灰色字即点亮。

如果上述步骤不行，就执行方法二：
1.关闭虚拟机；

2.在虚拟机将CD/DVD设置为VMware安装目录中的linux.iso（如ubuntu-16.04.5-desktop-i386.iso）、CD/DVD2设置和CD/DVD一样也是可以的（这里建议设置为自动检测，因为设置为自动检测也是可以的）和和软盘为自动检测三个步骤；

3.再重启虚拟机，灰色字即点亮。
** VMware 虚拟机如何连接网络
*** 一、首先查看自己的虚拟机服务有没有开启，选择电脑里面的服务查看；
1.计算机点击右键选择管理

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170103235137966.jpg @ 2020-10-29 15:25:51
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-25-51_20170103235137966.jpg]]

2.进入管理选择VM开头的服务如果没有开启的话就右键开启

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170103235345453.jpg @ 2020-10-29 15:26:15
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-26-15_20170103235345453.jpg]]
*** 二、虚拟机服务开启后就查看本地网络虚拟机的网卡启动没有
1.电脑右下角网络标志右键进入网络和共享中心
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170103235608659%20%281%29.jpg @ 2020-10-29 15:27:21
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-27-21_20170103235608659%2520%25281%2529.jpg]]

2.点击更改适配器，查看虚拟机的虚拟网卡启动没有，没有启动的话右键点击启动

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170103235743739.jpg @ 2020-10-29 15:27:39
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-27-39_20170103235743739.jpg]]

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170103235832318.jpg @ 2020-10-29 15:28:33
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-28-33_20170103235832318.jpg]]


3.网卡开启后设置ip地址，此处设置的ip和本机的ip没有关系，设置成你虚拟机里面运行的计算机需要的ip地址网段

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170104000142275.jpg @ 2020-10-29 15:28:43
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-28-43_20170104000142275.jpg]]
*** 三、此时你的本机设置完成了，该设置虚拟机了
1.打开虚拟机，选择你使用的操作系统打开详情页选择网络适配器，选择NAT模式并选择启动时连接，如下图；
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170104000401889.jpg @ 2020-10-29 15:29:52
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-29-52_20170104000401889.jpg]]
2.选择完后点击虚拟机页面上的编辑进入虚拟网络编辑器

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170104001142827.jpg @ 2020-10-29 15:30:06
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-30-06_20170104001142827.jpg]]

3.进来后会出现这个窗口，选择右下角更改设置，使用管理员进行修改
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170104001303104.jpg @ 2020-10-29 15:30:17
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-30-17_20170104001303104.jpg]]

4.更改完成后，更改下方的ip地址，此处的ip地址段和你在本机网络虚拟网卡（二-3）里面设置的ip要在一个网段里面,本机设置的是ip地址，而在此处设置的是ip网段
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170104001424828.jpg @ 2020-10-29 15:31:20
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-31-20_20170104001424828.jpg]]

5.选择DHCP,进行设置你的虚拟机分配虚拟计算机的ip地址范围

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170104001826299.jpg @ 2020-10-29 15:31:41
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-31-41_20170104001826299.jpg]]

6.设置完DHCP后进行网关的设置，选择NAT设置，设置你虚拟计算机的网关地址。
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170104001943940.jpg @ 2020-10-29 15:31:52
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-31-52_20170104001943940.jpg]]
*** 四、这时候，必要条件就已经配合结束了，开启虚拟计算机，进入IPv4的设置。
填写ip地址，IP地址要在你在虚拟机DHCP分配的ip地址（三-5）范围内
填写网关，就是在上面设置虚拟机网关的地址（三-6）
DNS服务器可以设置114.114.114.144 8.8.8.8 等。
*** 五、这时候基本就可以进行网络连接了，打开网页试一下，如果还连接不上，查看是否是哪一步没有设置对，在就重新启动虚拟计算机的网络。
** 修改时区和时间
~timedatectl set-timezone Asia/Shanghai~
* jupyter notebook
** 使用Anaconda安装
#+BEGIN_SRC bash
conda install jupyter notebook
#+END_SRC
** 使用pip命令安装
#+BEGIN_SRC python
pip3 install jupyter #python3.x
pip install jupyter	#python2.x
#+END_SRC

6.sudo fdisk -l，/dev/sda1空间从40G扩展到了58G
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20180117102533147.png @ 2020-06-02 19:29:31
[[file:jupyter_notebook/2020-06-02_19-29-31_20180117102533147.png]]

** 切换conda环境的方法
切换到想要的环境，比如说adda
~conda activate adda~
安装ipykernel：
~conda install ipykernel~
添加kernel进jupyter notebook：
~python -m ipykernel install --user --name [虚拟环境名] --display-name "kernel命名"~
如： ~python -m ipykernel install --name adda~

执行完这个语句之后，会自动在目录【C:\ProgramData\jupyter\kernels】(类似)生成一个【adda】文件夹，里面有kernel.json文件

现在打开jupyter notebook，里面就会显示有这个虚拟环境了

选择conda环境新建文件
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20200304105902203.png @ 2020-10-29 09:25:50

[[file:jupyter_notebook/2020-10-29_09-25-50_20200304105902203.png]]

此时，就可以看到创建的Python[conda env:tf-gpu]了，选择该kernel运行即可
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20200304105932199.png @ 2020-10-29 09:25:56
[[file:jupyter_notebook/2020-10-29_09-25-56_20200304105932199.png]]

** 配置jupyter notebook
生成配置文件： ~jupyter notebook --generate-config~

设置密码： ~jupyter notebook password~

修改配置文件: ~vim ~/.jupyter/jupyter_notebook_config.py~

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20200805124102787.png @ 2020-10-29 09:35:59
[[file:jupyter_notebook/2020-10-29_09-35-59_20200805124102787.png]]

** 远程访问jupyter notebook
通常情况下,打开 jupyter notebook即从本地地址localhost:8888打开jupyter notebook.

如果希望远程操控jupyter notebook,则需要进行一些设置.

*** 1. 检查配置文件是否存在

首相必须确认jupyter notebook 的配置文件 =jupyter_notebook_config.py= 是否存在.

不同系统的默认配置文件路径如下:

- Windows: =C:\Users\USERNAME\.jupyter\jupyter_notebook_config.py=
- OS X: =/Users/USERNAME/.jupyter/jupyter_notebook_config.py=
- Linux: =/home/USERNAME/.jupyter/jupyter_notebook_config.py=

如果系统上没有Jupyter 文件夹或者Jupyter 文件夹里没有配置文件,那么必须执行以下命令生成配置文件:
#+BEGIN_SRC bash
jupyter notebook --generate-config
#+END_SRC

这个命令会创建Jupyter文件夹并在文件夹内生成配置文件 =jupyter_notebook_config.py=

*** 2.生成密码

**** 2.1生成访问密码

从 jupyter notebook 5.0 版本开始,我们就可以通过自动方式生成访问密码.

设置访问密码的命令为 =jupyter notebook password= ，设置后的访问密码存储在 =jupyter_notebook_config.json= 里面。

#+BEGIN_SRC bash
> jupyter notebook password
Enter password:  ****
Verify password: ****
[NotebookPasswordApp] Wrote hashed password to /Users/you/.jupyter/jupyter_notebook_config.json
#+END_SRC


**** 2.2 生成hash密码

如果没有hash密码，那么我们每次通过浏览器远程访问Jupyter时，都需要输入一次密码。如果设置了hash密码，那么我们只需要在首次远程访问jupyter的时候输入一次密码，之后再次访问jupyter的时候就不用重复输入密码了。
在终端输入 =ipython= ，进入ipython环境后输入下列代码。

#+BEGIN_SRC bash
In [1]: from notebook.auth import passwd
In [2]: passwd()
Enter password:
Verify password:
Out[2]: 'sha1:67c9e60bb8b6:9ffede0825894254b2e042ea597d771089e11aed'
#+END_SRC

这里输出的就是hash密码，后面的操作需要用到这个hash密码，所以需要将其复制下来。

输入 =exit= 退出ipython环境。

*** 3.修改配置文件

打开 =jupyter_notebook_config.py= 文件,可以看到里面很多注释行。如果我们要修改 =jupyter_notebook_config.py= 里的某一行，必须先把行首的 =#= 去掉。

找到 =#c.NotebookApp.password = ' '= 这一行，将注释去掉，并修改为
 =c.NotebookApp.password = u'sha1:67c9e60bb8b6:9ffede0825894254b2e042ea597d771089e11aed`=

这里将之前复制的hash密码填入即可，注意密码前面的 =u= 不可省略，其作用是提示Python编译器，要按照给定的方式来解析引号中的字符串。

=c.NotebookApp.allow_remote_access = True= 将默认值False修改为True，表示允许外部访问。

=c.NotebookApp.ip = '*'= 这里的 =*= 表示允许所有IP皆可访问

=c.NotebookApp.open_browser = False= 禁止自动打开浏览器

=c.NotebookApp.notebook_dir = '/eswai/jupyter'= 这里可修改在浏览器打开jupyter notebook后的工作目录。

=c.NotebookApp.port = 9999= 设置一个固定的notebook服务会监听的IP端口（这里设置为9999），这个值可以任意，只要保证不和其他已经启用的端口号冲突即可，也可以不修改，默认为8888。

修改完成后在终端输入 =jupyter notebook= 命令，这样确保Jupyter重新加载jupyter_notebook_config.py，进而使得新配置起效。

之后我们只要在任意浏览器地址栏输入 =主机ip：9999= 即可远程登录jupyter notebook了。

如果是服务器上docker容器内的jupyter notebook，那么浏览器地址栏应该输入 =宿主机ip:宿主机端口=

这里的宿主机端口是创建容器时分配的宿主机端口，比如你创建容器时使用的端口映射参数为： =-p 8002:9999= ，那么远程登录地址为 =宿主机ip:8002= .

参考文档：

1. [[https://jupyter-notebook.readthedocs.io/en/latest/public_server.html][官方英文指南]]

2. [[https://www.jianshu.com/p/444c3ae23035][设置 jupyter notebook 可远程访问]]

3. [[https://blog.csdn.net/eswai/article/details/79437428][利用Docker环境配置jupyter notebook服务器]]

4. [[https://zhuanlan.zhihu.com/p/64524822][如何设置远程访问的Jupyter Notebook服务器-04（服务器篇）]]

* 统计文件个数
统计当前文件夹下文件的个数，包括子文件夹里的
#+BEGIN_SRC bash
ls -lR|grep "^-"|wc -l
#+END_SRC
统计文件夹下目录的个数，包括子文件夹里的
#+BEGIN_SRC bash
ls -lR|grep "^d"|wc -l
#+END_SRC
统计当前文件夹下文件的个数
#+BEGIN_SRC bash
ls -l |grep "^-"|wc -l
#+END_SRC
统计当前文件夹下目录的个数
#+BEGIN_SRC bash
ls -l |grep "^d"|wc -l
#+END_SRC
附：
统计输出信息的行数
#+BEGIN_SRC bash
wc -l
#+END_SRC
将长列表输出信息过滤一部分，只保留一般文件，如果只保留目录就是 ^d
#+BEGIN_SRC bash
grep "^-"
#+END_SRC
* Linux任务前后台的切换
Shell支持作用控制，有以下命令实现前后台切换：
1. command& 让进程在后台运行
2. jobs 查看后台运行的进程
3. fg %n 让后台运行的进程n到前台来
4. bg %n 让进程n到后台去
5. kill %n 杀死job

PS:"n"为jobs命令查看到的job编号，不是进程编号.

fg、bg、jobs、&、ctrl + z都是跟系统任务有关的，虽然现在基本上不怎么需要用到这些命令，但学会了也是很实用的.

& 最经常被用到,这个用在一个命令的最后，可以把这个命令放到后台执行
 
ctrl + z,可以将一个正在前台执行的命令放到后台，并且暂停

jobs,查看当前有多少在后台运行的命令
 
fg,将后台中的命令调至前台继续运行,如果后台中有多个命令，可以用 fg %jobnumber将选中的命令调出，%jobnumber是通过jobs命令查到的后台正在执行的命令的序号(不是pid)。

bg,将一个在后台暂停的命令，变成继续执行,如果后台中有多个命令，可以用bg %jobnumber将选中的命令调出，%jobnumber是通过jobs命令查到的后台正在执行的命令的序号(不是pid)。
 
** 实例：
假设你发现前台运行的一个程序需要很长的时间，但是需要干其他的事情，你就可以用 Ctrl-Z ，终止这个程序，然后可以看到系统提示：
~[1]+ Stopped /root/bin/rsync.sh~

如果没有此提示，则用 jobs 命令查看任务：
#+BEGIN_SRC bash
>>>jobs
[1]+ suspended /root/bin/rsync.sh &
#+END_SRC
然后我们可以把程序调度到后台执行：（bg 后面的数字为作业号）
#+BEGIN_SRC bash
>>>bg 1
[1]+ /root/bin/rsync.sh &
#+END_SRC
用 jobs 命令查看正在运行的任务：
#+BEGIN_SRC bash
>>>jobs
[1]+ Running /root/bin/rsync.sh &
#+END_SRC
如果想把它调回到前台运行，可以用
#+BEGIN_SRC bash
>>>fg 1
/root/bin/rsync.sh
#+END_SRC
这样，你在控制台上就只能等待这个任务完成了。 
* 显示当前GPU使用情况
Nvidia自带了一个nvidia-smi的命令行工具，会显示显存使用情况：
#+BEGIN_SRC bash
>>>nvidia-smi
#+END_SRC

* 实时查看GPU状况
1. 显示当前GPU使用情况
Nvidia自带了一个nvidia-smi的命令行工具，会显示显存使用情况：
#+BEGIN_SRC python 
$ nvidia-smi
#+END_SRC

* 解压缩命令
#+BEGIN_SRC bash
#压缩
tar -jcv -f filename.tar.bz2 要被压缩的文件或目录名称
#查询
tar -jtv -f filename.tar.bz2
#解压缩
tar -jxv -f filename.tar.bz2 -C 欲解压缩的目录
#+END_SRC

* 搭建c语言环境
** gcc和g++的区别
gcc 最开始的时候是 GNU C Compiler, 如你所知，就是一个c编译器。但是后来因为这个项目里边集成了更多其他不同语言的编译器，GCC就代表 the GNU Compiler Collection，所以表示一堆编译器的合集。

g++则是GCC的c++编译器。现在你在编译代码时调用的gcc，已经不是当初那个c语言编译器了，更确切的说他是一个驱动程序，根据代码的后缀名来判断调用c编译器还是c++编译器 (g++)。比如你的代码后缀是*.c，他会调用c编译器还有linker去链接c的library。如果你的代码后缀是cpp, 他会调用g++编译器，当然library call也是c++版本的。
** 安装gcc和g++
#+BEGIN_SRC bash
sudo apt-get install gcc
sudo apt-get install gcc
#+END_SRC

* ubuntu 设置root默认密码（初始密码）
ubuntu安装好后，root初始密码（默认密码）不知道，需要设置。

1、先用安装时候的用户登录进入系统

2、输入：sudo passwd  按回车

3、输入新密码，重复输入密码，最后提示passwd：password updated sucessfully

此时已完成root密码的设置

4、输入：su root

切换用户到root

* gcc、make、cmake的关系和区别
1.gcc是GNU Compiler Collection（就是GNU编译器套件），也可以简单认为是编译器，它可以编译很多种编程语言（括C、C++、Objective-C、Fortran、Java等等）。

2.当你的程序只有一个源文件时，直接就可以用gcc命令编译它。

3.但是当你的程序包含很多个源文件时，用gcc命令逐个去编译时，你就很容易混乱而且工作量大

4.所以出现了make工具
make工具可以看成是一个智能的批处理工具，它本身并没有编译和链接的功能，而是用类似于批处理的方式—通过调用makefile文件中用户指定的命令来进行编译和链接的。

5.makefile是什么？简单的说就像一首歌的乐谱，make工具就像指挥家，指挥家根据乐谱指挥整个乐团怎么样演奏，make工具就根据makefile中的命令进行编译和链接的。

6.makefile命令中就包含了调用gcc（也可以是别的编译器）去编译某个源文件的命令。

7.makefile在一些简单的工程完全可以人工手下，但是当工程非常大的时候，手写makefile也是非常麻烦的，如果换了个平台makefile又要重新修改。

8.这时候就出现了Cmake这个工具，cmake就可以更加简单的生成makefile文件给上面那个make用。当然cmake还有其他功能，就是可以跨平台生成对应平台能用的makefile，你不用再自己去修改了。

9.可是cmake根据什么生成makefile呢？它又要根据一个叫CMakeLists.txt文件（学名：组态档）去生成makefile。

10.到最后CMakeLists.txt文件谁写啊？亲，是你自己手写的。

11.当然如果你用IDE，类似VS这些一般它都能帮你弄好了，你只需要按一下那个三角形
12.cmake是make maker，生成各种可以直接控制编译过程的控制器的配置文件，比如makefile、各种IDE的配置文件。
13.make是一个简单的通过文件时间戳控制自动过程、处理依赖关系的软件，这个自动过程可以是编译一个项目。

* ssh 相关
** ssh Connection refused的解决方法
 没装openssh_server 或者openssh_client，解决方法:sudo apt-get install openssh_server openssh_client
 #+BEGIN_EXAMPLE
 The ssh binary, the SSH client, is provided by the openssh-client package, which is installed on your system.

 The ssh service runs the SSH server, provided by the openssh-server package, which isn’t installed on your system.

 The ssh package is a meta-package which installs both the client and the server.
 #+END_EXAMPLE
 开启openssh服务: sudo /etc/init.d/ssh start

 验证是否开启服务: ps -e | grep ssh

 如果有输出 sshd 证明已经开启ssh服务

** ubuntu开启SSH服务远程登录
 SSH分客户端openssh-client和openssh-server

 如果你只是想登陆别的机器的SSH只需要安装openssh-client（ubuntu有默认安装，如果没有则sudo apt-get install openssh-client），如果要使本机开放SSH服务就需要安装openssh-server。

  查看当前的ubuntu是否安装了ssh-server服务。默认只安装ssh-client服务
 dpkg -l | grep ssh

 安装ssh-server服务
 sudo apt-get install openssh-server

 再次查看安装的服务：
 dpkg -l | grep ssh

 然后确认ssh-server是否启动了：

 ps -e | grep ssh

 如果看到sshd那说明ssh-server已经启动了。
 如果没有则可以这样启动：sudo /etc/init.d/ssh start或sudo service ssh start
 配置相关：
 ssh-server配置文件位于/etc/ssh/sshd_config，在这里可以定义SSH的服务端口，默认端口是22，你可以自己定义成其他端口号，如222。（或把配置文件中的”PermitRootLogin without-password”加一个”#”号,把它注释掉，再增加一句”PermitRootLogin yes”）
 然后重启SSH服务：
 sudo /etc/init.d/ssh stop
 sudo /etc/init.d/ssh start
*** Unable to locate package openssh-server
  给一个docker的ubuntu容器安装openssh-server出现了这个问题
 #+BEGIN_SRC bash
 root@1c3148b444e2:/# apt-get install openssh-server
 Reading package lists... Done
 Building dependency tree       
 Reading state information... Done
 E: Unable to locate package openssh-server
 #+END_SRC
 原因解释：因为软件源出问题，导致无法找到或者下载软件，一般原因是刚安装的Ubuntu后没有更新软件源或者更新后但是没有sudo apt-get update，导致找不到软件。

 1 sudo apt-get update 
 （更新软件源）执行安装操作，如果不成功，执行2 

 2  sudo apt-get upgrade 
 （继续更新软件源）执行安装操作，应该能成功 
 最后执行sudo apt-get install -y openssh-server

** SSH、SCP和SFTP
*** SSH、SCP和SFTP都是SSH软件包的组成部分
 SSH 是 Secure Shell 的缩写，由 IETF 的网络小组（Network Working Group）所制定；SSH 为建立在应用层基础上的安全协议。SSH 是目前广泛采用的安全登录协议，专为远程登录会话和其他网络服务提供安全性的协议，替代以前不安全的Telnet协议。利用 SSH 协议可以有效防止远程管理过程中的信息泄露问题。

 SSH包括二个部分，服务端的SSHD（Secure Shell Daemon）和SSH客户端。我们通常所说的用SSH登录到某某主机，指的是用SSH客户端远程登录到某台主机（该主机运行了SSHD服务端程序）。

 SSH最初是UNIX系统上的一个程序，后来又迅速扩展到其他操作平台，目前几乎所有UNIX平台—包括HP-UX、Linux、AIX、Solaris、Digital UNIX、Irix，以及其他系统平台，都可运行SSH。
*** SCP和SFTP
 SCP是Secure Copy的简称，是用来与远程主机之间进行数据传输的协议，相当于经过加密的Copy命令。SCP数据传输使用 ssh协议，并且和ssh 使用相同的认证方式，提供相同的安全保证 。 根据实际需要，scp进行验证时会要求你输入密码或口令。

 SFTP=SSH File Transfer Protocol ，有时也被称作 Secure File Transfer Protocol 。SFTP是用SSH封装过的FTP协议，相当于经过加密的FTP协议，功能与FTP一样，只是传输数据经过加密。

 SFTP也有二个部分，服务端的SFTP-Server及SFTP Client。通常所说的用SFTP登录到某台主机，指的是用SFTP客户端登录到某台主机（该主机运行了SFTP-Server服务端程序）。
**** SCP和SFTP异同：

 不管SCP还是SFTP，都是SSH的功能之一，也都是使用SSH协议来传输文件的。

 不只是登录时的用户信息，相互传输的文件内容也是经过SSH加密的，所以说SCP和SFTP实现了安全的文件传输。

 SCP和CP命令相似，SFTP和FTP的使用方法也类似。SCP和SFTP的共同之处在于「使用SSH将文件加密才传输的」

 使用「WinSCP」或者「FileZilla」之类的客户端，还可以和Windows之间进行文件传输。

 SCP和SFTP的不同之处，首先就是之前提到的，SCP使用「SCP命令」，SFTP则类似「FTP处理文件」的使用方式。

 它们的不同之处还不止如此，还有「SCP比较简单，是轻量级的，SFTP的功能则比较多」。

 虽然还有很多不同之处，但二者的最大不同之处在于「SFTP在文件传输过程中中断的话，连接后还可以继续传输，但SCP不行」。

 由于各种原因导致的文件传输中断是经常讨论的话题，所以这个区别（SFTP支持断点续传，SCP则不支持）被认为是最大的区别。
*** 常见的SSH客户端
**** 图形化客户端：
 WinSCP，是一个Windows环境下使用SSH的开源图形化SFTP客户端。同时支持FTP、SCP、webdav协议。它的主要功能就是在本地与远程计算机间安全的复制文件。

 Xftp，是一个基于 MS windows 平台的功能强大的SFTP、FTP 文件传输软件。使用了 Xftp 以后，MS windows 用户能安全地在 UNIX/Linux 和 Windows PC 之间传输文件。

 FileZilla是一个免费开源的FTP软件，分为客户端版本和服务器版本，具备所有的FTP软件功能。支持FTP，SFTP(SSH File Transfer Protocol)， FTPS(FTP over SSL/TLS)等多种协议。

**** 终端工具类：
 PuTTY是一个Telnet、SSH、rlogin、纯TCP以及串行接口连接软件。PuTTY是一款开放源代码软件，使用MIT licence授权。

 Xshell 是一个强大的安全终端模拟软件，它支持SSH1, SSH2, SFTP以及Microsoft Windows 平台的TELNET 协议。
** SSH 密钥
 基于密钥的验证机制使用了密码学中的公钥，我们只需要向服务器证明客户端持有对应的私钥，而不需要公开其私钥。
 这样您就可以避免每次登录都输入密码的麻烦了秘密就可以登录。
 不过，私钥(通常是 ~/.ssh/id_rsa 或者 ~/.ssh/id_ed25519) 等效于您的密码，所以一定要好好保存它。

 ssh秘钥登录特点：1.安全；2.免输密码。

 对于安全级别较高的服务器，建议配好ssh登录后禁掉密码登录。

 缺点：略繁琐。如果你的只是临时登录一次，那么还是密码吧。

*** 生成秘钥
 秘钥对需要在你自己的机器上生成，然后把公钥放到服务器相应用户的~/.ssh目录

 使用 ssh-keygen 命令可以生成一对密钥

 执行下面命令,默认生成位置是~/.ssh

 ~ssh-keygen~

 系统会询问你文件名和秘钥密码，可以一路回车过去，会生成两个文件：
 - id_rsa 私钥
 - id_rsa.pub 公钥
 默认使用rsa算法，你也可以用比较详细的指令，如
 ~ssh-keygen -t rsa -b 1024 -f yourkeyname -C "备注"~

 | 参数   | 解释                                                  |
 |--------+-------------------------------------------------------|
 | -b     | 采用长度1024bit的密钥对,b=bits,最长4096，不过没啥必要 |
 | -t rsa | 采用rsa加密方式,t=type                                |
 | -f     | 生成文件名(文件路径),f=output_keyfiles                     |
 | -C     | 备注，C=comment                                       |

 更多参数可运行 man ssh-keygen

 您可以为密钥设置密码，防止有人持有您的私钥并使用它访问您的服务器。
 您可以使用 ssh-agent 或 gpg-agent ，这样就不需要每次都输入该密码了。
*** 在服务器上安装秘钥
 ssh 会查询 .ssh/authorized_keys 来确认那些用户可以被允许登录。

 把上一步生成的公钥发送到服务器(scp,FillZilla等)上，然后在服务器上执行下面命令

 cat id_rsa.pub >> ~/.ssh/authorized_keys


 或者直接在本地机器上执行命令： ~cat .ssh/id_rsa.pub | ssh foobar@remote 'cat >> ~/.ssh/authorized_keys'~

 如果支持 ssh-copy-id 的话，可以使用下面这种更简单的解决方案：

 ~ssh-copy-id -i .ssh/id_rsa.pub foobar@remote~

 如此便完成了公钥安装，有个小坑值得一提：authenrized_keys的权限必须是600或更小，否则会连接失败。

 保险起见，执行下面命令
 #+BEGIN_SRC bash
 chmod 600 ~/.ssh/authorized_keys
 chmod 700 ~/.ssh
 #+END_SRC
 另外，.ssh目录的owner必须是ssh登录用户，不能是root

*** 服务器ssh配置
 修改服务器上的ssh配置文件，位置：/etc/ssh/sshd_config
 #+BEGIN_EXAMPLE
 RSAAuthentication yes
 PubkeyAuthentication yes

 PermitRootLogin no //禁止root登录
 PasswordAuthentication yes //允许密码登录，根据你的情况设置
 #+END_EXAMPLE
 然后重启ssh服务

 service sshd restart

** SSH执行命令
 ssh 的一个经常被忽视的特性是它可以直接远程执行命令。

 ssh foobar@server ls 可以直接在用foobar的命令下执行 ls 命令。 

 想要配合管道来使用也可以， ssh foobar@server ls | grep PATTERN 会在本地查询远端 ls 的输出而 ls | ssh foobar@server grep PATTERN 会在远端对本地 ls 输出的结果进行查询。
** 通过 SSH 复制文件
 使用 ssh 复制文件有很多方法：
 - ssh+tee, 最简单的方法是执行 ssh 命令，然后通过这样的方法利用标准输入实现 cat localfile | ssh remote_server tee serverfile。回忆一下，tee 命令会将标准输出写入到一个文件；
 - scp ：当需要拷贝大量的文件或目录时，使用scp 命令则更加方便，因为它可以方便的遍历相关路径。语法如下：scp path/to/local_file remote_host:path/to/remote_file；
 - rsync 对 scp 进行了改进，它可以检测本地和远端的文件以防止重复拷贝。它还可以提供一些诸如符号连接、权限管理等精心打磨的功能。甚至还可以基于 --partial标记实现断点续传。rsync 的语法和scp类似；
** SSH 端口转发
SSH 端口转发功能能够将其他 TCP 端口的网络数据通过 SSH 链接来转发，并且自动提供了相应的加密及解密服务。其实这一技术就是我们常常听说的隧道(tunnel)技术，原因是 SSH 为其他 TCP 链接提供了一个安全的通道来进行传输。

SSH端口转发也被称作SSH隧道(SSH Tunnel)，因为它们都是通过SSH登陆之后，在SSH客户端与SSH服务端之间建立了一个隧道，从而进行通信。

我们知道，FTP 协议是以明文来传递数据的。但是我们可以让 FTP 客户端和服务器通过 SSH 隧道传输数据，从而实现安全的 FTP 数据传输。

更常见的情况是我们的应用经常被各种防火墙限制。常见的有禁止访问某些网站、禁用某类软件，同时你的所有网络行为都被监控并分析！同样的通过 SSH 隧道技术我们完全可以规避这些限制。

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-19_15-42-53.png @ 2021-10-19 15:43:34
[[file:ssh_%E7%9B%B8%E5%85%B3/2021-10-19_15-43-34_Snipaste_2021-10-19_15-42-53.png]]
如上图所示，通过 SSH 的端口转发， 应用程序的客户端和应用程序的服务器端不再直接通讯，而是转发到了 SSH 客户端及 SSH 服务端来通讯。这样就可以同时实现两个目的：数据的加密传输和穿透防火墙！
在具体的使用场景中，端口转发又被细分为本地端口转发、远程端口转发、动态端口转发等。
*** 本地端口转发
[[https://unix.stackexchange.com/questions/115897/whats-ssh-port-forwarding-and-whats-the-difference-between-ssh-local-and-remot][StackOverflow 文章示意图]]：

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-19_16-38-33.png @ 2021-10-19 16:38:37
[[file:ssh_%E7%9B%B8%E5%85%B3/2021-10-19_16-38-37_Snipaste_2021-10-19_16-38-33.png]]

顾名思义，本地端口转发是将应用【application client】对于本地主机A指定端口X的访问请求转发给主机B，交由主机B对另一指定主机C的指定端口Z发起访问。

命令如下：
#+begin_src bash
ssh -L 主机A端口X:主机C:主机C端口Z username@hostname
# 简单理解为：将对A:X的访问转变成对C:Z的访问
#+END_SRC
客户端在执行端口转发命令的同时，实际上也执行了基本的连接命令。多出来的部分中，「-L」旗标表示使用「本地端口转发」选项，之后是用冒号分隔开的三个需要指定的项。原理上，主机C可以是任何能够被主机B识别到的设备，也可以是主机B自身。

当主机C在其某端口提供某服务【application server】，主机A需要使用该服务却无法直接访问主机C或该端口时，如果发现有SSH：A→B的连接，且主机B能够直接访问主机C的该端口，本地端口转发就派上用场。

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-19_15-48-49.png @ 2021-10-19 15:48:54
[[file:ssh_%E7%9B%B8%E5%85%B3/2021-10-19_15-48-54_Snipaste_2021-10-19_15-48-49.png]]

此时，访问请求在主机A一侧发生，可以来自于主机A自身，也可以是其他与A连接的设备。图中Host A或Host B的阴影指代主机A或主机B一侧的网络系统。

实际上ssh本地端口转发命令的「-L」旗标后可以填写四个参数，完整格式为：
#+begin_src bash
ssh -L [收听接口:]收听端口:目标主机:目标端口 username@hostname
#+END_SRC
username@hostname 是 SSH 服务器所在的主机

命令中方括号内的部分，即第一个参数可以不写；它的默认值一般是0.0.0.0，意味着SSH隧道会收听所有接口，接受来自任何地址的应用访问请求并进行转发。而如果在此处填写了绑定地址（bind address），SSH隧道连接就会只处理来自绑定地址的应用请求，而对其他地址发来的请求置之不理；如同在（真实世界的）隧道入口设立哨卡，只对白名单牌号的车辆放行。例如在此处填写127.0.0.1，即可实现只有来自主机A本机的应用请求才被SSH隧道转发的效果。

需留意，收听接口是站在主机A的视角上去规定允许与A连接的设备，解决「能够使用SSH端口转发的应用请求从何处来」的问题，类似防火墙的入站；收听端口则依旧是主机A上的那个端口X，不能够跑到别的主机上去。

类似地，远程端口转发和动态端口转发也具有「收听接口」这一可不指明的参数，下文不再赘述。从安全或控制流量的角度，规定绑定地址是一项实用的功能。
**** 实例场景
***** 场景1 
主机B与主机C处于同一内网中，主机B能够与外界联系而主机C不能。这时不处于内网中的主机A如果想要访问主机C，就可以通过SSH连接主机B＋端口转发来进行。

台式机B上运行着虚拟机C，虚拟机使用虚拟机软件搭建的虚拟网络与宿主主机B相连接，但在主机B以外无法直接访问该虚拟网络。想要通过SSH，用与台式机B处于同一WiFi下的笔记本A来远程控制虚拟机C，（在A上）执行端口转发命令：
#+begin_src bash
ssh -L 22022:10.0.2.15:22 desktop_user@192.168.1.11	# cmd.1-1
#+END_SRC
其中，22022号端口是随便选的一个没被占用的端口；192.168.1.11是台式机B在WiFi中的IP；desktop_user是主机B上的用户名；10.0.2.15是虚拟机C在主机B为其搭建的虚拟网络中的IP；22号端口是默认的SSH端口。已知virtual_user是虚拟机C上的用户名，这时在笔记本A上执行应用的访问请求命令：
#+begin_src bash
ssh -p 22022 virtual_user@localhost	# cmd.1-2
#+END_SRC
我们在笔记本A上以SSH协议访问本机（localhost）的22022号端口，这个请求就像通过了隧道（SSH隧道）一样抵达台式机B，台式机B则把这个请求变为对虚拟机C的22号端口的访问，并为A返回结果。其中，使用「-p」旗标是为了访问主机A的特定端口而不是SSH默认的22号端口；由于我们在主机A上执行命令，A管自己叫localhost，假如在其他主机上执行则需相应地改为主机A的域名或IP等他们对A的称呼。

cmd.1-2中我们是将SSH当作普通应用使用的。参考Fig.1，cmd.1-1在A与B之间建立SSH隧道，此时A上的SSH客户端和B上的SSH服务器对应图中的SSH Client和SSH Server；cmd.1-2则表达应用的访问请求，此时A上的SSH客户端和C上的SSH服务器对应图中的application client和application server。

以上cmd.1-1和cmd.1-2合起来实际是想（在A上）进行：
#+begin_src bash
ssh -p 22 virtual_user@10.0.2.15	# cmd.1-3
#+END_SRC
当然，如果这cmd.1-3能被成功执行的话，就不需要端口转发了。
***** 场景2
防火墙阻止了主机A对主机B一些端口的连接，但主机B仍有部分端口是对主机A开放的。这时主机A如果需要访问主机B上被防火墙阻挡的端口，就可以通过SSH连接主机B＋端口转发来进行。需注意，这时所谓的主机C就是主机B。
某某云的云服务器B默认的防火墙设置仅开放了22号端口，其他入方向的访问都被屏蔽了。我们为云服务器B安装了桌面环境，现在想要在自己的计算机A上，通过VNC远程控制云服务器B的桌面。（在A上）执行端口转发命令：

ssh -L 5920:localhost:5901 cloud_user@server.example.com	# cmd.2-1
因为C就是B自己，所以C的位置填localhost；5920随便选；5901是云服务器B上VNC服务进程收听的端口；cloud_user是B上的用户名；http://server.example.com 是B的域名，换成公网IP也行。

下面在计算机A上打开RealVNC VNC Viewer（VNC客户端），输入VNC服务器地址：
#+begin_src bash
localhost:20
#+END_SRC
20=5920−5900，这是采用5901到5999之间端口时RealVNC的特殊设定。开始使用优雅（或许吧）的GUI来操作云服务器吧！

*** 远程端口转发
#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-19_16-44-15.png @ 2021-10-19 16:44:19
[[file:ssh_%E7%9B%B8%E5%85%B3/2021-10-19_16-44-19_Snipaste_2021-10-19_16-44-15.png]]
当主机C在其某端口提供某服务，主机B需要使用该服务却无法直接访问主机C或该端口时，如果发现有SSH：A→B的连接，且主机A能够直接访问主机C的该端口，远程端口转发就派上用场。

远程端口转发的结构如下图所示：

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-19_16-45-42.png @ 2021-10-19 16:45:46
[[file:ssh_%E7%9B%B8%E5%85%B3/2021-10-19_16-45-46_Snipaste_2021-10-19_16-45-42.png]]

需注意，此时访问请求在主机B一侧发生，而SSH连接的方向却没有变化，仍是由A到B的。因此「本地与远程端口转发互为镜像」的说法并不完全准确；严格意义上的镜像，SSH连接也要变为由B到A，那时则应该是在B上采用本地端口转发。可以看出，采取哪种端口转发主要取决于SSH连接建立的方向。

与本地端口转发的流动方向相反，远程端口转发是将对于远程主机B指定端口Y的访问请求转发给主机A，交由主机A对另一指定主机C的指定端口Z发起访问。命令如下：
#+begin_src bash
ssh -R 主机B端口Y:主机C:主机C端口Z username@hostname
# 简单理解为：将对B:Y的访问转变成对C:Z的访问
#+END_SRC
username@hostname不变，因为我们仍然以从主机A对主机B发起SSH连接为基础；「-R」旗标表示使用「远程端口转发」选项，之后是用冒号分隔开的三个需要指定的项。原理上，主机C可以是任何能够被主机A识别到的设备，也可以是主机A自身。

**** 示例场景

***** 场景1
主机A与主机C处于同一内网中，主机A能够与外界联系而主机C不能。这时（在主机A上）如果想让不处于内网中的主机B访问主机C，就可以通过SSH连接主机B＋端口转发来进行。
台式机A上运行着虚拟机C，虚拟机使用虚拟机软件搭建的虚拟网络与宿主主机A相连接，但在主机A以外无法直接访问该虚拟网络。想要通过SFTP，用与台式机A处于同一WiFi下的笔记本B来向虚拟机C传输文件，（在A上）执行端口转发命令：
#+begin_src bash
ssh -R 22122:10.0.2.16:22 laptop_user@192.168.1.233	# cmd.3-1
#+END_SRC
其中，22122号端口是随便选的一个没被占用的端口；192.168.1.233是笔记本B在WiFi中的IP；laptop_user是主机B上的用户名；10.0.2.16是虚拟机C在主机A为其搭建的虚拟网络中的IP；22号端口是默认的SFTP端口。已知virtual_user是虚拟机C上的用户名，这时在笔记本B上执行应用的访问请求命令：
#+begin_src bash
sftp -P 22122 virtual_user@localhost	# cmd.3-2
#+END_SRC
请注意这是一条运行在B上的应用命令；B上的SFTP客户端这时充当Fig.2中的application client。此处localhost是主机B对自己的称呼。对B的22122号端口的访问被转发至A，A访问C，即10.0.2.16的22号端口并将结果返回给B。于是B就通过远程端口转发成功访问了C上的SFTP服务器。

以上cmd.3-1和cmd.3-2合起来实际是想（在B上）进行：
#+begin_src bash
sftp -P 22 virtual_user@10.0.2.15	# cmd.3-3
#+END_SRC
当然，这cmd.3-3也是不能被直接成功执行的。

***** 场景2
处于内网之中的主机A可以访问公网，但不具有公网IP；公网中的主机B无法找到A，但为A开放各个端口的访问（A可以直接连接B，反之则不行）。这时A想要让B访问自己，就可以通过SSH连接主机B＋端口转发来进行。需注意，这时所谓的主机C就是主机A。

注意：OpenSSH服务器对于远程端口转发的设定，默认只接受远程主机B本机上的应用发起的请求。想要从其他连接到B的设备发起请求，需将「sshd_config」文件中「GatewayPorts」选项后的「no」修改为「yes」。

手头上计算机A运行着http服务，但A没有公网IP，其他设备不能使用该服务。恰好云服务器B有公网IP（甚至域名），便于被访问。在不将http服务迁移至云服务器B的前提下，可以使用SSH端口转发使其他设备通过访问B的方式访问A上的http服务。（在A上）执行端口转发命令：
#+begin_src bash
ssh -R 80:localhost:80 cloud_user@server.example.com	# cmd.4-1
#+END_SRC

这时C便是A自己（localhost）；80号端口是http默认端口，为简便两个都用默认；cloud_user还是B上的用户名；http://server.example.com 还是B的域名。

接下来在其他设备上打开浏览器，输入地址：
#+BEGIN_EXAMPLE
http://server.example.com/
#+END_EXAMPLE
于是大家可以通过访问 http://server.example.com 来访问本地计算机A提供的http服务了。

*** 动态端口转发
动态端口转发可以把本地主机A上运行的SSH客户端转变成一个SOCKS代理服务器；实际上它是一种特殊的本地端口转发，或者说叫它「动态本地端口转发」更科学。这个动态，就动在这种转发不规定目标地址（主机C）和目标端口（端口Z），而是去读取应用发起的请求，从请求中获取目标信息。
#+begin_src bash
ssh -D 主机A端口X username@hostname
#+END_SRC
好像很强，但有一个问题：之前使用固定的端口转发时，应用的访问请求都是指向被转发的那个端口X的，但现在应用的访问请求必须指向目标，以指定动态端口转发的目标。可如果不指向端口X，如何让数据走SSH隧道呢？这就要求我们在系统或应用（浏览器等）中设置一个使用SOCKS5协议、服务器为localhost、端口为X的代理，利用代理使请求走端口X。

这样应用的请求就从X进入隧道，抵达B后其中的目标信息被解析出来，B访问目标后再将结果通过隧道返回给A。比如在开启代理的A上的浏览器中访问http://zhihu.com ，经过端口转发，相当于是B在帮A访问 http://zhihu.com。 

**** 端口转发的停止
SSH端口转发完全基于基本的SSH连接，因此，通过在远程终端上执行exit命令、暴力关闭本地终端窗口、远程主机B关机、本地主机A关机等可以切断SSH连接的方式，即可停止SSH端口转发。就是这样。
** SSH config
SSH config是Linux系统下针对SSH客户端的一个参数配置方案，可以将一些关于SSH命令的参数放到配置文件中去，执行ssh命令的时候从文件中读取，简化命令行的操作。这篇短博客记录ssh config相关的配置问题和使用方法。

SSH 参数配置有3个层次：
- 命令行参数，如-p 10086, -i /path/to/identity_file 等选项来设置SSH的端口号或认证证书位置
- 针对某个用户的配置文件，所在路径为~/.ssh/config，默认是不存在的，需要手动创建
- 针对系统所有用户的配置文件，，所在路径为/etc/ssh/ssh_config

参数重要性的顺序也是1>2>3，即越近的配置重要性越高。这里主要讲述第2种情况下的配置方式，即针对~/.ssh/config文件的写法进行说明。

一个示例的文件如下：
#+BEGIN_EXAMPLE
# configuration 1
Host cluster
	HostName 192.168.11.11
	User tom


# configuration 2
Host=aliyun
	Hostname=202.44.2.2
	User tom
#+END_EXAMPLE
主要的规则如下：
- 每项配置都是参数名 参数值或参数值=参数名的形式，其中参数名不区分大小写，而参数值区分大小写，如上面的参数名HostName和Hostname是同一个参数
- 不同主机的配置通过Host参数来区分，一个配置文件里面可以有针对多个Host的配置
- 以#开头的是注释，会被忽略
- 同一个Host的配置内部，参数名 参数值和参数值=参数名的形式可以混用，如上例#2配置所示

下面详细展开常见的参数类型。
** 常见参数类型
参数可以在命令行通过man ssh_config来查看
*** Host
类似昵称，用于标识某个特定的配置，在ssh命令中使用，例如我们想要ssh连接到上例中的#1配置的主机，则在命令行执行如下命令即可：

ssh cluster


一个最有用的场景是使用scp在不同主机间传数据。没有配置之间，你得写很长的参数，如

scp a.txt tom@192.168.11.11:~/


尤其是IP地址记忆起来好麻烦啊。配置过上例中的文件后，这个任务可以简化成这样：

scp a.txt cluster:~/


省略了用户名和IP地址，方便多了。
*** HostName
需要ssh连接过去的主机名，一般是IP地址，也可以用%h来替代命令行参数
*** User
登录主机的用户名
*** IdentityFile
认证证书文件，默认位置是~/.ssh/id_rsa, ~/ssh/id_dsa等，如果采用默认的证书，可以不用设置此参数，除非你的证书放在某个自定义的目录，那么你就需要设置该参数来指向你的证书
*** Port
SSH访问主机的端口号，默认是22端口，同上，只有在非默认情况下才需要设置该值

* 设置合上笔记本盖子不休眠的方法
编辑下列文件：sudo gedit /etc/systemd/logind.conf
#+BEGIN_EXAMPLE
#HandlePowerKey按下电源键后的行为，默认power off
#HandleSleepKey 按下挂起键后的行为，默认suspend
#HandleHibernateKey按下休眠键后的行为，默认hibernate
#HandleLidSwitch合上笔记本盖后的行为，默认suspend（改为ignore；即合盖不休眠）在原文件中，还要去掉前面的#
#+END_EXAMPLE
最后重启服务

service systemd-logind restart
* cuda
** 如何查看显卡支持的CUDA版本
1. 在开始中找到并打开NVIDIA控制面板，如下图所示。
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/201702141648462427.jpg @ 2020-08-11 23:06:13
[[file:cuda/2020-08-11_23-06-13_201702141648462427.jpg]]


2. 打开NVIDIA控制面板，如下图所示。选择“系统信息”--“组件”，找到NVCUDA.DLL信息显示即为显卡支持的CUDA最高版本。

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170214164846247.jpg @ 2020-08-11 23:01:36
[[file:cuda/2020-08-11_23-01-36_20170214164846247.jpg]]

3. 在编译caffe时，若显卡的计算能力比较低的话，需要修改caffe-master\windows下的CommonSettings.props的属性表，现有的gpu计算能力参数：有compute_20,sm_20;compute_30,sm_30;compute_35,sm_35;compute_50,sm_50;compute_52,sm_52

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170214174642686.jpg @ 2020-08-11 23:07:19
[[file:cuda/2020-08-11_23-07-19_20170214174642686.jpg]]

* TensorFlow
用pip命令
#+BEGIN_SRC bash
pip install tensorflow
#+END_SRC

* 安装Pytorch
首先安装Anaconda.

然后进入pytorch官网,根据自己的情况进行选择,之后最下方红线位置就会显示你应该输入的安装命令

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-06-02_17-33-39.png @ 2020-06-02 17:33:46
[[file:%E5%AE%89%E8%A3%85Pytorch/2020-06-02_17-33-46_Snipaste_2020-06-02_17-33-39.png]]


将得到的命令粘贴到终端窗口中运行即可.

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20180117102533147.png @ 2020-06-02 17:28:16
[[file:%E5%AE%89%E8%A3%85Pytorch/2020-06-02_17-28-16_20180117102533147.png]]

验证pytorch是否安装成功:

terminal内输入python，进入python环境
然后输入下面的命令:
#+BEGIN_SRC bash
import torch
import torchvision
#+END_SRC

不报错的话就说明pytorch安装成功了，如图所示
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-06-02_17-29-56.png @ 2020-06-02 17:30:06
[[file:%E5%AE%89%E8%A3%85Pytorch/2020-06-02_17-30-06_Snipaste_2020-06-02_17-29-56.png]]

如果下载速度很慢的话，可以选择pip方式下载安装,如下图所示

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-06-02_17-26-40.png @ 2020-06-02 17:31:43
[[file:%E5%AE%89%E8%A3%85Pytorch/2020-06-02_17-31-43_Snipaste_2020-06-02_17-26-40.png]]

一般这个pip方式会比较快吧，但如果很不幸，你的pip方式也遇到网络很差的问题，那就下载文件吧……

在浏览器里新建任务，下载链接就是图中命令位置里的链接: [[https://download.............]]  把这个文件下载下来，实在不行去windows上面用迅雷下载这个文件，下载好了之后copy到你的Linux系统上去，然后找到文件位置，直接pip install 文件名

* OpenCV
** C++接口
先安装依赖
#+BEGIN_SRC bash
sudo apt-get install build-essential
sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev
sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev
#+END_SRC
然后，将压缩包解压，我下载我是opencv3.4.3版本，所以最后解压出来的文件夹就是opencv-3.4.3，接着，先用命令行进入该文件夹，然后执行命令，如下所示：
#+BEGIN_SRC bash
cd ~/opencv-3.4.3  # 进入opencv文件夹
mkdir build # 创建build文件夹
cd build # 进入build文件夹

#cmake指令，如果没有特殊要求建议就选择默认的就可以
#注意，后面的两个点千万不能省，代表了上级目录
cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local ..  
make -j7 # 多线程执行make任务

# 最后一步，安装库文件
sudo make install

#安装完成
#+END_SRC
** python接口
方法一：用pip命令即可
#+BEGIN_SRC bash
pip install opencv-python #安装opencv
pip install opencv-contrib-python #安装opencv的contrib扩展包
#+END_SRC

方法二：用conda安装
#+BEGIN_SRC bash
conda search opencv #查询一下conda里面可以安装的opencv
conda install opencv=3.1.0 #根据你想要安装的opencv版本（以3.1.0为例），输入指令
#+END_SRC
* Anaconda
Anaconda（官方网站）就是可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。如果日常工作或学习并不必要使用1,000多个库，那么可以考虑安装Miniconda
** Anaconda、conda、pip、virtualenv的区别
*** Anaconda
Anaconda是一个包含180+的科学包及其依赖项的发行版本。其包含的科学包包括：conda, numpy, scipy, ipython notebook等。
*** conda
conda是包及其依赖项和环境的管理工具。

- 适用语言：Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++, FORTRAN。

- 适用平台：Windows, macOS, Linux

- 用途：

(1)快速安装、运行和升级包及其依赖项。

(2)在计算机中便捷地创建、保存、加载和切换环境。

如果你需要的包要求不同版本的Python，你无需切换到不同的环境，因为conda同样是一个环境管理器。仅需要几条命令，你可以创建一个完全独立的环境来运行不同的Python版本，同时继续在你常规的环境中使用你常用的Python版本。——Conda官方网站
▪ conda为Python项目而创造，但可适用于上述的多种语言。

▪ conda包和环境管理器包含于Anaconda的所有版本当中。

** conda
 Python的版本比较多，并且它的库也非常广泛，同时库和库之间存在很多依赖关系，所以在库的安装和版本的管理上很麻烦。Conda是一个管理版本和Python环境的工具.

*** 源（channels）管理
**** 显示所有channel
#+BEGIN_SRC bash
conda config --show #显示出所有conda的config信息。
conda config --show channels #只看channels的信息
#+END_SRC
**** 增加源
#+BEGIN_SRC bash
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --set show_channel_urls yes  # 设置搜索时显示通道地址，这样就可以知道包的安装来源了。
#+END_SRC
添加完后，找到 .condarc 文件，删除里面的 defaults，这样能快点。当第一次执行 ~conda config~ 时，会生成配置文件.condarc


1、切换为清华源
#+BEGIN_SRC python
conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/   
conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge 
conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/
conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ 
# 设置搜索时显示通道地址
conda config --set show_channel_urls yes
#+END_SRC
镜像源地址由https改为http可以避免一些安装库时发生的错误
2、切换为中科大源
#+BEGIN_SRC python
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/
conda config --set show_channel_urls yes
#+END_SRC
3、切换回默认源
#+BEGIN_SRC python
conda config --remove-key channels
#+END_SRC
**** 移除镜像
#+BEGIN_SRC bash
conda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/  #这个命令是为了移除之前conda config --show channels显示的清华源。
#+END_SRC
*** Conda的环境管理
**** 创建环境
 #+BEGIN_SRC bash
 #创建一个名为py35的环境，指定Python版本是3.5（不用管是3.5.x，conda会为我们自动寻找3.５.x中的最新版本）
 conda create --name py35 python=3.5
 #+END_SRC

**** 激活环境
 #+BEGIN_SRC bash
 # 安装好后，使用activate激活某个环境
 activate py35 # for Windows
 source activate py35 # for Linux & Mac
 (py35) user@user-XPS-8920:~$
 #激活后，会发现terminal输入的地方多了py35的字样，实际上，此时系统做的事情就是把默认2.7环境从PATH中去除，再把3.4对应的命令加入PATH
 #+END_SRC

**** 返回主环境
 #+BEGIN_SRC bash
 # 如果想返回默认的python 2.7环境，运行
 deactivate py35 # for Windows
 source deactivate py35 # for Linux & Mac
 #+END_SRC

**** 删除环境
 #+BEGIN_SRC bash
 # 删除一个已有的环境
 conda remove --name py35 --all
 #+END_SRC
删除后将目录 anaconda3/envs下的环境文件夹删除
**** 复制（克隆）环境
conda本身的命令里是有移植这个选项的。 

假如前提是，在本地的conda里已经有一个AAA的环境，我想创建一个新环境跟它一模一样的叫BBB，那么这样一句就搞定了：

~conda create -n BBB --clone AAA~

但是如果是跨计算机呢。其实是一样的。

查询conda create命令的原来说明，是这样的：
#+BEGIN_EXAMPLE
–clone ENV 
Path to (or name of) existing local environment.
#+END_EXAMPLE
–clone这个参数后面的不仅可以是环境的名字，也可以是环境的路径。
**** 查看系统中的所有环境
 用户安装的不同Python环境会放在~/anaconda/envs目录下。

 查看当前系统中已经安装了哪些环境，使用:
 #+BEGIN_SRC bash
 conda info -e
 #+END_SRC
*** Conda的包管理
**** 安装库
 为当前环境安装库
 #+BEGIN_SRC bash
 conda install numpy
 # conda会从从远程搜索numpy的相关信息和依赖项目
 #+END_SRC
**** 查看已经安装的库
 #+BEGIN_SRC bash
 # 查看已经安装的packages
 conda list
 # 最新版的conda是从site-packages文件夹中搜索已经安装的包，可以显示出通过各种方式安装的包
 #+END_SRC
**** 查看某个环境的已安装包
 #+BEGIN_SRC bash
 # 查看某个指定环境的已安装包
 conda list -n py35
 #+END_SRC
**** 搜索package的信息
 #+BEGIN_SRC bash
 # 查找package信息
 conda search numpy
 #+END_SRC
**** 安装package到指定的环境
 #+BEGIN_SRC bash
 # 安装package
 conda install -n py35 numpy
 # 如果不用-n指定环境名称，则被安装在当前活跃环境
 # 也可以通过-c指定通过某个channel安装
 #+END_SRC
**** 更新package
 #+BEGIN_SRC bash
 # 更新package
 conda update -n py35 numpy
 #+END_SRC
**** 删除package
#+BEGIN_SRC bash
# 删除package
conda uninstall xxx   #卸载xxx文件包
#+END_SRC
**** 删除没用的包
#+BEGIN_SRC python
conda clean [-h] [-a] [-i] [-p] [-t] [-f]
                   [-c TEMPFILES [TEMPFILES ...]] [-d] [--json] [-q] [-v] [-y]
#+END_SRC
***** Removal Targets
-a, --all
Remove index cache, lock files, unused cache packages, and tarballs.

-i, --index-cache
Remove index cache.

-p, --packages
Remove unused packages from writable package caches. WARNING: This does not check for packages installed using symlinks back to the package cache.

-t, --tarballs
Remove cached package tarballs.

-f, --force-pkgs-dirs
Remove all writable package caches. This option is not included with the --all flag. WARNING: This will break environments with packages installed using symlinks back to the package cache.

-c, --tempfiles
Remove temporary files that could not be deleted earlier due to being in-use. Argument is path(s) to prefix(es) where files should be found and removed.
***** Output, Prompt, and Flow Control Options
-d, --dry-run
Only display what would have been done.

--json
Report all output as json. Suitable for using conda programmatically.

-q, --quiet
Do not display progress bar.

-v, --verbose
Can be used multiple times. Once for INFO, twice for DEBUG, three times for TRACE.

-y, --yes
Do not ask for confirmation.

Examples:

conda clean --tarballs
**** 更新conda
#+BEGIN_SRC bash
# 更新conda，保持conda最新
conda update conda
#+END_SRC
**** 更新anaconda
#+BEGIN_SRC bash
# 更新anaconda
conda update anaconda
#+END_SRC
**** 更新python
 #+BEGIN_SRC bash
 #假设当前环境是python 3.5, conda会将python升级为3.5.x系列的当前最新版本
 conda update python
 #+END_SRC
**** 批量导出、安装库
conda批量导出包含环境中所有组件的requirements.txt文件
#+BEGIN_SRC python
conda list -e > requirements.txt
#+END_SRC
conda批量安装requirements.txt文件中包含的组件依赖
#+BEGIN_SRC python
conda install --yes --file requirements.txt
#+END_SRC

** 使用国内镜像源安装pytorch
先设置镜像源，如清华的conda镜像
#+BEGIN_SRC python
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge 
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
conda config --set show_channel_urls yes
#+END_SRC
官方安装的命令是(版本1.6 CPU)：
#+BEGIN_SRC bash
conda install pytorch torchvision cpuonly -c pytorch
#+END_SRC
但要用国内源，我发现不能用-c这一段，直接用
#+BEGIN_SRC bash
conda install pytorch torchvision cpuonly
#+END_SRC

** 解决 conda install failed: conda.core.subdir_data.Response304ContentUnchanged
问题产生：

在install pkgs时，报错

Collecting package metadata (current_repodata.json): failed

具体为：

conda.core.subdir_data.Response304ContentUnchanged

 

解决：
works for me.
#+BEGIN_SRC bash
conda clean -i
#+END_SRC


清空cache后重新安装

suggestion from Github
#+BEGIN_SRC bash
conda config --remove channels conda-forge
#+END_SRC

疑似 forge 源出了问题
* 设置默认启动方式（图形界面或命令行界面）
在root用户权限下：

查看当前启动模式

systemctl get-default

更改模式命令：

systemctl set-default graphical.target由命令行模式更改为图形界面模式

systemctl set-default multi-user.target由图形界面模式更改为命令行模式

跟以前使用的linux版本一样，编辑 vi /etc/inittab 文件，修改系统初始化方式
* gdb
** gdb安装
打开终端，在终端里输入以下指令：
#+BEGIN_SRC bash
apt-get update
apt-get install  gdb
#+END_SRC
** 调试信息
一般来说GDB主要调试的是C/C++的程序。要调试C/C++的程序，首先在编译时，我们必须要把调试信息加到可执行文件中。使用编译器（cc/gcc/g++）的 -g 参数可以做到这一点。如：
#+begin_src bash
> gcc -g hello.c -o hello

> g++ -g hello.cpp -o hello
#+END_SRC
如果没有-g，你将看不见程序的函数名、变量名，所代替的全是运行时的内存地址。
** 启动gdb方式
1、gdb program
program 也就是你的执行文件，一般在当前目录下。

2、gdb program core
用gdb同时调试一个运行程序和core文件，core是程序非法执行后core dump后产生的文件。

3、gdb program 1234
如果你的程序是一个服务程序，那么你可以指定这个服务程序运行时的进程ID。gdb会自动attach上去，并调试他。program应该在PATH环境变量中搜索得到。

GDB启动时，可以加上一些GDB的启动开关，详细的开关可以用gdb -help查看。下面只列举一些比较常用的参数：

--symbols=SYMFILE
从指定文件中读取符号表。

--se=FILE
从指定文件中读取符号表信息，并把他用在可执行文件中。

--core=COREFILE
调试时core dump的core文件。

--directory=DIR
加入一个源文件的搜索路径。默认搜索路径是环境变量中PATH所定义的路径。
** GDB 的命令概貌
启动gdb后，就进入gdb的调试环境中，就可以使用gdb的命令开始调试程序了，gdb的命令可以使用help命令来查看，如下所示：
#+begin_src bash
root@linux:/home/benben# gdb
GNU gdb 5.1.1
Copyright 2002 Free Software Foundation, Inc.
GDB is free software, covered by the GNU General Public License, and you are
welcome to change it and/or distribute copies of it under certain conditions.
Type "show copying" to see the conditions.
There is absolutely no warranty for GDB. Type "show warranty" for details.
This GDB was configured as "i386-suse-linux".
(gdb) help
List of classes of commands:
 
aliases -- Aliases of other commands
breakpoints -- Making program stop at certain points
data -- Examining data
files -- Specifying and examining files
internals -- Maintenance commands
obscure -- Obscure features
running -- Running the program
stack -- Examining the stack
status -- Status inquiries
support -- Support facilities
tracepoints -- Tracing of program execution without stopping the program
user-defined -- User-defined commands
 
Type "help" followed by a class name for a list of commands in that class.
Type "help" followed by command name for full documentation.
Command name abbreviations are allowed if unambiguous.
(gdb)
#+END_SRC
gdb的命令很多，gdb把之分成许多个种类。help命令只是例出gdb的命令种类，如果要看种类中的命令，可以使用 help 命令，如：help breakpoints，查看设置断点的所有命令。也可以直接help 来查看命令的帮助。

gdb中，输入命令时，可以不用打全命令，只用打命令的前几个字符就可以了，当然，命令的前几个字符应该要标志着一个唯一的命令，在Linux下，你可以敲击两次TAB键来补齐命令的全称，如果有重复的，那么gdb会把其列出来。

示例一： 在进入函数func时，设置一个断点。可以敲入break func，或是直接就是b func
#+begin_src bash
(gdb) b func
Breakpoint 1 at 0x8048458: file hello.c, line 10.
#+END_SRC
示例二： 敲入b按两次TAB键，你会看到所有b打头的命令：
#+begin_src bash
(gdb) b
backtrace break bt
#+END_SRC
示例三： 只记得函数的前缀，可以这样：
#+begin_src bash
(gdb) b make_ <按TAB键>
（再按下一次TAB键，你会看到:）
make_a_section_from_file make_environ
make_abs_section make_function_type
make_blockvector make_pointer_type
make_cleanup make_reference_type
make_command make_symbol_completion_list
#+END_SRC
GDB把所有make开头的函数全部例出来给你查看。

要退出gdb时，只用发quit或命令简称q就行了。
** GDB 中运行UNIX的shell程序
在gdb环境中，你可以执行UNIX的shell的命令，使用gdb的shell命令来完成：
#+begin_src bash
shell
#+END_SRC
调用UNIX的shell来执行，环境变量SHELL中定义的UNIX的shell将会被用来执行，如果SHELL没有定义，那就使用UNIX的标准shell：/bin/sh。

退出用exit命令，回到gdb提示符

还有一个gdb命令是make：
#+begin_src bash
make
#+END_SRC
可以在gdb中执行make命令来重新build自己的程序。这个命令等价于“shell make ”。
** 在GDB中运行程序
当以 gdb 方式启动gdb后，gdb会在PATH路径和当前目录中搜索源文件。如要确认gdb是否读到源文件，可使用l或list命令，看看gdb是否能列出源代码。

在gdb中，运行程序使用r或是run命令。程序的运行，你有可能需要设置下面四方面的事。
1. 程序运行参数。
#+BEGIN_EXAMPLE
set args 可指定运行时参数。（如：set args 10 20 30 40 50 ）
show args 命令可以查看设置好的运行参数。
#+END_EXAMPLE
2. 运行环境。
#+BEGIN_EXAMPLE
path  可设定程序的运行路径。
show paths 查看程序的运行路径。
set env environmentVarname=value 设置环境变量。如：set env USER=benben
show env [varname] 查看环境变量，不带varname，打印出当前所有环境变量。
#+END_EXAMPLE
3. 工作目录。
#+BEGIN_EXAMPLE
cd 相当于shell的cd命令。
pwd 显示当前的所在目录。
#+END_EXAMPLE
4. 程序的输入输出。
#+BEGIN_EXAMPLE
info terminal 显示你程序用到的终端的模式。
使用重定向控制程序输出。如：run > outfile
tty命令可以设置输入输出使用的终端设备。如：tty /dev/tty1
#+END_EXAMPLE
** 调试已运行的程序
两种方法：
1. 在UNIX下用ps查看正在运行的程序的PID（进程ID），然后用gdb PID process-id 格式挂接正在运行的程序。
2. 先用gdb 关联上源代码，并进行gdb，在gdb中用attach process-id 命令来挂接进程的PID。并用detach来取消挂接的进程。
** 暂停 / 恢复程序运行
调试程序中，暂停程序运行是必须的，GDB可以方便地暂停程序的运行。你可以设置程序的在哪行停住，在什么条件下停住，在收到什么信号时停往等等。以便于你查看运行时的变量，以及运行时的流程。

当进程被gdb停住时，你可以使用info program 来查看程序的是否在运行，进程号，被暂停的原因。

在gdb中，我们可以有以下几种暂停方式：断点（BreakPoint）、观察点（WatchPoint）、捕捉点（CatchPoint）、信号（Signals）、线程停止（Thread Stops）。如果要恢复程序运行，可以使用c或是continue命令。
*** 设置断点（BreakPoint）
我们用break命令来设置断点。正面有几点设置断点的方法：
#+BEGIN_EXAMPLE
break function
在进入指定函数时停住。C++中可以使用class::function或function(type,type)格式来指定函数名。

break linenum
在指定行号停住。

break +offset
break -offset
在当前行号的前面或后面的offset行停住。offset为自然数。

break filename:linenum
在源文件filename的linenum行处停住。

break filename:function
在源文件filename的function函数的入口处停住。

break *address
在程序运行的内存地址处停住。

break
break命令没有参数时，表示在下一条指令处停住。

break ... if cond
...可以是上述的参数，condition表示条件，在条件成立时停住。比如在循环境体中，可以设置break if i=100，表示当i为100时停住程序。
#+END_EXAMPLE
查看断点时，可使用info命令，如下所示：（注：n表示断点号）
#+BEGIN_EXAMPLE
info breakpoints [n]
info break [n]
info watchpoints [n]
#+END_EXAMPLE
*** 设置观察点（WatchPoint）
观察点一般来观察某个表达式（变量也是一种表达式）的值是否有变化了，如果有变化，马上停住程序。我们有下面的几种方法来设置观察点：
#+BEGIN_EXAMPLE
watch expr
为表达式（变量）expr设置一个观察点。一量表达式值有变化时，马上停住程序。

rwatch expr
当表达式（变量）expr被读时，停住程序。

awatch expr
当表达式（变量）的值被读或被写时，停住程序。

info watchpoints
查看观察点、断点和捕捉点信息，同info break 一样.
#+END_EXAMPLE
*** 设置捕捉点（CatchPoint）

你可设置捕捉点来补捉程序运行时的一些事件。如：载入共享库（动态链接库）或是C++的异常。设置捕捉点的格式为：
#+BEGIN_EXAMPLE
catch event
#+END_EXAMPLE
当event发生时，停住程序。event可以是下面的内容：
- throw 一个C++抛出的异常。（throw为关键字）
- catch 一个C++捕捉到的异常。（catch为关键字）
- exec 调用系统调用exec时。（exec为关键字，目前此功能只在HP-UX下有用）
- fork 调用系统调用fork时。（fork为关键字，目前此功能只在HP-UX下有用）
- vfork 调用系统调用vfork时。（vfork为关键字，目前此功能只在HP-UX下有用）
- load 或 load 载入共享库（动态链接库）时。（load为关键字，目前此功能只在HP-UX下有用）
- unload 或 unload 卸载共享库（动态链接库）时。（unload为关键字，目前此功能只在HP-UX下有用）

#+BEGIN_EXAMPLE
tcatch event
只设置一次捕捉点，当程序停住以后，应点被自动删除。
#+END_EXAMPLE
*** 维护停止点
上面说了如何设置程序的停止点，GDB中的停止点也就是上述的三类。在GDB中，如果你觉得已定义好的停止点没有用了，你可以使用delete、clear、disable、enable这几个命令来进行维护。
#+BEGIN_EXAMPLE
clear
清除所有的已定义的停止点。

clear function
清除所有设置在函数上的停止点。

clear linenum
清除所有设置在指定行上的停止点。
clear filename:linenum
清除所有设置在指定文件：指定行上的停止点。

delete [breakpoints] [range...]
删除指定的断点，breakpoints为断点号。如果不指定断点号，则表示删除所有的断点。range 表示断点号的范围（如：3-7）。其简写命令为d。

比删除更好的一种方法是disable停止点，disable了的停止点，GDB不会删除，当你还需要时，enable即可，就好像回收站一样。

disable [breakpoints] [range...]
disable所指定的停止点，breakpoints为停止点号。如果什么都不指定，表示disable所有的停止点。简写命令是dis.

enable [breakpoints] [range...]
enable所指定的停止点，breakpoints为停止点号。

enable [breakpoints] once range...
enable所指定的停止点一次，当程序停止后，该停止点马上被GDB自动disable。

enable [breakpoints] delete range...
enable所指定的停止点一次，当程序停止后，该停止点马上被GDB自动删除。
#+END_EXAMPLE
*** 停止条件维护
前面在说到设置断点时，我们提到过可以设置一个条件，当条件成立时，程序自动停止，这是一个非常强大的功能，这里，我想专门说说这个条件的相关维护命令。一般来说，为断点设置一个条件，我们使用if关键词，后面跟其断点条件。并且，条件设置好后，我们可以用condition命令来修改断点的条件。（只有 break和watch命令支持if，catch目前暂不支持if）
#+BEGIN_EXAMPLE
condition bnum expression
修改断点号为bnum的停止条件为expression。

condition bnum
清除断点号为bnum的停止条件。
#+END_EXAMPLE
还有一个比较特殊的维护命令ignore，你可以指定程序运行时，忽略停止条件几次。
#+BEGIN_EXAMPLE
ignore bnum count
表示忽略断点号为bnum的停止条件count次。
#+END_EXAMPLE
*** 为停止点设定运行命令
我们可以使用GDB提供的command命令来设置停止点的运行命令。也就是说，当运行的程序在被停止住时，我们可以让其自动运行一些别的命令，这很有利行自动化调试。对基于GDB的自动化调试是一个强大的支持。
#+BEGIN_EXAMPLE
commands [bnum]
... command-list ...
end
#+END_EXAMPLE
为断点号bnum指写一个命令列表。当程序被该断点停住时，gdb会依次运行命令列表中的命令。

例如：
#+BEGIN_EXAMPLE
break foo if x>0
commands
printf "x is %d ",x
continue
end
#+END_EXAMPLE
断点设置在函数foo中，断点条件是x>0，如果程序被断住后，也就是，一旦x的值在foo函数中大于0，GDB会自动打印出x的值，并继续运行程序。

如果你要清除断点上的命令序列，那么只要简单的执行一下commands命令，并直接在打个end就行了。
*** 断点菜单
在C++中，可能会重复出现同一个名字的函数若干次（函数重载），在这种情况下，break 不能告诉GDB要停在哪个函数的入口。当然，你可以使用break 也就是把函数的参数类型告诉GDB，以指定一个函数。否则的话，GDB会给你列出一个断点菜单供你选择你所需要的断点。你只要输入你菜单列表中的编号就可以了。如：
#+begin_src bash
(gdb) b String::after
[0] cancel
[1] all
[2] file:String.cc; line number:867
[3] file:String.cc; line number:860
[4] file:String.cc; line number:875
[5] file:String.cc; line number:853
[6] file:String.cc; line number:846
[7] file:String.cc; line number:735
> 2 4 6
Breakpoint 1 at 0xb26c: file String.cc, line 867.
Breakpoint 2 at 0xb344: file String.cc, line 875.
Breakpoint 3 at 0xafcc: file String.cc, line 846.
Multiple breakpoints were set.
Use the "delete" command to delete unwanted breakpoints.
(gdb)
#+END_SRC
*** 恢复程序运行和单步调试
当程序被停住了，你可以用continue命令恢复程序的运行直到程序结束，或下一个断点到来。也可以使用step或next命令单步跟踪程序。
#+BEGIN_EXAMPLE
continue [ignore-count]
c [ignore-count]
fg [ignore-count]
#+END_EXAMPLE
恢复程序运行，直到程序结束，或是下一个断点到来。ignore-count表示忽略其后的断点次数。continue，c，fg三个命令都是一样的意思。
#+BEGIN_EXAMPLE
step [count]
单步跟踪，如果有函数调用，他会进入该函数。进入函数的前提是，此函数被编译有debug信息。很像VC等工具中的step in。后面可以加count也可以不加，不加表示一条条地执行，加表示执行后面的count条指令，然后再停住。

next [count]
同样单步跟踪，如果有函数调用，他不会进入该函数。很像VC等工具中的step over。后面可以加count也可以不加，不加表示一条条地执行，加表示执行后面的count条指令，然后再停住。

set step-mode on
打开step-mode模式，于是，在进行单步跟踪时，程序会因为没有debug信息而停住。这个参数有很利于查看机器码。

set step-mod off
关闭step-mode模式。This is the default.

show step-mode
Show whether gdb will stop in or step over functions without source line debug information.

finish
运行程序，直到当前函数完成返回。并打印函数返回时的堆栈地址和返回值及参数值等信息。

until 或 u
当你厌倦了在一个循环体内单步跟踪时，这个命令可以运行程序直到退出循环体。

until location
u location
Continue running your program until either the specified location is reached,or the current stack frame returns. location is any of the forms of argument acceptable to break. This form of the command uses breakpoints, and hence is quicker than until without an argument. The specified location is actually reached only if it is in the current frame. This implies that until can be used to skip over recursive function invocations.
#+END_EXAMPLE
For instance in the code below, if the current location is line 96, issuing until 99 will execute the program up to 
#+BEGIN_SRC 
int factorial (int value)
{
if (value &gt; 1) {
value *= factorial (value - 1);
}
return (value);
}
#+END_SRC
#+BEGIN_EXAMPLE
stepi 或 si 或 stepi repeatCount

单步跟踪一条机器指令！一条程序代码有可能由数条机器指令完成，stepi可以单步执行机器指令。It is often useful to do ‘display/i $pc’ when stepping by machine instructions. This makes gdb automatically display the next instruction to be executed, each time your program stops.

An argument is a repeat count, as in step.

nexti
nexti repeatCount
ni
Execute one machine instruction, but if it is a function call, proceed until the function returns.An argument is a repeat count, as in next.
#+END_EXAMPLE

** 函数的调用
#+begin_src bash
call name 调用和执行一个函数
(gdb) call gen_and_sork( 1234,1,0 )
(gdb) call printf(“abcd”)
#+END_SRC

** 寄存器的标准名字
- $pc 程序计数器
- $fp 帧指针（当前堆栈帧）
- $sp 栈指针
- $ps 处理器状态
* Linux 命令行快捷键
涉及在linux命令行下进行快速移动光标、命令编辑、编辑后执行历史命令、Bang(!)命令、控制命令等。让basher更有效率。

** 常用
ctrl+左右键:在单词之间跳转
ctrl+a:跳到本行的行首
ctrl+e:跳到页尾
Ctrl+u：删除当前光标前面的文字 （还有剪切功能）
ctrl+k：删除当前光标后面的文字(还有剪切功能)
Ctrl+L：进行清屏操作
Ctrl+y:粘贴Ctrl+u或ctrl+k剪切的内容
Ctrl+w:删除光标前面的单词的字符
Alt – d ：由光标位置开始，往右删除单词。往行尾删
说明
Ctrl – k: 先按住 Ctrl 键，然后再按 k 键；
Alt – k: 先按住 Alt 键，然后再按 k 键；
M – k：先单击 Esc 键，然后再按 k 键。

** 移动光标
Ctrl – a ：移到行首
Ctrl – e ：移到行尾
Ctrl – b ：往回(左)移动一个字符
Ctrl – f ：往后(右)移动一个字符
Alt – b ：往回(左)移动一个单词
Alt – f ：往后(右)移动一个单词
Ctrl – xx ：在命令行尾和光标之间移动
M-b ：往回(左)移动一个单词
M-f ：往后(右)移动一个单词

** 编辑命令
Ctrl – h ：删除光标左方位置的字符
Ctrl – d ：删除光标右方位置的字符（注意：当前命令行没有任何字符时，会注销系统或结束终端）
Ctrl – w ：由光标位置开始，往左删除单词。往行首删
Alt – d ：由光标位置开始，往右删除单词。往行尾删
M – d ：由光标位置开始，删除单词，直到该单词结束。
Ctrl – k ：由光标所在位置开始，删除右方所有的字符，直到该行结束。
Ctrl – u ：由光标所在位置开始，删除左方所有的字符，直到该行开始。
Ctrl – y ：粘贴之前删除的内容到光标后。
ctrl – t ：交换光标处和之前两个字符的位置。
Alt + . ：使用上一条命令的最后一个参数。
Ctrl – _ ：回复之前的状态。撤销操作。
Ctrl -a + Ctrl -k 或 Ctrl -e + Ctrl -u 或 Ctrl -k + Ctrl -u 组合可删除整行。

** Bang(!)命令
!! ：执行上一条命令。
^foo^bar ：把上一条命令里的foo替换为bar，并执行。
!wget ：执行最近的以wget开头的命令。
!wget:p ：仅打印最近的以wget开头的命令，不执行。
!$ ：上一条命令的最后一个参数， 与 Alt - . 和 $_ 相同。
!* ：上一条命令的所有参数
!*:p ：打印上一条命令是所有参数，也即 !*的内容。
^abc ：删除上一条命令中的abc。
^foo^bar ：将上一条命令中的 foo 替换为 bar
^foo^bar^ ：将上一条命令中的 foo 替换为 bar
!-n ：执行前n条命令，执行上一条命令： !-1， 执行前5条命令的格式是： !-5

** 查找历史命令
Ctrl – p ：显示当前命令的上一条历史命令
Ctrl – n ：显示当前命令的下一条历史命令
Ctrl – r ：搜索历史命令，随着输入会显示历史命令中的一条匹配命令，Enter键执行匹配命令；ESC键在命令行显示而不执行匹配命令。
Ctrl – g ：从历史搜索模式（Ctrl – r）退出。

** 控制命令
Ctrl – l ：清除屏幕，然后，在最上面重新显示目前光标所在的这一行的内容。
Ctrl – o ：执行当前命令，并选择上一条命令。
Ctrl – s ：阻止屏幕输出
Ctrl – q ：允许屏幕输出
Ctrl – c ：终止命令
Ctrl – z ：挂起命令

** 重复执行操作动作
M – 操作次数 操作动作 ： 指定操作次数，重复执行指定的操作。
* xargs
** 参数
-0 不仅可以将分隔符从默认的空格变成 NULL，还会将单引号、双引号、反斜线等统统默认为是普通字符
-d delim 分隔符，默认的xargs分隔符是回车，argument的分隔符是空格，这里修改的是xargs的分隔符
-p 当每次执行一个argument的时候询问一次用户
-n num 每次处理num个参数
** 参数使用例子
*** -E
我们正在处理一份日志文件 country.list 中的内容，将日志文件中的字符以空行作为分隔符依次 echo 出来，一旦遇到 korea 便终止退出：
#+BEGIN_SRC bash
[roc@roclinux ~]$ echo "china usa korea japan" > country.list
 
[roc@roclinux ~]$ cat country.list
china usa korea japan
 
[roc@roclinux ~]$ cat country.list | xargs -E 'korea' echo
china usa
#+END_SRC
*** -p
#+BEGIN_SRC bash
[roc@roclinux ~]$ find . -type f |xargs -p rm -f
rm -f ./china.txt ./usa.txt ./japan.txt ?...n
#+END_SRC
*** -n
#+BEGIN_SRC bash
[roc@roclinux 20160408]$ find . -type f |xargs -p -n 1 rm -f
rm -f ./china.txt ?...n
rm -f ./usa.txt ?...y
rm -f ./japan.txt ?...n
#+END_SRC
** 与管道的区别
管道可以实现：将前面的标准输出作为后面的“标准输入”。

管道无法实现：将前面的标准输出作为后面的“命令参数”。

** 注意事项
xargs 的标准输入中出现的“换行符、空格、制表符”都将被空格取代。下面来看一个带有换行符的例子：
#+BEGIN_SRC bash
#我们准备好了带有换行的标准输入
[roc@roclinux ~]$ echo -e "china.txt\njapan.txt"
china.txt
japan.txt
 
#可见, 换行符和空格的作用一样
[roc@roclinux ~]$ echo -e "china.txt\njapan.txt" | xargs cat
hello beijing
hello tokyo
#+END_SRC
* sed
sed采用的是流编辑模式，最明显的特点是，在 sed 处理数据之前，需要预先提供一组规则，sed 会按照此规则来编辑数据。

sed 会根据脚本命令来处理文本文件中的数据，这些命令要么从命令行中输入，要么存储在一个文本文件中，此命令执行数据的顺序如下：
- 每次仅读取一行内容；
- 根据提供的规则命令匹配并修改数据。注意，sed 默认不会直接修改源文件数据，而是会将数据复制到缓冲区中，修改也仅限于缓冲区中的数据；
- 将执行结果输出。

当一行数据匹配完成后，它会继续读取下一行数据，并重复这个过程，直到将文件中所有数据处理完毕。

sed 命令的基本格式如下：
#+begin_src bash
[root@localhost ~]# sed [选项] [脚本命令] 文件名
#+END_SRC

该命令常用的选项及含义，如下表所示。
| 选项            | 含义                                                                                                                        |
|-----------------+-----------------------------------------------------------------------------------------------------------------------------|
| -e 脚本命令     | 该选项会将其后跟的脚本命令添加到已有的命令中。                                                                              |
| -f 脚本命令文件 | 该选项会将其后文件中的脚本命令添加到已有的命令中。                                                                          |
| -n              | 默认情况下，sed 会在所有的脚本指定执行完毕后，会自动输出处理后的内容，而该选项会屏蔽启动输出，需使用 print 命令来完成输出。 |
| -i              | 此选项会直接修改源文件，要慎用。                                                                                            |

** sed选项
*** -n
sed 会将模式空间里的行经过处理后输出到标准输出，这是默认的处理方式。也就是说，除非你使用“d”来删除此行，否则经过“模式空间”处理的行都是会被输出到标准输出（屏幕）上的。

使用-n 选项只输出处理成功的行
#+begin_src bash
#还是先来看看原文件的内容
[roc@roclinux ~]$ cat roc.txt
1
2
3
4
5
 
#仔细看, 输出中出现了两个“4”
[roc@roclinux ~]$ sed ‘/4/p’ roc.txt
1
2
3
4
4
5
[roc@roclinux ~]$ sed -n '/4/p' roc.txt
4
#+END_SRC

** 特殊符号
*** $
数据流末尾
*** &
& 字符，在 sed 命令中，它表示的是“之前被匹配的部分”
#+begin_src bash
#按照惯例, 先展示文件的内容
[roc@roclinux ~]$ cat mysed.txt
Beijing
London
 
#我们使用到了&符号, 大家试着猜一猜它的作用
[roc@roclinux ~]$ sed 's/B.*/&2008/' mysed.txt
Beijing2008
London

[roc@roclinux 20160229]$ sed 's/Bei/&2008/' mysed.txt
Bei2008jing
London
#+END_SRC
*** （）括号
“sed 的预存储技术”，也就是命令中被“（”和“）”括起来的内容会被依次暂存起来，存储到 \1、\2、...里面。这样你就可以使用‘\N’形式来调用这些预存储的内容了。
#+begin_src bash
[roc@roclinux ~]$ echo "hello world" | sed 's/\(hello\).*/world \1/'
world hello
#+END_SRC

来继续看一个例子，我们希望只在每行的第一个和最后一个 Beijing 后面加上 2008 字符串，言下之意就是，除了每行的第一个和最后一个 2008 之外，这一行中间出现的 Beijing 后面就不要加 2008 啦。这个需求，真的是很复杂很个性化，但 sed 命令仍然可以很好地满足：
#先看下文件内容, 第一行中出现了4个Beijing
#+begin_src bash
[roc@roclinux ~]$ cat mysed.txt
Beijing Beijing Beijing Beijing
London London London London
 
[roc@roclinux ~]$ sed 's/\(Beijing\)\(.*\)\(Beijing\)/\12008\2\32008/' mysed.txt
Beijing2008 Beijing Beijing Beijing2008
London London London London
#+END_SRC
这个例子中我们再次使用了预存储技术，存储了三项内容，分别代表第一个 Beijing、中间的内容、最后的 Beijing。而针对\1和\3，我们在其后面追加了 2008 这个字符串。
** sed脚本命令
*** sed s 寻找并替换
此命令的基本格式为：
#+begin_src bash
[address]s/pattern/replacement/flags
#+END_SRC
其中，address 表示指定要操作的具体行，pattern 指的是需要替换的内容，replacement 指的是要替换的新内容。

关于指定具体操作行（address）的用法，这里先不做解释，文章后续会对其做详细介绍。

此命令中常用的 flags 标记如表 2 所示。

表 2 sed s命令flags标记及功能
| flags 标记 | 功能                                                                                                                                |
|------------+-------------------------------------------------------------------------------------------------------------------------------------|
| n          | 1~512 之间的数字，表示指定要替换的字符串出现第几次时才进行替换，例如，一行中有 3 个 A，但用户只想替换第二个 A，这是就用到这个标记； |
| g          | 对数据中所有匹配到的内容进行替换，如果没有 g，则只会在第一次匹配成功时做替换操作。例如，一行数据中有 3 个 A，则只会替换第一个 A；   |
| p          | 会打印与替换命令中指定的模式匹配的行。此标记通常与 -n 选项一起使用。                                                                |
| w file     | 将缓冲区中的内容写到指定的 file 文件中；                                                                                            |
| &          | 用正则表达式匹配的内容进行替换；                                                                                                    |
| \n         | 匹配第 n 个子串，该子串之前在 pattern 中用 \(\) 指定。                                                                              |
| \          | 转义（转义替换部分包含：&、\ 等）。                                                                                                 |

比如，可以指定 sed 用新文本替换第几处模式匹配的地方：
#+begin_src bash
[root@localhost ~]# sed 's/test/trial/2' data4.txt
This is a test of the trial script.
This is the second test of the trial script.
#+END_SRC

可以看到，使用数字 2 作为标记的结果就是，sed 编辑器只替换每行中第 2 次出现的匹配模式。

如果要用新文件替换所有匹配的字符串，可以使用 g 标记：
#+begin_src bash
[root@localhost ~]# sed 's/test/trial/g' data4.txt
This is a trial of the trial script.
This is the second trial of the trial script.
#+END_SRC
我们知道，-n 选项会禁止 sed 输出，但 p 标记会输出修改过的行，将二者匹配使用的效果就是只输出被替换命令修改过的行，例如：
#+begin_src bash
[root@localhost ~]# cat data5.txt
This is a test line.
This is a different line.
[root@localhost ~]# sed -n 's/test/trial/p' data5.txt
This is a trial line.
#+END_SRC
w 标记会将匹配后的结果保存到指定文件中，比如：
#+begin_src bash
[root@localhost ~]# sed 's/test/trial/w test.txt' data5.txt
This is a trial line.
This is a different line.
[root@localhost ~]#cat test.txt
This is a trial line.
#+END_SRC
在使用 s 脚本命令时，替换类似文件路径的字符串会比较麻烦，需要将路径中的正斜线进行转义，例如：
#+begin_src bash
[root@localhost ~]# sed 's/\/bin\/bash/\/bin\/csh/' /etc/passwd
#+END_SRC
*** sed d 删除行
此命令的基本格式为：
[address]d

如果需要删除文本中的特定行，可以用 d 脚本命令，它会删除指定行中的所有内容。但使用该命令时要特别小心，如果你忘记指定具体行的话，文件中的所有内容都会被删除，举个例子：
#+begin_src bash
[root@localhost ~]# cat data1.txt
The quick brown fox jumps over the lazy dog
The quick brown fox jumps over the lazy dog
The quick brown fox jumps over the lazy dog
The quick brown fox jumps over the lazy dog
[root@localhost ~]# sed 'd' data1.txt
#什么也不输出，证明成了空文件
#+END_SRC
当和指定地址一起使用时，删除命令显然能发挥出大的功用。可以从数据流中删除特定的文本行。

address 的具体写法后续会做详细介绍，这里举几个例子：

- 通过行号指定，比如删除 data6.txt 文件内容中的第 3 行：
#+begin_src bash
[root@localhost ~]# cat data6.txt
This is line number 1.
This is line number 2.
This is line number 3.
This is line number 4.
[root@localhost ~]# sed '3d' data6.txt
This is line number 1.
This is line number 2.
This is line number 4.
#+END_SRC
- 或者通过特定行区间指定，比如删除 data6.txt 文件内容中的第 2、3行：
#+begin_src bash
[root@localhost ~]# sed '2,3d' data6.txt
This is line number 1.
This is line number 4.
#+END_SRC
- 也可以使用两个文本模式来删除某个区间内的行，但这么做时要小心，你指定的第一个模式会“打开”行删除功能，第二个模式会“关闭”行删除功能，因此，sed 会删除两个指定行之间的所有行（包括指定的行），例如：
#+begin_src bash
[root@localhost ~]#sed '/1/,/3/d' data6.txt
#删除第 1~3 行的文本数据
This is line number 4.
#+END_SRC
- 或者通过特殊的文件结尾字符，比如删除 data6.txt 文件内容中第 3 行开始的所有的内容：
#+begin_src bash
[root@localhost ~]# sed '3,$d' data6.txt
This is line number 1.
This is line number 2.
#+END_SRC
在此强调，在默认情况下 sed 并不会修改原始文件，这里被删除的行只是从 sed 的输出中消失了，原始文件没做任何改变。
*** sed a 和 i 插入行
a 命令表示在指定行的后面附加一行，i 命令表示在指定行的前面插入一行，这里之所以要同时介绍这 2 个脚本命令，因为它们的基本格式完全相同，如下所示：
#+begin_src bash
[address]a（或 i）\新文本内容
#+END_SRC
下面分别就这 2 个命令，给读者举几个例子。比如说，将一个新行插入到数据流第三行前，执行命令如下：
#+begin_src bash
[root@localhost ~]# sed '3i\
> This is an inserted line.' data6.txt
This is line number 1.
This is line number 2.
This is an inserted line.
This is line number 3.
This is line number 4.
#+END_SRC
再比如说，将一个新行附加到数据流中第三行后，执行命令如下：
#+begin_src bash
[root@localhost ~]# sed '3a\
> This is an appended line.' data6.txt
This is line number 1.
This is line number 2.
This is line number 3.
This is an appended line.
This is line number 4.
#+END_SRC
如果你想将一个多行数据添加到数据流中，只需对要插入或附加的文本中的每一行末尾（除最后一行）添加反斜线即可，例如：
#+begin_src bash
[root@localhost ~]# sed '1i\
> This is one line of new text.\
> This is another line of new text.' data6.txt
This is one line of new text.
This is another line of new text.
This is line number 1.
This is line number 2.
This is line number 3.
This is line number 4.
#+END_SRC
可以看到，指定的两行都会被添加到数据流中。
*** sed c 替换整行
c 命令表示将指定行中的所有内容，替换成该选项后面的字符串。该命令的基本格式为：
#+begin_src bash
[address]c\用于替换的新文本
#+END_SRC
举个例子：
#+begin_src bash
[root@localhost ~]# sed '3c\
> This is a changed line of text.' data6.txt
This is line number 1.
This is line number 2.
This is a changed line of text.
This is line number 4.
#+END_SRC
在这个例子中，sed 编辑器会修改第三行中的文本，其实，下面的写法也可以实现此目的：
#+begin_src bash
[root@localhost ~]# sed '/number 3/c\
> This is a changed line of text.' data6.txt
This is line number 1.
This is line number 2.
This is a changed line of text.
This is line number 4.
#+END_SRC
*** sed y 替换单个字符
y 转换命令是唯一可以处理单个字符的 sed 脚本命令，其基本格式如下：
#+begin_src bash
[address]y/inchars/outchars/
#+END_SRC
转换命令会对 inchars 和 outchars 值进行一对一的映射，即 inchars 中的第一个字符会被转换为 outchars 中的第一个字符，第二个字符会被转换成 outchars 中的第二个字符...这个映射过程会一直持续到处理完指定字符。如果 inchars 和 outchars 的长度不同，则 sed 会产生一条错误消息。

举个简单例子：
#+begin_src bash
[root@localhost ~]# sed 'y/123/789/' data8.txt
This is line number 7.
This is line number 8.
This is line number 9.
This is line number 4.
This is line number 7 again.
This is yet another line.
This is the last line in the file.
#+END_SRC
可以看到，inchars 模式中指定字符的每个实例都会被替换成 outchars 模式中相同位置的那个字符。

转换命令是一个全局命令，也就是说，它会文本行中找到的所有指定字符自动进行转换，而不会考虑它们出现的位置，再打个比方：
#+begin_src bash
[root@localhost ~]# echo "This 1 is a test of 1 try." | sed 'y/123/456/'
This 4 is a test of 4 try.
#+END_SRC
sed 转换了在文本行中匹配到的字符 1 的两个实例，我们无法限定只转换在特定地方出现的字符。
*** sed p 打印命令
p 命令表示搜索符号条件的行，并输出该行的内容，此命令的基本格式为：
#+begin_src bash
[address]p
#+END_SRC
p 命令常见的用法是打印包含匹配文本模式的行，例如：
#+begin_src bash
[root@localhost ~]# cat data6.txt
This is line number 1.
This is line number 2.
This is line number 3.
This is line number 4.
[root@localhost ~]# sed -n '/number 3/p' data6.txt
This is line number 3.
[root@localhost ~]# sed '/number 2/p' data6.txt 
This is line number 1.
This is line number 2.
This is line number 2.
This is line number 3.
This is line number 4.
#+END_SRC
可以看到，用 -n 选项和 p 命令配合使用，我们可以禁止输出其他行，只打印包含匹配文本模式的行。

如果不加-n 选项，原有的内容也会打印出来

如果需要在修改之前查看行，也可以使用打印命令，比如与替换或修改命令一起使用。可以创建一个脚本在修改行之前显示该行，如下所示：
#+begin_src bash
[root@localhost ~]# sed -n '/3/{
> p
> s/line/test/p
> }' data6.txt
This is line number 3.
This is test number 3.
#+END_SRC
sed 命令会查找包含数字 3 的行，然后执行两条命令。首先，脚本用 p 命令来打印出原始行；然后它用 s 命令替换文本，并用 p 标记打印出替换结果。输出同时显示了原来的行文本和新的行文本。
*** sed w 将当前文件指定行写入另一个文件
w 命令用来将文本中指定行的内容写入文件中，此命令的基本格式如下：
#+begin_src bash
[address]w filename
#+END_SRC
这里的 filename 表示文件名，可以使用相对路径或绝对路径，但不管是哪种，运行 sed 命令的用户都必须有文件的写权限。

下面的例子是将数据流中的前两行打印到一个文本文件中：
#+begin_src bash
[root@localhost ~]# sed '1,2w test.txt' data6.txt
This is line number 1.
This is line number 2.
This is line number 3.
This is line number 4.
[root@localhost ~]# cat test.txt
This is line number 1.
This is line number 2.
#+END_SRC
当然，如果不想让行直接输出，可以用 -n 选项，再举个例子：
#+begin_src bash
[root@localhost ~]# cat data11.txt
Blum, R       Browncoat
McGuiness, A  Alliance
Bresnahan, C  Browncoat
Harken, C     Alliance
[root@localhost ~]# sed -n '/Browncoat/w Browncoats.txt' data11.txt
cat Browncoats.txt
Blum, R       Browncoat
Bresnahan, C  Browncoat
#+END_SRC
可以看到，通过使用 w 脚本命令，sed 可以实现将包含文本模式的数据行写入目标文件。
*** sed r 将另一个文件的内容写入当前文件的指定位置
r 命令用于将一个独立文件的数据插入到当前数据流的指定位置，该命令的基本格式为：
#+begin_src bash
[address]r filename
#+END_SRC
sed 命令会将 filename 文件中的内容插入到 address 指定行的后面，比如说：
#+begin_src bash
[root@localhost ~]# cat data12.txt
This is an added line.
This is the second added line.
[root@localhost ~]# sed '3r data12.txt' data6.txt
This is line number 1.
This is line number 2.
This is line number 3.
This is an added line.
This is the second added line.
This is line number 4.
#+END_SRC
如果你想将指定文件中的数据插入到数据流的末尾，可以使用 $ 地址符，例如：
#+begin_src bash
[root@localhost ~]# sed '$r data12.txt' data6.txt
This is line number 1.
This is line number 2.
This is line number 3.
This is line number 4.
This is an added line.
This is the second added line.
#+END_SRC
*** sed q 停止sed操作
q 命令的作用是使 sed 命令在第一次匹配任务结束后，退出 sed 程序，不再进行对后续数据的处理。

比如：
#+begin_src bash
[root@localhost ~]# sed '2q' test.txt
This is line number 1.
This is line number 2.
#+END_SRC
可以看到，sed 命令在打印输出第 2 行之后，就停止了，是 q 命令造成的，再比如：
#+begin_src bash
[root@localhost ~]# sed '/number 1/{ s/number 1/number 0/;q; }' test.txt
This is line number 0.
#+END_SRC
使用 q 命令之后，sed 命令会在匹配到 number 1 时，将其替换成 number 0，然后直接退出。
*** sed n 将下一行内容移入缓存空间，替换当前内容
n 选项可以将下一行内容移入缓存空降，替换掉当前缓存空间的内容；而N 选项是将下一行内容添加进当前缓存空间，原有的缓存空间内容并不会清空
#+begin_src bash
cs144@cs144vm:~/test$ cat mysed.txt 
Beijing 2003
Beijing 2004
Beijing 2005
Beijing 2006
Beijing 2007
cs144@cs144vm:~/test$ sed -n '/200/{n;y/eijing/EIIJNG/;p}' mysed.txt 
BEIIING 2004
BEIIING 2006
cs144@cs144vm:~/test$ sed -n '/200/{N;y/eijing/EIIJNG/;p}' mysed.txt 
BEIIING 2003
BEIIING 2004
BEIIING 2005
BEIIING 2006

#+END_SRC

** sed 脚本命令的寻址方式
前面在介绍各个脚本命令时，我们一直忽略了对 address 部分的介绍。对各个脚本命令来说，address 用来表明该脚本命令作用到文本中的具体行。

默认情况下，sed 命令会作用于文本数据的所有行。如果只想将命令作用于特定行或某些行，则必须写明 address 部分，表示的方法有以下 2 种：
1. 以数字形式指定行区间；
2. 用文本模式指定具体行区间。

以上两种形式都可以使用如下这 2 种格式，分别是：
#+begin_src bash
[address]脚本命令
#+END_SRC
或者
#+begin_src bash
address {
    多个脚本命令
}
#+END_SRC
以上两种形式在前面例子中都有具体实例，因此这里不再做过多赘述。
*** 以数字形式指定行区间
当使用数字方式的行寻址时，可以用行在文本流中的行位置来引用。sed 会将文本流中的第一行编号为 1，然后继续按顺序为接下来的行分配行号。

在脚本命令中，指定的地址可以是单个行号，或是用起始行号、逗号以及结尾行号指定的一定区间范围内的行。这里举一个 sed 命令作用到指定行号的例子：
#+begin_src bash
[root@localhost ~]#sed '2s/dog/cat/' data1.txt
The quick brown fox jumps over the lazy dog
The quick brown fox jumps over the lazy cat
The quick brown fox jumps over the lazy dog
The quick brown fox jumps over the lazy dog
#+END_SRC
可以看到，sed 只修改地址指定的第二行的文本。下面的例子中使用了行地址区间：
#+begin_src bash
[root@localhost ~]# sed '2,3s/dog/cat/' data1.txt
The quick brown fox jumps over the lazy dog
The quick brown fox jumps over the lazy cat
The quick brown fox jumps over the lazy cat
The quick brown fox jumps over the lazy dog
#+END_SRC
在此基础上，如果想将命令作用到文本中从某行开始的所有行，可以用特殊地址——美元符（$）：
#+begin_src bash
[root@localhost ~]# sed '2,$s/dog/cat/' data1.txt
The quick brown fox jumps over the lazy dog
The quick brown fox jumps over the lazy cat
The quick brown fox jumps over the lazy cat
The quick brown fox jumps over the lazy cat
#+END_SRC
*** 用文本模式指定行区间
sed 允许指定文本模式来过滤出命令要作用的行，格式如下：
#+begin_src bash
/pattern/command
#+END_SRC
注意，必须用正斜线将要指定的 pattern 封起来，sed 会将该命令作用到包含指定文本模式的行上。

举个例子，如果你想只修改用户 demo 的默认 shell，可以使用 sed 命令，执行命令如下：
#+begin_src bash
[root@localhost ~]# grep demo /etc/passwd
demo:x:502:502::/home/Samantha:/bin/bash
[root@localhost ~]# sed '/demo/s/bash/csh/' /etc/passwd
root:x:0:0:root:/root:/bin/bash
...
demo:x:502:502::/home/demo:/bin/csh
...
#+END_SRC
虽然使用固定文本模式能帮你过滤出特定的值，就跟上面这个用户名的例子一样，但其作用难免有限，因此，sed 允许在文本模式使用正则表达式指明作用的具体行。正则表达式允许创建高级文本模式匹配表达式来匹配各种数据。这些表达式结合了一系列通配符、特殊字符以及固定文本字符来生成能够匹配几乎任何形式文本的简练模式。

关于正则表达式，本节不做过多介绍.这里仅给读者提供一个简单示例：
#+begin_src bash
[root@localhost ~]# cat test.txt
<html>
<title>First Wed</title>
<body>
h1Helloh1
h2Helloh2
h3Helloh3
</body>
</html>
#使用正则表示式给所有第一个的h1、h2、h3添加<>，给第二个h1、h2、h3添加</>
[root@localhost ~]# cat sed.sh
/h[0-9]/{
    s//\<&\>/1
    s//\<\/&\>/2
}
[root@localhost ~]# sed -f sed.sh test.txt
<h1>Hello</h1>
<h2>Hello</h2>
<h3>Hello</h3>
#+END_SRC
*** /xxx/,/yyy/定位行范围
#+begin_src bash
#文件内容展示一下
[roc@roclinux ~]$ cat mysed.txt
Beijing 2003
Beijing 2004
Beijing 2005
Beijing 2006
Beijing 2007
Beijing 2008
Beijing 2007
 
#我们想展示匹配了2005的行和2007的行之间的内容
[roc@roclinux ~]$ sed -n ’/2005/,/2007/p’ mysed.txt
Beijing 2005
Beijing 2006
Beijing 2007
#+END_SRC
我们使用/2005/来匹配行范围的首行，用/2008/来匹配行范围的尾行。可以看到，在匹配尾行时，只要遇到第一个符合要求的行，就会停止，而不会再继续向后匹配了。所以，sed 命令只是匹配到了第一个 2007，并没有匹配到第二个 2007。

** sed 多行命令
默认情况下，sed会基于换行符的位置，将数据分成行，sed 会根据定义好的脚本命令一次处理一行数据。

但是，有时我们需要对跨多行的数据执行特定操作。比如说，在文本中查找一串字符串"abcdergalskgjalskgjl" ，它很有可能出现在两行中，每行各包含其中一部分。这时，如果用普通的 sed 编辑器命令来处理文本，就不可能发现这种被分开的情况。

幸运的是，sed 命令的设计人员已经考虑到了这种情况，并设计了对应的解决方案。sed 包含了三个可用来处理多行文本的特殊命令，分别是：
- Next 命令（N）：将数据流中的下一行加进来创建一个多行组来处理。
- Delete（D）：删除多行组中的一行。
- Print（P）：打印多行组中的一行。

注意，以上命令的缩写，都为大写
*** N 多行操作命令
N 命令会将下一行文本内容添加到缓冲区已有数据之后（之间用换行符分隔），从而使前后两个文本行同时位于缓冲区中，sed 命令会将这两行数据当成一行来处理。

下面这个例子演示的 N 命令的功能：
#+begin_src bash
[root@localhost ~]# cat data2.txt
This is the header line.
This is the first data line.
This is the second data line.
This is the last line.
[root@localhost ~]# sed '/first/{ N ; s/\n/ / }' data2.txt
This is the header line.
This is the first data line. This is the second data line.
This is the last line.
#+END_SRC
在这个例子中，sed 命令查找含有单词 first 的那行文本。找到该行后，它会用 N 命令将下一行合并到那行，然后用替换命令 s 将换行符替换成空格。结果是，文本文件中的两行在 sed 的输出中成了一行。

如果要在数据文件中查找一个可能会分散在两行中的文本短语，如何实现呢？这里给大家一个实例：
#+begin_src bash
[root@localhost ~]# cat data3.txt
On Tuesday, the Linux System
Administrator's group meeting will be held.
All System Administrators should attend.
Thank you for your attendance.
[root@localhost ~]# sed 'N ; s/System Administrator/Desktop User/' data3.txt
On Tuesday, the Linux Desktop User's group meeting will be held.
All Desktop Users should attend.
Thank you for your attendance.
#+END_SRC
用 N 命令将发现第一个单词的那行和下一行合并后，即使短语内出现了换行，你仍然可以找到它，这是因为，替换命令在 System 和 Administrator之间用了通配符（.）来匹配空格和换行符这两种情况。但当它匹配了换行符时，它就从字符串中删掉了换行符，导致两行合并成一行。这可能不是你想要的。

要解决这个问题，可以在 sed 脚本中用两个替换命令，一个用来匹配短语出现在多行中的情况，一个用来匹配短语出现在单行中的情况，比如：
#+begin_src bash
[root@localhost ~]# sed 'N
> s/System\nAdministrator/Desktop\nUser/
> s/System Administrator/Desktop User/
> ' data3.txt
On Tuesday, the Linux Desktop
User's group meeting will be held.
All Desktop Users should attend.
Thank you for your attendance.
#+END_SRC
第一个替换命令专门查找两个单词间的换行符，并将它放在了替换字符串中。这样就能在第一个替换命令专门在两个检索词之间寻找换行符，并将其纳入替换字符串。这样就允许在新文本的同样位置添加换行符了。

但这个脚本中仍有个小问题，即它总是在执行 sed 命令前将下一行文本读入到缓冲区中，当它到了后一行文本时，就没有下一行可读了，此时 N 命令会叫 sed 程序停止，这就导致，如果要匹配的文本正好在最后一行中，sed 命令将不会发现要匹配的数据。

解决这个 bug 的方法是，将单行命令放到 N 命令前面，将多行命令放到 N 命令后面，像这样：
#+begin_src bash
[root@localhost ~]# sed '
> s/System\nAdministrator/Desktop\nUser/
> N
> s/System Administrator/Desktop User/
> ' data3.txt
On Tuesday, the Linux Desktop
User's group meeting will be held.
All Desktop Users should attend.
Thank you for your attendance.
#+END_SRC
现在，查找单行中短语的替换命令在数据流的后一行也能正常工作，多行替换命令则会负责短语出现在数据流中间的情况。
*** D 多行删除命令
sed 不仅提供了单行删除命令（d），也提供了多行删除命令 D，其作用是只删除缓冲区中的第一行，也就是说，D 命令将缓冲区中第一个换行符（包括换行符）之前的内容删除掉。

比如说：
#+begin_src bash
[root@localhost ~]# cat data4.txt
On Tuesday, the Linux System
Administrator's group meeting will be held.
All System Administrators should attend.
[root@localhost ~]# sed 'N ; /System\nAdministrator/D' data4.txt
Administrator's group meeting will be held.
All System Administrators should attend.
#+END_SRC
文本的第二行被 N 命令加到了缓冲区(如果不加N是匹配不到东西的)，因此 sed 命令第一次匹配就是成功，而 D 命令会将缓冲区中第一个换行符之前（也就是第一行）的数据删除，所以，得到了如上所示的结果。

下面的例子中，它会删除数据流中出现在第一行前的空白行：
#+begin_src bash
[root@localhost ~]# cat data5.txt

This is the header line.
This is a data line.

This is the last line.
[root@localhost ~]# sed '/^$/{N ; /header/D}' data5.txt
This is the header line.
This is a data line.

This is the last line.
#+END_SRC
sed会查找空白行，然后用 N 命令来将下一文本行添加到缓冲区。此时如果缓冲区的内容中含有单词 header，则 D 命令会删除缓冲区中的第一行。
*** P 多行打印命令
同 d 和 D 之间的区别一样，P（大写）命令和单行打印命令 p（小写）不同，对于具有多行数据的缓冲区来说，它只会打印缓冲区中的第一行，也就是首个换行符之前的所有内容。

例如，test.txt 文件中的内容如下：
#+begin_src bash
[root@localhost ~]# cat test.txt
aaa
bbb
ccc
ddd
eee
fff
#+END_SRC
下面对 test.txt 文件中的内容分别用 p 命令和 P 命令后，产生的输出信息的对比。
#+begin_src bash
[root@localhost ~]# sed '/.*/N;P' test.txt
aaa
aaa
bbb
ccc
ccc
ddd
eee
eee
fff

[root@localhost ~]# sed -n '/.*/N;P' test.txt
aaa
ccc
eee

[root@localhost ~]# sed '/.*/N;p' test.txt
aaa
bbb
aaa
bbb
ccc
ddd
ccc
ddd
eee
fff
eee
fff

[root@localhost ~]# sed -n '/.*/N;p' test.txt
aaa
bbb
ccc
ddd
eee
fff
#+END_SRC
注意：N 将后面行加入缓存区后就不会处理后面行了，处理完将直接处理第三行内容
** sed 保持空间
前面我们一直说，sed 命令处理的是缓冲区中的内容，其实这里的缓冲区，应称为模式空间。值得一提的是，模式空间并不是 sed 命令保存文件的唯一空间。sed 还有另一块称为保持空间的缓冲区域，它可以用来临时存储一些数据。

下表列出了 5 条可用来操作保持空间的命令。

| 命令 | 功能                             |
|------+----------------------------------|
| h    | 将模式空间中的内容复制到保持空间 |
| H    | 将模式空间中的内容附加到保持空间 |
| g    | 将保持空间中的内容复制到模式空间 |
| G    | 将保持空间中的内容附加到模式空间 |
| x    | 交换模式空间和保持空间中的内容   |
通常，在使用 h 或 H 命令将字符串移动到保持空间后，最终还要用 g、G 或 x 命令将保存的字符串移回模式空间。保持空间最直接的作用是，一旦我们将模式空间中所有的文件复制到保持空间中，就可以清空模式空间来加载其他要处理的文本内容。

由于有两个缓冲区域，下面的例子中演示了如何用 h 和 g 命令来将数据在 sed 缓冲区之间移动。
#+begin_src bash
[root@localhost ~]# cat data2.txt
This is the header line.
This is the first data line.
This is the second data line.
This is the last line.
[root@localhost ~]# sed -n '/first/ {h ; p ; n ; p ; g ; p }' data2.txt
This is the first data line.
This is the second data line.
This is the first data line.
#+END_SRC
这个例子的运行过程是这样的：
- sed脚本命令用正则表达式过滤出含有单词first的行；
- 当含有单词 first 的行出现时，h 命令将该行放到保持空间；
- p 命令打印模式空间也就是第一个数据行的内容；
- n 命令提取数据流中的下一行（This is the second data line），并将它放到模式空间；
- p 命令打印模式空间的内容，现在是第二个数据行；
- g 命令将保持空间的内容（This is the first data line）放回模式空间，替换当前文本；
- p 命令打印模式空间的当前内容，现在变回第一个数据行了。
** sed改变指定流程
*** b 分支命令
通常，sed 程序的执行过程会从第一个脚本命令开始，一直执行到最后一个脚本命令（D 命令是个例外，它会强制 sed 返回到脚本的顶部，而不读取新的行）。sed 提供了 b 分支命令来改变命令脚本的执行流程，其结果与结构化编程类似。

b 分支命令基本格式为：
#+begin_src bash
[address]b [label]
#+END_SRC
其中，address 参数决定了哪些行的数据会触发分支命令，label 参数定义了要跳转到的位置。

需要注意的是，如果没有加 label 参数，跳转命令会跳转到脚本的结尾，比如：
#+begin_src bash
[root@localhost ~]# cat data2.txt
This is the header line.
This is the first data line.
This is the second data line.
This is the last line.
[root@localhost ~]# sed '{2,3b ; s/This is/Is this/ ; s/line./test?/}' data2.txt
Is this the header test?
This is the first data line.
This is the second data line.
Is this the last test?
#+END_SRC
可以看到，因为 b 命令未指定 label 参数，因此数据流中的第2行和第3行并没有执行那两个替换命令。

如果我们不想直接跳到脚本的结尾，可以为 b 命令指定一个标签（也就是格式中的 label，最多为 7 个字符长度）。在使用此该标签时，要以冒号开始（比如 :label2），并将其放到要跳过的脚本命令之后。这样，当 sed 命令匹配并处理该行文本时，会跳过标签之前所有的脚本命令，但会执行标签之后的脚本命令。

比如说：
#+begin_src bash
[root@localhost ~]# sed '{/first/b jump1 ; s/This is the/No jump on/
> :jump1
> s/This is the/Jump here on/}' data2.txt
No jump on header line
Jump here on first data line
No jump on second data line
No jump on last line
#+END_SRC
在这个例子中，如果文本行中出现了 first，程序的执行会直接跳到 jump1 标签之后的脚本行。如果分支命令的模式没有匹配，sed 会继续执行所有的脚本命令。

b 分支命令除了可以向后跳转，还可以向前跳转，例如：
#+begin_src bash
[root@localhost ~]# echo "This, is, a, test, to, remove, commas." | sed -n '{
> :start
> s/,//1p
> /,/b start
> }'
This is, a, test, to, remove, commas.
This is a, test, to, remove, commas.
This is a test, to, remove, commas.
This is a test to, remove, commas.
This is a test to remove, commas.
This is a test to remove commas.
#+END_SRC
在这个例子中，当缓冲区中的行内容中有逗号时，脚本命令就会一直循环执行，每次迭代都会删除文本中的第一个逗号，并打印字符串，直至内容中没有逗号。
*** t 测试命令
类似于 b 分支命令，t 命令也可以用来改变 sed 脚本的执行流程。t 测试命令会根据 s 替换命令的结果，如果匹配并替换成功，则脚本的执行会跳转到指定的标签；反之，t 命令无效。

测试命令使用与分支命令相同的格式：
#+begin_src bash
[address]t [label]
#+END_SRC
跟分支命令一样，在没有指定标签的情况下，如果 s 命令替换成功，sed 会跳转到脚本的结尾（相当于不执行任何脚本命令）。例如：
#+begin_src bash
[root@localhost ~]# sed '{
> s/first/matched/
> t
> s/This is the/No match on/
> }' data2.txt
No match on header line
This is the matched data line
No match on second data line
No match on last line
#+END_SRC
此例中，第一个替换命令会查找模式文本 first，如果匹配并替换成功，命令会直接跳过后面的替换命令；反之，如果第一个替换命令未能匹配成功，第二个替换命令就会被执行。

再举个例子：
#+begin_src bash
[root@localhost ~]#  echo "This, is, a, test, to, remove, commas. " | sed -n '{
> :start
> s/,//1p
> t start
> }'
This is, a, test, to, remove, commas.
This is a, test, to, remove, commas.
This is a test, to, remove, commas.
This is a test to, remove, commas.
This is a test to remove, commas.
This is a test to remove commas.
#+END_SRC
* awk
** 简介
awk是一个强大的文本分析工具，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。

awk有3个不同版本: awk、nawk和gawk，未作特别说明，一般指gawk，gawk 是 AWK 的 GNU 版本。

awk其名称得自于它的创始人 Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母。实际上 AWK 的确拥有自己的语言： AWK 程序设计语言 ， 三位创建者已将它正式定义为“样式扫描和处理语言”。它允许您创建简短的程序，这些程序读取输入文件、为数据排序、处理数据、对输入执行计算以及生成报表，还有无数其他的功能。
** 使用方法
#+begin_src bash
awk '{pattern + action}' {filenames}
#+END_SRC
尽管操作可能会很复杂，但语法总是这样，其中 pattern 表示 AWK 在数据中查找的内容，而 action 是在找到匹配内容时所执行的一系列命令。花括号（{}）不需要在程序中始终出现，但它们用于根据特定的模式对一系列指令进行分组。 pattern就是要表示的正则表达式，用斜杠括起来。

awk语言的最基本功能是在文件或者字符串中基于指定规则浏览和抽取信息，awk抽取信息后，才能进行其他文本操作。完整的awk脚本通常用来格式化文本文件中的信息。

通常，awk是以文件的一行为处理单位的。awk每接收文件的一行，然后执行相应的命令，来处理文本。
** 三种调用方式
1. 命令行方式
dawk [-F  field-separator]  'commands'  input-file(s)

其中，commands 是真正awk命令，[-F域分隔符]是可选的。 input-file(s) 是待处理的文件。

在awk中，文件的每一行中，由域分隔符分开的每一项称为一个域。通常，在不指名-F域分隔符的情况下，默认的域分隔符是空格。

2. shell脚本方式
将所有的awk命令插入一个文件，并使awk程序可执行，然后awk命令解释器作为脚本的首行，一遍通过键入脚本名称来调用。

相当于shell脚本首行的：#!/bin/sh

可以换成：#!/bin/awk

3. 将所有的awk命令插入一个单独文件，然后调用：

awk -f awk-script-file input-file(s)

其中，-f选项加载awk-script-file中的awk脚本，input-file(s)跟上面的是一样的。
* split
split 的作用很好描述，就是将文件按照一定规则进行拆分。一般情况下，我们可以按照文件大小来进行拆分，如果是文本文件的话，还可以按照行数来进行拆分，默认是 1000 行作为一个拆分单位。

默认情况下，分割后的文件的名称会以 x 作为前缀，以 aa、ab、ac 这样的双字母格式作为后缀，形成 xaa、xab 这样的名称格式。

我们来一起看看 split 的命令格式：
~split [-b ][-C ][-][-l ][要切割的文件][输出文件名前缀][-a ]~

最常用的选项，都在这里了：
- -b<字节>：指定按多少字节进行拆分，也可以指定 K、M、G、T 等单位。
- -<行数>或-l<行数>：指定每多少行要拆分成一个文件。
- 输出文件名前缀：设置拆分后的文件的名称前缀，split 会自动在前缀后加上编号，默认从 aa 开始。
- -a<后缀长度>：默认的后缀长度是 2，也就是按 aa、ab、ac 这样的格式依次编号。
** 例子
闲言少叙，我们现在就来介绍拆分的方法。先使用 dd 命令来生成一个 700MB 文件来作为我们的拆分对象：
#+BEGIN_SRC bash
[root@roclinux ~]$ dd if=/dev/zero bs=1024 count=700000 of=king_of_ring.avi
700000+0 records in
700000+0 records out
716800000 bytes (717 MB) copied, 12.9189 s, 55.5 MB/s
 
[root@roclinux ~]$  ls -l king_of_ring.avi
-rw-r--r-- 1 root root 716800000 Apr 12 13:01 king_of_ring.avi
#+END_SRC
美国大片的文件大小是 700MB，而我手边仅有的两个优盘，都是 512MB 大小的。我打算把文件以 400MB 作为一个拆分单位，来进行拆分。这里使用到了 split 的-b选项，来指定每个拆分文件的大小：
#+BEGIN_SRC bash
[root@roclinux ~]$ split -b 400M king_of_ring.avi
 
[root@roclinux ~]$ ls -l
total 1400008
-rw-r--r-- 1 root root 716800000 Apr 12 13:01 king_of_ring.avi
-rw-r--r-- 1 root root 419430400 Apr 12 13:04 xaa
-rw-r--r-- 1 root root 297369600 Apr 12 13:04 xab
#+END_SRC
看！分身完毕！咦，怎么多出了 xaa 和 xab 两个文件，这么奇怪的名字？

是的，你没看错，在没有明确指定拆分后文件的命名方式的情况下，split 会默认采用 x 字符作为文件前缀，采用类似 aa、ab、ac 的字符串依次作为文件后缀。于是，就出现了我们上面看到的 xaa、xab 了。

从文件大小来看，如我们所愿，电影文件的确被切割成了一个 400MB 的文件、一个 300MB 的文件，终于可以装到两个优盘里了。
** 切分后的合并
使用 cat 命令将拆分文件 xaa 和 xab 合并成一个文件，可以看出合并后的文件和源文件的大小是一致的：
#+BEGIN_SRC bash
[root@roclinux ~]$ cat xaa xab > king_of_ring_merge.avi
 
[root@roclinux ~]$ ls -l
total 2100012
-rw-r--r-- 1 root root 716800000 Apr 12 13:01 king_of_ring.avi
-rw-r--r-- 1 root root 716800000 Apr 12 13:07 king_of_ring_merge.avi
-rw-r--r-- 1 root root 419430400 Apr 12 13:04 xaa
-rw-r--r-- 1 root root 297369600 Apr 12 13:04 xab
#+END_SRC
对了，如果是在 Windows 下的话，我们要先运行 cmd，然后用 copy 命令来进行文件的合并：
#+BEGIN_SRC bash
copy /b xaa + xab king_of_ring.avi
#+END_SRC
格式上和 Linux 有些区别，但原理是一样的。
** 设置拆分文件的名称前缀
上面例子中，我们没有指定拆分文件的名称前缀，结果拆分后的文件名都是 aa、ab 这样的名称，这样的名称既不达意也不美观。

下面的例子，我们尝试以 king_of_ring_part_ 作为拆分后文件的名称前缀：
#+BEGIN_SRC bash
#我们指定了king_of_ring_part_前缀

[root@roclinux ~]$ split -b 400m king_of_ring.avi king_of_ring_part_
 
#可以看到, 文件名的可读性提高了很多
[root@roclinux ~]$ ls -l king*
-rw-r--r-- 1 root root 716800000 Feb 25 18:29 king_of_ring.avi
-rw-r--r-- 1 root root 419430400 Feb 25 19:24 king_of_ring_part_aa
-rw-r--r-- 1 root root 297369600 Feb 25 19:24 king_of_ring_part_ab
#+END_SRC

文件名的可读性是不是提高了不少，从文件名称就可以看出来是美国大片的拆分文件啦。
** 设置数字后缀
如果大家看不惯以 aa、ab 这种字母作为文件后缀，我们还可以通过-d选项来指定数字形式的文件后缀：
#+BEGIN_SRC bash
#使用了-d选项
[root@roclinux ~]$ split -b 400m -d king_of_ring.avi king_of_ring_part_
 
#后缀从原来的aa、ab变成了00、01
[root@roclinux ~]$ ls -l king*
-rw-r--r-- 1 root root 716800000 Feb 25 18:29 king_of_ring.avi
-rw-r--r-- 1 root root 419430400 Feb 25 19:24 king_of_ring_part_00
-rw-r--r-- 1 root root 297369600 Feb 25 19:24 king_of_ring_part_01
#+END_SRC
** 按照行数进行拆分
前面我们讲的是按照文件大小（如 400MB）进行文件拆分的方法，但是并非所有情况都适合于用文件大小作为拆分单元。比如，我们希望把 /etc/passwd 文件按照一个文件 10 行记录的方式进行拆分，又该怎么操作呢？
#+BEGIN_SRC bash
#使用-N来指定拆分的行数,本例中为-10
[root@roclinux ~]$ split -d -10 /etc/passwd my_passwd_
 
#可以看到拆分成功
[root@roclinux ~]$ wc -l my_passwd_*
  10 my_passwd_00
  10 my_passwd_01
   5 my_passwd_02
  25 total
#+END_SRC
** 合并后的校验
需要注意的是，在通过网络来传输大文件，或者在设备之间复制大文件的时候，可能会出现传输前后数据不一致的情况。

使用 split 来拆分大文件仅仅是故事的开始，操作完毕后化零为整、完璧归赵才是完美的结局。因此需要在合并文件后进行文件的完整性校验，推荐使用 md5sum 来计算和比对前后两个大文件的 md5 值。
#+BEGIN_SRC bash
#对原先的文件计算md5值
[root@roclinux ~]$ md5sum king_of_ring.avi
eacff27bf2db99c7301383b7d8c1c07c  king_of_ring.avi
 
#对合并后的文件计算md5值, 并与原值进行比较
[root@roclinux ~]$ md5sum king_of_ring_merge.avi
eacff27bf2db99c7301383b7d8c1c07c  king_of_ring_merge.avi
#+END_SRC
如果前后一致，那么恭喜你，文件合并成功！
* 设置Linux在未登录账号情况下自动连接wifi
期望机器能在通电进入系统后，即使没有登录账号也能自动连接wifi。可以使用Linux的网络管理工具的命令：

nmctl device wifi connect [ssid wifi名字] password [wifi密码]
* 使用git建立远程仓库，让别人git clone下来
首先, 如果你的ssh没有安装的话，要安装ssh服务端。ubuntu是很简单
sudo apt-get install openssh-server
1，建立你的git 目录。

ourunix@ubuntu:~$ mkdir testgit
ourunix@ubuntu:~$ cd testgit/
2,建立你的git仓库。
ourunix@ubuntu:~/testgit$ git init
Initialized empty Git repository in /home/wlp/testgit/.git/
3，添加你的需要的项目初始文件，这里我就只添加一张文档了。
ourunix@ubuntu:~/testgit$ echo "hello,git" > sayhi.txt
4，跟踪及提交到仓库。
ourunix@ubuntu:~/testgit$ git add sayhi.txt
ourunix@ubuntu:~/testgit$ git commit -m "2011.4.13" sayhi.txt
[master (root-commit) b87b535] 2011.4.13
1 files changed, 1 insertions(+), 0 deletions(-)
create mode 100644 sayhi.txt
5.在本地的git仓库"添加一个远程仓库",当然这个远程仓库还是你自己的这个目录。
ourunix@ubuntu:~/testgit$ git remote add origin ssh://你的用户名@你的IP/~/testgit/.git
这时候,本地的 .git/config 应该会改变
6.将本地的 master分支 ，跟踪到远程的分支
ourunix@ubuntu:~/testgit$ git push origin master
7,显示远程信息
ourunix@ubuntu:~/testgit$git remote show origin
8,利用其他局域网的电脑测试你的仓库
ourunix@ubuntu:~/test$ git clone ssh://你的用户名@你的IP/home/～/testgit/.git
Initialized empty Git repository in /home/wlp/test/git/.git/
xxx‘s password:
remote: Counting objects: 3, done.
Receiving objects: 100% (3/3), done.
remote: Total 3 (delta 0), reused 0 (delta 0)

9，大功告成
10. 修改远程分支地址
git remote set-url origin remote_git_address

http://blog.chinaunix.net/uid-22028680-id-3040436.html
* dpkg
* dpkg,rpm和yum以及apt-get的区别
一般来说著名的 Linux 系统基本上分两大类：
- RedHat 系列：Redhat、Centos、Fedora 等
- Debian 系列：Debian、Ubuntu 等

Dpkg (Debian系)：Ubuntu 
RPM (Red Hat系)：CentOS、Fedora
** RedHat 系列
- 常见的安装包格式 rpm 包，安装rpm包的命令是“rpm -参数”
- 包管理工具 yum
- 支持 tar 包
** Debian系列
- 常见的安装包格式 deb 包，安装 deb 包的命令是“dpkg -参数”
- 包管理工具 apt-get
- 支持 tar 包
* Linux 查看端口占用情况
Linux 查看端口占用情况可以使用 lsof 和 netstat 命令。
** lsof
lsof(list open files)是一个列出当前系统打开文件的工具。

lsof 查看端口占用语法格式：
#+BEGIN_EXAMPLE
lsof -i:端口号
#+END_EXAMPLE

*** 实例
查看服务器 8000 端口的占用情况：
#+begin_src bash
/:lsof -i:8000

COMMAND   PID USER   FD   TYPE   DEVICE SIZE/OFF NODE NAME
nodejs  26993 root   10u  IPv4 37999514      0t0  TCP *:8000 (LISTEN)
#+END_SRC
可以看到 8000 端口已经被轻 nodejs 服务占用。

lsof -i 需要 root 用户的权限来执行，如下图：
#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-09-03_15-42-19.png @ 2021-09-03 16:14:42
[[file:Linux_%E6%9F%A5%E7%9C%8B%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E6%83%85%E5%86%B5/2021-09-03_16-14-42_Snipaste_2021-09-03_15-42-19.png]]
更多 lsof 的命令如下：
#+begin_src bash
lsof -i:8080：查看8080端口占用
lsof abc.txt：显示开启文件abc.txt的进程
lsof -c abc：显示abc进程现在打开的文件
lsof -c -p 1234：列出进程号为1234的进程所打开的文件
lsof -g gid：显示归属gid的进程情况
lsof +d /usr/local/：显示目录下被进程开启的文件
lsof +D /usr/local/：同上，但是会搜索目录下的目录，时间较长
lsof -d 4：显示使用fd为4的进程
lsof -i -U：显示所有打开的端口和UNIX domain文件
#+END_SRC

** netstat
netstat -tunlp 用于显示 tcp，udp 的端口和进程等相关情况。

netstat 查看端口占用语法格式：
#+begin_src bash
netstat -tunlp | grep 端口号
#+END_SRC
-t (tcp) 仅显示tcp相关选项
-u (udp)仅显示udp相关选项
-n 拒绝显示别名，能显示数字的全部转化为数字
-l 仅列出在Listen(监听)的服务状态
-p 显示建立相关链接的程序名

例如查看 8000 端口的情况，使用以下命令：
#+begin_src bash
/:netstat -tunlp | grep 8000
tcp        0      0 0.0.0.0:8000            0.0.0.0:*               LISTEN      26993/nodejs   
#+END_SRC
更多命令：
#+begin_src bash
netstat -ntlp   //查看当前所有tcp端口
netstat -ntulp | grep 80   //查看所有80端口使用情况
netstat -ntulp | grep 3306   //查看所有3306端口使用情况
#+END_SRC
* netstat
Linux netstat 命令用于显示网络状态。

利用 netstat 指令可让你得知整个 Linux 系统的网络情况。

语法：netstat [-acCeFghilMnNoprstuvVwx][-A<网络类型>][--ip]

- -a或--all 显示所有连线中的Socket。
- -A<网络类型>或--<网络类型> 列出该网络类型连线中的相关地址。
- -c或--continuous 持续列出网络状态。
- -C或--cache 显示路由器配置的快取信息。
- -e或--extend 显示网络其他相关信息。
- -F或--fib 显示路由缓存。
- -g或--groups 显示多重广播功能群组组员名单。
- -h或--help 在线帮助。
- -i或--interfaces 显示网络界面信息表单。
- -l或--listening 显示监控中的服务器的Socket。
- -M或--masquerade 显示伪装的网络连线。
- -n或--numeric 直接使用IP地址，而不通过域名服务器。
- -N或--netlink或--symbolic 显示网络硬件外围设备的符号连接名称。
- -o或--timers 显示计时器。
- -p或--programs 显示正在使用Socket的程序识别码和程序名称。
- -r或--route 显示Routing Table。
- -s或--statistics 显示网络工作信息统计表。
- -t或--tcp 显示TCP传输协议的连线状况。
- -u或--udp 显示UDP传输协议的连线状况。
- -v或--verbose 显示指令执行过程。
- -V或--version 显示版本信息。
- -w或--raw 显示RAW传输协议的连线状况。
- -x或--unix 此参数的效果和指定"-A unix"参数相同。
- --ip或--inet 此参数的效果和指定"-A inet"参数相同。

** 实例
*** 显示详细的网络状况
# netstat -a
*** 显示当前户籍UDP连接状况
# netstat -nu
*** 显示UDP端口号的使用情况
# netstat -apu
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address        Foreign Address       State    PID/Program name  
udp    0   0 *:32768           *:*                   -          
udp    0   0 *:nfs            *:*                   -          
udp    0   0 *:641            *:*                   3006/rpc.statd   
udp    0   0 192.168.0.3:netbios-ns   *:*                   3537/nmbd      
udp    0   0 *:netbios-ns        *:*                   3537/nmbd      
udp    0   0 192.168.0.3:netbios-dgm   *:*                   3537/nmbd      
udp    0   0 *:netbios-dgm        *:*                   3537/nmbd      
udp    0   0 *:tftp           *:*                   3346/xinetd     
udp    0   0 *:999            *:*                   3366/rpc.rquotad  
udp    0   0 *:sunrpc          *:*                   2986/portmap    
udp    0   0 *:ipp            *:*                   6938/cupsd     
udp    0   0 *:1022           *:*                   3392/rpc.mountd   
udp    0   0 *:638            *:*                   3006/rpc.statd
*** 显示网卡列表
# netstat -i
Kernel Interface table
Iface    MTU Met  RX-OK RX-ERR RX-DRP RX-OVR  TX-OK TX-ERR TX-DRP TX-OVR Flg
eth0    1500  0  181864   0   0   0  141278   0   0   0 BMRU
lo    16436  0   3362   0   0   0   3362   0   0   0 LRU
*** 显示组播组的关系
# netstat -g
IPv6/IPv4 Group Memberships
Interface    RefCnt Group
--------------- ------ ---------------------
lo       1   ALL-SYSTEMS.MCAST.NET
eth0      1   ALL-SYSTEMS.MCAST.NET
lo       1   ff02::1
eth0      1   ff02::1:ff0a:b0c
eth0      1   ff02::1
*** 显示网络统计信息
# netstat -s
Ip:
  184695 total packets received
  0 forwarded
  0 incoming packets discarded
  184687 incoming packets delivered
  143917 requests sent out
  32 outgoing packets dropped
  30 dropped because of missing route
Icmp:
  676 ICMP messages received
  5 input ICMP message failed.
  ICMP input histogram:
    destination unreachable: 44
    echo requests: 287
    echo replies: 345
  304 ICMP messages sent
  0 ICMP messages failed
  ICMP output histogram:
    destination unreachable: 17
    echo replies: 287
Tcp:
  473 active connections openings
  28 passive connection openings
  4 failed connection attempts
  11 connection resets received
  1 connections established
  178253 segments received
  137936 segments send out
  29 segments retransmited
  0 bad segments received.
  336 resets sent
Udp:
  5714 packets received
  8 packets to unknown port received.
  0 packet receive errors
  5419 packets sent
TcpExt:
  1 resets received for embryonic SYN_RECV sockets
  ArpFilter: 0
  12 TCP sockets finished time wait in fast timer
  572 delayed acks sent
  3 delayed acks further delayed because of locked socket
  13766 packets directly queued to recvmsg prequeue.
  1101482 packets directly received from backlog
  19599861 packets directly received from prequeue
  46860 packets header predicted
  14541 packets header predicted and directly queued to user
  TCPPureAcks: 12259
  TCPHPAcks: 9119
  TCPRenoRecovery: 0
  TCPSackRecovery: 0
  TCPSACKReneging: 0
  TCPFACKReorder: 0
  TCPSACKReorder: 0
  TCPRenoReorder: 0
  TCPTSReorder: 0
  TCPFullUndo: 0
  TCPPartialUndo: 0
  TCPDSACKUndo: 0
  TCPLossUndo: 0
  TCPLoss: 0
  TCPLostRetransmit: 0
  TCPRenoFailures: 0
  TCPSackFailures: 0
  TCPLossFailures: 0
  TCPFastRetrans: 0
  TCPForwardRetrans: 0
  TCPSlowStartRetrans: 0
  TCPTimeouts: 29
  TCPRenoRecoveryFail: 0
  TCPSackRecoveryFail: 0
  TCPSchedulerFailed: 0
  TCPRcvCollapsed: 0
  TCPDSACKOldSent: 0
  TCPDSACKOfoSent: 0
  TCPDSACKRecv: 0
  TCPDSACKOfoRecv: 0
  TCPAbortOnSyn: 0
  TCPAbortOnData: 1
  TCPAbortOnClose: 0
  TCPAbortOnMemory: 0
  TCPAbortOnTimeout: 3
  TCPAbortOnLinger: 0
  TCPAbortFailed: 3
  TCPMemoryPressures: 0
*** 显示监听的套接口
# netstat -l
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address        Foreign Address       State   
tcp    0   0 *:32769           *:*             LISTEN   
tcp    0   0 *:nfs            *:*             LISTEN   
tcp    0   0 *:644            *:*             LISTEN   
tcp    0   0 *:1002           *:*             LISTEN   
tcp    0   0 *:netbios-ssn        *:*             LISTEN   
tcp    0   0 *:sunrpc          *:*             LISTEN   
tcp    0   0 vm-dev:ipp         *:*             LISTEN   
tcp    0   0 *:telnet          *:*             LISTEN   
tcp    0   0 *:601            *:*             LISTEN   
tcp    0   0 *:microsoft-ds       *:*             LISTEN   
tcp    0   0 *:http           *:*             LISTEN   
tcp    0   0 *:ssh            *:*             LISTEN   
tcp    0   0 *:https           *:*             LISTEN   
udp    0   0 *:32768           *:*                   
udp    0   0 *:nfs            *:*                   
udp    0   0 *:641            *:*                   
udp    0   0 192.168.0.3:netbios-ns   *:*                   
udp    0   0 *:netbios-ns        *:*                   
udp    0   0 192.168.0.3:netbios-dgm   *:*                   
udp    0   0 *:netbios-dgm        *:*                   
udp    0   0 *:tftp           *:*                   
udp    0   0 *:999            *:*                   
udp    0   0 *:sunrpc          *:*                   
udp    0   0 *:ipp            *:*                   
udp    0   0 *:1022           *:*                   
udp    0   0 *:638            *:*                   
Active UNIX domain sockets (only servers)
Proto RefCnt Flags    Type    State     I-Node Path
unix 2   [ ACC ]   STREAM   LISTENING   10621 @/tmp/fam-root-
unix 2   [ ACC ]   STREAM   LISTENING   7096  /var/run/acpid.socket
unix 2   [ ACC ]   STREAM   LISTENING   9792  /tmp/.gdm_socket
unix 2   [ ACC ]   STREAM   LISTENING   9927  /tmp/.X11-unix/X0
unix 2   [ ACC ]   STREAM   LISTENING   10489 /tmp/ssh-lbUnUf4552/agent.4552
unix 2   [ ACC ]   STREAM   LISTENING   10558 /tmp/ksocket-root/kdeinit__0
unix 2   [ ACC ]   STREAM   LISTENING   10560 /tmp/ksocket-root/kdeinit-:0
unix 2   [ ACC ]   STREAM   LISTENING   10570 /tmp/.ICE-unix/dcop4664-1270815442
unix 2   [ ACC ]   STREAM   LISTENING   10843 /tmp/.ICE-unix/4735
unix 2   [ ACC ]   STREAM   LISTENING   10591 /tmp/ksocket-root/klauncherah3arc.slave-socket
unix 2   [ ACC ]   STREAM   LISTENING   7763  /var/run/iiim/.iiimp-unix/9010
unix 2   [ ACC ]   STREAM   LISTENING   11047 /tmp/orbit-root/linc-1291-0-1e92c8082411
unix 2   [ ACC ]   STREAM   LISTENING   11053 /tmp/orbit-root/linc-128e-0-dc070659cbb3
unix 2   [ ACC ]   STREAM   LISTENING   8020  /var/run/dbus/system_bus_socket
unix 2   [ ACC ]   STREAM   LISTENING   58927 /tmp/mcop-root/vm-dev-2c28-4beba75f
unix 2   [ ACC ]   STREAM   LISTENING   7860  /tmp/.font-unix/fs7100
unix 2   [ ACC ]   STREAM   LISTENING   7658  /dev/gpmctl
unix 2   [ ACC ]   STREAM   LISTENING   10498 @/tmp/dbus-s2MLJGO5Ci
* vs code的sftp
ps：SFTP目前不能处理中文文件
** SFTP原理
SFTP原理是这样的：首先本地要有一个项目文件夹，同时远程也有一个项目文件夹，然后通过配置文件来同步二者。
SFTP可以查看远程项目所有文件，但不能直接操作，必须操作本地项目文件，再同步到远程项目。

现在我们本地和远程均有一个文件夹“sftpFolder”，用VsCode打开本地文件夹“sftpFolder”，然后执行 ctrl+shift+p ，搜索 SFTP:Config ，回车后，会生成一个“.vscode/sftp.json”，这个就是配置文件。
同时，如下图左侧会多了一个“远程目录”。

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-13_10-46-50.png @ 2021-10-13 10:47:31
[[file:vs_code%E7%9A%84sftp/2021-10-13_10-47-31_Snipaste_2021-10-13_10-46-50.png]]
** SFTP配置

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-13_10-47-59.png @ 2021-10-13 10:48:26
[[file:vs_code%E7%9A%84sftp/2021-10-13_10-48-26_Snipaste_2021-10-13_10-47-59.png]]
配置文件不能写注释，所以这里说明一下其中几个属性：
- uploadOnSave：本地更新文件保存会自动同步到远程文件（不会同步重命名文件和删除文件）
- downloadOnOpen：从远程服务器下载打开的文件
- ignore：忽略的文件（匹配的文件不会同步）
- watcher：监听器（可以重命名文件和删除文件）
    - autoUpload：文件变更会自动同步（修改和重命名）
    - autoDelete：文件删除会自动同步

* rz和sz命令使用
rz命令是方便从windows传文件到Linux，在windows下通过连接工具进入linux系统，cd到自己需要的目录，命令行输入rz，然后回车，之后会弹出一个选择框，选择我们需要上传的文件，然后add，最后上传就好了。

#: rz 

sz命令反过来，是从Linux传输文件到windows，同样Linux下我们需要传的文件所在目录，命令行输入sz，后面跟上需要传输的文件命，可以是一个文件，也可以跟多个文件名，同时传多个文件，然后回车，就可以传文件了。默认情况文件传到windows的用户下载目录下。

#: sz filename1 filename2 filename3

这两个命令传输传输小文件很方便也很快，但是遇到大文件经常需要很久，甚至传了一部分然后中断了，这时就需要nc命令出场了，传输大文件也非常快。

* Linux命令行连接WiFi
1. 安装nmcli
sudo apt-get install nmcli

2. 查看网络设备
sudo nmcli dev

3. 开启wifi
sudo nmcli r wifi on

4. 扫描wifi
sudo nmcli dev wifi

5. 连接wifi
sudo nmcli dev wifi connect "wifi名" password "密码"
