* apt
** 查看已安装软件
apt-get 命令 没有类似列出已安装软件包的简单的选项，但是 apt 有一个这样的命令：
#+begin_src bash
apt list --installed
#+END_SRC
这个会显示使用 apt 命令安装的所有的软件包。同时也会包含由于依赖而被安装的软件包。也就是说不仅会包含你曾经安装的程序，而且会包含大量库文件和间接安装的软件包。

apt 和 apt-get 命令都是基于 dpkg。也就是说用 dpkg 命令可以列出 Debian 系统的所有已经安装的软件包。
#+begin_src bash
dpkg-query -l
#+END_SRC
*** 参考文章
[[https://zhuanlan.zhihu.com/p/57472336][https://zhuanlan.zhihu.com/p/57472336]]
** apt-cache
1.apt-cache showpkg
显示软件包的一些常规信息
例: apt-cache showpkg openssh


2.apt-cache stats
显示相关的统计信息顯示相關的統計資訊

3.apt-cache dump
显示缓存中的每个软件包的简要描述信息

4.apt-cache unmet
显示不符合一致性的依赖关系

5.apt-cache show
显示指定软件包的记录信息。类似于rpm -qi

6.apt-cache search
查找软件包，类似于rpm -qa|grep package_name
例: apt-cache search openssh

7.apt-cache depends
显示软件包的依赖性关系

8.apt-cache pkgnames
列出所有的软件包
** apt-get
-y：yes，在命令行交互提示中，直接输入 yes
**** 修改源
1、原文件备份

sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak

2、编辑源列表文件

sudo vim /etc/apt/sources.list

3、将原来的列表删除，添加如下内容

阿里云源
#+BEGIN_EXAMPLE
deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
#+END_EXAMPLE
清华源 ubuntu18.04
#+BEGIN_EXAMPLE
# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释
# 若出现no longer has a Release file错误，把https该成http即可
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse
#+END_EXAMPLE
清华源 ubuntu 20.04 LTS
#+BEGIN_EXAMPLE
# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse

# 预发布软件源，不建议启用
# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-proposed main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-proposed main restricted universe multiverse
#+END_EXAMPLE
清华源 ubuntu 22.04
#+BEGIN_EXAMPLE
# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse

# 预发布软件源，不建议启用
# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse
#+END_EXAMPLE
4、更新源

更新软件包列表
sudo apt-get update
**** no longer has a Release file错误的解决方法
 把软件源里的https改成http就行了

 今天我sudo apt-get update遇到了一个
 #+BEGIN_SRC bash
 Hit:1 http://packages.microsoft.com/repos/code stable InRelease
 Ign:2 http://mirrors.tuna.tsinghua.edu.cn/ros/ubuntu bionic InRelease          
 Ign:3 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic InRelease             
 Hit:4 http://mirrors.tuna.tsinghua.edu.cn/ros/ubuntu bionic Release            
 Hit:5 http://dl.google.com/linux/chrome/deb stable InRelease                   
 Ign:6 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-updates InRelease     
 Ign:8 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-backports InRelease   
 Ign:9 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-security InRelease    
 Err:10 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic Release              
   Certificate verification failed: The certificate is NOT trusted. The certificate chain uses expired certificate.  Could not handshake: Error in the certificate verification. [IP: 101.6.15.130 443]
 Err:11 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-updates Release  
   Certificate verification failed: The certificate is NOT trusted. The certificate chain uses expired certificate.  Could not handshake: Error in the certificate verification. [IP: 101.6.15.130 443]
 Hit:12 http://archive.ubuntu.com/ubuntu bionic InRelease
 Hit:13 http://ppa.launchpad.net/videolan/master-daily/ubuntu bionic InRelease  
 Err:14 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-backports Release    
   Certificate verification failed: The certificate is NOT trusted. The certificate chain uses expired certificate.  Could not handshake: Error in the certificate verification. [IP: 101.6.15.130 443]
 Err:15 https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-security Release     
   Certificate verification failed: The certificate is NOT trusted. The certificate chain uses expired certificate.  Could not handshake: Error in the certificate verification. [IP: 101.6.15.130 443]
 Reading package lists... Done                                
 E: The repository 'https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic Release' no longer has a Release file.
 N: Updating from such a repository can't be done securely, and is therefore disabled by default.
 N: See apt-secure(8) manpage for repository creation and user configuration details.
 E: The repository 'https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-updates Release' no longer has a Release file.
 N: Updating from such a repository can't be done securely, and is therefore disabled by default.
 N: See apt-secure(8) manpage for repository creation and user configuration details.
 E: The repository 'https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-backports Release' no longer has a Release file.
 N: Updating from such a repository can't be done securely, and is therefore disabled by default.
 N: See apt-secure(8) manpage for repository creation and user configuration details.
 E: The repository 'https://mirrors.tuna.tsinghua.edu.cn/ubuntu bionic-security Release' no longer has a Release file.
 N: Updating from such a repository can't be done securely, and is therefore disabled by default.
 N: See apt-secure(8) manpage for repository creation and user configuration details.
 #+END_SRC
**** Ubuntu换源后，更新提示GPG error缺少公钥
#+BEGIN_EXAMPLE
W: GPG error: http://mirrors.aliyun.com trusty-security InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 40976EAF437D05B5 NO_PUBKEY 3B4FE6ACC0B21F32
W: GPG error: http://mirrors.aliyun.com trusty-updates InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 40976EAF437D05B5 NO_PUBKEY 3B4FE6ACC0B21F32
W: GPG error: http://mirrors.aliyun.com trusty-proposed InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 40976EAF437D05B5 NO_PUBKEY 3B4FE6ACC0B21F32
W: GPG error: http://mirrors.aliyun.com trusty-backports InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 40976EAF437D05B5 NO_PUBKEY 3B4FE6ACC0B21F32
W: GPG error: http://mirrors.aliyun.com trusty Release: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 40976EAF437D05B5 NO_PUBKEY 3B4FE6ACC0B21F32
#+END_EXAMPLE
Solution
#+begin_src bash
apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32
#+END_SRC
**** ubuntu-ports focal 证书过期
解决方法：[[https://github.com/tuna/issues/issues/1342]]、[[https://letsencrypt.org/docs/dst-root-ca-x3-expiration-september-2021/]]

先使用 http 源，然后再执行 apt upgrade 更新 ca-certificates 即可解决。
**** apt-get彻底卸载软件包
apt-get的卸载相关的命令有remove/purge/autoremove/clean/autoclean等。具体来说：

apt-get purge / apt-get --purge remove
删除已安装包（不保留配置文件)。
如软件包a，依赖软件包b，则执行该命令会删除a，而且不保留配置文件

apt-get autoremove
删除为了满足依赖而安装的，但现在不再需要的软件包（包括已安装包），保留配置文件。

apt-get remove
删除已安装的软件包（保留配置文件），不会删除依赖软件包，且保留配置文件。

apt-get autoclean
APT的底层包是dpkg, 而dpkg 安装Package时, 会将 *.deb 放在 /var/cache/apt/archives/中，apt-get autoclean 只会删除 /var/cache/apt/archives/ 已经过期的deb。

apt-get clean
使用 apt-get clean 会将 /var/cache/apt/archives/ 的 所有 deb 删掉，可以理解为 rm /var/cache/apt/archives/*.deb。

那么如何彻底卸载软件呢？

具体来说可以运行如下命令：
#+begin_src bash
# 删除软件及其配置文件
apt-get --purge remove <package>
# 删除没用的依赖包
apt-get autoremove <package>
# 此时dpkg的列表中有“rc”状态的软件包，可以执行如下命令做最后清理：
dpkg -l |grep ^rc|awk '{print $2}' |sudo xargs dpkg -P
#+END_SRC
***** 参考文章
[[https://blog.csdn.net/get_set/article/details/51276609][Ubuntu apt-get彻底卸载软件包]]
** apt-key
apt-key命令用于管理Debian Linux系统中的软件包密钥。

每个发布的deb包，都是通过密钥认证的，apt-key用来管理密钥。

用法：apt-key [--keyring file] [command] [arguments]

参数：
- apt-key add <file>          - 把下载的key添加到本地trusted数据库中
- apt-key del <keyid>         - 从本地trusted数据库通过keyid删除key
- apt-key export <keyid>      - 通过keyid导出key
- apt-key exportall           - 导出本地trusted数据库中的所有key
- apt-key update              - 通过key包来更新key,更新本地trusted数据库，删除过期没用的key
- apt-key net-update          - 通过网络更新key
- apt-key list                - 列出已保存在系统中key
- apt-key finger              - 列出所有验证指纹
- apt-key adv                 - 设置key的高级配置, Pass advanced options to gpg
* Anaconda
Anaconda（官方网站）就是可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。如果日常工作或学习并不必要使用1,000多个库，那么可以考虑安装Miniconda
** Anaconda、conda、pip、virtualenv的区别
*** Anaconda
Anaconda是一个包含180+的科学包及其依赖项的发行版本。其包含的科学包包括：conda, numpy, scipy, ipython notebook等。
*** conda
conda是包及其依赖项和环境的管理工具。

- 适用语言：Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++, FORTRAN。

- 适用平台：Windows, macOS, Linux

- 用途：

(1)快速安装、运行和升级包及其依赖项。

(2)在计算机中便捷地创建、保存、加载和切换环境。

如果你需要的包要求不同版本的Python，你无需切换到不同的环境，因为conda同样是一个环境管理器。仅需要几条命令，你可以创建一个完全独立的环境来运行不同的Python版本，同时继续在你常规的环境中使用你常用的Python版本。——Conda官方网站
▪ conda为Python项目而创造，但可适用于上述的多种语言。

▪ conda包和环境管理器包含于Anaconda的所有版本当中。

** conda
Python的版本比较多，并且它的库也非常广泛，同时库和库之间存在很多依赖关系，所以在库的安装和版本的管理上很麻烦。Conda是一个管理版本和Python环境的工具.
*** 源（channels）管理
**** 显示所有channel
#+BEGIN_SRC bash
conda config --show #显示出所有conda的config信息。
conda config --show channels #只看channels的信息
#+END_SRC
**** 增加源
#+BEGIN_SRC bash
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --set show_channel_urls yes  # 设置搜索时显示通道地址，这样就可以知道包的安装来源了。
#+END_SRC
添加完后，找到 .condarc 文件，删除里面的 defaults，这样能快点。当第一次执行 ~conda config~ 时，会生成配置文件.condarc


1、切换为清华源
#+BEGIN_SRC python
conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/   
conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge 
conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/
conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ 
# 设置搜索时显示通道地址
conda config --set show_channel_urls yes
#+END_SRC
镜像源地址由https改为http可以避免一些安装库时发生的错误
2、切换为中科大源
#+BEGIN_SRC python
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/
conda config --set show_channel_urls yes
#+END_SRC
3、切换回默认源
#+BEGIN_SRC python
conda config --remove-key channels
#+END_SRC
**** 移除镜像
#+BEGIN_SRC bash
conda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/  #这个命令是为了移除之前conda config --show channels显示的清华源。
#+END_SRC
*** Conda的环境管理
**** 创建环境
 #+BEGIN_SRC bash
 #创建一个名为py35的环境，指定Python版本是3.5（不用管是3.5.x，conda会为我们自动寻找3.５.x中的最新版本）
 conda create --name py35 python=3.5
 #+END_SRC

**** 激活环境
 #+BEGIN_SRC bash
 # 安装好后，使用activate激活某个环境
 activate py35 # for Windows
 source activate py35 # for Linux & Mac
 (py35) user@user-XPS-8920:~$
 #激活后，会发现terminal输入的地方多了py35的字样，实际上，此时系统做的事情就是把默认2.7环境从PATH中去除，再把3.4对应的命令加入PATH
 #+END_SRC

**** 返回主环境
 #+BEGIN_SRC bash
 # 如果想返回默认的python 2.7环境，运行
 deactivate py35 # for Windows
 source deactivate py35 # for Linux & Mac
 #+END_SRC

**** 删除环境
 #+BEGIN_SRC bash
 # 删除一个已有的环境
 conda remove --name py35 --all
 #+END_SRC
删除后将目录 anaconda3/envs下的环境文件夹删除
**** 复制（克隆）环境
conda本身的命令里是有移植这个选项的。 

假如前提是，在本地的conda里已经有一个AAA的环境，我想创建一个新环境跟它一模一样的叫BBB，那么这样一句就搞定了：

~conda create -n BBB --clone AAA~

但是如果是跨计算机呢。其实是一样的。

查询conda create命令的原来说明，是这样的：
#+BEGIN_EXAMPLE
–clone ENV 
Path to (or name of) existing local environment.
#+END_EXAMPLE
–clone这个参数后面的不仅可以是环境的名字，也可以是环境的路径。
**** 查看系统中的所有环境
 用户安装的不同Python环境会放在~/anaconda/envs目录下。

 查看当前系统中已经安装了哪些环境，使用:
 #+BEGIN_SRC bash
 conda info -e
 #+END_SRC
*** Conda的包管理
**** 安装库
 为当前环境安装库
 #+BEGIN_SRC bash
 conda install numpy
 # conda会从从远程搜索numpy的相关信息和依赖项目
 #+END_SRC
**** 查看已经安装的库
 #+BEGIN_SRC bash
 # 查看已经安装的packages
 conda list
 # 最新版的conda是从site-packages文件夹中搜索已经安装的包，可以显示出通过各种方式安装的包
 #+END_SRC
**** 查看某个环境的已安装包
 #+BEGIN_SRC bash
 # 查看某个指定环境的已安装包
 conda list -n py35
 #+END_SRC
**** 搜索package的信息
 #+BEGIN_SRC bash
 # 查找package信息
 conda search numpy
 #+END_SRC
**** 安装package到指定的环境
 #+BEGIN_SRC bash
 # 安装package
 conda install -n py35 numpy
 # 如果不用-n指定环境名称，则被安装在当前活跃环境
 # 也可以通过-c指定通过某个channel安装
 #+END_SRC
**** 更新package
 #+BEGIN_SRC bash
 # 更新package
 conda update -n py35 numpy
 #+END_SRC
**** 删除package
#+BEGIN_SRC bash
# 删除package
conda uninstall xxx   #卸载xxx文件包
#+END_SRC
**** 删除没用的包
#+BEGIN_SRC python
conda clean [-h] [-a] [-i] [-p] [-t] [-f]
                   [-c TEMPFILES [TEMPFILES ...]] [-d] [--json] [-q] [-v] [-y]
#+END_SRC
***** Removal Targets
-a, --all
Remove index cache, lock files, unused cache packages, and tarballs.

-i, --index-cache
Remove index cache.

-p, --packages
Remove unused packages from writable package caches. WARNING: This does not check for packages installed using symlinks back to the package cache.

-t, --tarballs
Remove cached package tarballs.

-f, --force-pkgs-dirs
Remove all writable package caches. This option is not included with the --all flag. WARNING: This will break environments with packages installed using symlinks back to the package cache.

-c, --tempfiles
Remove temporary files that could not be deleted earlier due to being in-use. Argument is path(s) to prefix(es) where files should be found and removed.
***** Output, Prompt, and Flow Control Options
-d, --dry-run
Only display what would have been done.

--json
Report all output as json. Suitable for using conda programmatically.

-q, --quiet
Do not display progress bar.

-v, --verbose
Can be used multiple times. Once for INFO, twice for DEBUG, three times for TRACE.

-y, --yes
Do not ask for confirmation.

Examples:

conda clean --tarballs
**** 更新conda
#+BEGIN_SRC bash
# 更新conda，保持conda最新
conda update conda
#+END_SRC
**** 更新anaconda
#+BEGIN_SRC bash
# 更新anaconda
conda update anaconda
#+END_SRC
**** 更新python
 #+BEGIN_SRC bash
 #假设当前环境是python 3.5, conda会将python升级为3.5.x系列的当前最新版本
 conda update python
 #+END_SRC
**** 批量导出、安装库
conda批量导出包含环境中所有组件的requirements.txt文件
#+BEGIN_SRC python
conda list -e > requirements.txt
#+END_SRC
conda批量安装requirements.txt文件中包含的组件依赖
#+BEGIN_SRC python
conda install --yes --file requirements.txt
#+END_SRC

*** 安装requirement.txt指定的依赖包
1. 生成requirement.txt文件
#+begin_src python
pip freeze > requirements.txt
#+END_SRC
安装requirement.txt文件依赖
#+begin_src python
pip install -r requirements.txt
#+END_SRC
2. 除了使用pip命令来生成及安装requirement.txt文件以外，也可以使用conda命令来安装。
#+begin_src python
conda install --yes --file requirements.txt
#+END_SRC
但是这里存在一个问题，如果requirements.txt中的包不可用，则会抛出“无包错误”。

使用下面这个命令可以解决这个问题
#+begin_src bash
$ while read requirement; do conda install --yes $requirement; done < requirements.txt
#+END_SRC
如果想要在conda命令无效时使用pip命令来代替，那么使用如下命令：
#+begin_src bash
$ while read requirement; do conda install --yes $requirement || pip install $requirement; done < requirements.txt
#+END_SRC

**** 参考文章
[[https://blog.csdn.net/Mao_Jonah/article/details/89502380][使用conda安装requirement.txt指定的依赖包]]
** 使用国内镜像源安装pytorch
先设置镜像源，如清华的conda镜像
#+BEGIN_SRC python
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge 
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
conda config --set show_channel_urls yes
#+END_SRC
官方安装的命令是(版本1.6 CPU)：
#+BEGIN_SRC bash
conda install pytorch torchvision cpuonly -c pytorch
#+END_SRC
但要用国内源，我发现不能用-c这一段，直接用
#+BEGIN_SRC bash
conda install pytorch torchvision cpuonly
#+END_SRC

** 卸载Anaconda要点
1. 安装 Anaconda-Clean package
打开 Anaconda Prompt， 输入如下命令：

conda install anaconda-clean

2. 输入如下命令卸载

anaconda-clean --yes

3. 删除整个anaconda目录：
由于Anaconda的安装文件都包含在一个目录中，所以直接将该目录删除即可。到包含整个anaconda目录的文件夹下，删除整个Anaconda目录：
rm -rf anaconda文件夹名

4. 建议——清理下.bashrc中的Anaconda路径：
#+BEGIN_EXAMPLE
1.到根目录下，打开终端并输入：
sudo gedit ~/.bashrc
2.在.bashrc文件末尾用#号注释掉之前添加的路径(或直接删除)：
#export PATH=/home/lq/anaconda3/bin:$PATH
保存并关闭文件
3.使其立即生效，在终端执行：
source ~/.bashrc
#+END_EXAMPLE


5. 关闭终端，然后再重启一个新的终端，这一步很重要，不然在原终端上还是绑定有anaconda.


** 解决 conda install failed: conda.core.subdir_data.Response304ContentUnchanged
问题产生：

在install pkgs时，报错

Collecting package metadata (current_repodata.json): failed

具体为：

conda.core.subdir_data.Response304ContentUnchanged

 

解决：
works for me.
#+BEGIN_SRC bash
conda clean -i
#+END_SRC


清空cache后重新安装

suggestion from Github
#+BEGIN_SRC bash
conda config --remove channels conda-forge
#+END_SRC

疑似 forge 源出了问题
* awk
** 简介
awk是一个强大的文本分析工具，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。

awk有3个不同版本: awk、nawk和gawk，未作特别说明，一般指gawk，gawk 是 AWK 的 GNU 版本。

awk其名称得自于它的创始人 Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母。实际上 AWK 的确拥有自己的语言： AWK 程序设计语言 ， 三位创建者已将它正式定义为“样式扫描和处理语言”。它允许您创建简短的程序，这些程序读取输入文件、为数据排序、处理数据、对输入执行计算以及生成报表，还有无数其他的功能。
** 使用方法
#+begin_src bash
awk '{pattern + action}' {filenames}
#+END_SRC
尽管操作可能会很复杂，但语法总是这样，其中 pattern 表示 AWK 在数据中查找的内容，而 action 是在找到匹配内容时所执行的一系列命令。花括号（{}）不需要在程序中始终出现，但它们用于根据特定的模式对一系列指令进行分组。 pattern就是要表示的正则表达式，用斜杠括起来。

awk语言的最基本功能是在文件或者字符串中基于指定规则浏览和抽取信息，awk抽取信息后，才能进行其他文本操作。完整的awk脚本通常用来格式化文本文件中的信息。

通常，awk是以文件的一行为处理单位的。awk每接收文件的一行，然后执行相应的命令，来处理文本。
** 三种调用方式
1. 命令行方式
dawk [-F  field-separator]  'commands'  input-file(s)

其中，commands 是真正awk命令，[-F域分隔符]是可选的。 input-file(s) 是待处理的文件。

在awk中，文件的每一行中，由域分隔符分开的每一项称为一个域。通常，在不指名-F域分隔符的情况下，默认的域分隔符是空格。

2. shell脚本方式
将所有的awk命令插入一个文件，并使awk程序可执行，然后awk命令解释器作为脚本的首行，一遍通过键入脚本名称来调用。

相当于shell脚本首行的：#!/bin/sh

可以换成：#!/bin/awk

3. 将所有的awk命令插入一个单独文件，然后调用：

awk -f awk-script-file input-file(s)

其中，-f选项加载awk-script-file中的awk脚本，input-file(s)跟上面的是一样的。
* Aria2
** 安装方法
#+begin_src bash
yum install aria2  #CentOS系统
apt-get install aria2  #Debian/Ubuntu系统
#+END_SRC
** 用法
1、直链下载
下载直链文件，只需在命令后附加地址，如：

aria2c http://xx.com/xx
如果需要重命名为yy的话加上--out或者-o参数，如：

aria2c --out=yy http://xx.com/xx
aria2c -o yy http://xx.com/xx
使用aria2的分段和多线程下载功能可以加快文件的下载速度，对于下载大文件时特别有用。-x 分段下载，-s 多线程下载，如：

aria2c -s 2 -x 2 http://xx.com/xx
这将使用2个连接和2个线程来下载该文件。

2、BT下载
种子和磁力下载：

aria2c ‘xxx.torrnet‘
aria2c '磁力链接'
列出种子内容：

aria2c -S xxx.torrent
下载种子内编号为1、4、5、6、7的文件，如：

aria2c --select-file=1,4-7 xxx.torrent
设置bt端口：

aria2c --listen-port=3653 ‘xxx.torrent’
3、限速下载
单个文件最大下载速度：

aria2c --max-download-limit=300K -s10 -x10 'http://xx.com/xx'
整体下载最大速度：

aria2c --max-overall-download-limit=300k -s10 -x10 'http://xx.com/xx'
这些基本都是常用的几个命令，更多的可以使用man aria2c和aria2c -h查看。
* Axel
这是wget的出色替代者，是一款轻量级下载实用工具。它实际上是个加速器，因为它打开了多路http连接，可下载独立文件片段，因而文件下载起来更快速。
* bash脚本
** 判断表达式
*** 文件判断
以下表达式用来判断文件状态。
- [ -a file ]：如果 file 存在，则为true。
- [ -b file ]：如果 file 存在并且是一个块（设备）文件，则为true。
- [ -c file ]：如果 file 存在并且是一个字符（设备）文件，则为true。
- [ -d file ]：如果 file 存在并且是一个目录，则为true。
- [ -e file ]：如果 file 存在，则为true。
- [ -f file ]：如果 file 存在并且是一个普通文件，则为true。
- [ -g file ]：如果 file 存在并且设置了组 ID，则为true。
- [ -G file ]：如果 file 存在并且属于有效的组 ID，则为true。
- [ -h file ]：如果 file 存在并且是符号链接，则为true。
- [ -k file ]：如果 file 存在并且设置了它的“sticky bit”，则为true。
- [ -L file ]：如果 file 存在并且是一个符号链接，则为true。
- [ -N file ]：如果 file 存在并且自上次读取后已被修改，则为true。
- [ -O file ]：如果 file 存在并且属于有效的用户 ID，则为true。
- [ -p file ]：如果 file 存在并且是一个命名管道，则为true。
- [ -r file ]：如果 file 存在并且可读（当前用户有可读权限），则为true。
- [ -s file ]：如果 file 存在且其长度大于零，则为true。
- [ -S file ]：如果 file 存在且是一个网络 socket，则为true。
- [ -t fd ]：如果 fd 是一个文件描述符，并且重定向到终端，则为true。 这可以用来判断是否重定向了标准输入／输出／错误。
- [ -u file ]：如果 file 存在并且设置了 setuid 位，则为true。
- [ -w file ]：如果 file 存在并且可写（当前用户拥有可写权限），则为true。
- [ -x file ]：如果 file 存在并且可执行（有效用户有执行／搜索权限），则为true。
- [ file1 -nt file2 ]：如果 FILE1 比 FILE2 的更新时间最近，或者 FILE1 存在而 FILE2 不存在，则为true。
- [ file1 -ot file2 ]：如果 FILE1 比 FILE2 的更新时间更旧，或者 FILE2 存在而 FILE1 不存在，则为true。
- [ FILE1 -ef FILE2 ]：如果 FILE1 和 FILE2 引用相同的设备和 inode 编号，则为true。
*** 字符串判断
以下表达式用来判断字符串。
- [ string ]：如果string不为空（长度大于0），则判断为真。
- [ -n string ]：如果字符串string的长度大于零，则判断为真。
- [ -z string ]：如果字符串string的长度为零，则判断为真。
- [ string1 = string2 ]：如果string1和string2相同，则判断为真。
- [ string1 == string2 ] 等同于[ string1 = string2 ]。
- [ string1 != string2 ]：如果string1和string2不相同，则判断为真。
- [ string1 '>' string2 ]：如果按照字典顺序string1排列在string2之后，则判断为真。
- [ string1 '<' string2 ]：如果按照字典顺序string1排列在string2之前，则判断为真。

注意，test命令内部的>和<，必须用引号引起来（或者是用反斜杠转义）。否则，它们会被 shell 解释为重定向操作符。
*** 整数判断
下面的表达式用于判断整数。
- [ integer1 -eq integer2 ]：如果integer1等于integer2，则为true。
- [ integer1 -ne integer2 ]：如果integer1不等于integer2，则为true。
- [ integer1 -le integer2 ]：如果integer1小于或等于integer2，则为true。
- [ integer1 -lt integer2 ]：如果integer1小于integer2，则为true。
- [ integer1 -ge integer2 ]：如果integer1大于或等于integer2，则为true。
- [ integer1 -gt integer2 ]：如果integer1大于integer2，则为true。
*** 逻辑运算
通过逻辑运算，可以把多个test判断表达式结合起来，创造更复杂的判断。三种逻辑运算AND，OR，和NOT，都有自己的专用符号。
- AND运算：符号&&，也可使用参数-a。
- OR运算：符号||，也可使用参数-o。
- NOT运算：符号!。

* cuda
** 如何查看显卡支持的CUDA版本
1. 在开始中找到并打开NVIDIA控制面板，如下图所示。
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/201702141648462427.jpg @ 2020-08-11 23:06:13
[[file:cuda/2020-08-11_23-06-13_201702141648462427.jpg]]


2. 打开NVIDIA控制面板，如下图所示。选择“系统信息”--“组件”，找到NVCUDA.DLL信息显示即为显卡支持的CUDA最高版本。

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170214164846247.jpg @ 2020-08-11 23:01:36
[[file:cuda/2020-08-11_23-01-36_20170214164846247.jpg]]

3. 在编译caffe时，若显卡的计算能力比较低的话，需要修改caffe-master\windows下的CommonSettings.props的属性表，现有的gpu计算能力参数：有compute_20,sm_20;compute_30,sm_30;compute_35,sm_35;compute_50,sm_50;compute_52,sm_52

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170214174642686.jpg @ 2020-08-11 23:07:19
[[file:cuda/2020-08-11_23-07-19_20170214174642686.jpg]]

** 关于pytorch、cuda、cudnn、显卡驱动之间的关系
pytorch本质上只是一个深度学习框架，帮我们封装好了数据加载、损失函数、优化器、网络等信息，但是他调用GPU进行加速还需要cuda驱动，这里也经常遇到一个名字叫cudnn，而显卡驱动与cuda以及cudnn也不是一个东西，下面是大致的介绍：

英伟达显卡驱动：
英伟达显卡驱动是为我们提供显示功能的，我们点亮屏幕，让游戏画面能够很流畅需要它，它和我们其他几个名词没有关系，因为剩下的几个名词都是我们进行更高级的GPU功能准备的。

Cuda：
Cuda驱动可以在官网下载，它是NVIDIA推出的用于自家GPU的并行计算框架，也就是说CUDA只能在NVIDIA的GPU上运行，而且只有当要解决的计算问题是可以大量并行计算的时候才能发挥CUDA的作用，而我们需要进行的深度学习跑神经网络的时候需要使用使用。

cuDNN：
cuDNN是一个专门用于神经网络的加速包，注意，它跟我们的CUDA没有完全一一对应的关系，即每一个版本的CUDA可能有好几个版本的cuDNN与之对应，但一般是有大致对应关系的（见后图）。

pytorch（torch）框架：
pytorch是一个深度学习框架，封装好了很多网络和深度学习相关的工具方便我们调用，而不用我们一个个去单独写了，他分为CPU和GPU版本，其他框架还有TensorFlow，Caffe等。

那么这几个概念就很清晰了，显卡驱动复杂给我们进行屏幕显示、游戏的画质渲染，而我们在调用pytorch的时候实际上是调用cuDNN这个加速包，这个包调用cuda驱动这个并行加速框架来调用GPU的流处理器进行计算的。

在网上可以查到cuda与pytorch版本的大致对应关系：[[https://pytorch.org/get-started/previous-versions/][INSTALLING PREVIOUS VERSIONS OF PYTORCH]]

关于cudnn可以直接在官网搜索搜索cuda对应的cudnn版本进行安装，官网链接如下：[[https://developer.nvidia.com/rdp/cudnn-archive#a-collapse742-10][cuDNN Archive]]

** 查看 CUDA 和 cuDNN 版本的方法
*** Linux
**** 查看 CUDA 版本
方法一：
nvcc --version

或

nvcc -V

如果 nvcc 没有安装，那么用方法二。

方法二：

去安装目录下查看：
#+begin_src bash
cat /usr/local/cuda/version.txt
#+END_SRC
**** 查看 cuDNN 版本
#+begin_src bash
cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2
#+END_SRC
如果没有，那么可能没有安装 cuDNN。

如果是为了使用 PyTorch/TensorFlow，在 Linux 服务器上推荐使用 conda 安装，使用 conda 可以很方便安装 PyTorch/TensorFlow 以及对应版本的 CUDA 和 cuDNN。
*** Windows
**** 查看 CUDA 版本
在命令行中执行：

nvcc --version

或者进入 CUDA 的安装目录查看：

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA
**** 查看 cuDNN 版本
进入 CUDA 的安装目录查看文件 cudnn.h ：（注意修改v9.0）

C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\include\cudnn.h

如下所示，cuDNN 版本为 7.2.1 :

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_14-43-33.png @ 2021-11-22 14:43:40
[[file:cuda/2021-11-22_14-43-40_Snipaste_2021-11-22_14-43-33.png]]

如果不知道安装路径，或者安装了多个版本的 CUDA，可以去环境变量内查看 CUDA_PATH 或 path。
*** 使用 PyTorch 查看 CUDA 和 cuDNN 版本
#+begin_src python
import torch
print(torch.__version__)

print(torch.version.cuda)
print(torch.backends.cudnn.version())
#+END_SRC
*** 参考文章
[[https://www.cnblogs.com/wuliytTaotao/p/11453265.html#%25E6%259F%25A5%25E7%259C%258B-cudnn-%25E7%2589%2588%25E6%259C%25AC][Linux 和 Windows 查看 CUDA 和 cuDNN 版本]]
** 升级cuda
安装cuda时需要重启机器，所以我放弃升级了。
这个教程没有完全写完。
#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-24_21-09-38.png @ 2021-11-24 21:10:03
[[file:cuda/2021-11-24_21-10-03_Snipaste_2021-11-24_21-09-38.png]]

先在NVIDIA官网找到对应版本的cuda，执行命令
#+begin_src bash
wget http://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda_11.0.2_450.51.05_linux.run
sudo sh cuda_11.0.2_450.51.05_linux.run
#+END_SRC
安装过程中全选，收到报错提示，查看cuda安装log
#+begin_src bash
cat /var/log/cuda-installer.log
#+END_SRC
提示：
#+BEGIN_EXAMPLE
[INFO]: Driver installation detected by command: apt list --installed | grep -e nvidia-driver-[0-9][0-9][0-9] -e nvidia-[0-9][0-9][0-9]
[INFO]: Cleaning up window
[INFO]: Complete
[INFO]: Checking compiler version...
[INFO]: gcc location: /usr/bin/gcc
[INFO]: gcc version: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)
[INFO]: Initializing menu
[INFO]: Setup complete
[INFO]: Components to install:
[INFO]: Driver
[INFO]: 450.51.05
[INFO]: Executing NVIDIA-Linux-x86_64-450.51.05.run --ui=none --no-questions --accept-license --disable-nouveau --no-cc-version-check --install-libglvnd  2>&1
[INFO]: Finished with code: 256
[ERROR]: Install of driver component failed.
[ERROR]: Install of 450.51.05 failed, quitting
#+END_EXAMPLE
查看nvidia-installer.log
#+begin_src bash
cat /var/log/nvidia-installer.log
#+END_SRC
#+BEGIN_EXAMPLE
nvidia-installer log file '/var/log/nvidia-installer.log'
creation time: Wed Nov 24 13:06:21 2021
installer version: 465.19.01

PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

nvidia-installer command line:
    ./nvidia-installer
    --ui=none
    --no-questions
    --accept-license
    --disable-nouveau
    --no-cc-version-check
    --install-libglvnd

Using built-in stream user interface
-> Detected 12 CPUs online; setting concurrency level to 12.
ERROR: Unable to find the module utility `modprobe`; please make sure you have the package 'module-init-tools' or 'kmod' installed.  If you do have 'module-init-tools' or 'kmod' installed, then please check that `modprobe` is in your PATH.
#+END_EXAMPLE
按提示安装module-init-tools、kmod
#+begin_src bash
apt-get install module-init-tools kmod
#+END_SRC
* cp命令
Linux cp（英文全拼：copy file）命令主要用于复制文件或目录。

语法
#+BEGIN_EXAMPLE
cp [options] source dest
或
cp [options] source... directory
#+END_EXAMPLE

参数说明：
- -a：此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容。其作用等于dpR参数组合。
- -d：复制时保留链接。这里所说的链接相当于 Windows 系统中的快捷方式。
- -f：覆盖已经存在的目标文件而不给出提示。
- -i：与 -f 选项相反，在覆盖目标文件之前给出提示，要求用户确认是否覆盖，回答 y 时目标文件将被覆盖。
- -p：除复制文件的内容外，还把修改时间和访问权限也复制到新文件中。
- -r：若给出的源文件是一个目录文件，此时将复制该目录下所有的子目录和文件。
- -l：不复制文件，只是生成链接文件。
* chsh命令
Linux chsh命令用于更改使用者 shell 设定。

使用权限：所有使用者。

语法:shell>> chsh

通过 -s 参数改变当前的shell设置
** 实例
#+begin_src bash
shell>> chsh
Changing fihanging shell for user1
Password: [del]
New shell [/bin/tcsh]: ### [是目前使用的 shell]
[del]
shell>> chsh -l ### 展示 /etc/shells 档案内容
/bin/bash
/bin/sh
/bin/ash
/bin/bsh
/bin/tcsh
/bin/csh
#+END_SRC
改变当前的shell。当前的shell 设置为/bin/bash，通过chsh命令，改变shell的设置/bin/csh。
#+begin_src bash
$ chsh
Changing shell for root.
New shell [/bin/bash]: /bin/csh //输入新的shell地址
Shell changed.
#+END_SRC

通过 -s 参数改变当前的shell设置
#+begin_src bash
$ chsh -s /bin/csh //改变当前设置为 /bin/csh
Changing shell for root.
Shell not changed.
#+END_SRC
* cmake
** 简介
什么是cmake

你或许听过好几种 Make 工具，例如 GNU Make ，QT 的 qmake ，微软的 MSnmake，BSD Make（pmake），Makepp，等等。这些 Make 工具遵循着不同的规范和标准，所执行的 Makefile 格式也千差万别。这样就带来了一个严峻的问题：如果软件想跨平台，必须要保证能够在不同平台编译。而如果使用上面的 Make 工具，就得为每一种标准写一次 Makefile ，这将是一件让人抓狂的工作。

CMake就是针对上面问题所设计的工具：它首先允许开发者编写一种平台无关的 CMakeList.txt 文件来定制整个编译流程，然后再根据目标用户的平台进一步生成所需的本地化 Makefile 和工程文件，如 Unix 的 Makefile 或 Windows 的 Visual Studio 工程。从而做到“Write once, run everywhere”。显然，CMake 是一个比上述几种 make 更高级的编译配置工具。一些使用 CMake 作为项目架构系统的知名开源项目有 VTK、ITK、KDE、OpenCV、OSG 等。

在 linux 平台下使用 CMake 生成 Makefile 并编译的流程如下：
- 编写 CMake 配置文件 CMakeLists.txt 。
- 执行命令 cmake PATH 或者 ccmake PATH 生成 Makefile。其中， PATH 是 CMakeLists.txt 所在的目录。（ccmake 和 cmake 的区别在于前者提供了一个交互式的界面）
- 使用 make 命令进行编译。
** 入门案例
*** 单个源文件
本节对应的源代码所在目录：Demo1。

对于简单的项目，只需要写几行代码就可以了。例如，假设现在我们的项目中只有一个源文件 main.cc ，该程序的用途是计算一个数的指数幂。
#+BEGIN_SRC c++
#include <stdio.h>
#include <stdlib.h>
/**
 * power - Calculate the power of number.
 * @param base: Base value.
 * @param exponent: Exponent value.
 *
 * @return base raised to the power exponent.
 */
double power(double base, int exponent)
{
  int result = base;
  int i;
  
  if (exponent == 0) {
    return 1;
  }
  
  for(i = 1; i < exponent; ++i){
    result = result * base;
  }
  return result;
}
int main(int argc, char *argv[])
{
  if (argc < 3){
    printf("Usage: %s base exponent \n", argv[0]);
    return 1;
  }
  double base = atof(argv[1]);
  int exponent = atoi(argv[2]);
  double result = power(base, exponent);
  printf("%g ^ %d is %g\n", base, exponent, result);
  return 0;
}

#+END_SRC

首先编写 CMakeLists.txt 文件，并保存在与 main.cc 源文件同个目录下：
#+BEGIN_SRC c++
# CMake 最低版本号要求
cmake_minimum_required(VERSION 2.8)
# 项目信息
project(Demo1)
# 指定生成目标
add_executable(Demo main.cc)
#+END_SRC
CMakeLists.txt 的语法比较简单，由命令、注释和空格组成，其中命令是不区分大小写的。符号 # 后面的内容被认为是注释。命令由命令名称、小括号和参数组成，参数之间使用空格进行间隔。

对于上面的 CMakeLists.txt 文件，依次出现了几个命令：
- cmake_minimum_required：指定运行此配置文件所需的 CMake 的最低版本；
- project：参数值是 Demo1，该命令表示项目的名称是 Demo1 。
- add_executable： 将名为 main.cc 的源文件编译成一个名称为 Demo 的可执行文件。

编译项目:之后，在当前目录执行 cmake . ，得到 Makefile 后再使用 make 命令编译得到 Demo1 可执行文件。
#+BEGIN_SRC c++
[ehome@xman Demo1]$ cmake .
-- The C compiler identification is GNU 4.8.2
-- The CXX compiler identification is GNU 4.8.2
-- Check for working C compiler: /usr/sbin/cc
-- Check for working C compiler: /usr/sbin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/sbin/c++
-- Check for working CXX compiler: /usr/sbin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ehome/Documents/programming/C/power/Demo1
[ehome@xman Demo1]$ make
Scanning dependencies of target Demo
[100%] Building C object CMakeFiles/Demo.dir/main.cc.o
Linking C executable Demo
[100%] Built target Demo
[ehome@xman Demo1]$ ./Demo 5 4
5 ^ 4 is 625
[ehome@xman Demo1]$ ./Demo 7 3
7 ^ 3 is 343
[ehome@xman Demo1]$ ./Demo 2 10
2 ^ 10 is 1024

#+END_SRC
*** 多个源文件
同一目录，多个源文件

本小节对应的源代码所在目录：Demo2。

上面的例子只有单个源文件。现在假如把 power 函数单独写进一个名为MathFunctions.c 的源文件里，使得这个工程变成如下的形式：
#+BEGIN_EXAMPLE
./Demo2
    |
    +--- main.cc
    |
    +--- MathFunctions.cc
    |
    +--- MathFunctions.h
#+END_EXAMPLE
这个时候，CMakeLists.txt 可以改成如下的形式：
#+BEGIN_SRC c++
# CMake 最低版本号要求
cmake_minimum_required(VERSION 2.8)
# 项目信息
project(Demo2)
# 指定生成目标
add_executable(Demo main.cc MathFunctions.cc)
#+END_SRC
唯一的改动只是在 add_executable 命令中增加了一个 MathFunctions.cc 源文件。这样写当然没什么问题，但是如果源文件很多，把所有源文件的名字都加进去将是一件烦人的工作。更省事的方法是使用 aux_source_directory 命令，该命令会查找指定目录下的所有源文件，然后将结果存进指定变量名。其语法如下：
#+BEGIN_SRC c++
aux_source_directory(<dir> <variable>)
#+END_SRC
因此，可以修改 CMakeLists.txt 如下：
#+BEGIN_SRC c++
# CMake 最低版本号要求
cmake_minimum_required(VERSION 2.8)
# 项目信息
project (Demo2)
# 查找当前目录下的所有源文件
# 并将名称保存到 DIR_SRCS 变量
aux_source_directory(. DIR_SRCS)
# 指定生成目标
add_executable(Demo ${DIR_SRCS})
#+END_SRC
这样，CMake 会将当前目录所有源文件的文件名赋值给变量 DIR_SRCS ，再指示变量 DIR_SRCS 中的源文件需要编译成一个名称为 Demo 的可执行文件。
*** 多个目录，多个源文件
本小节对应的源代码所在目录：Demo3。

现在进一步将 MathFunctions.h 和 MathFunctions.cc 文件移动到 math 目录下。
#+BEGIN_EXAMPLE
./Demo3
    |
    +--- main.cc
    |
    +--- math/
          |
          +--- MathFunctions.cc
          |
          +--- MathFunctions.h
#+END_EXAMPLE
对于这种情况，需要分别在项目根目录 Demo3 和 math 目录里各编写一个 CMakeLists.txt 文件。为了方便，我们可以先将 math 目录里的文件编译成静态库再由 main 函数调用。

根目录中的 CMakeLists.txt ：
#+BEGIN_SRC c++
# CMake 最低版本号要求
cmake_minimum_required(VERSION 2.8)
# 项目信息
project(Demo3)
# 查找当前目录下的所有源文件
# 并将名称保存到 DIR_SRCS 变量
aux_source_directory(. DIR_SRCS)
# 添加 math 子目录
add_subdirectory(math)
# 指定生成目标
add_executable(Demo main.cc)
# 添加链接库
target_link_libraries(Demo MathFunctions)
#+END_SRC
该文件添加了下面的内容: 第3行，使用命令 add_subdirectory 指明本项目包含一个子目录 math，这样 math 目录下的 CMakeLists.txt 文件和源代码也会被处理 。第6行，使用命令 target_link_libraries 指明可执行文件 main 需要连接一个名为 MathFunctions 的链接库 。

子目录中的 CMakeLists.txt：
#+BEGIN_SRC c++
# 查找当前目录下的所有源文件
# 并将名称保存到 DIR_LIB_SRCS 变量
aux_source_directory(. DIR_LIB_SRCS)
# 生成链接库
add_library (MathFunctions ${DIR_LIB_SRCS})
#+END_SRC
在该文件中使用命令 add_library 将 src 目录中的源文件编译为静态链接库。
*** 自定义编译选项
本节对应的源代码所在目录：Demo4。

CMake 允许为项目增加编译选项，从而可以根据用户的环境和需求选择最合适的编译方案。

例如，可以将 MathFunctions 库设为一个可选的库，如果该选项为 ON ，就使用该库定义的数学函数来进行运算。否则就调用标准库中的数学函数库。

修改 CMakeLists 文件：我们要做的第一步是在顶层的 CMakeLists.txt 文件中添加该选项： 
#+BEGIN_SRC c++
# CMake 最低版本号要求
cmake_minimum_required(VERSION 2.8)
# 项目信息
project(Demo4)
# 加入一个配置头文件，用于处理 CMake 对源码的设置
configure_file(
"${PROJECT_SOURCE_DIR}/config.h.in"
"${PROJECT_BINARY_DIR}/config.h"
)
# 是否使用自己的 MathFunctions 库
option(USE_MYMATH
"Use provided math implementation" ON)
# 是否加入 MathFunctions 库
if (USE_MYMATH)
 include_directories("${PROJECT_SOURCE_DIR}/math")
add_subdirectory(math)
set (EXTRA_LIBS ${EXTRA_LIBS} MathFunctions)
endif (USE_MYMATH)
# 查找当前目录下的所有源文件
# 并将名称保存到 DIR_SRCS 变量
aux_source_directory(. DIR_SRCS)
# 指定生成目标
add_executable(Demo ${DIR_SRCS})
target_link_libraries (Demo ${EXTRA_LIBS})
#+END_SRC
其中：

第7行的 configure_file 命令用于加入一个配置头文件 config.h ，这个文件由 CMake 从 config.h.in 生成，通过这样的机制，将可以通过预定义一些参数和变量来控制代码的生成。

第13行的 option 命令添加了一个 USE_MYMATH 选项，并且默认值为 ON 。

第17行根据 USE_MYMATH 变量的值来决定是否使用我们自己编写的 MathFunctions 库。
** 语法的基本原则
- 变量使用${}方式取值，但是在 IF 控制语句中是直接使用变量名
- 指令(参数 1 参数 2...) 参数使用括弧括起，参数之间使用空格或分号分开。 
- 指令是大小写无关的，参数和变量是大小写相关的。推荐全部使用大写指令

** PROJECT
可以用来指定工程的名字和支持的语言，默认支持所有语言

- PROJECT (HELLO)   指定了工程的名字，并且支持所有语言（建议）
- PROJECT (HELLO CXX)      指定了工程的名字，并且支持语言是C++
- PROJECT (HELLO C CXX)      指定了工程的名字，并且支持语言是C和C++

该指定隐式定义了两个CMAKE的变量
- <projectname>_BINARY_DIR，例如： HELLO_BINARY_DIR
- <projectname>_SOURCE_DIR，例如： HELLO_SOURCE_DIR

MESSAGE关键字就可以直接使用者两个变量，当前都指向当前的工作目录。
** set
set 有三种，分别为: 
- 设置一般变量(Set Normal Variable) 
- 设置缓存变量(Set Cache Entry)
- 设置环境变量(Set Environment Variable)
#+BEGIN_SRC c++
// 1. 设置一般变量(Set Normal Variable)
set(<variable> <value>... [PARENT_SCOPE])

// 2. 设置缓存变量(Set Cache Entry)
set(<variable> <value>... CACHE <type> <docstring> [FORCE])

// 3. 设置环境变量(Set Environment Variable)
set(ENV{<variable>} [<value>])
#+END_SRC

注意：SET(SRC_LIST main.cpp) 可以写成 SET(SRC_LIST “main.cpp”)，
如果源文件名中含有空格（例如 m ain.cpp），就必须要加双引号
*** 设置一般变量set(<variable> <value>... [PARENT_SCOPE])
#+BEGIN_SRC bash
set(<variable> <value>... [PARENT_SCOPE])
#+END_SRC
将一个或者多个值 <value>... 赋值给变量 <variable>， 多个值之间以分号（；）分隔。
**** 参数
- PARENT_SCOPE，配置该选项后，表示该变量在父级作用域上有效, 在当前作用域上是无效的;
**** 例子
目录结构如下:
#+BEGIN_EXAMPLE
rs:cmake-set$ tree
.
├── CMakeLists.txt
├── main.cpp
└── subdir
    └── CMakeLists.txt
#+END_EXAMPLE
顶层的　CMakeLists.txt
#+BEGIN_SRC c++
cmake_minimum_required(VERSION 3.5)

project(DEMO VERSION 10.2.1.3 LANGUAGES CXX C ASM )

# 设置一般变量
set(PNAME rsenjoyer)
set(PGRADE 80 85 90 95)

message("top name = ${PNAME}")  # top name = rsenjoyer
message("top grade = ${PGRADE}") #top grade = 80;85;90;95

add_subdirectory(subdir)

message("top after name = ${PNAME}")  # top name = rsenjoyer

add_executable(DEMO main.cpp)
#+END_SRC
子目录的 CMakeLists.txt
#+BEGIN_SRC c++
set(PNAME jack)
message("sub name = ${PNAME}") # sub name = jack

# 仅仅会改变父级的　PNAME，　对当前的变量不会更改
set(PNAME rose PARENT_SCOPE)
message("sub name = ${PNAME}") #sub name = jack
#+END_SRC
*** 设置缓存变量(Set Cache Entry)
什么是缓存变量，缓存变量可以理解为当第一次运行cmake时，这些变量缓存到一份文件中(即编译目录下的CMakeCache.txt)。当再次运行cmake时，这些变量会直接使用缓存值，可以利用ccmake或者cmake-gui等工具来重新赋值。缓存变量在整个cmake运行过程中都可以起作用。

当使用CACHE时，且缓存(cache)中没有该变量时，变量被创建并存入缓存(cache)中，如果原缓存(cache)中有该变量，也不会改变原缓存中该变量的值，除非后面使用FORCE。
#+BEGIN_SRC c++
set(<variable> <value>... CACHE <type> <docstring> [FORCE])
#+END_SRC
作用:
- 设置变量并缓存到 CMakeCache.txt
- 默认不会覆盖已缓存(已存在于 CMakeCache.txt )的变量；
**** 参数
- 类型 type,必须为以下的一种
    - BOOL,布尔值(ON/OFF)
    - FILEPATH,文件路径
    - PATH,目录路径
    - STRING,字符串
    - INTERNAL,单行文字.INTERNAL将变量为内部变量，即cmake-gui不会向用户显示这类变量，而其它类型的缓存变量用户都可以通cmake-gui按照特定的类型改变。
- 描述字符串 <docstring>: 单行文字,用于 CMAKE-GUI 的时提示用户
- FORCE 用于是否强制更新缓存里面的值，配置后，每次都会强制更新　CMakeCache.txt 里面的值
**** 例子
#+BEGIN_SRC c++
set(FOO, "x" CACHE <type>)  
//原缓存中没有FOO则将FOO赋值为x且存入cache中。
//原缓存中有FOO则不做任何改变，即便原cache中FOO存的不是x。
set(FOO, "x" CACHE <type><docstring> FORCE) 　　　
//即便原cache中存在FOO也会创建另一个FOO，官方文档原话(If FORCE is specified, the value of the cache variable 
//is set, even if the variable is already in the cache.This should normally be avoided, as it will 
//remove any changes to the cache variable’s value by the user.)，小弟笨拙没有搞懂。
#+END_SRC
**** 注意
1. CACHE与PARENT_SCOPE不能一起使用。
2. 同一名称(例FOO)的一般变量和缓存变量可以同时存在，但在调用该变量时(${FOO})会在先取一般变量的值，一般变量中没有再取缓存变量的值。
一些栗子：
#+BEGIN_EXAMPLE
set(FOO “x”)　　　　　　　　//设置一般变量FOO，不会触及cache，但是会隐藏cache中的FOO。

set(FOO “x” CACHE ...)　　//忽视相同名称的一般变量，在cache中检查FOO是否存在，
#+END_EXAMPLE
3. 当改变cache中的变量时，同名的一般变量会被删除。一般不建议使用相同名称的一般变量和缓存变量。

然而在有些工程中可以很好的借助这一交互例如：

一个工程利用ADD_SUBDIRECTOTY()添加子工程，子工程有它自己的CMakeList.txt。如果在父工程和子工程中都对同一缓存变量赋值，cmake时父工程率先将变量存入cache中，子工程直接在cache中调用该值，保证了父子工程的一致性。当父工程需要改变该变量，而子程序需要利用原值时，可以直接在父工程中设置同名称的一般变量即可。

*** 设置环境变量(Set Environment Variable)
#+BEGIN_SRC c++
set(ENV{<variable>} [<value>])
#+END_SRC
作用
- 设置环境变量 <variable>,值为 <value>
- 如果 <value> 不存在或者为空字符串 表示清除该环境变量
** SET_TARGET_PROPERTIES
这条指令可以用来设置输出的名称，对于动态库，还可以用来指定动态库版本和 API 版本。
cmake 在构建一个新的target 时，会尝试清理掉其他使用这个名字的库。

SET_TARGET_PROPERTIES(hello_static PROPERTIES  OUTPUT_NAME "hello")

** ADD_EXECUTABLE
生成可执行文件

#+begin_src bash
ADD_EXECUTABLE(hello ${SRC_LIST})     
#+END_SRC
生成的可执行文件名是hello，源文件读取变量SRC_LIST中的内容

也可以直接写 ADD_EXECUTABLE(hello main.cpp) 。
后缀可以不行，他会自动去找.c和.cpp，最好不要这样写，可能会有这两个文件main.cpp和main

** ADD_SUBDIRECTORY
#+begin_src bash
ADD_SUBDIRECTORY(source_dir [binary_dir] [EXCLUDE_FROM_ALL])
#+END_SRC
这个指令用于向当前工程添加存放源文件的子目录，并可以指定中间二进制和目标二进制存放的位置。

每个目录下都要有一个CMakeLists.txt
#+begin_src bash
[root@localhost cmake]# tree
.
├── build
├── CMakeLists.txt
└── src
    ├── CMakeLists.txt
    └── main.cpp
#+END_SRC
外层CMakeLists.txt：
#+begin_src bash
PROJECT(HELLO)
ADD_SUBDIRECTORY(src)
#+END_SRC
src下的CMakeLists.txt：
#+begin_src bash
ADD_EXECUTABLE(hello main.cpp)
#+END_SRC
ADD_SUBDIRECTORY用于将外层的CMakeLists.txt与src目录下的CMakeLists.txt关联起来。

下面的命令将 src 子目录加入工程并指定编译输出(包含编译中间结果)路径为bin 目录
#+begin_src bash
ADD_SUBDIRECTORY(src bin)
#+END_SRC
如果不进行 bin 目录的指定，那么编译结果(包括中间结果)都将存放在build/src 目录

** ADD_LIBRARY
ADD_LIBRARY(hello SHARED ${LIBHELLO_SRC})

- hello：就是正常的库名，生成的名字前面会加上lib，最终产生的文件是libhello.so
- SHARED，动态库    STATIC，静态库
- ${LIBHELLO_SRC} ：源文件

*** 同时生成动态库与静态库
在cmake中，同时生成静态库与动态库的时候，需要一些技巧，因为cmake中不能生成同名的静态库和动态库，只能先生成不同名的库之后再把其中一个库的名字改名。
#+begin_src bash
# 生成动态库目标
add_library(MathFunctions SHARED ${srcs})
# 生成静态库目标
add_library(MathFunctions_static STATIC ${srcs})
 
# 指定静态库的输出名称
set_target_properties(MathFunctions_static PROPERTIES OUTPUT_NAME "MathFunctions")
# 使动态库和静态库同时存在
set_target_properties(MathFunctions PROPERTIES CLEAN_DIRECT_OUTPUT 1)
set_target_properties(MathFunctions_static PROPERTIES CLEAN_DIRECT_OUTPUT 1)
 
# 指定动态库版本
# VERSION 动态库版本
# SOVERSION API版本
set_target_properties(person PROPERTIES VERSION 1.0 SOVERSION 1)
 
# 将动态库与动态库同时安装到lib目录中
install (TARGETS MathFunctions MathFunctions_static DESTINATION lib)
#+END_SRC
CLEAN_DIRECT_OUTPUT 部分用于指示在生成具有相同名字的（OUTPUT_NAME）的目标时，是否清理上次生成的内容。由于这时的动态和静态库都使用 hello 这个名字，因此需要设置此不见标志。
** 变量
*** 变量的作用域
- Function Scope: 在函数内部定义,仅仅在当前函数以及所调用的子函数内有效;
- Directory Scope: 在当前目录的定义的变量,当调用子目录时候,子目录会复制一份父级目录内的变量到子目录中
- Persistent Cache: 持久化的缓存,一般由CACHE 存储起来.
*** 变量的搜索路径
- 在当前 Function Scope 调用内查找,找到后使用,未找到进行下一步;
- 在当前目录下面查找,找到使用,未找到下一步;
- 在 CACHE 中寻找,找到使用,未找到,则为空.
*** Variables for Languages
**** CMAKE_COMPILER_IS_GNUCXX
True if the C++ (CXX) compiler is GNU. Use CMAKE_CXX_COMPILER_ID instead
*** Variables that Change Behavior.
**** CMAKE_MODULE_PATH
这个变量用来定义自己的 cmake 模块所在的路径。如果你的工程比较复杂,有可能会自己编写一些 cmake 模块,这些 cmake 模块是随你的工程发布的,为了让 cmake 在处理CMakeLists.txt 时找到这些模块,你需要通过 SET 指令,将自己的 cmake 模块路径设置一下。
比如 
#+BEGIN_SRC c++
SET(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)
#+END_SRC
这时候你就可以通过 INCLUDE 指令来调用自己的模块了。
*** Variables that Provide Information
**** CMAKE_SOURCE_DIR
#+BEGIN_EXAMPLE
The path to the top level of the source tree.

This is the full path to the top level of the current CMake source tree. For an in-source build, this would be the same as CMAKE_BINARY_DIR.

When run in -P script mode, CMake sets the variables CMAKE_BINARY_DIR, CMAKE_SOURCE_DIR, CMAKE_CURRENT_BINARY_DIR and CMAKE_CURRENT_SOURCE_DIR to the current working directory.
#+END_EXAMPLE
PROJECT_SOURCE_DIR,<projectname>_SOURCE_DIR

这三个变量指代的内容是一致的,不论采用何种编译方式,都是工程顶层目录。
也就是在 in source 编译时,他跟 CMAKE_BINARY_DIR 等变量一致。
PROJECT_SOURCE_DIR 跟其他指令稍有区别,现在,你可以理解为他们是一致的。

** STREQUAL
STREQUAL 用于比较字符串，相同返回 true 。
** file命令
#+BEGIN_SRC c++
file(GLOB <variable>
     [LIST_DIRECTORIES true|false] [RELATIVE <path>] [CONFIGURE_DEPENDS]
     [<globbing-expressions>...])
file(GLOB_RECURSE <variable> [FOLLOW_SYMLINKS]
     [LIST_DIRECTORIES true|false] [RELATIVE <path>] [CONFIGURE_DEPENDS]
     [<globbing-expressions>...])
#+END_SRC
Generate a list of files that match the <globbing-expressions> and store it into the <variable>. Globbing expressions are similar to regular expressions, but much simpler. If RELATIVE flag is specified, the results will be returned as relative paths to the given path. The results will be ordered lexicographically.

On Windows and macOS, globbing is case-insensitive even if the underlying filesystem is case-sensitive (both filenames and globbing expressions are converted to lowercase before matching). On other platforms, globbing is case-sensitive.

If the CONFIGURE_DEPENDS flag is specified, CMake will add logic to the main build system check target to rerun the flagged GLOB commands at build time. If any of the outputs change, CMake will regenerate the build system.

产生一个匹配 <globbing-expressions> 的文件列表并将它存储到变量 <variable> 中。文件名替代表达式和正则表达式相似，但更简单。如果 RELATIVE 标志位被设定，将返回指定路径的相对路径。结果将按字典顺序排序。

如果 CONFIGURE_DEPENDS 标志位被指定，CMake 将在编译时给主构建系统添加逻辑来检查目标，以重新运行 GLOB 标志的命令。如果任何输出被改变，CMake都将重新生成这个构建系统。
** include指令
[[https://blog.csdn.net/qq_38410730/article/details/102677143][参考文章]]
*** include指令
include指令一般用于语句的复用，也就是说，如果有一些语句需要在很多CMakeLists.txt文件中使用，为避免重复编写，可以将其写在.cmake文件中，然后在需要的CMakeLists.txt文件中进行include操作就行了。

include指令的结构为：
#+BEGIN_EXAMPLE
include(<file|module> [OPTIONAL] [RESULT_VARIABLE <var>]
                      [NO_POLICY_SCOPE])
#+END_EXAMPLE
虽然，有不少的可选参数，但是一般情况下，都是直接写：
#+BEGIN_EXAMPLE
include(file|module)
#+END_EXAMPLE

注意，为了使CMakeLists.txt能够找到该文件，需要指定文件完整路径(绝对路径或相对路径)，当然如果指定了CMAKE_MODULE_PATH，就可以直接include该目录下的.cmake文件了。

.cmake文件里面通常是什么信息呢？

.cmake文件里包含了一些cmake命令和一些宏/函数，当CMakeLists.txt包含该.cmake文件时，当编译运行时，该.cmake里的一些命令就会在该包含处得到执行，并且在包含以后的地方能够调用该.cmake里的一些宏和函数。

什么是宏？什么是函数？
*** 宏和函数的定义
先看一下关键字：cmake的宏是MACRO，函数是function。它们的用法是：
#+BEGIN_SRC c++
macro(<name> [arg1 [arg2 [arg3 ...]]])
  COMMAND1(ARGS ...)            # 命令语句
  COMMAND2(ARGS ...)
  ...
endmacro()

function(<name> [arg1 [arg2 [arg3 ...]]])
  COMMAND1(ARGS ...)            # 命令语句
  COMMAND2(ARGS ...)
  ...
function()
#+END_SRC
定义一个名称为name的宏（函数），arg1...是传入的参数。我们除了可以用${arg1}来引用变量以外，系统为我们提供了一些特殊的变量：

| 变量  | 说明                                                 |
|-------+------------------------------------------------------|
| argv# | #是一个下标，0指向第一个参数，累加                   |
| argv  | 所有的定义时要求传入的参数                           |
| argn  | 定义时要求传入的参数以外的参数                       |
| argc  | 传入的实际参数的个数，也就是调用函数是传入的参数个数 |

*** 宏和函数的区别
那么宏和函数之间的区别是什么呢？

其实和C/C++里面宏和函数之间的区别差不多，宏就是字符串替换，函数就是使用变量，在命令中途可以对改变量进行修改。

以StackOverflow的例子来了解一下区别：

首先创建一个CMakeLists.txt：
#+BEGIN_EXAMPLE
cmake_minimum_required(VERSION 3.0)
include(test.cmake)
#+END_EXAMPLE
在同目录下创建文件test.cmake：
#+BEGIN_EXAMPLE
set(var "ABC")

macro(Moo arg)
  message("arg = ${arg}")
  set(arg "abc")
  message("# After change the value of arg.")
  message("arg = ${arg}")
endmacro()
message("=== Call macro ===")
Moo(${var})

function(Foo arg)
  message("arg = ${arg}")
  set(arg "abc")
  message("# After change the value of arg.")
  message("arg = ${arg}")
endfunction()
message("=== Call function ===")
Foo(${var})
#+END_EXAMPLE
运行cmake：
#+BEGIN_SRC bash
mkdir build && cd build
cmake ..
#+END_SRC
运行后的输出结果是：
#+BEGIN_EXAMPLE
-- The C compiler identification is GNU 5.4.0
-- The CXX compiler identification is GNU 5.4.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
=== Call macro ===
arg = ABC
# After change the value of arg.
arg = ABC
=== Call function ===
arg = ABC
# After change the value of arg.
arg = abc
-- Configuring done
-- Generating done
-- Build files have been written to: /home/yngzmiao/test/build
#+END_EXAMPLE
从这里可以看出，宏实现的仅仅是字符串替换，宏定义的过程中是无法进行修改的，而函数却是可以的。

*** 宏和函数参数的差异
一般情况下，从上面的例子就能看出宏和函数的用法了，但很多情况下，我们自以为的“懂了”都是假懂。比如一不小心，就会出错。

更换test.cmake为下面的内容，并运行：
#+BEGIN_EXAMPLE
set(var "ABC")

macro(Moo arg)
  message("arg = ${arg}")
  set(arg "abc")
  message("# After change the value of arg.")
  message("arg = ${arg}")
endmacro()
message("=== Call macro ===")
Moo(var)

function(Foo arg)
  message("arg = ${arg}")
  set(arg "abc")
  message("# After change the value of arg.")
  message("arg = ${arg}")
endfunction()
message("=== Call function ===")
Foo(var)
#+END_EXAMPLE
运行后的输出结果是：
#+BEGIN_EXAMPLE
-- The C compiler identification is GNU 5.4.0
-- The CXX compiler identification is GNU 5.4.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
=== Call macro ===
arg = var
# After change the value of arg.
arg = var
=== Call function ===
arg = var
# After change the value of arg.
arg = abc
-- Configuring done
-- Generating done
-- Build files have been written to: /home/yngzmiao/test/build
#+END_EXAMPLE
对比两段程序可以看出其中的区别：无论是宏还是函数，当调用的时候如果使用的是set出来的变量，都必须通过${}将变量的内容传递进去，而不能只写上变量名。

这是将实参传递给形参时的注意点，但在宏和函数的实现过程中，还有需要注意的内容。

例子：
#+BEGIN_EXAMPLE
set(var "ABC")

macro(Moo arg)
  if (arg STREQUAL "ABC")
    message("arg1 = ${arg}")
  endif()
  if (${arg} STREQUAL "ABC")
    message("arg2 = ${arg}")
  endif()
endmacro()
message("=== Call macro ===")
Moo(${var})

function(Foo arg)
  if (arg STREQUAL "ABC")
    message("arg1 = ${arg}")
  endif()
  if (${arg} STREQUAL "ABC")
    message("arg2 = ${arg}")
  endif()
endfunction()
message("=== Call function ===")
Foo(${var})
#+END_EXAMPLE
运行后的输出结果是：
#+BEGIN_EXAMPLE
-- The C compiler identification is GNU 5.4.0
-- The CXX compiler identification is GNU 5.4.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
=== Call macro ===
arg2 = ABC
=== Call function ===
arg1 = ABC
arg2 = ABC
-- Configuring done
-- Generating done
-- Build files have been written to: /home/yngzmiao/test/build
#+END_EXAMPLE
可以看出，在宏和函数的实现过程中，宏的参数由于不是传统意义上的变量，而是字符串替换，因此需要通过${}取出内容。而函数却不一定需要这样。

也就是说，对于macro宏而言：
#+BEGIN_EXAMPLE
if(argv0)                         # 错误用法
if(${argv0})                      # 正确用法
if(defined argv0)                 # 错误用法
if(defined ${argv0})              # 正确用法
#+END_EXAMPLE
也就是说，对于宏和函数的参数而言：
- 当宏和函数调用的时候，如果传递的是经set设置的变量，必须通过${}取出内容；
- 在宏的定义过程中，对变量进行的操作必须通过${}取出内容，而函数就没有这个必要。
** install
install 用于将项目生成的库文件、头文件、可执行文件或相关文件等安装到指定位置（系统目录，或发行包目录）。

INSTALL的安装可以包括：二进制、动态库、静态库以及文件、目录、脚本等
#+begin_src bash
install(TARGETS <target>... [...])
install(IMPORTED_RUNTIME_ARTIFACTS <target>... [...])
install({FILES | PROGRAMS} <file>... [...])
install(DIRECTORY <dir>... [...])
install(SCRIPT <file> [...])
install(CODE <code> [...])
install(EXPORT <export-name> [...])
install(RUNTIME_DEPENDENCY_SET <set-name> [...])
#+END_SRC
DESTINATION：
1. 写绝对路径
2. 可以写相对路径，相对路径实际路径是：${CMAKE_INSTALL_PREFIX}/<DESTINATION 定义的路径>

CMAKE_INSTALL_PREFIX  默认是在 /usr/local/

cmake -DCMAKE_INSTALL_PREFIX=/usr    在cmake的时候指定CMAKE_INSTALL_PREFIX变量的路径

注意：abc 和 abc/有很大的区别
- 目录名不以/结尾：这个目录将被安装为目标路径下的
- 目录名以/结尾：将这个目录中的内容安装到目标路径
** cmake命令选项

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-05-01_15-36-45.png @ 2021-05-01 15:36:52
[[file:cmake/2021-05-01_15-36-52_Snipaste_2021-05-01_15-36-45.png]]

** message
向终端输出用户自定义的信息

主要包含三种信息：
- SEND_ERROR，产生错误，生成过程被跳过。
- SATUS，输出前缀为—的信息。
- FATAL_ERROR，立即终止所有 cmake 过程.
*** General messages
#+BEGIN_SRC c++
message([<mode>] "message text" ...)
#+END_SRC
Record the specified message text in the log. If more than one message string is given, they are concatenated into a single message with no separator between the strings.

The optional <mode> keyword determines the type of message, which influences the way the message is handled:

The CMake command-line tool displays STATUS to TRACE messages on stdout with the message preceded by two hyphens and a space. All other message types are sent to stderr and are not prefixed with hyphens. The CMake GUI displays all messages in its log area. The curses interface shows STATUS to TRACE messages one at a time on a status line and other messages in an interactive pop-up box. The --log-level command-line option to each of these tools can be used to control which messages will be shown.

New in version 3.17: To make a log level persist between CMake runs, the CMAKE_MESSAGE_LOG_LEVEL variable can be set instead. Note that the command line option takes precedence over the cache variable.

New in version 3.16: Messages of log levels NOTICE and below will have each line preceded by the content of the CMAKE_MESSAGE_INDENT variable (converted to a single string by concatenating its list items). For STATUS to TRACE messages, this indenting content will be inserted after the hyphens.

New in version 3.17: Messages of log levels NOTICE and below can also have each line preceded with context of the form [some.context.example]. The content between the square brackets is obtained by converting the CMAKE_MESSAGE_CONTEXT list variable to a dot-separated string. The message context will always appear before any indenting content but after any automatically added leading hyphens. By default, message context is not shown, it has to be explicitly enabled by giving the cmake --log-context command-line option or by setting the CMAKE_MESSAGE_CONTEXT_SHOW variable to true. See the CMAKE_MESSAGE_CONTEXT documentation for usage examples.

CMake Warning and Error message text displays using a simple markup language. Non-indented text is formatted in line-wrapped paragraphs delimited by newlines. Indented text is considered pre-formatted.
**** mode
- FATAL_ERROR: CMake Error, stop processing and generation.立即终止所有cmake过程。
- SEND_ERROR: CMake Error, continue processing, but skip generation.
- WARNING: CMake Warning, continue processing.
- AUTHOR_WARNING: CMake Warning (dev), continue processing.
- DEPRECATION: CMake Deprecation Error or Warning if variable CMAKE_ERROR_DEPRECATED or CMAKE_WARN_DEPRECATED is enabled, respectively, else no message.
- (none) or NOTICE: Important message printed to stderr to attract user's attention.
- STATUS： The main interesting messages that project users might be interested in. Ideally these should be concise, no more than a single line, but still informative.产生带前缀 - 的信息。
- VERBOSE： Detailed informational messages intended for project users. These messages should provide additional details that won't be of interest in most cases, but which may be useful to those building the project when they want deeper insight into what's happening.
- DEBUG： Detailed informational messages intended for developers working on the project itself as opposed to users who just want to build it. These messages will not typically be of interest to other users building the project and will often be closely related to internal implementation details.
- TRACE： Fine-grained messages with very low-level implementation details. Messages using this log level would normally only be temporary and would expect to be removed before releasing the project, packaging up the files, etc.

*** Reporting checks
A common pattern in CMake output is a message indicating the start of some sort of check, followed by another message reporting the result of that check. For example:
#+BEGIN_SRC c++
message(STATUS "Looking for someheader.h")
#... do the checks, set checkSuccess with the result
if(checkSuccess)
  message(STATUS "Looking for someheader.h - found")
else()
  message(STATUS "Looking for someheader.h - not found")
endif()
#+END_SRC
This can be more robustly and conveniently expressed using the CHECK_... keyword form of the message() command:
#+BEGIN_SRC c++
message(<checkState> "message" ...)where <checkState> must be one of the following:
#+END_SRC
- CHECK_START:Record a concise message about the check about to be performed.
- CHECK_PASS:Record a successful result for a check.
- CHECK_FAIL:Record an unsuccessful result for a check.

When recording a check result, the command repeats the message from the most recently started check for which no result has yet been reported, then some separator characters and then the message text provided after the CHECK_PASS or CHECK_FAIL keyword. Check messages are always reported at STATUS log level.

Checks may be nested and every CHECK_START should have exactly one matching CHECK_PASS or CHECK_FAIL. The CMAKE_MESSAGE_INDENT variable can also be used to add indenting to nested checks if desired. For example:
#+BEGIN_SRC c++
message(CHECK_START "Finding my things")
list(APPEND CMAKE_MESSAGE_INDENT "  ")
unset(missingComponents)

message(CHECK_START "Finding partA")
# ... do check, assume we find A
message(CHECK_PASS "found")

message(CHECK_START "Finding partB")
# ... do check, assume we don't find B
list(APPEND missingComponents B)
message(CHECK_FAIL "not found")

list(POP_BACK CMAKE_MESSAGE_INDENT)
if(missingComponents)
  message(CHECK_FAIL "missing components: ${missingComponents}")
else()
  message(CHECK_PASS "all components found")
endif()
#+END_SRC
Output from the above would appear something like the following:
#+BEGIN_EXAMPLE
-- Finding my things
--   Finding partA
--   Finding partA - found
--   Finding partB
--   Finding partB - not found
-- Finding my things - missing components: B
#+END_EXAMPLE
** include
include 用于加载makefile文件

-include 表示当文件不存在时，make不会报错
*** 参考文章
[[https://www.gnu.org/software/make/manual/html_node/Include.html][3.3 Including Other Makefiles]]
[[https://blog.csdn.net/xiaozhi_su/article/details/4202779][Makefile中指示符“include”、“-include”和“sinclude”的区别]]
[[https://stackoverflow.com/questions/16981464/difference-between-include-and-include-in-a-makefile][Difference between "include" and "-include" in a makefile]]
** Makefile选项CFLAGS,LDFLAGS,LIBS
CFLAGS 表示用于 C 编译器的选项，
CXXFLAGS 表示用于 C++ 编译器的选项。
这两个变量实际上涵盖了编译和汇编两个步骤。

CFLAGS： 指定头文件（.h文件）的路径，如：CFLAGS=-I/usr/include -I/path/include。同样地，安装一个包时会在安装路径下建立一个include目录，当安装过程中出现问题时，试着把以前安装的包的include目录加入到该变量中来。

LDFLAGS：gcc 等编译器会用到的一些优化参数，也可以在里面指定库文件的位置。用法：LDFLAGS=-L/usr/lib -L/path/to/your/lib。每安装一个包都几乎一定的会在安装目录里建立一个lib目录。如果明明安装了某个包，而安装另一个包时，它愣是说找不到，可以抒那个包的lib路径加入的LDFALGS中试一下。

LIBS：告诉链接器要链接哪些库文件，如LIBS = -lpthread -liconv

简单地说，LDFLAGS是告诉链接器从哪里寻找库文件，而LIBS是告诉链接器要链接哪些库文件。不过使用时链接阶段这两个参数都会加上，所以你即使将这两个的值互换，也没有问题。

有时候LDFLAGS指定-L虽然能让链接器找到库进行链接，但是运行时链接器却找不到这个库，如果要让软件运行时库文件的路径也得到扩展，那么我们需要增加这两个库给"-Wl,R"：

LDFLAGS = -L/var/xxx/lib -L/opt/mysql/lib -Wl,R/var/xxx/lib -Wl,R/opt/mysql/lib

如果在执行./configure以前设置环境变量export LDFLAGS="-L/var/xxx/lib -L/opt/mysql/lib -Wl,R/var/xxx/lib -Wl,R/opt/mysql/lib" ，注意设置环境变量等号两边不可以有空格，而且要加上引号（shell的用法）。那么执行configure以后，Makefile将会设置这个选项，链接时会有这个参数，编译出来的可执行程序的库文件搜索路径就得到扩展了。
*** 参考文章
[[https://www.cnblogs.com/taskiller/archive/2012/12/14/2817650.html][Makefile选项CFLAGS,LDFLAGS,LIBS - Taskiller - 博客园]]
** =和:=的区别
如果使用=，make会将整个makefile展开后，再决定变量的值。也就是说，变量的值将会是整个makefile中最后被指定的值。看例子：
#+begin_src bash
x = foo
y = $(x) bar
x = xyz
#+END_SRC

在上例中，y的值将会是 xyz bar ，而不是 foo bar 。
#+begin_src bash
x := foo
y := $(x) bar
x := xyz
#+END_SRC
而:=表示变量在赋值时就先设置为该值，而不是整个makefile展开后的最终值，后面如果再有赋值，则会将前面的赋值覆盖，使用新的值。

在上例中，y的值将会是 foo bar ，而不是 xyz bar 了。

#+BEGIN_EXAMPLE
Simple assignment :=
A simple assignment expression is evaluated only once, at the very first occurrence. For example, if CC :=${GCC} ${FLAGS} during the first encounter is evaluated to gcc -W then each time ${CC} occurs it will be replaced with gcc -W.

Recursive assignment =
A Recursive assignment expression is evaluated everytime the variable is encountered in the code. For example, a statement like CC = ${GCC} {FLAGS} will be evaluated only when an action like ${CC} file.c is executed. However, if the variable GCC is reassigned i.e GCC=c++ then the ${CC} will be converted to c++ -W after the reassignment.

Conditional assignment ?=
Conditional assignment assigns a value to a variable only if it does not have a value

Appending +=
Assume that CC = gcc then the appending operator is used like CC += -w
then CC now has the value gcc -W
#+END_EXAMPLE
For more check out these [[https://github.com/amjadmajid/Makefile][tutorials]]
*** 参考文章
[[https://www.cnblogs.com/wanqieddy/archive/2011/09/21/2184257.html][Makefile 中:= ?= += =的区别- wanqi - 博客园]]
[[https://www.sunxidong.com/237.html][Makefile中“=”、“:=”、“?=”、“+=”的区别 - 孙希栋的博客]]
[[https://stackoverflow.com/questions/4879592/whats-the-difference-between-and-in-makefile][What's the difference between := and = in Makefile?]]
** 外部构建方式举例
外部构建会把生成的临时文件放在build目录下，不会对源文件有任何影响。
强烈建议使用外部构建方式

1. 建立一个build目录，可以在任何地方，建议在当前目录下

2. 进入build，运行cmake ..    当然..表示上一级目录，你可以写CMakeLists.txt所在的绝对路径，生产的文件都在build目录下了

3. 在build目录下，运行make来构建工程

注意外部构建的两个变量
1. <projectname>_BINARY_DIR，编译路径 也就是 /root/cmake/bulid
2. <projectname>_SOURCE_DIR，还是工程路径

* declare命令
Linux declare命令用于声明 shell 变量。

declare [-aAfFgilnrtux] [-p] [name[=value] ...]


** 参数说明
#+BEGIN_EXAMPLE
-f 将操作或显示限制为函数名及函数定义。
-F 只显示函数名（调试时附加行号和源文件）。
-g 在shell函数中使用时创建全局变量；其他情况下忽略。
-p 显示每个名称的属性和值。

*设置属性的选项:
-a 创建数组（如果支持）。
-A 创建关联数组（如果支持）。
-i 增加整型属性。
+i 删除整型属性。
-l 增加小写属性，变量的值将转换为小写。
+l 删除小写属性。
-n 增加引用属性（如果该选项存在）。
+n 删除引用属性（如果该选项存在）。
-r 增加只读属性。
-t 增加追踪属性。
+t 删除追踪属性。
-u 增加大写属性，变量的值将转换为大写。
+u 删除大写属性。
-x 增加导出属性。
+x 删除导出属性。
#+END_EXAMPLE

** 例子
#+begin_src bash
# 声明变量
declare reference_website='https://wangchujiang.com/linux-command/'

# 显示所有包含整型属性的变量和值。
declare -i
# 定义变量b并赋值为3，具有整型属性。
declare -i b=5
# 显示属性，返回 declare -i b="5"。
declare -p b
# 删除整型属性。
declare +i b
# 显示属性，返回 declare -- b="5"。
declare -p b
# 根据变量属性强制转换值的英文大小写。
declare -u uc_var='abc'
declare -l lc_var='ABC'
# 显示'ABC abc';
echo "${uc_var} ${lc_var}"
# 定义函数内的全局变量
function test(){
  declare -g a=3
  # 或者
  local -g b=3
  # 或者
  c=3
  # 让我们查看它们的属性。
  declare -p a b c
}
# 执行函数。
test
# 返回结果。
# declare -- a="3"
# declare -- b="3"
# declare -- c="3"

# 定义函数外的全局变量
declare a=3
b=3
declare –p a b
# 返回结果如下。
# declare -- a="3"
# declare -- b="3"

# 定义局部变量
function test2(){
  local -i a=3
  declare -i b=3
}
test2
# 没有该变量（已经被销毁了）
echo "${a} ${b}"
# 因此，我们日常脚本中最常见的类似于'a=3'实际上是声明并赋值了一个全局变量。
# 在接下来的 **讨论** 环节会延伸讨论全局和局部变量问题。
# 注意，不能使用 `+a` 或 `+A` 取消数组，也不能使用 `+r` 取消只读属性。

# 定义只读数组，设置属性的同时定义赋值。
declare -ar season=('Spring' 'Summer' 'Autumn' 'Winter')
# 或者这样。
season=('Spring' 'Summer' 'Autumn' 'Winter')
declare -ar season
# 显示所有数组。
declare -a
# 定义关联数组。

declare -A fruits=(['apple']='red' ['banana']='yellow')
# 显示所有关联数组。
declare -A
# 显示所有变量的属性和值并显示函数的定义，输出很长。
declare
# 显示所有变量的属性和值。
declare -p
# 显示所有全局变量的属性和值。
declare -g
# 显示全部函数名和函数定义。
declare -f
# 只显示全部函数名。
declare -F

# 定义两个函数。
function func_a(){ echo $(date +"%F %T"); }
function func_b(){ cd /; ls -lh --sort=time; }
# 显示一到多个函数名和函数定义。
declare -f func_a func_b
# 只显示一到多个函数名，验证某个名称是否已经定义为函数时有用。
declare -F func_a func_b
# 最好不要让函数名和变量名相同。
#+END_SRC
* df 查看磁盘空间
df -hl：查看磁盘剩余空间

df -h：查看每个根路径的分区大小

du -sh [目录名]：返回该目录的大小

du -sm [文件夹]：返回该文件夹总M数

du -h [目录名]：查看指定文件夹下的所有文件大小（包含子文件夹）
* DISPLAY环境变量的作用
在Linux/Unix 类操作系统上, DISPLAY用来设置将图形显示到何处. 
直接登陆图形界面或者登陆命令行界面后使用startx启动图形, DISPLAY环境变量将自动设置为:0.0, 此时可以打开终端, 输出图形程序的名称(比如xclock)来启动程序, 图形将显示在本地窗口上

在终端上输入printenv查看当前环境变量, 输出结果中有如下内容: DISPLAY=:0.0

使用xdpyinfo可以查看到当前显示的更详细的信息。

DISPLAY 环境变量格式如下host:NumA.NumB, 
- host指Xserver所在的主机主机名或者ip地址, 图形将显示在这一机器上, 可以是启动了图形界面的Linux/Unix机器, 也可以是安装了Exceed, - X-Deep/32等Windows平台运行的Xserver的Windows机器. 如果Host为空, 则表示Xserver运行于本机, 并且图形程序(Xclient)使用unix socket方式连接到Xserver, 而不是TCP方式. 
- 使用TCP方式连接时, NumA为连接的端口减去6000的值, 如果NumA为0, 则表示连接到6000端口; 使用unix socket方式连接时则表示连接的unix socket的路径, 如果为0, 则表示连接到/tmp/.X11-unix/X0 . 
- NumB则几乎总是0.

Xserver默认情况下不允许别的用户的图形程序的图形显示在当前屏幕上. 

如果使用su username或者su - username切换到别的用户, 并且使用命令
#+begin_src bash
export DISPLAY=:0.0
#+END_SRC
设置DISPLAY环境变量, 运行图形程序(如xclock)时会收到如下错误:
#+BEGIN_EXAMPLE
Xlib: connection to ":0.0" refused by server Xlib: No protocol specified Error: Can't open display: :0.0
#+END_EXAMPLE
如果需要别的用户的图形显示在当前屏幕上, 则应以当前登陆的用户, 也就是切换身份前的用户执行如下命令：
#+begin_src bash
xhost +
#+END_SRC
这个命令将允许别的用户启动的图形程序将图形显示在当前屏幕上。

* dos2unix和unix2dos
dos2unix是将Windows格式文件转换为Unix、Linux格式的实用命令。Windows格式文件的换行符为\r\n ,而Unix&Linux文件的换行符为\n. dos2unix命令其实就是将文件中的\r\n 转换为\n。

而unix2dos则是和dos2unix互为孪生的一个命令，它是将Linux&Unix格式文件转换为Windows格式文件的命令。

命令语法：
#+begin_src bash
dos2unix [options] [-c convmode] [-o file ...] [-n infile outfile ...]
unix2dos [options] [-c convmode] [-o file ...] [-n infile outfile ...]
#+END_SRC
dos2unix 可以一次转换多个文件:
dos2unix filename1 filename2 filename3
** 命令参数
此命令参数是Red Hat Enterprise Linux Server release 5.7下dos2unix命令参数，不同版本Linux的dos2nnix命令参数有可能不同。


参数 描叙
-h 显示命令dos2unix联机帮助信息。
-k 保持文件时间戳不变
-q 静默模式，不输出转换结果信息等
-V 显示命令版本信息
-c 转换模式
-o 在源文件转换，默认参数
-n 保留原本的旧档，将转换后的内容输出到新档案.默认都会直接在原来的文件上修改，如果需要保留源文件，那么可以使用参数-n,格式为dos2unix -n oldfilename newfilename
** 示例
将Windows格式文本转换为Unix&Linux格式文件
#+begin_src bash
[root@DB-Server myscript]# cat -v test.sh 
. /home/oracle/.bash_profile^M
echo ' '^M
date^M
echo ' '^M
^M
sqlplus test/test @/home/oracle/scripts/test.sql^M
^M
echo ' '^M
date^M
echo ' '^M
[root@DB-Server myscript]# dos2unix test.sh 
dos2unix: converting file test.sh to UNIX format ...
[root@DB-Server myscript]# cat -v test.sh 
. /home/oracle/.bash_profile
echo ' '
date
echo ' '
 
sqlplus test/test @/home/oracle/scripts/test.sql
 
echo ' '
date
echo ' '
#+END_SRC

* dpkg
* dpkg,rpm和yum以及apt-get的区别
一般来说著名的 Linux 系统基本上分两大类：
- RedHat 系列：Redhat、Centos、Fedora 等
- Debian 系列：Debian、Ubuntu 等

Dpkg (Debian系)：Ubuntu 
RPM (Red Hat系)：CentOS、Fedora
** RedHat 系列
- 常见的安装包格式 rpm 包，安装rpm包的命令是“rpm -参数”
- 包管理工具 yum
- 支持 tar 包
** Debian系列
- 常见的安装包格式 deb 包，安装 deb 包的命令是“dpkg -参数”
- 包管理工具 apt-get
- 支持 tar 包
* du命令：查看文件夹和文件的磁盘占用情况
du 命令，全称是 disk usage，用来展示磁盘使用量的统计信息。

语法:
#+begin_src bash
du [-abcDhHklmsSx][-L <符号连接>][-X <文件>][--block-size][--exclude=<目录或文件>][--max-depth=<目录层数>][--help][--version][目录或文件]
#+END_SRC
** 参数
- -a或-all 显示目录中个别文件的大小。
- -b或-bytes 显示目录或文件大小时，以byte为单位。
- -c或--total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和。
- -D或--dereference-args 显示指定符号连接的源文件大小。
- -h或--human-readable 以K，M，G为单位，提高信息的可读性。
- -H或--si 与-h参数相同，但是K，M，G是以1000为换算单位。
- -k或--kilobytes 以1024 bytes为单位。
- -l或--count-links 重复计算硬件连接的文件。
- -L<符号连接>或--dereference<符号连接> 显示选项中所指定符号连接的源文件大小。
- -m或--megabytes 以1MB为单位。
- -s或--summarize 仅显示总计。
- -S或--separate-dirs 显示个别目录的大小时，并不含其子目录的大小。
- -x或--one-file-xystem 以一开始处理时的文件系统为准，若遇上其它不同的文件系统目录则略过。
- -X<文件>或--exclude-from=<文件> 在<文件>指定目录或文件。
- --exclude=<目录或文件> 略过指定的目录或文件。
- --max-depth=<目录层数> 超过指定层数的目录后，予以忽略。
- --help 显示帮助。
- --version 显示版本信息。
** 实例
显示目录或者文件所占空间:
#+begin_src bash
>>> du
608     ./test6
308     ./test4
4       ./scf/lib
4       ./scf/service/deploy/product
4       ./scf/service/deploy/info
12      ./scf/service/deploy
16      ./scf/service
4       ./scf/doc
4       ./scf/bin
32      ./scf
8       ./test3
1288    .
#+END_SRC
只显示当前目录下面的子目录的目录大小和当前目录的总的大小，最下面的1288为当前目录的总大小

显示指定文件所占空间:
#+begin_src bash
>>> du log2012.log 
300     log2012.log
#+END_SRC

方便阅读的格式显示test目录所占空间情况：
#+begin_src bash
>>> du -h test
608K    test/test6
308K    test/test4
4.0K    test/scf/lib
4.0K    test/scf/service/deploy/product
4.0K    test/scf/service/deploy/info
12K     test/scf/service/deploy
16K     test/scf/service
4.0K    test/scf/doc
4.0K    test/scf/bin
32K     test/scf
8.0K    test/test3
1.3M    test
#+END_SRC

** 参考文章
[[https://www.runoob.com/linux/linux-comm-du.html][菜鸟教程]]

* ELF文件格式
ELF (Executable and Linkable Format)是一种为可执行文件，目标文件，共享链接库和内核转储(core dumps)准备的标准文件格式。 Linux和很多类Unix操作系统都使用这个格式。 让我们来看一下64位ELF文件格式的结构以及内核源码中有关于它的一些定义。

一个ELF文件由以下三部分组成：
- ELF头(ELF header) - 描述文件的主要特性：类型，CPU架构，入口地址，现有部分的大小和偏移等等；
- 节头表(Section header table) - 包含对节(sections)的描述。
- 程序头表(Program header table) - 列举了所有有效的段(segments)和他们的属性。 程序头表需要加载器将文件中的节加载到虚拟内存段中；


#+DOWNLOADED: screenshot @ 2022-05-10 21:51:39
[[file:images/linux%E7%AC%94%E8%AE%B0/ELF%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F/2022-05-10_21-51-39_screenshot.png]]

注意：段（Segment）与节（Section）的区别：段是程序执行的必要组成，当多个目标文件链接成一个可执行文件时，会将相同权限的节合并到一个段中。相比而言，节的粒度更小。
** ELF头(ELF header)
ELF头(ELF header)位于文件的开始位置。 它的主要目的是定位文件的其他部分。 文件头主要包含以下字段：
- ELF文件鉴定 - 一个字节数组用来确认文件是否是一个ELF文件，并且提供普通文件特征的信息；
- 文件类型 - 确定文件类型。 这个字段描述文件是一个重定位文件，或可执行文件,或...；
- 目标结构；
- ELF文件格式的版本；
- 程序入口地址；
- 程序头表的文件偏移；
- 节头表的文件偏移；
- ELF头(ELF header)的大小；
- 程序头表的表项大小；
- 其他字段...

你可以在内核源码种找到表示ELF64 header的结构体 elf64_hdr：
#+BEGIN_SRC c
typedef struct elf64_hdr {
    unsigned char    e_ident[EI_NIDENT];
    Elf64_Half e_type;
    Elf64_Half e_machine;
    Elf64_Word e_version;
    Elf64_Addr e_entry;
    Elf64_Off e_phoff;
    Elf64_Off e_shoff;
    Elf64_Word e_flags;
    Elf64_Half e_ehsize;
    Elf64_Half e_phentsize;
    Elf64_Half e_phnum;
    Elf64_Half e_shentsize;
    Elf64_Half e_shnum;
    Elf64_Half e_shstrndx;
} Elf64_Ehdr;
#+END_SRC
这个结构体定义在 [[https://github.com/torvalds/linux/blob/master/include/uapi/linux/elf.h#L312][elf.h]]

我们可以使用readelf工具来查看ELF Header。
#+begin_src bash
$ readelf -h hello.o

ELF Header:
  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00
  Class:                             ELF64
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              REL (Relocatable file)
  Machine:                           Advanced Micro Devices X86-64
  Version:                           0x1
  Entry point address:               0x0
  Start of program headers:          0 (bytes into file)
  Start of section headers:          672 (bytes into file)
  Flags:                             0x0
  Size of this header:               64 (bytes)
  Size of program headers:           0 (bytes)
  Number of program headers:         0
  Size of section headers:           64 (bytes)
  Number of section headers:         13
  Section header string table index: 10
#+END_SRC

ELF文件结构示意图中定义的Elf_Ehdr的各个成员的含义与readelf具有对应关系。如下所示
1. e_ident	Magic: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00
Class: ELF32
Data: 2’s complement, little end
Version: 1(current)
OS/ABI: UNIX - System V
ABI Version: 0
2. e_type	Type: REL (Relocatable file)
ELF文件类型
3. e_machine	Machine: Advanced Micro Devices X86-64
ELF文件的CPI平台属性
4. e_version	Version: 0x1
ELF版本号。一般为常数1
5. e_entry	Entry point address: 0x0
入口地址，规定ELF程序的入口虚拟地址，操作系统在加载完该程序后从这个地址开始执行进程的指令。可重定位指令一般没有入口地址，则该值为0
6. e_phoff	Start of program headers: 0(bytes into file)
7. e_shoff	Start of section headers: 672 (bytes into file)
Section Header Table 在文件中的偏移
8. e_word	Flags: 0x0
ELF标志位，用来标识一些ELF文件平台相关的属性。
9. e_ehsize	Size of this header: 64 (bytes)
ELF Header本身的大小
10. e_phentsize	Size of program headers: 0 (bytes)
11. e_phnum	Number of program headers: 0
12. e_shentsize	Size of section headers: 64 (bytes)
单个Section Header大小
13. e_shnum	Number of section headers: 13
Section Header的数量
14. e_shstrndx	Section header string table index: 10
Section Header字符串表在Section Header Table中的索引
*** ELF魔数
每种可执行文件的格式的开头几个字节都是很特殊的，特别是开头4个字节，通常被称为魔数（Magic Number）。
通过对魔数的判断可以确定文件的格式和类型。如：ELF的可执行文件格式的头4个字节为0x7F、e、l、f；Java的可执行文件格式的头4个字节为c、a、f、e；
如果被执行的是Shell脚本或perl、python等解释型语言的脚本，那么它的第一行往往是#!/bin/sh或#!/usr/bin/perl或#!/usr/bin/python，此时前两个字节#和!就构成了魔数，系统一旦判断到这两个字节，就对后面的字符串进行解析，以确定具体的解释程序路径。

*** ELF文件类型
ELF文件主要有三种类型，可以通过ELF Header中的e_type成员进行区分。
- 可重定位文件（Relocatable File）：ETL_REL。一般为.o文件。可以被链接成可执行文件或共享目标文件。静态链接库属于可重定位文件。
- 可执行文件（Executable File）：ET_EXEC。可以直接执行的程序。
- 共享目标文件（Shared Object File）：ET_DYN。一般为.so文件。有两种情况可以使用。
    - 链接器将其与其他可重定位文件、共享目标文件链接成新的目标文件；
    - 动态链接器将其与其他共享目标文件、结合一个可执行文件，创建进程映像。


#+DOWNLOADED: screenshot @ 2022-05-10 22:07:24
[[file:images/linux%E7%AC%94%E8%AE%B0/ELF%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F/2022-05-10_22-07-24_screenshot.png]]

** 节(Section Header Table)
所有的数据都存储在ELF文件的节(sections)中。 
ELF 节头表是一个节头数组。
每一个节头都描述了其所对应的节的信息，如节名、节大小、在文件中的偏移、读写权限等。编译器、链接器、装载器都是通过节头表来定位和访问各个节的属性的。

我们通过节头表中的索引(index)来确认节(sections)。 节头表表项包含以下字段：
- 节的名字；
- 节的类型；
- 节的属性；
- 内存地址；
- 文件中的偏移；
- 节的大小；
- 到其他节的链接；
- 各种各样的信息；
- 地址对齐；
- 这个表项的大小，如果有的话；

而且，在linux内核中结构体 elf64_shdr 如下所示:
#+BEGIN_SRC c
typedef struct elf64_shdr {
    Elf64_Word sh_name;
    Elf64_Word sh_type;
    Elf64_Xword sh_flags;
    Elf64_Addr sh_addr;
    Elf64_Off sh_offset;
    Elf64_Xword sh_size;
    Elf64_Word sh_link;
    Elf64_Word sh_info;
    Elf64_Xword sh_addralign;
    Elf64_Xword sh_entsize;
} Elf64_Shdr;
#+END_SRC
我们可以使用readelf工具来查看节头表。
#+begin_src bash
$ readelf -S hello.o

There are 13 section headers, starting at offset 0x2a0:

Section Headers:
  [Nr] Name              Type             Address           Offset
       Size              EntSize          Flags  Link  Info  Align
  [ 0]                   NULL             0000000000000000  00000000
       0000000000000000  0000000000000000           0     0     0
  [ 1] .text             PROGBITS         0000000000000000  00000040
       0000000000000015  0000000000000000  AX       0     0     1
  [ 2] .rela.text        RELA             0000000000000000  000001f0
       0000000000000030  0000000000000018   I      11     1     8
  [ 3] .data             PROGBITS         0000000000000000  00000055
       0000000000000000  0000000000000000  WA       0     0     1
  [ 4] .bss              NOBITS           0000000000000000  00000055
       0000000000000000  0000000000000000  WA       0     0     1
  [ 5] .rodata           PROGBITS         0000000000000000  00000055
       000000000000000d  0000000000000000   A       0     0     1
  [ 6] .comment          PROGBITS         0000000000000000  00000062
       0000000000000035  0000000000000001  MS       0     0     1
  [ 7] .note.GNU-stack   PROGBITS         0000000000000000  00000097
       0000000000000000  0000000000000000           0     0     1
  [ 8] .eh_frame         PROGBITS         0000000000000000  00000098
       0000000000000038  0000000000000000   A       0     0     8
  [ 9] .rela.eh_frame    RELA             0000000000000000  00000220
       0000000000000018  0000000000000018   I      11     8     8
  [10] .shstrtab         STRTAB           0000000000000000  00000238
       0000000000000061  0000000000000000           0     0     1
  [11] .symtab           SYMTAB           0000000000000000  000000d0
       0000000000000108  0000000000000018          12     9     8
  [12] .strtab           STRTAB           0000000000000000  000001d8
       0000000000000013  0000000000000000           0     0     1
Key to Flags:
  W (write), A (alloc), X (execute), M (merge), S (strings), l (large)
  I (info), L (link order), G (group), T (TLS), E (exclude), x (unknown)
  O (extra OS processing required) o (OS specific), p (processor specific)
#+END_SRC
ELF文件结构示意图中定义的Elf_Shdr的各个成员的含义与readelf具有对应关系。如下所示：
1. sh_name	节名
节名是一个字符串，保存在一个名为.shstrtab的字符串表（可通过Section Header索引到）。sh_name的值实际上是其节名字符串在.shstrtab中的偏移值
2. sh_type	节类型
3. sh_flags	节标志位
4. sh_addr	节地址：节的虚拟地址
如果该节可以被加载，则sh_addr为该节被加载后在进程地址空间中的虚拟地址；否则sh_addr为0
5. sh_offset	节偏移
如果该节存在于文件中，则表示该节在文件中的偏移；否则无意义，如sh_offset对于BSS 节来说是没有意义的
6. sh_size	节大小
7. sh_link、sh_info	节链接信息
8. sh_addralign	节地址对齐方式
9. sh_entsize	节项大小
有些节包含了一些固定大小的项，如符号表，其包含的每个符号所在的大小都一样的，对于这种节，sh_entsize表示每个项的大小。如果为0，则表示该节不包含固定大小的项。
*** 节类型（sh_type）
节名是一个字符串，只是在链接和编译过程中有意义，但它并不能真正地表示节的类型。对于编译器和链接器来说，主要决定节的属性是节的类型（sh_type）和节的标志位（sh_flags）。

节的类型相关常量以SHT_开头，上述readelf -S命令执行的结果省略了该前缀。常见的节类型如下表所示：
| 常量         | 值 | 含义                                 |
|--------------+----+--------------------------------------|
| SHT_NULL     |  0 | 无效节                               |
| SHT_PROGBITS |  1 | 程序节。代码节、数据节都是这种类型。 |
| SHT_SYMTAB   |  2 | 符号表                               |
| SHT_STRTAB   |  3 | 字符串表                             |
| SHT_RELA     |  4 | 重定位表。该节包含了重定位信息。     |
| SHT_HASH     |  5 | 符号表的哈希表                       |
| SHT_DYNAMIC  |  6 | 动态链接信息                         |
| SHT_NOTE     |  7 | 提示性信息                           |
| SHT_NOBITS   |  8 | 表示该节在文件中没有内容。如.bss节   |
| SHT_REL      |  9 | 该节包含了重定位信息                 |
| SHT_SHLIB    | 10 | 保留                                 |
| SHT_DNYSYM   | 11 | 动态链接的符号表                     |
*** 节标志位（sh_flag）
节标志位表示该节在进程虚拟地址空间中的属性。如是否可写、是否可执行等。相关常量以SHF_开头。常见的节标志位如下表所示：
| 常量          | 值 | 含义                                                                                                       |
|---------------+----+------------------------------------------------------------------------------------------------------------|
| SHF_WRITE     |  1 | 表示该节在进程空间中可写                                                                                   |
| SHF_ALLOC     |  2 | 表示该节在进程空间中需要分配空间。有些包含指示或控制信息的节不需要在进程空间中分配空间，就不会有这个标志。 |
| SHF_EXECINSTR |  4 | 表示该节在进程空间中可以被执行                                                                             |
*** 节链接信息（sh_link、sh_info）
如果节的类型是与链接相关的（无论是动态链接还是静态链接），如重定位表、符号表、等，则sh_link、sh_info两个成员所包含的意义如下所示。其他类型的节，这两个成员没有意义。
| sh_type     | sh_link                                | sh_info                              |
|-------------+----------------------------------------+--------------------------------------|
| SHT_DYNAMIC | 该节所使用的字符串表在节头表中的下标   | 0                                    |
| SHT_HASH    | 该节所使用的符号表在节头表中的下标     | 0                                    |
| SHT_REL     | 该节所使用的相应符号表在节头表中的下标 | 该重定位表所作用的节在节头表中的下标 |
| SHT_RELA    | 该节所使用的相应符号表在节头表中的下标 | 该重定位表所作用的节在节头表中的下标 |
| SHT_SYMTAB  | 操作系统相关                           | 操作系统相关                         |
| SHT_DYNSYM  | 操作系统相关                           | 操作系统相关                         |
| other       | SHN_UNDEF                              | 0                                    |
** ELF Sections
*** 节的分类
.text节
.text节是保存了程序代码指令的代码节。一段可执行程序，如果存在Phdr，则.text节就会存在于text段中。由于.text节保存了程序代码，所以节类型为SHT_PROGBITS。

.rodata节
rodata节保存了只读的数据，如一行C语言代码中的字符串。由于.rodata节是只读的，所以只能存在于一个可执行文件的只读段中。因此，只能在text段（不是data段）中找到.rodata节。由于.rodata节是只读的，所以节类型为SHT_PROGBITS。

.plt节（过程链接表）
.plt节也称为过程链接表（Procedure Linkage Table），其包含了动态链接器调用从共享库导入的函数所必需的相关代码。由于.plt节保存了代码，所以节类型为SHT_PROGBITS。

.data节
.data节存在于data段中，其保存了初始化的全局变量等数据。由于.data节保存了程序的变量数据，所以节类型为SHT_PROGBITS。

.bss节
.bss节存在于data段中，占用空间不超过4字节，仅表示这个节本省的空间。.bss节保存了未进行初始化的全局数据。程序加载时数据被初始化为0，在程序执行期间可以进行赋值。由于.bss节未保存实际的数据，所以节类型为SHT_NOBITS。

.got.plt节（全局偏移表-过程链接表）
.got节保存了全局偏移表。.got节和.plt节一起提供了对导入的共享库函数的访问入口，由动态链接器在运行时进行修改。由于.got.plt节与程序执行有关，所以节类型为SHT_PROGBITS。

.dynsym节（动态链接符号表）
.dynsym节保存在text段中。其保存了从共享库导入的动态符号表。节类型为SHT_DYNSYM。

.dynstr节（动态链接字符串表）
.dynstr保存了动态链接字符串表，表中存放了一系列字符串，这些字符串代表了符号名称，以空字符作为终止符。

.rel.*节（重定位表）
重定位表保存了重定位相关的信息，这些信息描述了如何在链接或运行时，对ELF目标文件的某部分或者进程镜像进行补充或修改。由于重定位表保存了重定位相关的数据，所以节类型为SHT_REL。

.hash节
.hash节也称为.gnu.hash，其保存了一个用于查找符号的散列表。

.symtab节（符号表）
.symtab节是一个ElfN_Sym的数组，保存了符号信息。节类型为SHT_SYMTAB。

.strtab节（字符串表）
.strtab节保存的是符号字符串表，表中的内容会被.symtab的ElfN_Sym结构中的st_name引用。节类型为SHT_STRTAB。

.ctors节和.dtors节
.ctors（构造器）节和.dtors（析构器）节分别保存了指向构造函数和析构函数的函数指针，构造函数是在main函数执行之前需要执行的代码；析构函数是在main函数之后需要执行的代码。
*** 符号表
节的分类中我们介绍了.dynsym节和.symtab节，两者都是符号表。那么它们到底有什么区别呢？存在什么关系呢？

符号是对某些类型的数据或代码（如全局变量或函数）的符号引用，函数名或变量名就是符号名。例如，printf()函数会在动态链接符号表.dynsym中存有一个指向该函数的符号项（以Elf_Sym数据结构表示）。在大多数共享库和动态链接可执行文件中，存在两个符号表。即.dynsym和.symtab。

.dynsym保存了引用来自外部文件符号的全局符号。如printf库函数。.dynsym保存的符号是.symtab所保存符合的子集，.symtab中还保存了可执行文件的本地符号。如全局变量，代码中定义的本地函数等。

既然.dynsym是.symtab的子集，那为何要同时存在两个符号表呢？

通过readelf -S命令可以查看可执行文件的输出，一部分节标志位（sh_flags）被标记为了A（ALLOC）、WA（WRITE/ALLOC）、AX（ALLOC/EXEC）。其中，.dynsym被标记为ALLOC，而.symtab则没有标记。

ALLOC表示有该标记的节会在运行时分配并装载进入内存，而.symtab不是在运行时必需的，因此不会被装载到内存中。.dynsym保存的符号只能在运行时被解析，因此是运行时动态链接器所需的唯一符号。.dynsym对于动态链接可执行文件的执行是必需的，而.symtab只是用来进行调试和链接的。

#+DOWNLOADED: screenshot @ 2022-05-10 22:33:15
[[file:images/linux%E7%AC%94%E8%AE%B0/ELF%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F/2022-05-10_22-33-15_screenshot.png]]
上图所示为通过符号表索引字符串表的示意图。符号表中的每一项都是一个Elf_Sym结构，对应可以在字符串表中索引得到一个字符串。该数据结构中成员的含义如下表所示：

| 成员     | 含义                                                   |
|----------+--------------------------------------------------------|
| st_name  | 符号名。该值为该符号名在字符串表中的偏移地址。         |
| st_value | 符号对应的值。存放符号的值（可能是地址或位置偏移量）。 |
| st_size  | 符号的大小。                                           |
| st_other | 0                                                      |
| st_shndx | 符号所在的节                                           |
| st_info  | 符号类型及绑定属性                                     |
使用readelf工具我们也能够看到符号表的相关信息。
#+begin_src bash
$ readelf -s hello.o

Symbol table '.symtab' contains 11 entries:
   Num:    Value          Size Type    Bind   Vis      Ndx Name
     0: 0000000000000000     0 NOTYPE  LOCAL  DEFAULT  UND
     1: 0000000000000000     0 FILE    LOCAL  DEFAULT  ABS hello.c
     2: 0000000000000000     0 SECTION LOCAL  DEFAULT    1
     3: 0000000000000000     0 SECTION LOCAL  DEFAULT    3
     4: 0000000000000000     0 SECTION LOCAL  DEFAULT    4
     5: 0000000000000000     0 SECTION LOCAL  DEFAULT    5
     6: 0000000000000000     0 SECTION LOCAL  DEFAULT    7
     7: 0000000000000000     0 SECTION LOCAL  DEFAULT    8
     8: 0000000000000000     0 SECTION LOCAL  DEFAULT    6
     9: 0000000000000000    21 FUNC    GLOBAL DEFAULT    1 main
    10: 0000000000000000     0 NOTYPE  GLOBAL DEFAULT  UND puts
#+END_SRC
*** 字符串表
类似于符号表，在大多数共享库和动态链接可执行文件中，也存在两个字符串表。即.dynstr和.strtab，分别对应于.dynsym和symtab。此外，还有一个.shstrtab的节头字符串表，用于保存节头表中用到的字符串，可通过sh_name进行索引。

ELF文件中所有字符表的结构基本一致，如上图所示。
*** 重定位表
重定位就是将符号定义和符号引用进行连接的过程。可重定位文件需要包含描述如何修改节内容的相关信息，从而使可执行文件和共享目标文件能够保存进程的程序镜像所需要的正确信息。

重定位表是进行重定位的重要依据。我们可以使用objdump工具查看目标文件的重定位表：
#+begin_src bash
$ objdump -r hello.o


hello.o:     file format elf64-x86-64

RELOCATION RECORDS FOR [.text]:
OFFSET           TYPE              VALUE
0000000000000005 R_X86_64_32       .rodata
000000000000000a R_X86_64_PC32     puts-0x0000000000000004


RELOCATION RECORDS FOR [.eh_frame]:
OFFSET           TYPE              VALUE
0000000000000020 R_X86_64_PC32     .text
#+END_SRC
重定位表是一个Elf_Rel类型的数组结构，每一项对应一个需要进行重定位的项。
其成员含义如下所示：
1. r_offset	重定位入口的偏移。
对于可重定位文件来说，这个值是该重定位入口所要修正的位置的第一个字节相对于节起始的偏移
对于可执行文件或共享对象文件来说，这个值是该重定位入口所要修正的位置的第一个字节的虚拟地址
2. r_info	重定位入口的类型和符号
因为不同处理器的指令系统不一样，所以重定位所要修正的指令地址格式也不一样。每种处理器都有自己的一套重定位入口的类型。
对于可执行文件和共享目标文件来说，它们的重定位入口是动态链接类型的。
重定位是目标文件链接成为可执行文件的关键。我们将在后面的进行介绍。
** 程序头表(Program header table)
在可执行文件或者共享链接库中所有的节(sections)都被分为多个段(segments)。 程序头是一个结构的数组，每一个结构都表示一个段(segments)。 
它的结构就像这样：
#+BEGIN_SRC c
typedef struct elf64_phdr {
    Elf64_Word p_type;
    Elf64_Word p_flags;
    Elf64_Off p_offset;
    Elf64_Addr p_vaddr;
    Elf64_Addr p_paddr;
    Elf64_Xword p_filesz;
    Elf64_Xword p_memsz;
    Elf64_Xword p_align;
} Elf64_Phdr;
#+END_SRC
在内核源码中。

elf64_phdr 定义在相同的 elf.h 文件中.

** 目标文件的格式
目前，PC平台流行的 可执行文件格式（Executable） 主要包含如下两种，它们都是 COFF（Common File Format） 格式的变种。
- Windows下的 PE（Portable Executable）
- Linux下的 ELF（Executable Linkable Format）

目标文件就是源代码经过编译后但未进行连接的那些中间文件（Windows的.obj和Linux的.o），它与可执行文件的格式非常相似，所以一般跟可执行文件格式一起采用同一种格式存储。
在Windows下采用PE-COFF文件格式；Linux下采用ELF文件格式。

事实上，除了可执行文件外，动态链接库（DDL，Dynamic Linking Library）、静态链接库（Static Linking Library） 均采用可执行文件格式存储。
它们在Window下均按照PE-COFF格式存储；Linux下均按照ELF格式存储。只是文件名后缀不同而已。
- 动态链接库：Windows的.dll、Linux的.so
- 静态链接库：Windows的.lib、Linux的.a

** 参考文章
[[http://chuquan.me/2018/05/21/elf-introduce/][计算机那些事(4)——ELF文件结构 - 楚权的世界]]
[[https://xinqiu.gitbooks.io/linux-inside-zh/content/Theory/linux-theory-2.html][ELF 文件格式· Linux Inside 中文版]]
* export 命令
Linux export 命令用于设置或显示环境变量。

在 shell 中执行程序时，shell 会提供一组环境变量。export 可新增，修改或删除环境变量，供后续执行的程序使用。export 的效力仅限于该次登陆操作。

语法：
export [-fnp][变量名称]=[变量设置值]

参数说明：
-f 　代表[变量名称]中为函数名称。
-n 　删除指定的变量。变量实际上并未删除，只是不会输出到后续指令的执行环境中。
-p 　列出所有的shell赋予程序的环境变量。

** 实例
列出当前所有的环境变量
#+begin_src bash
>>>export -p //列出当前的环境变量值
declare -x HOME=“/root“
declare -x LANG=“zh_CN.UTF-8“
declare -x LANGUAGE=“zh_CN:zh“
declare -x LESSCLOSE=“/usr/bin/lesspipe %s %s“
declare -x LESSOPEN=“| /usr/bin/lesspipe %s“
declare -x LOGNAME=“root“
declare -x LS_COLORS=““
declare -x MAIL=“/var/mail/root“
declare -x OLDPWD
declare -x PATH=“/opt/toolchains/arm920t-eabi/bin:/opt/toolchains/arm920t-eabi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games“
declare -x PWD=“/root“
declare -x SHELL=“/bin/bash“
declare -x SHLVL=“1“
declare -x SPEECHD_PORT=“6560“
declare -x SSH_CLIENT=“192.168.1.65 1674 22“
declare -x SSH_CONNECTION=“192.168.1.65 1674 192.168.1.3 22“
declare -x SSH_TTY=“/dev/pts/2“
declare -x TERM=“XTERM“
declare -x USER=“root“
declare -x XDG_SESSION_COOKIE=“93b5d3d03e032c0cf892a4474bebda9f-1273864738.954257-340206484“
#+END_SRC
定义环境变量
#+begin_src bash
>>>export MYENV //定义环境变量
>>>export -p //列出当前的环境变量
declare -x HOME=“/root“
declare -x LANG=“zh_CN.UTF-8“
declare -x LANGUAGE=“zh_CN:zh“
declare -x LESSCLOSE=“/usr/bin/lesspipe %s %s“
declare -x LESSOPEN=“| /usr/bin/lesspipe %s“
declare -x LOGNAME=“root“
declare -x LS_COLORS=““
declare -x MAIL=“/var/mail/root“
declare -x MYENV
declare -x OLDPWD
declare -x PATH=“/opt/toolchains/arm920t-eabi/bin:/opt/toolchains/arm920t-eabi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games“
declare -x PWD=“/root“
declare -x SHELL=“/bin/bash“
declare -x SHLVL=“1“
declare -x SPEECHD_PORT=“6560“
declare -x SSH_CLIENT=“192.168.1.65 1674 22“
declare -x SSH_CONNECTION=“192.168.1.65 1674 192.168.1.3 22“
declare -x SSH_TTY=“/dev/pts/2“
declare -x TERM=“XTERM“
declare -x USER=“root“
declare -x XDG_SESSION_COOKIE=“93b5d3d03e032c0cf892a4474bebda9f-1273864738.954257-340206484“
#+END_SRC
定义环境变量赋值
#+begin_src bash
>>>export MYENV=7 //定义环境变量并赋值
>>>export -p
declare -x HOME=“/root“
declare -x LANG=“zh_CN.UTF-8“
declare -x LANGUAGE=“zh_CN:zh“
declare -x LESSCLOSE=“/usr/bin/lesspipe %s %s“
declare -x LESSOPEN=“| /usr/bin/lesspipe %s“
declare -x LOGNAME=“root“
declare -x LS_COLORS=““
declare -x MAIL=“/var/mail/root“
declare -x MYENV=“7“
declare -x OLDPWD
declare -x PATH=“/opt/toolchains/arm920t-eabi/bin:/opt/toolchains/arm920t-eabi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games“
declare -x PWD=“/root“
declare -x SHELL=“/bin/bash“
declare -x SHLVL=“1“
declare -x SPEECHD_PORT=“6560“
declare -x SSH_CLIENT=“192.168.1.65 1674 22“
declare -x SSH_CONNECTION=“192.168.1.65 1674 192.168.1.3 22“
declare -x SSH_TTY=“/dev/pts/2“
declare -x TERM=“XTERM“
declare -x USER=“root“
declare -x XDG_SESSION_COOKIE=“93b5d3d03e032c0cf892a4474bebda9f-1273864738.954257-340206484“
#+END_SRC

* expr命令
expr命令是一个手工命令行计数器，用于在UNIX/LINUX下求表达式变量的值，一般用于整数值，也可用于字符串。

语法：expr 表达式
表达式说明:
- 用空格隔开每个项；
- 用反斜杠 \ 放在 shell 特定的字符前面；
- 对包含空格和其他特殊字符的字符串要用引号括起来
** 实例
1、计算字串长度
#+begin_src bash
> expr length “this is a test”
 14
#+END_SRC

2、抓取字串
#+begin_src bash
> expr substr “this is a test” 3 5
is is
#+END_SRC
3、抓取第一个字符数字串出现的位置
#+begin_src bash
> expr index "sarasara"  a
 2
#+END_SRC
4、整数运算
#+begin_src bash
> expr 14 % 9
5
> expr 10 + 10
20
> expr 1000 + 900
1900
> expr 30 / 3 / 2
5
> expr 30 \* 3 (使用乘号时，必须用反斜线屏蔽其特定含义。因为shell可能会误解显示星号的意义)
90
> expr 30 * 3
expr: Syntax error
#+END_SRC
** 参考文章
[[https://www.runoob.com/linux/linux-comm-expr.html][Linux expr命令 - 菜鸟教程]]
* emacs 安装
源码下载地址：http://mirrors.ocf.berkeley.edu/gnu/emacs/

编译安装：
#+begin_src bash
./configure --prefix=/opt/emacs/
#+END_SRC
如果出现以下错误：
#+begin_src bash
checking for libXaw... configure: error: No X toolkit could be found.
If you are sure you want Emacs compiled without an X toolkit, pass
  --with-x-toolkit=no
to configure.  Otherwise, install the development libraries for the toolkit
that you want to use (e.g. Gtk+) and re-run configure.
#+END_SRC
解决方案：

安装依赖：
#+begin_src bash
sudo apt-get install build-essential texinfo libx11-dev libxpm-dev libjpeg-dev libpng-dev libgif-dev libtiff-dev libgtk2.0-dev libgtk-3-dev libncurses-dev libxpm-dev automake autoconf
#+END_SRC
如果出现以下错误：
#+begin_src bash
configure: error: The following required libraries were not found:
     gnutls
Maybe some development libraries/packages are missing?
If you don't want to link with them give
     --with-gnutls=no
as options to configure
#+END_SRC
解决方案：
#+begin_src bash
sudo apt-get install gnutls-dev
# 或者
./configure --with-gnutls=ifavailable
#+END_SRC
如果出现以下警告：
#+begin_src bash
configure: WARNING: This configuration installs a 'movemail' program
that does not retrieve POP3 email.  By default, Emacs 25 and earlier
installed a 'movemail' program that retrieved POP3 email via only
insecure channels, a practice that is no longer recommended but that
you can continue to support by using './configure --with-pop'.
configure: You might want to install GNU Mailutils
<https://mailutils.org> and use './configure --with-mailutils'.
#+END_SRC
解决方案：
#+begin_src bash
./configure --prefix=/opt/emacs/ --with-mailutils --with-pop
#+END_SRC
最后：
#+begin_src bash
make && make install
#+END_SRC
添加软连接
#+begin_src bash
ln -s /opt/emacs/bin/emacs /usr/bin/emacs
#+END_SRC

** 参考文章
[[https://www.cnblogs.com/felixwang2/p/10281092.html][emacs源码安装]]
[[https://stackoverflow.com/questions/52722096/build-emacs-and-gnutls-not-found/52722866][build emacs and gnutls not found]]
* gcc、make、cmake的关系和区别
1.gcc是GNU Compiler Collection（就是GNU编译器套件），也可以简单认为是编译器，它可以编译很多种编程语言（括C、C++、Objective-C、Fortran、Java等等）。

2.当你的程序只有一个源文件时，直接就可以用gcc命令编译它。

3.但是当你的程序包含很多个源文件时，用gcc命令逐个去编译时，你就很容易混乱而且工作量大

4.所以出现了make工具
make工具可以看成是一个智能的批处理工具，它本身并没有编译和链接的功能，而是用类似于批处理的方式—通过调用makefile文件中用户指定的命令来进行编译和链接的。

5.makefile是什么？简单的说就像一首歌的乐谱，make工具就像指挥家，指挥家根据乐谱指挥整个乐团怎么样演奏，make工具就根据makefile中的命令进行编译和链接的。

6.makefile命令中就包含了调用gcc（也可以是别的编译器）去编译某个源文件的命令。

7.makefile在一些简单的工程完全可以人工手下，但是当工程非常大的时候，手写makefile也是非常麻烦的，如果换了个平台makefile又要重新修改。

8.这时候就出现了Cmake这个工具，cmake就可以更加简单的生成makefile文件给上面那个make用。当然cmake还有其他功能，就是可以跨平台生成对应平台能用的makefile，你不用再自己去修改了。

9.可是cmake根据什么生成makefile呢？它又要根据一个叫CMakeLists.txt文件（学名：组态档）去生成makefile。

10.到最后CMakeLists.txt文件谁写啊？亲，是你自己手写的。

11.当然如果你用IDE，类似VS这些一般它都能帮你弄好了，你只需要按一下那个三角形
12.cmake是make maker，生成各种可以直接控制编译过程的控制器的配置文件，比如makefile、各种IDE的配置文件。
13.make是一个简单的通过文件时间戳控制自动过程、处理依赖关系的软件，这个自动过程可以是编译一个项目。

* gdb
** gdb安装
打开终端，在终端里输入以下指令：
#+BEGIN_SRC bash
apt-get update
apt-get install  gdb
#+END_SRC
** gdb原理
*** 概述
为了方便描述，先写一个最最简单的C程序：
#+begin_src bash
#include <stdio.h>

int main(int argc, char *argv[])
{
    int a = 1;
    int b = 2;
    int c = a + b;
    printf("c = %d \n", c);
    return 0;
}
#+END_SRC
编译命令:
#+begin_src bash
$ gcc -g test.c -o test
#+END_SRC
我们对可执行程序 test 进行调试，输入命令：
#+begin_src bash
$ gdb ./test
#+END_SRC
系统首先会启动gdb进程，这个进程会调用系统函数fork()来创建一个子进程，这个子进程做两件事情： 
1. 调用系统函数ptrace(PTRACE_TRACEME，[其他参数])； 
2. 通过execc来加载、执行可执行程序test，那么test程序就在这个子进程中开始执行了。

#+DOWNLOADED: screenshot @ 2022-05-16 15:27:19
[[file:images/linux%E7%AC%94%E8%AE%B0/gdb/2022-05-16_15-27-19_screenshot.png]]
*** ptrach
ptrash函数原型是：
#+BEGIN_EXAMPLE
#include <sys/ptrace.h>
long ptrace(enum __ptrace_request request, pid_t pid, void *addr, void *data);
#+END_EXAMPLE
我们先来看一下 man 中对这个函数的简介：

#+DOWNLOADED: screenshot @ 2022-05-16 15:34:35
[[file:images/linux%E7%AC%94%E8%AE%B0/gdb/2022-05-16_15-34-35_screenshot.png]]
tracer就是调试程序，可以理解为gdb程序；tracee就是被调试程序，对应于图中的目标程序test。老外一般喜欢用-er和-ee来表示主动和被动的关系，例如：employer就是雇主(老板)，employee就是苦逼的被雇佣者(打工人)。

ptrace系统函数是Linux内核提供的一个用于进程跟踪的系统调用，通过它，一个进程(gdb)可以读写另外一个进程(test)的指令空间、数据空间、堆栈和寄存器的值。而且gdb进程接管了test进程的所有信号，也就是说系统向test进程发送的所有信号，都被gdb进程接收到，这样一来，test进程的执行就被gdb控制了，从而达到调试的目的。

也就是说，如果没有gdb调试，操作系统与目标进程之间是直接交互的；如果使用gdb来调试程序，那么操作系统发送给目标进程的信号就会被gdb截获，gdb根据信号的属性来决定：在继续运行目标程序时是否把当前截获的信号转交给目标程序，如此一来，目标程序就在gdb发来的信号指挥下进行相应的动作。
*** GDB如何调试已经执行的服务进程
ptrace系统函数的第一个参数是一个枚举类型的值，其中重要的是2个：PTRACE_TRACEME和PTRACE_ATTACH<。

在上面的讲解中，子进程在调用ptrace系统函数时使用的参数是PTRACE_TRACEME，注意是子进程调用ptrace，相当于子进程对操作系统说：gdb进程是我的爸爸，以后你有任何想发给我的信号，请直接发给gdb进程吧！

如果想对一个已经执行的进程B进行调试，那么就要在gdb这个父进程中调用ptrace(PTRACE_ATTACH,[其他参数])，此时，gdb进程会attach(绑定)到已经执行的进程B，gdb把进程B收养成为自己的子进程，而子进程B的行为等同于它进行了一次 PTRACE_TRACEME操作。此时gdb进程会发送SIGSTO信号给子进程B，子进程B接收到SIGSTOP信号后，就会暂停执行进入TASK_STOPED状态，表示自己准备好被调试了。

#+DOWNLOADED: screenshot @ 2022-05-16 15:39:50
[[file:images/linux%E7%AC%94%E8%AE%B0/gdb/2022-05-16_15-39-50_screenshot.png]]
，不论是调试一个新程序，还是调试一个已经处于执行中状态的服务程序，通过ptrace系统调用，最终的结果都是：gdb程序是父进程，被调试程序是子进程，子进程的所有信号都被父进程gdb来接管，并且父进程gdb可查看、修改子进程的内部信息，包括：堆栈、寄存器等。
*** GDB如何实现断点指令
#+begin_src bash
int main(int argc, char *argv[])
{
    int a = 1;
    int b = 2;
    int c = a + b;
    printf("c = %d \n", c);
    return 0;
}
#+END_SRC
来看一下编译出来的反汇编代码是什么样的，编译指令：
#+begin_src bash
gcc -S test.c; cat test.S)
#+END_SRC

#+DOWNLOADED: screenshot @ 2022-05-16 15:40:56
[[file:images/linux%E7%AC%94%E8%AE%B0/gdb/2022-05-16_15-40-56_screenshot.png]]
我们把源码和汇编代码放在一起，方便理解：
#+DOWNLOADED: screenshot @ 2022-05-16 15:41:31
[[file:images/linux%E7%AC%94%E8%AE%B0/gdb/2022-05-16_15-41-31_screenshot.png]]
在调试窗口输入设置断点指令“break 5”，此时gdb做2件事情： 1. 对第5行源码所对应的第10行汇编代码存储到断点链表中。 2. 在汇编代码的第10行，插入中断指令INT3，也就是说：汇编代码中的第10行被替换为INT3。

#+DOWNLOADED: screenshot @ 2022-05-16 15:42:21
[[file:images/linux%E7%AC%94%E8%AE%B0/gdb/2022-05-16_15-42-21_screenshot.png]]
然后，在调试窗口继续输入执行指令“run”(一直执行，直到遇到断点就暂停)，汇编代码中PC指针(一个内部指针，指向即将执行的那行代码)执行第10行时，发现是INT3指令，于是操作系统就发送一个SIGTRAP信号给test进程。

此刻，第10行汇编代码被执行过了，PC指针就指向第11行了。

#+DOWNLOADED: screenshot @ 2022-05-16 15:42:44
[[file:images/linux%E7%AC%94%E8%AE%B0/gdb/2022-05-16_15-42-44_screenshot.png]]
上面已经说过，操作系统发给test的任何信号，都被gdb接管了，也就是说gdb会首先接收到这SIGTRAP个信号，gdb发现当前汇编代码执行的是第10行，于是到断点链表中查找，发现链表中存储了第10行的代码，说明第10行被设置了断点。于是gdb又做了2个操作： 1. 把汇编代码中的第10行"INT3"替换为断点链表中原来的代码。 2. 把 PC 指针回退一步，也即是设置为指向第10 行。

然后，gdb继续等待用户的调试指令。
#+DOWNLOADED: screenshot @ 2022-05-16 15:43:01
[[file:images/linux%E7%AC%94%E8%AE%B0/gdb/2022-05-16_15-43-01_screenshot.png]]
此刻，就相当于下一条执行的指令是汇编代码中的第10行，也就是源码中的第5行。从我们调试者角度看，就是被调试程序在第5行断点处暂停了下来，此时我们可以继续输入其他调试指令来debug，比如：查看变量值、查看堆栈信息、修改局部变量的值等等。
*** GDB如何实现单步指令next
假设此时程序停止在源码的第6行，即汇编代码的第11行：

#+DOWNLOADED: screenshot @ 2022-05-16 15:43:49
[[file:images/linux%E7%AC%94%E8%AE%B0/gdb/2022-05-16_15-43-49_screenshot.png]]
在调试窗口输入单步执行指令next，我们的目的是执行一行代码，也就是把源码中第6行代码执行完，然后停止在第7行。gdb在接收到next执行时，会计算出第7行源码，应该对应到汇编代码的第14行，于是gdb就控制汇编代码中的PC指针一直执行，直到第13行执行结束，也就是PC指向第14行时，就停止下来，然后继续等待用户输入调试指令。

#+DOWNLOADED: screenshot @ 2022-05-16 15:44:04
[[file:images/linux%E7%AC%94%E8%AE%B0/gdb/2022-05-16_15-44-04_screenshot.png]]
*** 参考文章
[[https://zhuanlan.zhihu.com/p/336922639][原来gdb的底层调试原理这么简单]]
** 符号表
A Debugging Symbol Table maps instructions in the compiled binary program to their corresponding variable, function, or line in the source code. This mapping could be something like:

Program instruction --> item name, item type, original file, line number defined.

Symbol tables may be embedded into the program or stored as a separate file. So if you plan to debug your program, then it is required to create a symbol table which will have the required information to debug the program.

We can infer the following facts about symbol tables:
- A symbol table works for a particular version of the program – if the program changes, a new table must be created.
- Debug builds are often larger and slower than retail (non-debug) builds; debug builds contain the symbol table and other ancillary information.
- If you wish to debug a binary program you did not compile yourself, you must get the symbol tables from the author.

To let GDB be able to read all that information line by line from the symbol table, we need to compile it a bit differently. Normally we compile our programs as:
#+begin_src bash
gcc hello.cc -o hello 
#+END_SRC
Instead of doing this, we need to compile with the -g flag as shown below:
#+begin_src bash
gcc -g hello.cc -o hello 
#+END_SRC

*** 参考文章
[[https://www.tutorialspoint.com/gnu_debugger/gdb_debugging_symbols.htm][GDB - Debugging Symbols - Tutorialspoint]]
** 调试信息
一般来说GDB主要调试的是C/C++的程序。要调试C/C++的程序，首先在编译时，我们必须要把调试信息加到可执行文件中。使用编译器（cc/gcc/g++）的 -g 参数可以做到这一点。如：
#+begin_src bash
> gcc -g hello.c -o hello

> g++ -g hello.cpp -o hello
#+END_SRC
如果没有-g，你将看不见程序的函数名、变量名，所代替的全是运行时的内存地址。
** 启动gdb方式
1、gdb program
program 也就是你的执行文件，一般在当前目录下。

2、gdb program core
用gdb同时调试一个运行程序和core文件，core是程序非法执行后core dump后产生的文件。

3、gdb program 1234
如果你的程序是一个服务程序，那么你可以指定这个服务程序运行时的进程ID。gdb会自动attach上去，并调试他。program应该在PATH环境变量中搜索得到。

GDB启动时，可以加上一些GDB的启动开关，详细的开关可以用gdb -help查看。下面只列举一些比较常用的参数：

--symbols=SYMFILE
从指定文件中读取符号表。

--se=FILE
从指定文件中读取符号表信息，并把他用在可执行文件中。

--core=COREFILE
调试时core dump的core文件。

--directory=DIR
加入一个源文件的搜索路径。默认搜索路径是环境变量中PATH所定义的路径。
** GDB 的命令概貌
启动gdb后，就进入gdb的调试环境中，就可以使用gdb的命令开始调试程序了，gdb的命令可以使用help命令来查看，如下所示：
#+begin_src bash
root@linux:/home/benben# gdb
GNU gdb 5.1.1
Copyright 2002 Free Software Foundation, Inc.
GDB is free software, covered by the GNU General Public License, and you are
welcome to change it and/or distribute copies of it under certain conditions.
Type "show copying" to see the conditions.
There is absolutely no warranty for GDB. Type "show warranty" for details.
This GDB was configured as "i386-suse-linux".
(gdb) help
List of classes of commands:
 
aliases -- Aliases of other commands
breakpoints -- Making program stop at certain points
data -- Examining data
files -- Specifying and examining files
internals -- Maintenance commands
obscure -- Obscure features
running -- Running the program
stack -- Examining the stack
status -- Status inquiries
support -- Support facilities
tracepoints -- Tracing of program execution without stopping the program
user-defined -- User-defined commands
 
Type "help" followed by a class name for a list of commands in that class.
Type "help" followed by command name for full documentation.
Command name abbreviations are allowed if unambiguous.
(gdb)
#+END_SRC
gdb的命令很多，gdb把之分成许多个种类。help命令只是例出gdb的命令种类，如果要看种类中的命令，可以使用 help 命令，如：help breakpoints，查看设置断点的所有命令。也可以直接help 来查看命令的帮助。

gdb中，输入命令时，可以不用打全命令，只用打命令的前几个字符就可以了，当然，命令的前几个字符应该要标志着一个唯一的命令，在Linux下，你可以敲击两次TAB键来补齐命令的全称，如果有重复的，那么gdb会把其列出来。

示例一： 在进入函数func时，设置一个断点。可以敲入break func，或是直接就是b func
#+begin_src bash
(gdb) b func
Breakpoint 1 at 0x8048458: file hello.c, line 10.
#+END_SRC
示例二： 敲入b按两次TAB键，你会看到所有b打头的命令：
#+begin_src bash
(gdb) b
backtrace break bt
#+END_SRC
示例三： 只记得函数的前缀，可以这样：
#+begin_src bash
(gdb) b make_ <按TAB键>
（再按下一次TAB键，你会看到:）
make_a_section_from_file make_environ
make_abs_section make_function_type
make_blockvector make_pointer_type
make_cleanup make_reference_type
make_command make_symbol_completion_list
#+END_SRC
GDB把所有make开头的函数全部例出来给你查看。

要退出gdb时，只用发quit或命令简称q就行了。
** GDB 中运行UNIX的shell程序
在gdb环境中，你可以执行UNIX的shell的命令，使用gdb的shell命令来完成：
#+begin_src bash
shell
#+END_SRC
调用UNIX的shell来执行，环境变量SHELL中定义的UNIX的shell将会被用来执行，如果SHELL没有定义，那就使用UNIX的标准shell：/bin/sh。

退出用exit命令，回到gdb提示符

还有一个gdb命令是make：
#+begin_src bash
make
#+END_SRC
可以在gdb中执行make命令来重新build自己的程序。这个命令等价于“shell make ”。
** 在GDB中运行程序
当以 gdb 方式启动gdb后，gdb会在PATH路径和当前目录中搜索源文件。如要确认gdb是否读到源文件，可使用l或list命令，看看gdb是否能列出源代码。

在gdb中，运行程序使用r或是run命令。程序的运行，你有可能需要设置下面四方面的事。
1. 程序运行参数。
#+BEGIN_EXAMPLE
set args 可指定运行时参数。（如：set args 10 20 30 40 50 ）
show args 命令可以查看设置好的运行参数。
#+END_EXAMPLE
2. 运行环境。
#+BEGIN_EXAMPLE
path  可设定程序的运行路径。
show paths 查看程序的运行路径。
set env environmentVarname=value 设置环境变量。如：set env USER=benben
show env [varname] 查看环境变量，不带varname，打印出当前所有环境变量。
#+END_EXAMPLE
3. 工作目录。
#+BEGIN_EXAMPLE
cd 相当于shell的cd命令。
pwd 显示当前的所在目录。
#+END_EXAMPLE
4. 程序的输入输出。
#+BEGIN_EXAMPLE
info terminal 显示你程序用到的终端的模式。
使用重定向控制程序输出。如：run > outfile
tty命令可以设置输入输出使用的终端设备。如：tty /dev/tty1
#+END_EXAMPLE
** 调试已运行的程序
两种方法：
1. 在UNIX下用ps查看正在运行的程序的PID（进程ID），然后用gdb PID process-id 格式挂接正在运行的程序。
2. 先用gdb 关联上源代码，并进行gdb，在gdb中用attach process-id 命令来挂接进程的PID。并用detach来取消挂接的进程。
** 暂停 / 恢复程序运行
调试程序中，暂停程序运行是必须的，GDB可以方便地暂停程序的运行。你可以设置程序的在哪行停住，在什么条件下停住，在收到什么信号时停往等等。以便于你查看运行时的变量，以及运行时的流程。

当进程被gdb停住时，你可以使用info program 来查看程序的是否在运行，进程号，被暂停的原因。

在gdb中，我们可以有以下几种暂停方式：断点（BreakPoint）、观察点（WatchPoint）、捕捉点（CatchPoint）、信号（Signals）、线程停止（Thread Stops）。如果要恢复程序运行，可以使用c或是continue命令。
*** 设置断点（BreakPoint）
我们用break命令来设置断点。正面有几点设置断点的方法：
#+BEGIN_EXAMPLE
break function
在进入指定函数时停住。C++中可以使用class::function或function(type,type)格式来指定函数名。

break linenum
在指定行号停住。

break +offset
break -offset
在当前行号的前面或后面的offset行停住。offset为自然数。

break filename:linenum
在源文件filename的linenum行处停住。

break filename:function
在源文件filename的function函数的入口处停住。

break *address
在程序运行的内存地址处停住。

break
break命令没有参数时，表示在下一条指令处停住。

break ... if cond
...可以是上述的参数，condition表示条件，在条件成立时停住。比如在循环境体中，可以设置break if i=100，表示当i为100时停住程序。
#+END_EXAMPLE
查看断点时，可使用info命令，如下所示：（注：n表示断点号）
#+BEGIN_EXAMPLE
info breakpoints [n]
info break [n]
info watchpoints [n]
#+END_EXAMPLE
*** 设置观察点（WatchPoint）
观察点一般来观察某个表达式（变量也是一种表达式）的值是否有变化了，如果有变化，马上停住程序。我们有下面的几种方法来设置观察点：
#+BEGIN_EXAMPLE
watch expr
为表达式（变量）expr设置一个观察点。一量表达式值有变化时，马上停住程序。

rwatch expr
当表达式（变量）expr被读时，停住程序。

awatch expr
当表达式（变量）的值被读或被写时，停住程序。

info watchpoints
查看观察点、断点和捕捉点信息，同info break 一样.
#+END_EXAMPLE
*** 设置捕捉点（CatchPoint）

你可设置捕捉点来补捉程序运行时的一些事件。如：载入共享库（动态链接库）或是C++的异常。设置捕捉点的格式为：
#+BEGIN_EXAMPLE
catch event
#+END_EXAMPLE
当event发生时，停住程序。event可以是下面的内容：
- throw 一个C++抛出的异常。（throw为关键字）
- catch 一个C++捕捉到的异常。（catch为关键字）
- exec 调用系统调用exec时。（exec为关键字，目前此功能只在HP-UX下有用）
- fork 调用系统调用fork时。（fork为关键字，目前此功能只在HP-UX下有用）
- vfork 调用系统调用vfork时。（vfork为关键字，目前此功能只在HP-UX下有用）
- load 或 load 载入共享库（动态链接库）时。（load为关键字，目前此功能只在HP-UX下有用）
- unload 或 unload 卸载共享库（动态链接库）时。（unload为关键字，目前此功能只在HP-UX下有用）

#+BEGIN_EXAMPLE
tcatch event
只设置一次捕捉点，当程序停住以后，应点被自动删除。
#+END_EXAMPLE
*** 维护停止点
上面说了如何设置程序的停止点，GDB中的停止点也就是上述的三类。在GDB中，如果你觉得已定义好的停止点没有用了，你可以使用delete、clear、disable、enable这几个命令来进行维护。
#+BEGIN_EXAMPLE
clear
清除所有的已定义的停止点。

clear function
清除所有设置在函数上的停止点。

clear linenum
清除所有设置在指定行上的停止点。
clear filename:linenum
清除所有设置在指定文件：指定行上的停止点。

delete [breakpoints] [range...]
删除指定的断点，breakpoints为断点号。如果不指定断点号，则表示删除所有的断点。range 表示断点号的范围（如：3-7）。其简写命令为d。

比删除更好的一种方法是disable停止点，disable了的停止点，GDB不会删除，当你还需要时，enable即可，就好像回收站一样。

disable [breakpoints] [range...]
disable所指定的停止点，breakpoints为停止点号。如果什么都不指定，表示disable所有的停止点。简写命令是dis.

enable [breakpoints] [range...]
enable所指定的停止点，breakpoints为停止点号。

enable [breakpoints] once range...
enable所指定的停止点一次，当程序停止后，该停止点马上被GDB自动disable。

enable [breakpoints] delete range...
enable所指定的停止点一次，当程序停止后，该停止点马上被GDB自动删除。
#+END_EXAMPLE
*** 停止条件维护
前面在说到设置断点时，我们提到过可以设置一个条件，当条件成立时，程序自动停止，这是一个非常强大的功能，这里，我想专门说说这个条件的相关维护命令。一般来说，为断点设置一个条件，我们使用if关键词，后面跟其断点条件。并且，条件设置好后，我们可以用condition命令来修改断点的条件。（只有 break和watch命令支持if，catch目前暂不支持if）
#+BEGIN_EXAMPLE
condition bnum expression
修改断点号为bnum的停止条件为expression。

condition bnum
清除断点号为bnum的停止条件。
#+END_EXAMPLE
还有一个比较特殊的维护命令ignore，你可以指定程序运行时，忽略停止条件几次。
#+BEGIN_EXAMPLE
ignore bnum count
表示忽略断点号为bnum的停止条件count次。
#+END_EXAMPLE
*** 为停止点设定运行命令
我们可以使用GDB提供的command命令来设置停止点的运行命令。也就是说，当运行的程序在被停止住时，我们可以让其自动运行一些别的命令，这很有利行自动化调试。对基于GDB的自动化调试是一个强大的支持。
#+BEGIN_EXAMPLE
commands [bnum]
... command-list ...
end
#+END_EXAMPLE
为断点号bnum指写一个命令列表。当程序被该断点停住时，gdb会依次运行命令列表中的命令。

例如：
#+BEGIN_EXAMPLE
break foo if x>0
commands
printf "x is %d ",x
continue
end
#+END_EXAMPLE
断点设置在函数foo中，断点条件是x>0，如果程序被断住后，也就是，一旦x的值在foo函数中大于0，GDB会自动打印出x的值，并继续运行程序。

如果你要清除断点上的命令序列，那么只要简单的执行一下commands命令，并直接在打个end就行了。
*** 断点菜单
在C++中，可能会重复出现同一个名字的函数若干次（函数重载），在这种情况下，break 不能告诉GDB要停在哪个函数的入口。当然，你可以使用break 也就是把函数的参数类型告诉GDB，以指定一个函数。否则的话，GDB会给你列出一个断点菜单供你选择你所需要的断点。你只要输入你菜单列表中的编号就可以了。如：
#+begin_src bash
(gdb) b String::after
[0] cancel
[1] all
[2] file:String.cc; line number:867
[3] file:String.cc; line number:860
[4] file:String.cc; line number:875
[5] file:String.cc; line number:853
[6] file:String.cc; line number:846
[7] file:String.cc; line number:735
> 2 4 6
Breakpoint 1 at 0xb26c: file String.cc, line 867.
Breakpoint 2 at 0xb344: file String.cc, line 875.
Breakpoint 3 at 0xafcc: file String.cc, line 846.
Multiple breakpoints were set.
Use the "delete" command to delete unwanted breakpoints.
(gdb)
#+END_SRC
*** 恢复程序运行和单步调试
当程序被停住了，你可以用continue命令恢复程序的运行直到程序结束，或下一个断点到来。也可以使用step或next命令单步跟踪程序。
#+BEGIN_EXAMPLE
continue [ignore-count]
c [ignore-count]
fg [ignore-count]
#+END_EXAMPLE
恢复程序运行，直到程序结束，或是下一个断点到来。ignore-count表示忽略其后的断点次数。continue，c，fg三个命令都是一样的意思。
#+BEGIN_EXAMPLE
step [count]
单步跟踪，如果有函数调用，他会进入该函数。进入函数的前提是，此函数被编译有debug信息。很像VC等工具中的step in。后面可以加count也可以不加，不加表示一条条地执行，加表示执行后面的count条指令，然后再停住。

next [count]
同样单步跟踪，如果有函数调用，他不会进入该函数。很像VC等工具中的step over。后面可以加count也可以不加，不加表示一条条地执行，加表示执行后面的count条指令，然后再停住。

set step-mode on
打开step-mode模式，于是，在进行单步跟踪时，程序会因为没有debug信息而停住。这个参数有很利于查看机器码。

set step-mod off
关闭step-mode模式。This is the default.

show step-mode
Show whether gdb will stop in or step over functions without source line debug information.

finish
运行程序，直到当前函数完成返回。并打印函数返回时的堆栈地址和返回值及参数值等信息。

until 或 u
当你厌倦了在一个循环体内单步跟踪时，这个命令可以运行程序直到退出循环体。

until location
u location
Continue running your program until either the specified location is reached,or the current stack frame returns. location is any of the forms of argument acceptable to break. This form of the command uses breakpoints, and hence is quicker than until without an argument. The specified location is actually reached only if it is in the current frame. This implies that until can be used to skip over recursive function invocations.
#+END_EXAMPLE
For instance in the code below, if the current location is line 96, issuing until 99 will execute the program up to 
#+BEGIN_SRC 
int factorial (int value)
{
if (value &gt; 1) {
value *= factorial (value - 1);
}
return (value);
}
#+END_SRC
#+BEGIN_EXAMPLE
stepi 或 si 或 stepi repeatCount

单步跟踪一条机器指令！一条程序代码有可能由数条机器指令完成，stepi可以单步执行机器指令。It is often useful to do ‘display/i $pc’ when stepping by machine instructions. This makes gdb automatically display the next instruction to be executed, each time your program stops.

An argument is a repeat count, as in step.

nexti
nexti repeatCount
ni
Execute one machine instruction, but if it is a function call, proceed until the function returns.An argument is a repeat count, as in next.
#+END_EXAMPLE

** 打印所有全局变量和局部变量
#+begin_src bash
#查看全局和静态变量
info variables

#查看当前stack frame局部变量
info locals

#查看当前stack frame参数
info args
#+END_SRC

** 函数的调用
#+begin_src bash
call name 调用和执行一个函数
(gdb) call gen_and_sork( 1234,1,0 )
(gdb) call printf(“abcd”)
#+END_SRC

** 寄存器的标准名字
- $pc 程序计数器
- $fp 帧指针（当前堆栈帧）
- $sp 栈指针
- $ps 处理器状态
** 查看当前位置 - where
** layout的使用
layout用于分割窗口，可以一边查看代码，一边测试。主要有以下几种用法：
- layout src：显示源代码窗口
- layout asm：显示汇编窗口
- layout regs：显示源代码/汇编和寄存器窗口
- layout split：显示源代码和汇编窗口
- layout next：显示下一个layout
- layout prev：显示上一个layout
- Ctrl + L：刷新窗口
- Ctrl + x，再按1：单窗口模式，显示一个窗口
- Ctrl + x，再按2：双窗口模式，显示两个窗口
- Ctrl + x，再按a：回到传统模式，即退出layout，回到执行layout之前的调试窗口。

** print和display命令：查看变量的值
*** print
打印变量或者表达式的值，print 命令可以缩写为 p

#+begin_src bash
p *argv    # 打印数组argv首个元素
p *argv@2  # 打印数组前两个元素
p *argv@argc  # 打印数组所有元素
#+END_SRC

*** display
使用 display 命令查看变量或表达式的值，每当程序暂停执行（例如单步执行）时，GDB 调试器都会自动帮我们打印出来，而 print 命令则不会。

display 命令没有缩写形式，常用的语法格式如下 2 种：
#+begin_src bash
(gdb) display expr
(gdb) display/fmt expr
#+END_SRC
其中，expr 表示要查看的目标变量或表达式；参数 fmt 用于指定输出变量或表达式的格式，下表罗列了常用的一些 fmt 参数。
| /fmt | 功 能                                |
|------+--------------------------------------|
| /x   | 以十六进制的形式打印出整数。         |
| /d   | 以有符号、十进制的形式打印出整数。   |
| /u   | 以无符号、十进制的形式打印出整数。   |
| /o   | 以八进制的形式打印出整数。           |
| /t   | 以二进制的形式打印出整数。           |
| /f   | 以浮点数的形式打印变量或表达式的值。 |
| /c   | 以字符形式打印变量或表达式的值。     |

注意，display 命令和 /fmt 之间不要留有空格。以 /x 为例，应写为 (gdb)display/x expr。
*** undisplay：取消追踪观察变量
*** 参考文章
[[http://c.biancheng.net/view/8238.html][GDB print和display命令：查看变量的值]]
** si,ni,s,n的区别
n/s都是C语言级的断点定位。 s会进入C函数内部,但是不会进入没有定位信息的函数（比如没有加-g编译的代码，因为其没有C代码的行数标记，没办法定位），n不会。

ni/si都是汇编级别的断点定位。si会进入汇编和C函数内部,ni不会。


归纳:当要进入没有调试信息的库函数调试的时候，用si是唯一的方法。

当进入有调试信息的函数，用si和s都可以，但是他们不同，si是定位到汇编级别的第一个语句，但是s是进入到C级别的第一个语句
** 查看函数调用栈
查看调用栈信息：
- backtrace：查看函数调用的顺序（函数调用栈的信息），可以用bt缩写
- backtrace n: 显示程序的调用栈信息，只显示栈顶n桢(frame)
- backtrace –n: 显示程序的调用栈信息，只显示栈底部n桢(frame)
- set backtrace limit n: 设置bt显示的最大桢层
- where, info stack：都是bt的别名，功能一样

查看桢信息：
- frame n: 查看第n桢的信息，切换到栈编号为n的上下文中,frame可以用f缩写
- frame addr: 查看pc地址为addr的桢的相关信息
- up n: 查看当前桢上面第n桢的信息
- down n: 查看当前桢下面第n桢的信息

查看更加详细的信息：
- info frame、info frame n或者info frame addr ：查看当前函数调用的栈帧信息数
- info args：查看当前桢中的参数
- info locals：查看当前桢中的局部变量
- info catch：查看当前桢中的异常处理器（exception handlers

** watch：被设置观察点的变量发生修改时，打印显示
i watch：显示观察点
** examine命令
examine命令缩写为x

用于显示内存内容或地址。

格式：
x/<n/f/u>  <addr>

n:是正整数，表示需要显示的内存单元的个数，即从当前地址向后显示n个内存单元的内容，
一个内存单元的大小由第三个参数u定义。

f:表示addr指向的内存内容的输出格式，s对应输出字符串，此处需特别注意输出整型数据的格式：
  x 按十六进制格式显示变量.
  d 按十进制格式显示变量。
  u 按十进制格式显示无符号整型。
  o 按八进制格式显示变量。
  t 按二进制格式显示变量。
  a 按十六进制格式显示变量。
  c 按字符格式显示变量。
  f 按浮点数格式显示变量。

u:就是指以多少个字节作为一个内存单元-unit,默认为4。u还可以用被一些字符表示:
  如b=1 byte, h=2 bytes,w=4 bytes,g=8 bytes.

<addr>:表示内存地址。

#+begin_src bash
>>> x/s argv
>>> x/d argv+6
>>> x/d argv+8
>>> x/2c $a1
#+END_SRC
** tui界面
TUI（TextUser Interface）为GDB调试的文本用户界面，可以方便地显示源代码、汇编和寄存器文本窗口

Tui界面可以通过运行gdbtui或gdb-tui命令进入(其它变种gdb也一样，如arm-none-eabi-gdb-tui)，当然也可以进入gdb界面后使用TUI快捷键打开，如C-x C-a快捷键
*** TUI Overview
在TUI模式中，可以显示以下几个窗口：
- 命令窗口:
用于 GDB调试时的命令输入和命令结果输出显示，与普通 GDB窗口无异。
- 源代码窗口:
用于显示程序源代码，包括当前运行行、中断以中断标识等。
- 汇编窗口:
显示当前程序的汇编代码。
- 寄存器窗口:
显示处理器的寄存器内容，当寄存器内容发生改变时会高亮显示。

源代码窗口和汇编窗口会高亮显示程序运行位置并以'>'符号标记。有两个特殊标记用于标识断点，第一个标记用于标识断点类型：
- B:程序至少有一次运行到了该断点
- b:程序没有运行到过该断点
- H:程序至少有一次运行到了该硬件断点
- h:程序没有运行到过该硬件断点 

第二个标记用于标识断点使能与否:
- +:断点使能 Breakpointis enabled. 
- -:断点被禁用 Breakpointis disabled. 

在命令窗口上方有一行状态栏，显示效果如下图所示，主要显示内容有：
#+DOWNLOADED: screenshot @ 2022-05-06 16:28:13
[[file:images/linux%E7%AC%94%E8%AE%B0/gdb/2022-05-06_16-28-13_screenshot.png]]
- target:
Indicates the current GDB target. (see Specifying a Debugging Target).

- process
Gives the current process or thread number. When no process is being debugged, this field is set to No process.

- function
Gives the current function name for the selected frame. The name is demangled if demangling is turned on (see Print Settings). When there is no symbol corresponding to the current program counter, the string ?? is displayed.

- line
Indicates the current line number for the selected frame. When the current line number is not known, the string ?? is displayed.

- pc
Indicates the current program counter address.
*** TUI 快捷键
The following key bindings are installed for both TUI mode and the GDB standard mode.

C-x C-a 或
C-x a  或
C-x A (按住Ctrl+x后松开再按a，以下快捷键操作方式相同)
进入或退出 TUI模式。

C-x 1
使 TUI只显示一个窗口。 Usea TUI layout with only one window. The layout will either be`source' or `assembly'.When the TUI mode is not active, it will switch to the TUI mode.Think of this key binding as the Emacs C-x 1binding. 

C-x 2
使TUI显示两个窗口，连接使用此快捷键可在三种窗口组合(只能同时显示两个，共3种组合)中不断切换。
Usea TUI layout with at least two windows. When the current layoutalready has two windows, the next layout with two windows is used.When a new layout is chosen, one window will always be common to theprevious layout and the new one. Think of it as the Emacs C-x2 binding. 

C-x o
更换激活窗口
Changethe active window. The TUI associates several key bindings (likescrolling and arrow keys) with the active window. This command givesthe focus to the next TUI window. Think of it as the Emacs C-xo binding. 

C-x s
在 TUI模式和 TUISingleKey模式之间切换
Switchin and out of the TUI SingleKey mode that binds single keys to GDBcommands (see section 22.3TUI Single Key Mode). 

下列快捷键只在TUI模式才能有效：

PgUp
激活窗口的内容向上滚动一页 Scroll the active window one page up. 

PgDn
激活窗口的内容向下滚动一页 Scroll the active window one page down. 

Up
激活窗口的内容向上滚动一行 Scroll the active window one line up. 

Down
激动窗口的内容向下滚动一行 Scroll the active window one line down. 

Left
激活窗口的内容向左移动一列 Scroll the active window one column left. 

Right
激活窗口的内容向右移动一列 Scroll the active window one column right. 

C-L
更新屏幕 Refresh the screen. 

当源代码和汇编窗口同时显示时，以上快捷键会同步更新两个窗口的内容。

Becausethe arrow keys scroll the active window in the TUI mode, they are notavailable for their normal use by readline unless the command windowhas the focus. When another window is active, you must use otherreadline key bindings such as C-p, C-n,C-b and C-f to control the command window. 
*** TUI Single Key Mode
The TUI also provides a SingleKey mode, which binds several frequently used GDB commands to single keys. Type C-x s to switch into this mode, where the following key bindings are used:

c
continue

d
down

f
finish

n
next

o
nexti. The shortcut letter ‘o’ stands for “step Over”.

q
exit the SingleKey mode.

r
run

s
step

i
stepi. The shortcut letter ‘i’ stands for “step Into”.

u
up

v
info locals

w
where

Other keys temporarily switch to the GDB command prompt. The key that was pressed is inserted in the editing buffer so that it is possible to type most GDB commands without interaction with the TUI SingleKey mode. Once the command is entered the TUI SingleKey mode is restored. The only way to permanently leave this mode is by typing q or C-x s.

If GDB was built with Readline 8.0 or later, the TUI SingleKey keymap will be named ‘SingleKey’. This can be used in .inputrc to add additional bindings to this keymap.
*** TUI-specific Commands 
TheTUI has specific commands to control the text windows. These commandsare always available, even when GDB is not in the TUI mode. When GDBis in the standard mode, most of these commands will automaticallyswitch to the TUI mode. 当处理GDB标准模式时，下列的大多数命令会自动切换到TUI模式。

tui enable
Activate TUI mode. The last active TUI window layout will be used if TUI mode has previously been used in the current debugging session, otherwise a default layout is used.

tui disable
Disable TUI mode, returning to the console interpreter.

info win：显示正在显示的窗口大小信息
Listand give the size of all displayed windows. 、

tui new-layout name window weight [window weight…]
#+BEGIN_EXAMPLE
Create a new TUI layout. The new layout will be named name, and can be accessed using the layout command (see below).

Each window parameter is either the name of a window to display, or a window description. The windows will be displayed from top to bottom in the order listed.

The names of the windows are the same as the ones given to the focus command (see below); additional, the status window can be specified. Note that, because it is of fixed height, the weight assigned to the status window is of no importance. It is conventional to use ‘0’ here.

A window description looks a bit like an invocation of tui new-layout, and is of the form {[-horizontal]window weight [window weight…]}.

This specifies a sub-layout. If -horizontal is given, the windows in this description will be arranged side-by-side, rather than top-to-bottom.

Each weight is an integer. It is the weight of this window relative to all the other windows in the layout. These numbers are used to calculate how much of the screen is given to each window.

For example:

(gdb) tui new-layout example src 1 regs 1 status 0 cmd 1
Here, the new layout is called ‘example’. It shows the source and register windows, followed by the status window, and then finally the command window. The non-status windows all have the same weight, so the terminal will be split into three roughly equal sections.

Here is a more complex example, showing a horizontal layout:

(gdb) tui new-layout example {-horizontal src 1 asm 1} 2 status 0 cmd 1
This will result in side-by-side source and assembly windows; with the status and command window being beneath these, filling the entire width of the terminal. Because they have weight 2, the source and assembly windows will be twice the height of the command window.
#+END_EXAMPLE

layout next：显示下一个窗口
Displaythe next layout. 

layout prev：显示上一个窗口
Displaythe previous layout. 

layout src：显示源代码窗口
Displaythe source window only. 

layout asm：显示汇编窗口
Displaythe assembly window only. 

layout split：显示源代码和汇编窗口
Displaythe source and assembly window. 

layout regs：显示寄存器窗口
Displaythe register window together with the source or assembly window. 

focus next：将一个窗口置为激活状态
Make the next window active for scrolling. 

focus prev：将上一个窗口置为激活状态
Make the previous window active for scrolling. 

focus src：将源代码窗口置为激活状态
Make the source window active for scrolling. 

focus asm：将汇编窗口置为激活状态
Make the assembly window active for scrolling. 

focus regs：将寄存器窗口置为激活状态
Make the register window active for scrolling. 

focus cmd：将命令行窗口置为激活状态
Make the command window active for scrolling. 

refresh：更新窗口，与 C-L快捷键同
Refresh the screen. This is similar to typing C-L.

tui reg float：寄存器窗口显示内容为浮点寄存器
Showthe floating point registers in the register window. 

tui reg general：寄存器窗口显示内容为普通寄存器
Show the general registers in the register window. 

tui reg next：显示下一组寄存器
Show the next register group. The list of register groups as well astheir order is target specific. The predefined register groups are the following:

 general, float,system, vector,all, save,restore. 

tui reg system ：显示上一组寄存器
Show the system registers in the register window. 

update ：更新源代码窗口到当前运行点
Update the source window and the current execution point. 

winheight winname +count：增加指定窗口的高度 
winheight winname -count：减小指定窗口的高度
Change the height of the window name by count lines. Positive counts increase the height, while negative counts decrease it. The name parameter can be one of src (the source window), cmd (the command window), asm (the disassembly window), or regs (the register display window).
*** TUI Configuration Variables
Several configuration variables control the appearance of TUI windows.

set tui border-kind kind
Select the border appearance for the source, assembly and register windows. The possible values are the following:

space
Use a space character to draw the border.

ascii
Use ASCII characters ‘+’, ‘-’ and ‘|’ to draw the border.

acs
Use the Alternate Character Set to draw the border. The border is drawn using character line graphics if the terminal supports them.

set tui border-mode mode
set tui active-border-mode mode
Select the display attributes for the borders of the inactive windows or the active window. The mode can be one of the following:

normal
Use normal attributes to display the border.

standout
Use standout mode.

reverse
Use reverse video mode.

half
Use half bright mode.

half-standout
Use half bright and standout mode.

bold
Use extra bright or bold mode.

bold-standout
Use extra bright or bold and standout mode.

set tui tab-width nchars
Set the width of tab stops to be nchars characters. This setting affects the display of TAB characters in the source and assembly windows.

set tui compact-source [on|off]
Set whether the TUI source window is displayed in “compact” form. The default display uses more space for line numbers and starts the source text at the next tab stop; the compact display uses only as much space as is needed for the line numbers in the current file, and only a single space to separate the line numbers from the source.

Note that the colors of the TUI borders can be controlled using the appropriate set style commands. See Output Styling.
*** 参考文章
[[https://sourceware.org/gdb/onlinedocs/gdb/TUI.html#TUI][25.GDB Text User Interface]]
[[https://blog.csdn.net/xu415/article/details/19021759][GDB调试之TUI界面 - CSDN博客]]
** apropos
apropos regexp命令查找所有符合regexp正则表达式的命令信息：
#+begin_src bash
(gdb) apropos set
awatch -- Set a watchpoint for an expression
b -- Set breakpoint at specified line or function
br -- Set breakpoint at specified line or function
bre -- Set breakpoint at specified line or function
brea -- Set breakpoint at specified line or function
......
#+END_SRC

** 
代码的虚拟地址是在编译(链接)期生成的，而代码编译后的结果一般是一个ELF(Executable Linkable Format)文件。ELF文件记录了我们代码中每个函数的虚拟地址，此外还会有一些其他有助于我们的信息。
我们可以使用指令查看一下user/_sleep这个ELF文件的格式。
新开一个终端，输入命令readelf -a user/_sleep

b *0xde6
print $pc
info reg 打印全部32个用户寄存器
print/x $satp 查看satp寄存器
x/3i 0xde4
* grep命令
Linux grep 命令用于查找文件里符合条件的字符串。

grep 指令用于查找内容包含指定的范本样式的文件，如果发现某文件的内容符合所指定的范本样式，预设 grep 指令会把含有范本样式的那一列显示出来。若不指定任何文件名称，或是所给予的文件名为 -，则 grep 指令会从标准输入设备读取数据。

语法：
#+begin_src bash
grep [-abcEFGhHilLnqrsvVwxy][-A<显示行数>][-B<显示列数>][-C<显示列数>][-d<进行动作>][-e<范本样式>][-f<范本文件>][--help][范本样式][文件或目录...]
#+END_SRC
参数：
- -a 或 --text : 不要忽略二进制的数据。
- -A<显示行数> 或 --after-context=<显示行数> : 除了显示符合范本样式的那一列之外，并显示该行之后的内容。
- -b 或 --byte-offset : 在显示符合样式的那一行之前，标示出该行第一个字符的编号。
- -B<显示行数> 或 --before-context=<显示行数> : 除了显示符合样式的那一行之外，并显示该行之前的内容。
- -c 或 --count : 计算符合样式的列数。
- -C<显示行数> 或 --context=<显示行数>或-<显示行数> : 除了显示符合样式的那一行之外，并显示该行之前后的内容。
- -d <动作> 或 --directories=<动作> : 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep指令将回报信息并停止动作。
- -e<范本样式> 或 --regexp=<范本样式> : 指定字符串做为查找文件内容的样式。
- -E 或 --extended-regexp : 将样式为延伸的正则表达式来使用。
- -f<规则文件> 或 --file=<规则文件> : 指定规则文件，其内容含有一个或多个规则样式，让grep查找符合规则条件的文件内容，格式为每行一个规则样式。
- -F 或 --fixed-regexp : 将样式视为固定字符串的列表。
- -G 或 --basic-regexp : 将样式视为普通的表示法来使用。
- -h 或 --no-filename : 在显示符合样式的那一行之前，不标示该行所属的文件名称。
- -H 或 --with-filename : 在显示符合样式的那一行之前，表示该行所属的文件名称。
- -i 或 --ignore-case : 忽略字符大小写的差别。
- -l 或 --file-with-matches : 列出文件内容符合指定的样式的文件名称。
- -L 或 --files-without-match : 列出文件内容不符合指定的样式的文件名称。
- -n 或 --line-number : 在显示符合样式的那一行之前，标示出该行的列数编号。
- -o 或 --only-matching : 只显示匹配PATTERN 部分。
- -q 或 --quiet或--silent : 不显示任何信息。
- -r 或 --recursive : 此参数的效果和指定"-d recurse"参数相同。
- -s 或 --no-messages : 不显示错误信息。
- -v 或 --invert-match : 显示不包含匹配文本的所有行。
- -V 或 --version : 显示版本信息。
- -w 或 --word-regexp : 只显示全字符合的列。
- -x --line-regexp : 只显示全列符合的列。
- -y : 此参数的效果和指定"-i"参数相同。

* id命令
Linux id命令用于显示用户的ID，以及所属群组的ID。

id会显示用户以及所属群组的实际与有效ID。若两个ID相同，则仅显示实际ID。若仅指定用户名称，则显示目前用户的ID。

语法：id [-gGnru][--help][--version][用户名称]
参数说明：
- -g或--group 　显示用户所属群组的ID。
- -G或--groups 　显示用户所属附加群组的ID。
- -n或--name 　显示用户，所属群组或附加群组的名称。
- -r或--real 　显示实际ID。
- -u或--user 　显示用户ID。
- -help 　显示帮助。
- -version 　显示版本信息。
** 实例
显示当前用户信息
#+begin_src bash
>>>id //显示当前用户ID
uid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel) context=root:system_r:unconfined_t
#+END_SRC
显示用户群组的ID
#+begin_src bash
>>>id -g
0
#+END_SRC
显示所有群组的ID
#+begin_src bash
>>>id -g
0 1 2 3 4 5 6 10
#+END_SRC
显示指定用户信息
#+begin_src bash
>>>id hnlinux
#+END_SRC
** 参考文章
[[https://www.runoob.com/linux/linux-comm-id.html][Linux id命令 - 菜鸟教程]]
* jupyter notebook
** 使用Anaconda安装
#+BEGIN_SRC bash
conda install jupyter notebook
#+END_SRC
** 使用pip命令安装
#+BEGIN_SRC python
pip3 install jupyter #python3.x
pip install jupyter	#python2.x
#+END_SRC

6.sudo fdisk -l，/dev/sda1空间从40G扩展到了58G
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20180117102533147.png @ 2020-06-02 19:29:31
[[file:jupyter_notebook/2020-06-02_19-29-31_20180117102533147.png]]

** 切换conda环境的方法
切换到想要的环境，比如说adda
~conda activate adda~
安装ipykernel：
~conda install ipykernel~
添加kernel进jupyter notebook：
~python -m ipykernel install --user --name [虚拟环境名] --display-name "kernel命名"~
如： ~python -m ipykernel install --name adda~

执行完这个语句之后，会自动在目录【C:\ProgramData\jupyter\kernels】(类似)生成一个【adda】文件夹，里面有kernel.json文件

现在打开jupyter notebook，里面就会显示有这个虚拟环境了

选择conda环境新建文件
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20200304105902203.png @ 2020-10-29 09:25:50

[[file:jupyter_notebook/2020-10-29_09-25-50_20200304105902203.png]]

此时，就可以看到创建的Python[conda env:tf-gpu]了，选择该kernel运行即可
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20200304105932199.png @ 2020-10-29 09:25:56
[[file:jupyter_notebook/2020-10-29_09-25-56_20200304105932199.png]]

** 配置jupyter notebook
生成配置文件： ~jupyter notebook --generate-config~

设置密码： ~jupyter notebook password~

修改配置文件: ~vim ~/.jupyter/jupyter_notebook_config.py~

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20200805124102787.png @ 2020-10-29 09:35:59
[[file:jupyter_notebook/2020-10-29_09-35-59_20200805124102787.png]]

** 远程访问jupyter notebook
通常情况下,打开 jupyter notebook即从本地地址localhost:8888打开jupyter notebook.

如果希望远程操控jupyter notebook,则需要进行一些设置.

*** 1. 检查配置文件是否存在

首相必须确认jupyter notebook 的配置文件 =jupyter_notebook_config.py= 是否存在.

不同系统的默认配置文件路径如下:

- Windows: =C:\Users\USERNAME\.jupyter\jupyter_notebook_config.py=
- OS X: =/Users/USERNAME/.jupyter/jupyter_notebook_config.py=
- Linux: =/home/USERNAME/.jupyter/jupyter_notebook_config.py=

如果系统上没有Jupyter 文件夹或者Jupyter 文件夹里没有配置文件,那么必须执行以下命令生成配置文件:
#+BEGIN_SRC bash
jupyter notebook --generate-config
#+END_SRC

这个命令会创建Jupyter文件夹并在文件夹内生成配置文件 =jupyter_notebook_config.py=

*** 2.生成密码

**** 2.1生成访问密码

从 jupyter notebook 5.0 版本开始,我们就可以通过自动方式生成访问密码.

设置访问密码的命令为 =jupyter notebook password= ，设置后的访问密码存储在 =jupyter_notebook_config.json= 里面。

#+BEGIN_SRC bash
> jupyter notebook password
Enter password:  ****
Verify password: ****
[NotebookPasswordApp] Wrote hashed password to /Users/you/.jupyter/jupyter_notebook_config.json
#+END_SRC


**** 2.2 生成hash密码

如果没有hash密码，那么我们每次通过浏览器远程访问Jupyter时，都需要输入一次密码。如果设置了hash密码，那么我们只需要在首次远程访问jupyter的时候输入一次密码，之后再次访问jupyter的时候就不用重复输入密码了。
在终端输入 =ipython= ，进入ipython环境后输入下列代码。

#+BEGIN_SRC bash
In [1]: from notebook.auth import passwd
In [2]: passwd()
Enter password:
Verify password:
Out[2]: 'sha1:67c9e60bb8b6:9ffede0825894254b2e042ea597d771089e11aed'
#+END_SRC

这里输出的就是hash密码，后面的操作需要用到这个hash密码，所以需要将其复制下来。

输入 =exit= 退出ipython环境。

*** 3.修改配置文件

打开 =jupyter_notebook_config.py= 文件,可以看到里面很多注释行。如果我们要修改 =jupyter_notebook_config.py= 里的某一行，必须先把行首的 =#= 去掉。

找到 =#c.NotebookApp.password = ' '= 这一行，将注释去掉，并修改为
 =c.NotebookApp.password = u'sha1:67c9e60bb8b6:9ffede0825894254b2e042ea597d771089e11aed`=

这里将之前复制的hash密码填入即可，注意密码前面的 =u= 不可省略，其作用是提示Python编译器，要按照给定的方式来解析引号中的字符串。

=c.NotebookApp.allow_remote_access = True= 将默认值False修改为True，表示允许外部访问。

=c.NotebookApp.ip = '*'= 这里的 =*= 表示允许所有IP皆可访问

=c.NotebookApp.open_browser = False= 禁止自动打开浏览器

=c.NotebookApp.notebook_dir = '/eswai/jupyter'= 这里可修改在浏览器打开jupyter notebook后的工作目录。

=c.NotebookApp.port = 9999= 设置一个固定的notebook服务会监听的IP端口（这里设置为9999），这个值可以任意，只要保证不和其他已经启用的端口号冲突即可，也可以不修改，默认为8888。

修改完成后在终端输入 =jupyter notebook= 命令，这样确保Jupyter重新加载jupyter_notebook_config.py，进而使得新配置起效。

之后我们只要在任意浏览器地址栏输入 =主机ip：9999= 即可远程登录jupyter notebook了。

如果是服务器上docker容器内的jupyter notebook，那么浏览器地址栏应该输入 =宿主机ip:宿主机端口=

这里的宿主机端口是创建容器时分配的宿主机端口，比如你创建容器时使用的端口映射参数为： =-p 8002:9999= ，那么远程登录地址为 =宿主机ip:8002= .

参考文档：

1. [[https://jupyter-notebook.readthedocs.io/en/latest/public_server.html][官方英文指南]]

2. [[https://www.jianshu.com/p/444c3ae23035][设置 jupyter notebook 可远程访问]]

3. [[https://blog.csdn.net/eswai/article/details/79437428][利用Docker环境配置jupyter notebook服务器]]

4. [[https://zhuanlan.zhihu.com/p/64524822][如何设置远程访问的Jupyter Notebook服务器-04（服务器篇）]]

* Linux 任务前后台的切换
Shell支持作用控制，有以下命令实现前后台切换：
1. command& 让进程在后台运行
2. jobs 查看后台运行的进程
3. fg %n 让后台运行的进程n到前台来
4. bg %n 让进程n到后台去
5. kill %n 杀死job

PS:"n"为jobs命令查看到的job编号，不是进程编号.

fg、bg、jobs、&、ctrl + z都是跟系统任务有关的，虽然现在基本上不怎么需要用到这些命令，但学会了也是很实用的.

& 最经常被用到,这个用在一个命令的最后，可以把这个命令放到后台执行
 
ctrl + z,可以将一个正在前台执行的命令放到后台，并且暂停

jobs,查看当前有多少在后台运行的命令
 
fg,将后台中的命令调至前台继续运行,如果后台中有多个命令，可以用 fg %jobnumber将选中的命令调出，%jobnumber是通过jobs命令查到的后台正在执行的命令的序号(不是pid)。

bg,将一个在后台暂停的命令，变成继续执行,如果后台中有多个命令，可以用bg %jobnumber将选中的命令调出，%jobnumber是通过jobs命令查到的后台正在执行的命令的序号(不是pid)。
 
** 实例：
假设你发现前台运行的一个程序需要很长的时间，但是需要干其他的事情，你就可以用 Ctrl-Z ，终止这个程序，然后可以看到系统提示：
~[1]+ Stopped /root/bin/rsync.sh~

如果没有此提示，则用 jobs 命令查看任务：
#+BEGIN_SRC bash
>>>jobs
[1]+ suspended /root/bin/rsync.sh &
#+END_SRC
然后我们可以把程序调度到后台执行：（bg 后面的数字为作业号）
#+BEGIN_SRC bash
>>>bg 1
[1]+ /root/bin/rsync.sh &
#+END_SRC
用 jobs 命令查看正在运行的任务：
#+BEGIN_SRC bash
>>>jobs
[1]+ Running /root/bin/rsync.sh &
#+END_SRC
如果想把它调回到前台运行，可以用
#+BEGIN_SRC bash
>>>fg 1
/root/bin/rsync.sh
#+END_SRC
这样，你在控制台上就只能等待这个任务完成了。 
* Linux 命令行快捷键
涉及在linux命令行下进行快速移动光标、命令编辑、编辑后执行历史命令、Bang(!)命令、控制命令等。让basher更有效率。

** 常用
ctrl+左右键:在单词之间跳转
ctrl+a:跳到本行的行首
ctrl+e:跳到页尾
Ctrl+u：删除当前光标前面的文字 （还有剪切功能）
ctrl+k：删除当前光标后面的文字(还有剪切功能)
Ctrl+L：进行清屏操作
Ctrl+y:粘贴Ctrl+u或ctrl+k剪切的内容
Ctrl+w:删除光标前面的单词的字符
Alt – d ：由光标位置开始，往右删除单词。往行尾删
说明
Ctrl – k: 先按住 Ctrl 键，然后再按 k 键；
Alt – k: 先按住 Alt 键，然后再按 k 键；
M – k：先单击 Esc 键，然后再按 k 键。

** 移动光标
Ctrl – a ：移到行首
Ctrl – e ：移到行尾
Ctrl – b ：往回(左)移动一个字符
Ctrl – f ：往后(右)移动一个字符
Alt – b ：往回(左)移动一个单词
Alt – f ：往后(右)移动一个单词
Ctrl – xx ：在命令行尾和光标之间移动
M-b ：往回(左)移动一个单词
M-f ：往后(右)移动一个单词

** 编辑命令
Ctrl – h ：删除光标左方位置的字符
Ctrl – d ：删除光标右方位置的字符（注意：当前命令行没有任何字符时，会注销系统或结束终端）
Ctrl – w ：由光标位置开始，往左删除单词。往行首删
Alt – d ：由光标位置开始，往右删除单词。往行尾删
M – d ：由光标位置开始，删除单词，直到该单词结束。
Ctrl – k ：由光标所在位置开始，删除右方所有的字符，直到该行结束。
Ctrl – u ：由光标所在位置开始，删除左方所有的字符，直到该行开始。
Ctrl – y ：粘贴之前删除的内容到光标后。
ctrl – t ：交换光标处和之前两个字符的位置。
Alt + . ：使用上一条命令的最后一个参数。
Ctrl – _ ：回复之前的状态。撤销操作。
Ctrl -a + Ctrl -k 或 Ctrl -e + Ctrl -u 或 Ctrl -k + Ctrl -u 组合可删除整行。

** Bang(!)命令
!! ：执行上一条命令。
^foo^bar ：把上一条命令里的foo替换为bar，并执行。
!wget ：执行最近的以wget开头的命令。
!wget:p ：仅打印最近的以wget开头的命令，不执行。
!$ ：上一条命令的最后一个参数， 与 Alt - . 和 $_ 相同。
!* ：上一条命令的所有参数
!*:p ：打印上一条命令是所有参数，也即 !*的内容。
^abc ：删除上一条命令中的abc。
^foo^bar ：将上一条命令中的 foo 替换为 bar
^foo^bar^ ：将上一条命令中的 foo 替换为 bar
!-n ：执行前n条命令，执行上一条命令： !-1， 执行前5条命令的格式是： !-5

** 查找历史命令
Ctrl – p ：显示当前命令的上一条历史命令
Ctrl – n ：显示当前命令的下一条历史命令
Ctrl – r ：搜索历史命令，随着输入会显示历史命令中的一条匹配命令，Enter键执行匹配命令；ESC键在命令行显示而不执行匹配命令。
Ctrl – g ：从历史搜索模式（Ctrl – r）退出。

** 控制命令
Ctrl – l ：清除屏幕，然后，在最上面重新显示目前光标所在的这一行的内容。
Ctrl – o ：执行当前命令，并选择上一条命令。
Ctrl – s ：阻止屏幕输出
Ctrl – q ：允许屏幕输出
Ctrl – c ：终止命令
Ctrl – z ：挂起命令

** 重复执行操作动作
M – 操作次数 操作动作 ： 指定操作次数，重复执行指定的操作。
* Linux 命令行连接WiFi
1. 安装nmcli
sudo apt-get install nmcli

2. 查看网络设备
sudo nmcli dev

3. 开启wifi
sudo nmcli r wifi on

4. 扫描wifi
sudo nmcli dev wifi

5. 连接wifi
sudo nmcli dev wifi connect "wifi名" password "密码"
* Linux 让终端走代理的几种方法
参看文章：
[[https://zhuanlan.zhihu.com/p/46973701][Linux 让终端走代理的几种方法]]

可以用命令$ curl cip.cc 检查终端是否处于代理状态
** 方法一：（推荐使用）
为什么说这个方法推荐使用呢？因为他只作用于当前终端中，不会影响环境，而且命令比较简单
在终端中直接运行：

export http_proxy=http://proxyAddress:port

如果你是SSR,并且走的http的代理端口是12333，想执行wget或者curl来下载国外的东西，可以使用如下命令：

export http_proxy=http://127.0.0.1:12333

如果是https那么就经过如下命令：

export https_proxy=http://127.0.0.1:12333

下面这是cad实验室的代理设置：
#+begin_src bash
export http_proxy="socks5://proxy.in.zjulearning.org:7070"
export https_proxy="socks5://proxy.in.zjulearning.org:7070"
#+END_SRC

Linux上代理的临时取消
#+begin_src bash
unset  http_proxy
unset https_proxy
#+END_SRC

** 方法二 ：
这个办法的好处是把代理服务器永久保存了，下次就可以直接用了

把代理服务器地址写入shell配置文件.bashrc或者.zshrc 直接在.bashrc或者.zshrc添加下面内容
#+begin_src bash
export http_proxy="http://localhost:port"
export https_proxy="http://localhost:port"
#+END_SRC
或者走socket5协议（ss,ssr）的话，代理端口是1080
#+begin_src bash
export http_proxy="socks5://127.0.0.1:1080"
export https_proxy="socks5://127.0.0.1:1080"
#+END_SRC
或者干脆直接设置ALL_PROXY

export ALL_PROXY=socks5://127.0.0.1:1080

最后在执行如下命令应用设置

source ~/.bashrc

或者通过设置alias简写来简化操作，每次要用的时候输入setproxy，不用了就unsetproxy。
#+begin_src bash
alias setproxy="export ALL_PROXY=socks5://127.0.0.1:1080" 
alias unsetproxy="unset ALL_PROXY"
#+END_SRC
** 方法三:
改相应工具的配置，比如apt的配置

sudo vim /etc/apt/apt.conf

在文件末尾加入下面这行

Acquire::http::Proxy "http://proxyAddress:port"
** python走socks5代理的设置
python如果要使用socks5，除了需要上面的代理外，还要安装pysocks库.

参考文章：[[https://stackoverflow.com/questions/38794015/pythons-requests-missing-dependencies-for-socks-support-when-using-socks5-fro][Python's requests "Missing dependencies for SOCKS support" when using SOCKS5 from Terminal]]
* Linux 中查看某个软件的安装路径
** 查看文件安装路径：
由于软件安装的地方不止一个地方，所有先说查看文件安装的所有路径(地址)。

这里以Oracle为例。比如说我安装了Oracle，但是不知道文件都安装在哪些地方、放在哪些文件夹里，可以用下面的命令查看所有的文件路径

在终端输入：
#+begin_src bash
whereis oracle
#+END_SRC

回车，如果你安装好了Oracle，就会显示文件安装的地址，例如我的显示(安装地址可能会不同)
#+BEGIN_EXAMPLE
oracle: /usr/bin/oracle /usr/lib/oracle /usr/share/oracle /usr/share/man/man1/oracle.1.gz
#+END_EXAMPLE

可以看出来，Oracle安装在是个目录里。

如果你没有安装Oracle或者Oracle安装没成功，则不会显示文件路径出来。只提示:
#+begin_src bash
oracle:
#+END_SRC
** 查询运行文件所在路径：

如果你只要查询文件的运行文件所在地址，直接用下面的命令就可以了(还是以Oracle为例)：
#+begin_src bash
which oracle
#+END_SRC
结果会显示：
#+BEGIN_EXAMPLE
/usr/bin/oracle
#+END_EXAMPLE

* Linux shell中2>&1的含义解释
| 名称                 | 代码 | 操作符           |
|----------------------+------+------------------|
| 标准输入(stdin)      |    0 | < 或 <<          |
| 标准输出(stdout)     |    1 | >, >>, 1> 或 1>> |
| 标准错误输出(stderr) |    2 | 2> 或 2>>        |

从上表看的出来，我们平时使用的
#+begin_src bash
echo "hello" > t.log 
#+END_SRC
其实也可以写成
#+begin_src bash
echo "hello" 1> t.log
#+END_SRC
** 关于2>&1的含义
1. 含义：将标准错误输出重定向到标准输出
2. 符号>&是一个整体，不可分开
3. 2>1的写法其实是将标准错误输出重定向到名为"1"的文件里去了。
4. 写成2&>1也是不可以的

#+begin_src bash
# 标准错误输出和标准输出都定向到log中
nohup java -jar app.jar >log 2>&1 &
#+END_SRC
1. 本来1----->屏幕 （1指向屏幕）
2. 执行>log后， 1----->log (1指向log)
3. 执行2>&1后， 2----->1 (2指向1，而1指向log,因此2也指向了log)

">log 2>&1"可以有以下两种简写方式
#+begin_src bash
&>log
>&log
#+END_SRC
** 参考文章
[[https://blog.csdn.net/zhaominpro/article/details/82630528][Linux shell中2>&1的含义解释]]
* ldd命令
作用：用来查看程式运行所需的共享库,常用来解决程式因缺少某个库文件而不能运行的一些问题。ldd不是个可执行程式，而只是个shell脚本。

格式：
ldd [选项] 文件

选项：
- --version：打印ldd的版本号
- -v --verbose：打印所有信息，例如包括符号的版本信息
- -d --data-relocs：执行符号重部署，并报告缺少的目标对象（只对ELF格式适用）
- -r --function-relocs：对目标对象和函数执行重新部署，并报告缺少的目标对象和函数（只对ELF格式适用）
- --help：用法信息

* less命令
less 与 more 类似，less 可以随意浏览文件，支持翻页和搜索，支持向上翻页和向下翻页。

语法：
less [参数] 文件 

参数说明：
- -b <缓冲区大小> 设置缓冲区的大小
- -e 当文件显示结束后，自动离开
- -f 强迫打开特殊文件，例如外围设备代号、目录和二进制文件
- -F，就会有类似 tail -f 的效果，读取写入文件的最新内容， 按 ctrl+C 停止。
- -g 只标志最后搜索的关键词
- -i 忽略搜索时的大小写
- -m 显示类似more命令的百分比
- -N 显示每行的行号
- -o <文件名> 将less 输出的内容在指定文件中保存起来
- -Q 不使用警告音
- -v 进入编辑模型， shift+ZZ 保存退出到 less 查看模式。
- -s 显示连续空行为一行
- -S 行过长时间将超出部分舍弃
- -x <数字> 将"tab"键显示为规定的数字空格
- /字符串：向下搜索"字符串"的功能
- ?字符串：向上搜索"字符串"的功能
- n：重复前一个搜索（与 / 或 ? 有关）
- N：反向重复前一个搜索（与 / 或 ? 有关）
- b 向上翻一页
- d 向后翻半页
- h 显示帮助界面
- Q 退出less 命令
- u 向前滚动半页
- y 向前滚动一行
- 空格键 滚动一页
- 回车键 滚动一行
- [pagedown]： 向下翻动一页
- [pageup]： 向上翻动一页

** 附加备注
1.全屏导航
#+BEGIN_EXAMPLE
ctrl + F - 向前移动一屏
ctrl + B - 向后移动一屏
ctrl + D - 向前移动半屏
ctrl + U - 向后移动半屏
#+END_EXAMPLE
2.单行导航
#+BEGIN_EXAMPLE
j - 下一行
k - 上一行
#+END_EXAMPLE

3.其它导航
#+BEGIN_EXAMPLE
G - 移动到最后一行
g - 移动到第一行
q / ZZ - 退出 less 命令
#+END_EXAMPLE

4.其它有用的命令
#+BEGIN_EXAMPLE
v - 使用配置的编辑器编辑当前文件
h - 显示 less 的帮助文档
&pattern - 仅显示匹配模式的行，而不是整个文件
#+END_EXAMPLE

5.标记导航
当使用 less 查看大文件时，可以在任何一个位置作标记，可以通过命令导航到标有特定标记的文本位置：
#+BEGIN_EXAMPLE
ma - 使用 a 标记文本的当前位置
'a - 导航到标记 a 处
:e 查看下一个文件， 用 :n 和 :p 来回切换。
#+END_EXAMPLE
** 实例
1、查看文件

less log2013.log
2、ps查看进程信息并通过less分页显示

ps -ef |less
3、查看命令历史使用记录并通过less分页显示
#+begin_src bash
[root@localhost test]# history | less
22  scp -r tomcat6.0.32 root@192.168.120.203:/opt/soft
23  cd ..
24  scp -r web root@192.168.120.203:/opt/
25  cd soft
26  ls
……省略……
#+END_SRC
4、浏览多个文件
#+begin_src bash
less log2013.log log2014.log
#+END_SRC
可以按 :e 查看下一个文件， 用 :n 和 :p 来回切换。
说明：
输入 ：n后，切换到 log2014.log
输入 ：p 后，切换到log2013.log
* locale命令
locale是linux系统中多语言环境的设置接口，Locale根据计算机用户所使用的语言，所在国家或者地区，以及当地的文化传统所定义的一个软件运行时的语言环境。

通过locale来设置程序运行的不同语言环境，locale由ANSI C提供支持。locale的命名规则为<语言>_<地区>.<字符集编码>，如zh_CN.UTF-8，zh代表中文，CN代表大陆地区，UTF-8表示字符集。在locale环境中，有一组变量，代表国际化环境中的不同设置。locale是linux系统中多语言环境的设置接口，Locale根据计算机用户所使用的语言，所在国家或者地区，以及当地的文化传统所定义的一个软件运行时的语言环境。

通过locale来设置程序运行的不同语言环境，locale由ANSI C提供支持。locale的命名规则为<语言>_<地区>.<字符集编码>，如zh_CN.UTF-8，zh代表中文，CN代表大陆地区，UTF-8表示字符集。在locale环境中，有一组变量，代表国际化环境中的不同设置。

语法格式：locale [参数]

常用参数：
- -a 查看当前系统所有可用locale
- -m 写入可用字符映射的名称
- -c 写入选定类别的名称
- -k 写入选定关键字的名称

** 查看当前locale设置:
#+begin_src bash
[root@linuxcool ~]# locale
LANG=en_US.UTF-8
LANGUAGE=en_US:en
LC_CTYPE="en_US.UTF-8"
LC_NUMERIC=en_US.UTF-8
LC_TIME=en_US.UTF-8
LC_COLLATE="en_US.UTF-8"
LC_MONETARY=en_US.UTF-8
LC_MESSAGES="en_US.UTF-8"
LC_PAPER=en_US.UTF-8
LC_NAME=en_US.UTF-8
LC_ADDRESS=en_US.UTF-8
LC_TELEPHONE=en_US.UTF-8
LC_MEASUREMENT=en_US.UTF-8
LC_IDENTIFICATION=en_US.UTF-8
LC_ALL=
#+END_SRC
分别介绍下
#+BEGIN_EXAMPLE
LANG：LANG的优先级是最低的，它是所有LC_*变量的默认值，下方所有以LC_开头变量（LC_ALL除外）中，如果存在没有设置变量值的变量，那么系统将会使用LANG的变量值来给这个变量进行赋值。如果变量有值，则保持不变

LC_CTYPE：用于字符分类和字符串处理，控制所有字符的处理方式，包括字符编码，字符是单字节还是多字节，如何打印等，非常重要的一个变量。
LC_NUMERIC：用于格式化非货币的数字显示
LC_TIME：用于格式化时间和日期
LC_COLLATE：用于比较和排序
LC_MONETARY：用于格式化货币单位
LC_MESSAGES：用于控制程序输出时所使用的语言，主要是提示信息，错误信息，状态信息，标题，标签，按钮和菜单等
LC_PAPER：默认纸张尺寸大小
LC_NAME：姓名书写方式
LC_ADDRESS：地址书写方式
LC_TELEPHONE：电话号码书写方式
LC_MEASUREMENT：度量衡表达方式
LC_IDENTIFICATION：locale对自身包含信息的概述

LC_ALL：它不是环境变量，它是一个宏，它可通过该变量的设置覆盖所有LC_*变量，这个变量设置之后，可以废除LC_*的设置值，使得这些变量的设置值与LC_ALL的值一致，注意LANG变量不受影响。

优先级：LC_ALL > LC_* > LANG
#+END_EXAMPLE

** 查看可用的语言环境：
#+begin_src bash
[root@linuxcool ~]# locale -a
C
C.UTF-8
POSIX
#+END_SRC
上面所列的，C是系统默认的locale，POSIX是C的别名，这是标准的C locale ，它所指定的属性和行为由ISO C标准所指定，当我们新安装完一个系统时，默认的locale就是C或POSIX（C就是ASCII编码）
** 设置系统的locale
修改/etc/profile文件
#+begin_src bash
#在最下面增加
export LC_ALL=zh_CN.utf8
export LANG=zh_CN.utf8
#+END_SRC
source一下配置文件，使其生效

修改/etc/default/locale
#+begin_src bash
LANG=“en_US.UTF-8”
LANGUAGE=“en_US:en”
#+END_SRC
注销一下，使其生效

修改/etc/locale.gen文件
#+begin_src bash
...
#en_SG ISO-8859-1
en_US.UTF-8 UTF-8
#en_US ISO-8859-1
…
#+END_SRC
将注释打开即可

修改完成后，执行下locale-gen命令使其生效

命令行模式下修改
#+begin_src bash
localectl set-locale LANG=en_US.UTF-8
#+END_SRC

创建/etc/locale.conf文件
#+begin_src bash
LANG=en_AU.UTF-8
LC_COLLATE=C
LC_TIME=en_DK.UTF-8
#+END_SRC
source使其生效
** locale 没有en_us的解决方法
#+begin_src bash
apt-get update

# Install locales package
apt-get install -y locales

# Uncomment en_US.UTF-8 for inclusion in generation
sed -i 's/^# *\(en_US.UTF-8\)/\1/' /etc/locale.gen

# Generate locale
locale-gen

# Export env vars
echo "export LC_ALL=en_US.UTF-8" >> ~/.bashrc
echo "export LANG=en_US.UTF-8" >> ~/.bashrc
echo "export LANGUAGE=en_US.UTF-8" >> ~/.bashrc
#+END_SRC
* ls命令
-a, –all 列出目录下的所有文件，包括以 . 开头的隐含文件

-A 同-a，但不列出“.”(表示当前目录)和“…”(表示当前目录的父目录)。

-c 配合 -lt：根据 ctime 排序及显示 ctime (文件状态最后更改的时间)配合 -l：显示 ctime 但根据名称排序否则：根据 ctime 排序

-C 每栏由上至下列出项目

–color[=WHEN] 控制是否使用色彩分辨文件。WHEN 可以是’never’、'always’或’auto’其中之一

-d, –directory 将目录象文件一样显示，而不是显示其下的文件。

-D, –dired 产生适合 Emacs 的 dired 模式使用的结果

-f 对输出的文件不进行排序，-aU 选项生效，-lst 选项失效

-g 类似 -l,但不列出所有者

-G, –no-group 不列出任何有关组的信息

-h, –human-readable 以容易理解的格式列出文件大小 (例如 1K 234M 2G)

–si 类似 -h,但文件大小取 1000 的次方而不是 1024

-H, –dereference-command-line 使用命令列中的符号链接指示的真正目的地

–indicator-style=方式 指定在每个项目名称后加上指示符号<方式>：none (默认)，classify (-F)，file-type (-p)

-i, –inode 印出每个文件的 inode 号

-I, –ignore=样式 不印出任何符合 shell 万用字符<样式>的项目

-k 即 –block-size=1K,以 k 字节的形式表示文件的大小。

-l 除了文件名之外，还将文件的权限、所有者、文件大小等信息详细列出来。

-L, –dereference 当显示符号链接的文件信息时，显示符号链接所指示的对象而并非符号链接本身的信息

-m 所有项目以逗号分隔，并填满整行行宽

-o 类似 -l,显示文件的除组信息外的详细信息。

-r, –reverse 依相反次序排列

-R, –recursive 同时列出所有子目录层

-s, –size 以块大小为单位列出所有文件的大小

-S 根据文件大小排序
** 仅列出目录
仅列出目录，下面是 4 种不同的方法。

1. 利用 ls 命令的 -d 选项：
#+begin_src bash
$ ls -d */
Desktop/  pic/  shell/  src/
#+END_SRC

2. 利用 ls 命令的 -F 选项：
#+begin_src bash
$ ls -F |grep "/$"
Desktop/
pic/
shell/
src/
#+END_SRC
-F 选项会给输出的不同文件类型加上一个后缀，比如普通文件会在其后加一个 * 符号，管道文件会在其后加上一个 | 符号，而目录则在其后加上一个 / 符号，因此使用上面的方法也可以实现仅列出目录。

3. 利用 ls 命令的 -l 选项：
#+begin_src bash
$ls -l |grep "^d"
drwxr-xr-x 2 root root 4096 2011-05-08 01:46 Desktop
drwxr-xr-x 2 root root 4096 2012-03-26 10:03 pic
drwxr-xr-x 2 root root 4096 2012-03-30 17:21 shell
drwxr-xr-x 3 root root 4096 2012-03-22 22:18 src
#+END_SRC


上面列出了目录的详细信息，如果只想列出目录名本身，那么可以：
#+begin_src bash
$ls -l |grep "^d" |awk '{print $8}'
Desktop
pic
shell
src
#+END_SRC
** 只显示隐藏文件
可以用ls .* 来显示。但是默认情况下，如果是目录，会显示目录包含的文件，此时可以用-d来避免。
#+begin_src bash
ls -d .*
#+END_SRC
** 基于文件名，大小，时间排序
linux ls命令中，-f 直接列出结果，而不进行排序（ls默认会以文件名排序）；-S 基于文件大小进行排序；-t 基于文件修改时间进行排序；-r 将排序结果反向输出，例如：原本文件名由小到大，反向则由大到小；

1. 基于文件名排序
ls -fl

2. 基于文件大小排序
ls -Sr

3. 基于文件时间排序
ls -tr 
* netstat
Linux netstat 命令用于显示网络状态。

利用 netstat 指令可让你得知整个 Linux 系统的网络情况。

语法：netstat [-acCeFghilMnNoprstuvVwx][-A<网络类型>][--ip]

- -a或--all 显示所有连线中的Socket。
- -A<网络类型>或--<网络类型> 列出该网络类型连线中的相关地址。
- -c或--continuous 持续列出网络状态。
- -C或--cache 显示路由器配置的快取信息。
- -e或--extend 显示网络其他相关信息。
- -F或--fib 显示路由缓存。
- -g或--groups 显示多重广播功能群组组员名单。
- -h或--help 在线帮助。
- -i或--interfaces 显示网络界面信息表单。
- -l或--listening 显示监控中的服务器的Socket。
- -M或--masquerade 显示伪装的网络连线。
- -n或--numeric 直接使用IP地址，而不通过域名服务器。
- -N或--netlink或--symbolic 显示网络硬件外围设备的符号连接名称。
- -o或--timers 显示计时器。
- -p或--programs 显示正在使用Socket的程序识别码和程序名称。
- -r或--route 显示Routing Table。
- -s或--statistics 显示网络工作信息统计表。
- -t或--tcp 显示TCP传输协议的连线状况。
- -u或--udp 显示UDP传输协议的连线状况。
- -v或--verbose 显示指令执行过程。
- -V或--version 显示版本信息。
- -w或--raw 显示RAW传输协议的连线状况。
- -x或--unix 此参数的效果和指定"-A unix"参数相同。
- --ip或--inet 此参数的效果和指定"-A inet"参数相同。

** 实例
*** 显示详细的网络状况
# netstat -a
*** 显示当前户籍UDP连接状况
# netstat -nu
*** 显示UDP端口号的使用情况
# netstat -apu
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address        Foreign Address       State    PID/Program name  
udp    0   0 *:32768           *:*                   -          
udp    0   0 *:nfs            *:*                   -          
udp    0   0 *:641            *:*                   3006/rpc.statd   
udp    0   0 192.168.0.3:netbios-ns   *:*                   3537/nmbd      
udp    0   0 *:netbios-ns        *:*                   3537/nmbd      
udp    0   0 192.168.0.3:netbios-dgm   *:*                   3537/nmbd      
udp    0   0 *:netbios-dgm        *:*                   3537/nmbd      
udp    0   0 *:tftp           *:*                   3346/xinetd     
udp    0   0 *:999            *:*                   3366/rpc.rquotad  
udp    0   0 *:sunrpc          *:*                   2986/portmap    
udp    0   0 *:ipp            *:*                   6938/cupsd     
udp    0   0 *:1022           *:*                   3392/rpc.mountd   
udp    0   0 *:638            *:*                   3006/rpc.statd
*** 显示网卡列表
# netstat -i
Kernel Interface table
Iface    MTU Met  RX-OK RX-ERR RX-DRP RX-OVR  TX-OK TX-ERR TX-DRP TX-OVR Flg
eth0    1500  0  181864   0   0   0  141278   0   0   0 BMRU
lo    16436  0   3362   0   0   0   3362   0   0   0 LRU
*** 显示组播组的关系
# netstat -g
IPv6/IPv4 Group Memberships
Interface    RefCnt Group
--------------- ------ ---------------------
lo       1   ALL-SYSTEMS.MCAST.NET
eth0      1   ALL-SYSTEMS.MCAST.NET
lo       1   ff02::1
eth0      1   ff02::1:ff0a:b0c
eth0      1   ff02::1
*** 显示网络统计信息
# netstat -s
Ip:
  184695 total packets received
  0 forwarded
  0 incoming packets discarded
  184687 incoming packets delivered
  143917 requests sent out
  32 outgoing packets dropped
  30 dropped because of missing route
Icmp:
  676 ICMP messages received
  5 input ICMP message failed.
  ICMP input histogram:
    destination unreachable: 44
    echo requests: 287
    echo replies: 345
  304 ICMP messages sent
  0 ICMP messages failed
  ICMP output histogram:
    destination unreachable: 17
    echo replies: 287
Tcp:
  473 active connections openings
  28 passive connection openings
  4 failed connection attempts
  11 connection resets received
  1 connections established
  178253 segments received
  137936 segments send out
  29 segments retransmited
  0 bad segments received.
  336 resets sent
Udp:
  5714 packets received
  8 packets to unknown port received.
  0 packet receive errors
  5419 packets sent
TcpExt:
  1 resets received for embryonic SYN_RECV sockets
  ArpFilter: 0
  12 TCP sockets finished time wait in fast timer
  572 delayed acks sent
  3 delayed acks further delayed because of locked socket
  13766 packets directly queued to recvmsg prequeue.
  1101482 packets directly received from backlog
  19599861 packets directly received from prequeue
  46860 packets header predicted
  14541 packets header predicted and directly queued to user
  TCPPureAcks: 12259
  TCPHPAcks: 9119
  TCPRenoRecovery: 0
  TCPSackRecovery: 0
  TCPSACKReneging: 0
  TCPFACKReorder: 0
  TCPSACKReorder: 0
  TCPRenoReorder: 0
  TCPTSReorder: 0
  TCPFullUndo: 0
  TCPPartialUndo: 0
  TCPDSACKUndo: 0
  TCPLossUndo: 0
  TCPLoss: 0
  TCPLostRetransmit: 0
  TCPRenoFailures: 0
  TCPSackFailures: 0
  TCPLossFailures: 0
  TCPFastRetrans: 0
  TCPForwardRetrans: 0
  TCPSlowStartRetrans: 0
  TCPTimeouts: 29
  TCPRenoRecoveryFail: 0
  TCPSackRecoveryFail: 0
  TCPSchedulerFailed: 0
  TCPRcvCollapsed: 0
  TCPDSACKOldSent: 0
  TCPDSACKOfoSent: 0
  TCPDSACKRecv: 0
  TCPDSACKOfoRecv: 0
  TCPAbortOnSyn: 0
  TCPAbortOnData: 1
  TCPAbortOnClose: 0
  TCPAbortOnMemory: 0
  TCPAbortOnTimeout: 3
  TCPAbortOnLinger: 0
  TCPAbortFailed: 3
  TCPMemoryPressures: 0
*** 显示监听的套接口
# netstat -l
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address        Foreign Address       State   
tcp    0   0 *:32769           *:*             LISTEN   
tcp    0   0 *:nfs            *:*             LISTEN   
tcp    0   0 *:644            *:*             LISTEN   
tcp    0   0 *:1002           *:*             LISTEN   
tcp    0   0 *:netbios-ssn        *:*             LISTEN   
tcp    0   0 *:sunrpc          *:*             LISTEN   
tcp    0   0 vm-dev:ipp         *:*             LISTEN   
tcp    0   0 *:telnet          *:*             LISTEN   
tcp    0   0 *:601            *:*             LISTEN   
tcp    0   0 *:microsoft-ds       *:*             LISTEN   
tcp    0   0 *:http           *:*             LISTEN   
tcp    0   0 *:ssh            *:*             LISTEN   
tcp    0   0 *:https           *:*             LISTEN   
udp    0   0 *:32768           *:*                   
udp    0   0 *:nfs            *:*                   
udp    0   0 *:641            *:*                   
udp    0   0 192.168.0.3:netbios-ns   *:*                   
udp    0   0 *:netbios-ns        *:*                   
udp    0   0 192.168.0.3:netbios-dgm   *:*                   
udp    0   0 *:netbios-dgm        *:*                   
udp    0   0 *:tftp           *:*                   
udp    0   0 *:999            *:*                   
udp    0   0 *:sunrpc          *:*                   
udp    0   0 *:ipp            *:*                   
udp    0   0 *:1022           *:*                   
udp    0   0 *:638            *:*                   
Active UNIX domain sockets (only servers)
Proto RefCnt Flags    Type    State     I-Node Path
unix 2   [ ACC ]   STREAM   LISTENING   10621 @/tmp/fam-root-
unix 2   [ ACC ]   STREAM   LISTENING   7096  /var/run/acpid.socket
unix 2   [ ACC ]   STREAM   LISTENING   9792  /tmp/.gdm_socket
unix 2   [ ACC ]   STREAM   LISTENING   9927  /tmp/.X11-unix/X0
unix 2   [ ACC ]   STREAM   LISTENING   10489 /tmp/ssh-lbUnUf4552/agent.4552
unix 2   [ ACC ]   STREAM   LISTENING   10558 /tmp/ksocket-root/kdeinit__0
unix 2   [ ACC ]   STREAM   LISTENING   10560 /tmp/ksocket-root/kdeinit-:0
unix 2   [ ACC ]   STREAM   LISTENING   10570 /tmp/.ICE-unix/dcop4664-1270815442
unix 2   [ ACC ]   STREAM   LISTENING   10843 /tmp/.ICE-unix/4735
unix 2   [ ACC ]   STREAM   LISTENING   10591 /tmp/ksocket-root/klauncherah3arc.slave-socket
unix 2   [ ACC ]   STREAM   LISTENING   7763  /var/run/iiim/.iiimp-unix/9010
unix 2   [ ACC ]   STREAM   LISTENING   11047 /tmp/orbit-root/linc-1291-0-1e92c8082411
unix 2   [ ACC ]   STREAM   LISTENING   11053 /tmp/orbit-root/linc-128e-0-dc070659cbb3
unix 2   [ ACC ]   STREAM   LISTENING   8020  /var/run/dbus/system_bus_socket
unix 2   [ ACC ]   STREAM   LISTENING   58927 /tmp/mcop-root/vm-dev-2c28-4beba75f
unix 2   [ ACC ]   STREAM   LISTENING   7860  /tmp/.font-unix/fs7100
unix 2   [ ACC ]   STREAM   LISTENING   7658  /dev/gpmctl
unix 2   [ ACC ]   STREAM   LISTENING   10498 @/tmp/dbus-s2MLJGO5Ci
* Node.js安装
在Ubuntu系统上安装nodejs有很多种方法，分别为：apt-get在线安装，下载Node.js源码自己编译安装，下载编译好的文件，使用npm安装等方式。

** 在线安装
在线安装并不推荐，比较坑的一点是安装后node命令不可用，nondejs命令可用。使用在线安装步骤为：在我们安装 nodejs 之前，推荐你将系统更新到最新的补丁和升级包，所以请登录到系统中使用超级用户运行如下命令：

apt-get update，之后apt-get install nodejs。

此外在Node.js官网提供了一种在线安装的方式，我们可以通过

curl -sL https://deb.nodesource.com/setup_7.x | sudo -E bash - 

sudo apt-get install -y nodejs来安装node7.0及以上版本的nodejs。注意：如果在虚拟机上搭建的Linux系统，在执行以上在线安装的命令时网速可能会相当慢，解决办法可以参考这篇文章。

#+DOWNLOADED: screenshot @ 2022-03-07 22:34:16
[[file:images/linux%E7%AC%94%E8%AE%B0/Node.js%E5%AE%89%E8%A3%85/2022-03-07_22-34-16_screenshot.png]]

** 源码编译安装
通过源码编译安装。用以下命令来升级系统，并且安装一些Node.js必要的包

apt-get update

apt-get install python gcc make g++

接下来来到Node.js官网下载专区，在下图中红框内选中的标为Source Code的版本上点击鼠标

右键，选择复制下载链接。

#+DOWNLOADED: screenshot @ 2022-03-08 09:52:41
[[file:images/linux%E7%AC%94%E8%AE%B0/Node.js%E5%AE%89%E8%A3%85/2022-03-08_09-52-41_screenshot.png]]
执行以下命令：

wget https://nodejs.org/dist/v6.9.2/node-v6.9.2.tar.gz（该地址为Source Code下载地址）

tar -zxvf node-v6.9.2.tar.gz 解压下载的Source Code。

解压完成后依次执行：
#+begin_src bash
cd node-v6.9.2
./configure
make
sudo make install
#+END_SRC
此外，我们还可以使用git在github上将源码clone下来，同样执行以上命令编译安装。

** 下载编译好的文件进行安装
下载编译好的文件。简单说就是解压后，bin文件夹中存在node及npm，如果进入到对应文件中执行命令一点问题没有，不过不是全局的。所以设置为全局就可以了。

wget https://nodejs.org/dist/v6.9.2/node-v6.9.2-linux-x64.tar.xz  --下载
xz -d node-v6.9.2-linux-x64.tar.xz --解压为tar类型
tar -xvf node-v6.9.2-linux-x64.tar  --解压

解压完成后pwd查看当前下载目录，并执行以下命令设置全局：
ln -s /home/zlliu/tcl/node-v6.9.2-linux-64/bin/node /usr/local/node
ln -s /home/zlliu/tcl/node-v6.9.2-linux-64/bin/npm /usr/local/npm
其中/home/zlliu/tcl/这个路径是我下载nodejs存放的路径，你应该改成自己的存放路径。

** 使用npm安装
使用npm安装，首先要下载npm。同样可使用apt-get或者下载npm源码进行安装，安装后可使用npm各种命令，如npm ls，npm install <packageName>等等，我们可用npm指定安装nodejs的版本。
* makefile
** 语法规则
#+begin_src bash
target ... : prerequisites ...
    command
    ...
    ...

#+END_SRC
或是这样：
#+begin_src bash
targets : prerequisites ; command
    command
    ...
#+END_SRC
这是一个文件的依赖关系，
也就是说，target这一个或多个的目标文件依赖于prerequisites中的文件，其生成规则定义在command中。

在Makefile中的命令，必须要以 Tab 键开始。
*** target
可以是一个object file（目标文件），也可以是一个执行文件，还可以是一个标签（label），以空格分开，可以使用通配符。


*** prerequisites
生成该target所依赖的文件和/或target。

如果其中的某个文件要比目标文件要新，那么，目标就被认为是“过时的”，被认为是需要重生成的。
*** command
该target要执行的命令（任意的shell命令）

如果其不与“target:prerequisites”在一行，那么，必须以 Tab 键开头，如果和prerequisites在一行，那么可以用分号做为分隔。

如果命令太长，你可以使用反斜杠（ \ ）作为换行符。make对一行上有多少个字符没有限制。规则告诉make两件事，文件的依赖关系和如何生成目标文件。

一般来说，make会以UNIX的标准Shell，也就是 /bin/sh 来执行命令。
** make工作方式
#+begin_src makefile
edit : main.o kbd.o command.o display.o \
        insert.o search.o files.o utils.o
    cc -o edit main.o kbd.o command.o display.o \
        insert.o search.o files.o utils.o

main.o : main.c defs.h
    cc -c main.c
kbd.o : kbd.c defs.h command.h
    cc -c kbd.c
command.o : command.c defs.h command.h
    cc -c command.c
display.o : display.c defs.h buffer.h
    cc -c display.c
insert.o : insert.c defs.h buffer.h
    cc -c insert.c
search.o : search.c defs.h buffer.h
    cc -c search.c
files.o : files.c defs.h buffer.h command.h
    cc -c files.c
utils.o : utils.c defs.h
    cc -c utils.c
clean :
    rm edit main.o kbd.o command.o display.o \
        insert.o search.o files.o utils.o
#+END_SRC
make会在当前目录下找名字叫“Makefile”或“makefile”的文件。

如果找到，它会找文件中的第一个目标文件（target），在上面的例子中，他会找到“edit”这个文件，并把这个文件作为最终的目标文件。

如果edit文件不存在，或是edit所依赖的后面的 .o 文件的文件修改时间要比 edit 这个文件新，那么，他就会执行后面所定义的命令来生成 edit 这个文件。

如果 edit 所依赖的 .o 文件也不存在，那么make会在当前文件中找目标为 .o 文件的依赖性，如果找到则再根据那一个规则生成 .o 文件。（这有点像一个堆栈的过程）

当然，你的C文件和H文件是存在的啦，于是make会生成 .o 文件，然后再用 .o 文件生成make的终极任务，也就是执行文件 edit 了。
** 变量
变量的定义：
#+BEGIN_SRC makefile
objects = main.o kbd.o command.o display.o \
     insert.o search.o files.o utils.o
#+END_SRC
makefile以 $(objects) 的方式来使用变量。

#+BEGIN_SRC makefile
objects = main.o kbd.o command.o display.o \
    insert.o search.o files.o utils.o

edit : $(objects)
    cc -o edit $(objects)
main.o : main.c defs.h
    cc -c main.c
kbd.o : kbd.c defs.h command.h
    cc -c kbd.c
command.o : command.c defs.h command.h
    cc -c command.c
display.o : display.c defs.h buffer.h
    cc -c display.c
insert.o : insert.c defs.h buffer.h
    cc -c insert.c
search.o : search.c defs.h buffer.h
    cc -c search.c
files.o : files.c defs.h buffer.h command.h
    cc -c files.c
utils.o : utils.c defs.h
    cc -c utils.c
clean :
    rm edit $(objects)
#+END_SRC
*** 使用变量
在Makefile中的定义的变量，就像是C/C++语言中的宏一样，他代表了一个文本字串，在Makefile中执行的时候其会自动原模原样地展开在所使用的地方。其与C/C++所不同的是，你可以在Makefile中改变其值。在Makefile中，变量可以使用在“目标”，“依赖目标”， “命令”或是Makefile的其它部分中。

变量的命名字可以包含字符、数字，下划线（可以是数字开头），但不应该含有 : 、 # 、 = 或是空字符（空格、回车等）。变量是大小写敏感的，“foo”、“Foo”和“FOO”是三个不同的变量名。传统的Makefile的变量名是全大写的命名方式，但我推荐使用大小写搭配的变量名，如：MakeFlags。这样可以避免和系统的变量冲突，而发生意外的事情。

有一些变量是很奇怪字串，如 $< 、 $@ 等，这些是自动化变量
*** 变量的基础
变量在声明时需要给予初值，而在使用时，需要给在变量名前加上 $ 符号，但最好用小括号 () 或是大括号 {} 把变量给包括起来。如果你要使用真实的 $ 字符，那么你需要用 $$ 来表示。

变量可以使用在许多地方，如规则中的“目标”、“依赖”、“命令”以及新的变量中。先看一个例子：
#+begin_src makefile
objects = program.o foo.o utils.o
program : $(objects)
    cc -o program $(objects)

$(objects) : defs.h
#+END_SRC
变量会在使用它的地方精确地展开，就像C/C++中的宏一样，例如：
#+begin_src makefile
foo = c
prog.o : prog.$(foo)
    $(foo)$(foo) -$(foo) prog.$(foo)

#+END_SRC
展开后得到：
#+begin_src makefile
prog.o : prog.c
    cc -c prog.c

#+END_SRC
当然，千万不要在你的Makefile中这样干，这里只是举个例子来表明Makefile中的变量在使用处展开的真实样子。可见其就是一个“替代”的原理。

另外，给变量加上括号完全是为了更加安全地使用这个变量，在上面的例子中，如果你不想给变量加上括号，那也可以，但我还是强烈建议你给变量加上括号。
*** 变量中的变量
在定义变量的值时，我们可以使用其它变量来构造变量的值，在Makefile中有两种方式来在用变量定义变量的值。

先看第一种方式，也就是简单的使用 = 号，在 = 左侧是变量，右侧是变量的值，右侧变量的值可以定义在文件的任何一处，也就是说，右侧中的变量不一定非要是已定义好的值，其也可以使用后面定义的值。如：
#+begin_src makefile
foo = $(bar)
bar = $(ugh)
ugh = Huh?

all:
    echo $(foo)

#+END_SRC
我们执行“make all”将会打出变量 $(foo) 的值是 Huh? （ $(foo) 的值是 $(bar) ， $(bar) 的值是 $(ugh) ， $(ugh) 的值是 Huh? ）可见，变量是可以使用后面的变量来定义的。

这个功能有好的地方，也有不好的地方，好的地方是，我们可以把变量的真实值推到后面来定义，如：
#+begin_src makefile
CFLAGS = $(include_dirs) -O
include_dirs = -Ifoo -Ibar

#+END_SRC
当 CFLAGS 在命令中被展开时，会是 -Ifoo -Ibar -O 。但这种形式也有不好的地方，那就是递归定义，如：
#+begin_src makefile
CFLAGS = $(CFLAGS) -O

#+END_SRC
或：
#+begin_src makefile
A = $(B)
B = $(A)

#+END_SRC
这会让make陷入无限的变量展开过程中去，当然，我们的make是有能力检测这样的定义，并会报错。还有就是如果在变量中使用函数，那么，这种方式会让我们的make运行时非常慢，更糟糕的是，他会使用得两个make的函数“wildcard”和“shell”发生不可预知的错误。因为你不会知道这两个函数会被调用多少次。

为了避免上面的这种方法，我们可以使用make中的另一种用变量来定义变量的方法。这种方法使用的是 := 操作符，如：
#+begin_src makefile
x := foo
y := $(x) bar
x := later

#+END_SRC
其等价于：
#+begin_src makefile
y := foo bar
x := later

#+END_SRC
值得一提的是，这种方法，前面的变量不能使用后面的变量，只能使用前面已定义好了的变量。如果是这样：
#+begin_src makefile
y := $(x) bar
x := foo

#+END_SRC
那么，y的值是“bar”，而不是“foo bar”。

上面都是一些比较简单的变量使用了，让我们来看一个复杂的例子，其中包括了make的函数、条件表达式和一个系统变量“MAKELEVEL”的使用：
#+begin_src makefile
ifeq (0,${MAKELEVEL})
cur-dir   := $(shell pwd)
whoami    := $(shell whoami)
host-type := $(shell arch)
MAKE := ${MAKE} host-type=${host-type} whoami=${whoami}
endif

#+END_SRC
关于条件表达式和函数，我们在后面再说，对于系统变量“MAKELEVEL”，其意思是，如果我们的make有一个嵌套执行的动作（参见前面的“嵌套使用make”），那么，这个变量会记录了我们的当前Makefile的调用层数。

下面再介绍两个定义变量时我们需要知道的，请先看一个例子，如果我们要定义一个变量，其值是一个空格，那么我们可以这样来：
#+begin_src makefile
nullstring :=
space := $(nullstring) # end of the line

#+END_SRC
nullstring是一个Empty变量，其中什么也没有，而我们的space的值是一个空格。因为在操作符的右边是很难描述一个空格的，这里采用的技术很管用，先用一个Empty变量来标明变量的值开始了，而后面采用“#”注释符来表示变量定义的终止，这样，我们可以定义出其值是一个空格的变量。请注意这里关于“#”的使用，注释符“#”的这种特性值得我们注意，如果我们这样定义一个变量：
#+begin_src makefile
dir := /foo/bar    # directory to put the frobs in

#+END_SRC
dir这个变量的值是“/foo/bar”，后面还跟了4个空格，如果我们这样使用这样变量来指定别的目录——“$(dir)/file”那么就完蛋了。

还有一个比较有用的操作符是 ?= ，先看示例：
#+begin_src makefile
FOO ?= bar

#+END_SRC
其含义是，如果FOO没有被定义过，那么变量FOO的值就是“bar”，如果FOO先前被定义过，那么这条语将什么也不做，其等价于：
#+begin_src makefile
ifeq ($(origin FOO), undefined)
    FOO = bar
endif

#+END_SRC
*** 变量高级用法（替换和嵌套使用）
这里介绍两种变量的高级使用方法，第一种是变量值的替换。

我们可以替换变量中的共有的部分，其格式是 $(var:a=b) 或是 ${var:a=b} ，其意思是，把变量“var”中所有以“a”字串“结尾”的“a”替换成“b”字串。这里的“结尾”意思是“空格”或是“结束符”。

还是看一个示例吧：
#+begin_src makefile
foo := a.o b.o c.o
bar := $(foo:.o=.c)

#+END_SRC
这个示例中，我们先定义了一个 $(foo) 变量，而第二行的意思是把 $(foo) 中所有以 .o 字串“结尾”全部替换成 .c ，所以我们的 $(bar) 的值就是“a.c b.c c.c”。

另外一种变量替换的技术是以“静态模式”（参见前面章节）定义的，如：
#+begin_src makefile
foo := a.o b.o c.o
bar := $(foo:%.o=%.c)

#+END_SRC
这依赖于被替换字串中的有相同的模式，模式中必须包含一个 % 字符，这个例子同样让 $(bar) 变量的值为“a.c b.c c.c”。

第二种高级用法是——“把变量的值再当成变量”。先看一个例子：
#+begin_src makefile
x = y
y = z
a := $($(x))

#+END_SRC
在这个例子中，$(x)的值是“y”，所以$($(x))就是$(y)，于是$(a)的值就是“z”。（注意，是“x=y”，而不是“x=$(y)”）

我们还可以使用更多的层次：
#+begin_src makefile
x = y
y = z
z = u
a := $($($(x)))

#+END_SRC
这里的 $(a) 的值是“u”。

让我们再复杂一点，使用上“在变量定义中使用变量”的第一个方式，来看一个例子：
#+begin_src makefile
x = $(y)
y = z
z = Hello
a := $($(x))

#+END_SRC
这里的 $($(x)) 被替换成了 $($(y)) ，因为 $(y) 值是“z”，所以，最终结果是： a:=$(z) ，也就是“Hello”。

再复杂一点，我们再加上函数：
#+begin_src makefile
x = variable1
variable2 := Hello
y = $(subst 1,2,$(x))
z = y
a := $($($(z)))

#+END_SRC
这个例子中， $($($(z))) 扩展为 $($(y)) ，而其再次被扩展为 $($(subst 1,2,$(x))) 。 $(x) 的值是“variable1”，subst函数把“variable1”中的所有“1”字串替换成“2”字串，于是，“variable1”变成 “variable2”，再取其值，所以，最终， $(a) 的值就是 $(variable2) 的值——“Hello”。（喔，好不容易）

在这种方式中，或要可以使用多个变量来组成一个变量的名字，然后再取其值：
#+begin_src makefile
first_second = Hello
a = first
b = second
all = $($a_$b)

#+END_SRC
这里的 $a_$b 组成了“first_second”，于是， $(all) 的值就是“Hello”。

再来看看结合第一种技术的例子：
#+begin_src makefile
a_objects := a.o b.o c.o
1_objects := 1.o 2.o 3.o

sources := $($(a1)_objects:.o=.c)

#+END_SRC
这个例子中，如果 $(a1) 的值是“a”的话，那么， $(sources) 的值就是“a.c b.c c.c”；如果 $(a1) 的值是“1”，那么 $(sources) 的值是“1.c 2.c 3.c”。

再来看一个这种技术和“函数”与“条件语句”一同使用的例子：
#+begin_src makefile
ifdef do_sort
    func := sort
else
    func := strip
endif

bar := a d b g q c

foo := $($(func) $(bar))

#+END_SRC
这个示例中，如果定义了“do_sort”，那么： foo := $(sort a d b g q c) ，于是 $(foo) 的值就是 “a b c d g q”，而如果没有定义“do_sort”，那么： foo := $(strip a d b g q c) ，调用的就是strip函数。

当然，“把变量的值再当成变量”这种技术，同样可以用在操作符的左边:
#+begin_src makefile
dir = foo
$(dir)_sources := $(wildcard $(dir)/*.c)
define $(dir)_print
lpr $($(dir)_sources)
endef

#+END_SRC
这个例子中定义了三个变量：“dir”，“foo_sources”和“foo_print”。
*** 追加变量值
我们可以使用 += 操作符给变量追加值，如：
#+begin_src makefile
objects = main.o foo.o bar.o utils.o
objects += another.o

#+END_SRC
于是，我们的 $(objects) 值变成：“main.o foo.o bar.o utils.o another.o”（another.o被追加进去了）

使用 += 操作符，可以模拟为下面的这种例子：
#+begin_src makefile
objects = main.o foo.o bar.o utils.o
objects := $(objects) another.o

#+END_SRC
所不同的是，用 += 更为简洁。

如果变量之前没有定义过，那么， += 会自动变成 = ，如果前面有变量定义，那么 += 会继承于前次操作的赋值符。如果前一次的是 := ，那么 += 会以 := 作为其赋值符，如：
#+begin_src makefile
variable := value
variable += more

#+END_SRC
等价于：
#+begin_src makefile
variable := value
variable := $(variable) more

#+END_SRC
但如果是这种情况：
#+begin_src makefile
variable = value
variable += more

#+END_SRC
由于前次的赋值符是 = ，所以 += 也会以 = 来做为赋值，那么岂不会发生变量的递补归定义，这是很不好的，所以make会自动为我们解决这个问题，我们不必担心这个问题。
*** override 指示符
如果有变量是通常make的命令行参数设置的，那么Makefile中对这个变量的赋值会被忽略。如果你想在Makefile中设置这类参数的值，那么，你可以使用“override”指示符。其语法是:
#+begin_src makefile
override <variable>; = <value>;

override <variable>; := <value>;

#+END_SRC
当然，你还可以追加:
#+begin_src makefile
override <variable>; += <more text>;

#+END_SRC
对于多行的变量定义，我们用define指示符，在define指示符前，也同样可以使用override指示符，如:
#+begin_src makefile
override define foo
bar
endef

#+END_SRC
*** 多行变量
还有一种设置变量值的方法是使用define关键字。使用define关键字设置变量的值可以有换行，这有利于定义一系列的命令（前面我们讲过“命令包”的技术就是利用这个关键字）。

define指示符后面跟的是变量的名字，而重起一行定义变量的值，定义是以endef 关键字结束。其工作方式和“=”操作符一样。变量的值可以包含函数、命令、文字，或是其它变量。因为命令需要以[Tab]键开头，所以如果你用define定义的命令变量中没有以 Tab 键开头，那么make 就不会把其认为是命令。

下面的这个示例展示了define的用法:
#+begin_src makefile
define two-lines
echo foo
echo $(bar)
endef

#+END_SRC
*** 环境变量
make运行时的系统环境变量可以在make开始运行时被载入到Makefile文件中，但是如果Makefile中已定义了这个变量，或是这个变量由make命令行带入，那么系统的环境变量的值将被覆盖。（如果make指定了“-e”参数，那么，系统环境变量将覆盖Makefile中定义的变量）

因此，如果我们在环境变量中设置了 CFLAGS 环境变量，那么我们就可以在所有的Makefile中使用这个变量了。这对于我们使用统一的编译参数有比较大的好处。如果Makefile中定义了CFLAGS，那么则会使用Makefile中的这个变量，如果没有定义则使用系统环境变量的值，一个共性和个性的统一，很像“全局变量”和“局部变量”的特性。

当make嵌套调用时（参见前面的“嵌套调用”章节），上层Makefile中定义的变量会以系统环境变量的方式传递到下层的Makefile 中。当然，默认情况下，只有通过命令行设置的变量会被传递。而定义在文件中的变量，如果要向下层Makefile传递，则需要使用export关键字来声明。（参见前面章节）

当然，我并不推荐把许多的变量都定义在系统环境中，这样，在我们执行不用的Makefile时，拥有的是同一套系统变量，这可能会带来更多的麻烦。
*** 目标变量
前面我们所讲的在Makefile中定义的变量都是“全局变量”，在整个文件，我们都可以访问这些变量。当然，“自动化变量”除外，如 $< 等这种类量的自动化变量就属于“规则型变量”，这种变量的值依赖于规则的目标和依赖目标的定义。

当然，我也同样可以为某个目标设置局部变量，这种变量被称为“Target-specific Variable”，它可以和“全局变量”同名，因为它的作用范围只在这条规则以及连带规则中，所以其值也只在作用范围内有效。而不会影响规则链以外的全局变量的值。

其语法是：
#+begin_src makefile
<target ...> : <variable-assignment>;

<target ...> : overide <variable-assignment>

#+END_SRC
<variable-assignment>;可以是前面讲过的各种赋值表达式，如 = 、 := 、 += 或是 ?= 。第二个语法是针对于make命令行带入的变量，或是系统环境变量。

这个特性非常的有用，当我们设置了这样一个变量，这个变量会作用到由这个目标所引发的所有的规则中去。如：
#+begin_src makefile
prog : CFLAGS = -g
prog : prog.o foo.o bar.o
    $(CC) $(CFLAGS) prog.o foo.o bar.o

prog.o : prog.c
    $(CC) $(CFLAGS) prog.c

foo.o : foo.c
    $(CC) $(CFLAGS) foo.c

bar.o : bar.c
    $(CC) $(CFLAGS) bar.c

#+END_SRC
在这个示例中，不管全局的 $(CFLAGS) 的值是什么，在prog目标，以及其所引发的所有规则中（prog.o foo.o bar.o的规则）， $(CFLAGS) 的值都是 -g
*** 模式变量
在GNU的make中，还支持模式变量（Pattern-specific Variable），通过上面的目标变量中，我们知道，变量可以定义在某个目标上。模式变量的好处就是，我们可以给定一种“模式”，可以把变量定义在符合这种模式的所有目标上。

我们知道，make的“模式”一般是至少含有一个 % 的，所以，我们可以以如下方式给所有以 .o 结尾的目标定义目标变量：
#+begin_src makefile
%.o : CFLAGS = -O

#+END_SRC
同样，模式变量的语法和“目标变量”一样：
#+begin_src makefile
<pattern ...>; : <variable-assignment>;

<pattern ...>; : override <variable-assignment>;

#+END_SRC
override同样是针对于系统环境传入的变量，或是make命令行指定的变量。
** make的推导以及文件功能
只要make看到一个 .o 文件，它就会自动的把 .c 文件加在依赖关系中，
如果make找到一个 whatever.o ，那么 whatever.c 就会是 whatever.o 的依赖文件。并且 cc -c whatever.c 也会被推导出来
#+BEGIN_SRC makefile
objects = main.o kbd.o command.o display.o \
    insert.o search.o files.o utils.o

edit : $(objects)
    cc -o edit $(objects)

main.o : defs.h
kbd.o : defs.h command.h
command.o : defs.h command.h
display.o : defs.h buffer.h
insert.o : defs.h buffer.h
search.o : defs.h buffer.h
files.o : defs.h buffer.h command.h
utils.o : defs.h

.PHONY : clean
clean :
    rm edit $(objects)
#+END_SRC
这种方法，也就是make的“隐晦规则”。上面文件内容中， .PHONY 表示 clean 是个伪目标文件。

可以用文件功能将上面重复出现的.h文件收拢起来写到一起
#+BEGIN_SRC makefile
objects = main.o kbd.o command.o display.o \
    insert.o search.o files.o utils.o

edit : $(objects)
    cc -o edit $(objects)

$(objects) : defs.h
kbd.o command.o files.o : command.h
display.o insert.o search.o files.o : buffer.h

.PHONY : clean
clean :
    rm edit $(objects)
#+END_SRC
** 清空目标文件的规则
每个Makefile中都应该写一个清空目标文件（ .o 和执行文件）的规则，这不仅便于重编译，也很利于保持文件的清洁。
一般的风格都是：
#+BEGIN_SRC makefile
clean:
    rm edit $(objects)
#+END_SRC
更为稳健的做法是：
#+BEGIN_SRC makefile
.PHONY : clean
clean :
    -rm edit $(objects)
#+END_SRC
前面说过， .PHONY 表示 clean 是一个“伪目标”。
而在 rm 命令前面加了一个小减号的意思就是，也许某些文件出现问题，但不要管，继续做后面的事。
** Makefile的文件名
默认的情况下，make命令会在当前目录下按顺序找寻文件名为“GNUmakefile”、“makefile”、“Makefile”的文件，找到了解释这个文件。

可以使用别的文件名来书写Makefile，比如：“Make.Linux”，“Make.Solaris”，“Make.AIX”等，

如果要指定特定的Makefile，你可以使用make的 -f 和 --file 参数，如： make -f Make.Linux 或 make --file Make.AIX 。
** include
在Makefile使用 include 关键字可以把别的Makefile包含进来，这很像C语言的 #include ，被包含的文件会原模原样的放在当前文件的包含位置。 

include 的语法是：
#+begin_src bash
include <filename>
#+END_SRC
filename 可以是当前操作系统Shell的文件模式（可以包含路径和通配符）。

在 include 前面可以有一些空字符，但是绝不能是 Tab 键开始。 
include 和 <filename> 可以用一个或多个空格隔开。
举个例子，你有这样几个Makefile： a.mk 、 b.mk 、 c.mk ，还有一个文件叫 foo.make ，以及一个变量 $(bar) ，其包含了 e.mk 和 f.mk ，那么，下面的语句：
#+begin_src bash
include foo.make *.mk $(bar)
#+END_SRC
等价于：
#+begin_src bash
include foo.make a.mk b.mk c.mk e.mk f.mk
#+END_SRC
make命令开始时，会找寻 include 所指出的其它Makefile，并把其内容安置在当前的位置。就好像C/C++的 #include 指令一样。
如果文件都没有指定绝对路径或是相对路径的话，make会在当前目录下首先寻找，
如果当前目录下没有找到，那么，make还会在下面的几个目录下找：
- 如果make执行时，有 -I 或 --include-dir 参数，那么make就会在这个参数所指定的目录下去寻找。
- 如果目录 <prefix>/include （一般是： /usr/local/bin 或 /usr/include ）存在的话，make也会去找。

如果有文件没有找到的话，make会生成一条警告信息，但不会马上出现致命错误。
它会继续载入其它的文件，一旦完成makefile的读取，make会再重试这些没有找到，或是不能读取的文件，如果还是不行，make才会出现一条致命信息。
如果你想让make不理那些无法读取的文件，而继续执行，你可以在include前加一个减号“-”。如：
#+begin_src bash
-include <filename>
#+END_SRC
其表示，无论include过程中出现什么错误，都不要报错继续执行。和其它版本make兼容的相关命令是sinclude，其作用和这一个是一样的。
** 环境变量MAKEFILES¶
如果你的当前环境中定义了环境变量 MAKEFILES ，那么，make会把这个变量中的值做一个类似于 include 的动作。
这个变量中的值是其它的Makefile，用空格分隔。只是，它和 include 不同的是，从这个环境变量中引入的Makefile的“目标”不会起作用，如果环境变量中定义的文件发现错误，make也会不理。

但是在这里我还是建议不要使用这个环境变量，因为只要这个变量一被定义，那么当你使用make时，所有的Makefile都会受到它的影响，这绝不是你想看到的。
在这里提这个事，只是为了告诉大家，也许有时候你的Makefile出现了怪事，那么你可以看看当前环境中有没有定义这个变量。
** 通配符
make支持三个通配符： * ， ? 和 ~ 。
这和Unix的B-Shell是相同的。

#+begin_src makefile
objects = *.o
#+END_SRC
上面例子中objects的值就是 *.o ，而不是 *.o 展开后的值。

Makefile中的变量其实就是C/C++中的宏。如果你要让通配符在变量中展开，也就是让objects的值是所有 .o 的文件名的集合，那么，你可以这样：
#+begin_src makefile
objects := $(wildcard *.o)
#+END_SRC

另给一个变量使用通配符的例子：
1. 列出一确定文件夹中的所有 .c 文件。
#+begin_src makefile
objects := $(wildcard *.c)
#+END_SRC
2. 列出(1)中所有文件对应的 .o 文件，在（3）中我们可以看到它是由make自动编译出的:
#+begin_src makefile
$(patsubst %.c,%.o,$(wildcard *.c))
#+END_SRC
3. 由(1)(2)两步，可写出编译并链接所有 .c 和 .o 文件
#+begin_src makefile
objects := $(patsubst %.c,%.o,$(wildcard *.c))
foo : $(objects)
    cc -o foo $(objects)
#+END_SRC
这种用法由关键字“wildcard”，“patsubst”指出，
** VPATH文件寻找
如果没有指明VPATH变量，make只会在当前的目录中去找寻依赖文件和目标文件。

如果定义了VPATH变量，那么，make就会在当前目录找不到的情况下，到所指定的目录中去找寻文件。

#+begin_src makefile
VPATH = src:../headers
#+END_SRC
上面的定义指定两个目录，“src”和“../headers”，make会按照这个顺序进行搜索。目录由“冒号”分隔。（当然，当前目录永远是最高优先搜索的地方）

另一个设置文件搜索路径的方法是使用make的“vpath”关键字（注意，它是全小写的），这不是变量，这是一个make的关键字，这和上面提到的那个VPATH变量很类似，但是它更为灵活。它可以指定不同的文件在不同的搜索目录中。

这是一个很灵活的功能。它的使用方法有三种：
#+BEGIN_EXAMPLE
vpath <pattern> <directories>
为符合模式<pattern>的文件指定搜索目录<directories>。

vpath <pattern>
清除符合模式<pattern>的文件的搜索目录。

vpath
清除所有已被设置好了的文件搜索目录。
#+END_EXAMPLE
vpath使用方法中的<pattern>需要包含 % 字符。 

% 的意思是匹配零或若干字符，（需引用 % ，使用 \ ）例如， %.h 表示所有以 .h 结尾的文件。

<pattern>指定了要搜索的文件集，而<directories>则指定了< pattern>的文件集的搜索的目录。例如：
#+begin_src makefile
vpath %.h ../headers
#+END_SRC
该语句表示，要求make在“../headers”目录下搜索所有以 .h 结尾的文件。（如果某文件在当前目录没有找到的话）

我们可以连续地使用vpath语句，以指定不同搜索策略。

如果连续的vpath语句中出现了相同的<pattern> ，或是被重复了的<pattern>，那么，make会按照vpath语句的先后顺序来执行搜索。如：
#+begin_src makefile
vpath %.c foo
vpath %   blish
vpath %.c bar
#+END_SRC
其表示 .c 结尾的文件，先在“foo”目录，然后是“blish”，最后是“bar”目录。
#+begin_src makefile
vpath %.c foo:bar
vpath %   blish
#+END_SRC
而上面的语句则表示 .c 结尾的文件，先在“foo”目录，然后是“bar”目录，最后才是“blish”目录。
** 伪目标
伪目标是这样一个目标：它不代表一个真正的文件名，在执行make时可以指定这个目标来执行其所在规则定义的命令，有时我们也可以将一个伪目标称为标签。使用伪目标有两点原因：
1. 避免在我们的Makefile中定义的只执行命令的的目标（此目标的目的为了执行执行一系列命令，而不需要创建这个目标）和工作目录下的实际文件出现名字冲突。
2. 提高执行make时的效率.

果我们需要书写这样一个规则：规则所定义的命令不是去创建目标文件，而是使用make指定具体的目标来执一些特定的命令。像下边那样：
#+begin_src makefile
clean: 
rm *.o temp 
#+END_SRC
规则中rm不是创建文件clean的命令，只是删除当前目录下的所有.o文件和temp文件。

在工作目录下不存在clean这个文件时，我们输入make clean后，rm *.o temp总会被执行。这是我们的初衷。

但当前工作目录下存在文件clean时情况就不一样了，在我们输入make clean时。

规则没有依赖文件，所以目标被认为是最新的而不去执行规则作定义的命令，命令rm将不会被执行。这并不是我们的初衷。

为了避免这个问题，我们可以将目标clean明确的声明为伪目标。

将一个目标声明为伪目标需要将它作为特殊目标.PHONY的依赖。如下：
#+begin_src makefile
.PHONY : clean 
#+END_SRC
这样目标clean就是一个伪目标，无论当前目录下是否存在clean这个文件。

当我们输入make clean之后，rm命令都会被执行。

而且，当一个目标被声明为伪目标后，make在执行此规则时不会试图去查找隐含规则来创建这个目标。

这样也提高了make的执行效率，同时我们也不用担心由于目标和文件名重名而使我们的期望失败。

在书写伪目标规则时，首先需要声明目标是一个伪目标，之后才是伪目标的规则定义。

目标clean书写格式应该如下：
#+begin_src makefile
.PHONY: clean 
clean: 
rm *.o temp 
#+END_SRC
伪目标的另外一使用场合在make的并行和递归执行过程中。

此情况下一般存在一个变量，其定义为所有需要make的子目录。

对多个目录进行make的实现方式可以在一个规则中可以使用shell的循环来完成。

如下：
#+begin_src makefile
SUBDIRS = foo bar baz 
subdirs: 
for dir in $(SUBDIRS); do \ 
$(MAKE) -C $$dir; \ 
done 
#+END_SRC
但这种实现方法存在以下几个问题。

当子目录执行make出现错误时，make不会退出。

就是说，在对某一个目录执行make失败以后，会继续对其他的目录进行make。

在最终执行失败的情况下，我们很难根据错误的提示定位出具体是是那个目录下的Makefile出现错误。

这给问题定位造成了很大的困难。为了避免这样的问题，我们可以在命令行部分加入错误的监测，在命令执行错误后make退出。

不幸的是，如果在执行make时使用了-k选项，此方式将失效。

另外一个问题就是使用这种shell的循环方式时，没有用到make对目录的并行处理功能，因为规则的命令是一条完整的shell命令，不能被并行的执行。

我们可以通过伪目标方式来克服以上实现方式所存在的两个问题。
#+begin_src makefile
SUBDIRS = foo bar baz 
.PHONY: subdirs $(SUBDIRS) 
subdirs: $(SUBDIRS) 
$(SUBDIRS): 
$(MAKE) -C $@ 
foo: baz 
#+END_SRC
上边的实现中使用了一个没有命令行的规则foo: baz，用来限制子目录的make顺序。

此规则的含义时在处理foo目录之前，需要等待baz目录处理完成。

在书写一个并行执行make的Makefile时，目录的处理顺序是需要特别注意的。

一般情况下，一个伪目标不作为一个另外一个目标文件的依赖。

这是因为当一个目标文件的依赖包含伪目标时，每一次在执行这个规则时伪目标所定义的命令都会被执行（因为它是规则的依赖，重建规则目标文件时需要首先重建它的依赖）。

当伪目标没有作为任何目标（此目标是一个可被创建或者已存在的文件）的依赖时，我们只能通过make的命令行选项明确指定这个伪目标，来执行它所定义的命令。例如我们的make clean。

伪目标一般没有依赖的文件。但是，我们也可以为伪目标指定所依赖的文件。

伪目标同样可以作为“默认目标”，只要将其放在第一个。

一个示例就是，如果你的Makefile需要一口气生成若干个可执行文件，但你只想简单地敲一个make完事，并且，所有的目标文件都写在一个Makefile中，那么你可以使用“伪目标”这个特性,下边就是一个例子：
#+begin_src makefile
#sample Makefile 
all : prog1 prog2 prog3 
.PHONY : all 
prog1 : prog1.o utils.o 
cc -o prog1 prog1.o utils.o 
prog2 : prog2.o 
cc -o prog2 prog2.o 
prog3 : prog3.o sort.o utils.o 
cc -o prog3 prog3.o sort.o utils.o 
#+END_SRC
我们知道，Makefile中的第一个目标会被作为其默认目标。

我们声明了一个“all”的伪目标，其依赖于其它三个目标。

由于默认目标的特性是，总是被执行的，但由于“all”又是一个伪目标，伪目标只是一个标签不会生成文件，所以不会有“all”文件产生。

为了完成对它的更新，make会创建（不存在）或者重建（已存在）目标all的所有依赖文件（prog1、prog2和prog3）。

.PHONY : all 声明了“all”这个目标为“伪目标”。（注：这里的显式“.PHONY : all” 不写的话一般情况也可以正确的执行，这样make可通过隐式规则推导出， “all” 是一个伪目标，执行make不会生成“all”文件，而执行后面的多个目标。建议：显式写出是一个好习惯。）

当需要单独更新某一个程序时，我们可以通过make的命令行选项来明确指定需要重建的程序。（例如： make prog1）。

当一个伪目标作为另外一个伪目标依赖时，make将其作为另外一个伪目标的子例程来处理（可以这样理解：其作为另外一个伪目标的必须执行的部分，就行C语言中的函数调用一样）。

下边的例子就是这种用法：
#+begin_src makefile
.PHONY: cleanall cleanobj cleandiff 
cleanall : cleanobj cleandiff 
rm program 
cleanobj : 
rm *.o 
cleandiff : 
rm *.diff
#+END_SRC
cleanobj和cleandiff这两个伪目标有点像子程序的意思（执行目标clearall时会触发它们所定义的命令被执行）。

我们可以输入make cleanall和make cleanobj和make cleandiff命令来达到清除不同种类文件的目的。

例子首先通过特殊目标.PHONY声明了多个伪目标，它们之间使用空各分割，之后才是各个伪目标的规则定义。

说明：
通常在清除文件的伪目标所定义的命令中rm使用选项–f（--force）来防止在缺少删除文件时出错并退出，使make clean过程失败。

也可以在rm之前加上-来防止rm错误退出，这种方式时make会提示错误信息但不会退出。

为了不看到这些讨厌的信息，需要使用上述的第一种方式。

另外make存在一个内嵌隐含变量RM，它被定义为：RM = rm –f。因此在书写clean规则的命令行时可以使用变量$(RM)来代替rm，这样可以免出现一些不必要的麻烦！这是我们推荐的用法。
*** 参考文献
[[https://www.zybuluo.com/lishuhuakai/note/210174][Makefile伪目标]]
** 多目标
Makefile的规则中的目标可以不止一个，其支持多目标。

有可能我们的多个目标同时依赖于一个文件，并且其生成的命令大体类似。于是我们就能把其合并起来。

当然，多个目标的生成规则的执行命令不是同一个，这可能会给我们带来麻烦，

不过好在我们可以使用一个自动化变量 $@ ，这个变量表示目前规则中所有的目标的集合。
#+begin_src makefile
bigoutput littleoutput : text.g
    generate text.g -$(subst output,,$@) > $@
#+END_SRC
上述规则等价于：
#+begin_src makefile
bigoutput : text.g
    generate text.g -big > bigoutput
littleoutput : text.g
    generate text.g -little > littleoutput
#+END_SRC
其中， -$(subst output,,$@) 中的 $ 表示执行一个Makefile的函数，函数名为subst，后面的为参数。

这里的这个函数是替换字符串的意思， $@ 表示目标的集合，就像一个数组， $@ 依次取出目标，并执于命令。
** 静态模式
静态模式可以更加容易地定义多目标的规则，可以让我们的规则变得更加的有弹性和灵活。我们还是先来看一下语法：
#+begin_src makefile
<targets ...> : <target-pattern> : <prereq-patterns ...>
    <commands>
    ...
#+END_SRC
targets定义了一系列的目标文件，可以有通配符。是目标的一个集合。

target-pattern是指明了targets的模式，也就是的目标集模式。

prereq-patterns是目标的依赖模式，它对target-pattern形成的模式再进行一次依赖目标的定义。

如果我们的<target-pattern>定义成 %.o ，意思是我们的<target>;集合中都是以 .o 结尾的，而如果我们的<prereq-patterns>定义成 %.c ，意思是对<target-pattern>所形成的目标集进行二次定义，其计算方法是，取<target-pattern>模式中的 % （也就是去掉了 .o 这个结尾），并为其加上 .c 这个结尾，形成的新集合。

所以，我们的“目标模式”或是“依赖模式”中都应该有 % 这个字符，如果你的文件名中有 % 那么你可以使用反斜杠 \ 进行转义，来标明真实的 % 字符。

看一个例子：
#+begin_src makefile
objects = foo.o bar.o

all: $(objects)

$(objects): %.o: %.c
    $(CC) -c $(CFLAGS) $< -o $@
#+END_SRC
上面的例子中，指明了我们的目标从$object中获取， %.o 表明要所有以 .o 结尾的目标，也就是 foo.o bar.o ，也就是变量 $object 集合的模式，而依赖模式 %.c 则取模式 %.o 的 % ，也就是 foo bar ，并为其加下 .c 的后缀，于是，我们的依赖目标就是 foo.c bar.c 。而命令中的 $< 和 $@ 则是自动化变量， $< 表示第一个依赖文件， $@ 表示目标集（也就是“foo.o bar.o”）。于是，上面的规则展开后等价于下面的规则：
#+begin_src makefile
foo.o : foo.c
    $(CC) -c $(CFLAGS) foo.c -o foo.o
bar.o : bar.c
    $(CC) -c $(CFLAGS) bar.c -o bar.o
#+END_SRC
试想，如果我们的 %.o 有几百个，那么我们只要用这种很简单的“静态模式规则”就可以写完一堆规则，实在是太有效率了。“静态模式规则”的用法很灵活，如果用得好，那会是一个很强大的功能。再看一个例子：
#+begin_src makefile
files = foo.elc bar.o lose.o

$(filter %.o,$(files)): %.o: %.c
    $(CC) -c $(CFLAGS) $< -o $@
$(filter %.elc,$(files)): %.elc: %.el
    emacs -f batch-byte-compile $<
#+END_SRC
$(filter %.o,$(files))表示调用Makefile的filter函数，过滤“$files”集，只要其中模式为“%.o”的内容。其它的内容，我就不用多说了吧。这个例子展示了Makefile中更大的弹性。
** 自动生成依赖性
在Makefile中，我们的依赖关系可能会需要包含一系列的头文件，比如，如果我们的main.c中有一句 #include "defs.h" ，那么我们的依赖关系应该是：
#+begin_src makefile
main.o : main.c defs.h
#+END_SRC
但是，如果是一个比较大型的工程，你必需清楚哪些C文件包含了哪些头文件，并且，你在加入或删除头文件时，也需要小心地修改Makefile，这是一个很没有维护性的工作。为了避免这种繁重而又容易出错的事情，我们可以使用C/C++编译的一个功能。大多数的C/C++编译器都支持一个“-M”的选项，即自动找寻源文件中包含的头文件，并生成一个依赖关系。例如，如果我们执行下面的命令:
#+begin_src bash
cc -M main.c
#+END_SRC
其输出是：
#+begin_src makefile
main.o : main.c defs.h
#+END_SRC

于是由编译器自动生成的依赖关系，这样一来，你就不必再手动书写若干文件的依赖关系，而由编译器自动生成了。需要提醒一句的是，如果你使用GNU的C/C++编译器，你得用 -MM 参数，不然， -M 参数会把一些标准库的头文件也包含进来。

gcc -M main.c的输出是:
#+begin_src makefile
main.o: main.c defs.h /usr/include/stdio.h /usr/include/features.h \
    /usr/include/sys/cdefs.h /usr/include/gnu/stubs.h \
    /usr/lib/gcc-lib/i486-suse-linux/2.95.3/include/stddef.h \
    /usr/include/bits/types.h /usr/include/bits/pthreadtypes.h \
    /usr/include/bits/sched.h /usr/include/libio.h \
    /usr/include/_G_config.h /usr/include/wchar.h \
    /usr/include/bits/wchar.h /usr/include/gconv.h \
    /usr/lib/gcc-lib/i486-suse-linux/2.95.3/include/stdarg.h \
    /usr/include/bits/stdio_lim.h
#+END_SRC
gcc -MM main.c的输出则是:
#+begin_src makefile
main.o: main.c defs.h
#+END_SRC
那么，编译器的这个功能如何与我们的Makefile联系在一起呢。因为这样一来，我们的Makefile也要根据这些源文件重新生成，让 Makefile自已依赖于源文件？这个功能并不现实，不过我们可以有其它手段来迂回地实现这一功能。GNU组织建议把编译器为每一个源文件的自动生成的依赖关系放到一个文件中，为每一个 name.c 的文件都生成一个 name.d 的Makefile文件， .d 文件中就存放对应 .c 文件的依赖关系。

于是，我们可以写出 .c 文件和 .d 文件的依赖关系，并让make自动更新或生成 .d 文件，并把其包含在我们的主Makefile中，这样，我们就可以自动化地生成每个文件的依赖关系了。

这里，我们给出了一个模式规则来产生 .d 文件：
#+begin_src makefile
%.d: %.c
    @set -e; rm -f $@; \
    $(CC) -M $(CPPFLAGS) $< > $@.$$$$; \
    sed 's,\($*\)\.o[ :]*,\1.o $@ : ,g' < $@.$$$$ > $@; \
    rm -f $@.$$$$
#+END_SRC
这个规则的意思是，所有的 .d 文件依赖于 .c 文件， rm -f $@ 的意思是删除所有的目标，也就是 .d 文件，第二行的意思是，为每个依赖文件 $< ，也就是 .c 文件生成依赖文件， $@ 表示模式 %.d 文件，如果有一个C文件是name.c，那么 % 就是 name ， $$$$ 意为一个随机编号，第二行生成的文件有可能是“name.d.12345”，第三行使用sed命令做了一个替换，关于sed命令的用法请参看相关的使用文档。第四行就是删除临时文件。

总而言之，这个模式要做的事就是在编译器生成的依赖关系中加入 .d 文件的依赖，即把依赖关系：
#+begin_src makefile
main.o : main.c defs.h
#+END_SRC
转成：
#+begin_src makefile
main.o main.d : main.c defs.h
#+END_SRC
于是，我们的 .d 文件也会自动更新了，并会自动生成了，当然，你还可以在这个 .d 文件中加入的不只是依赖关系，包括生成的命令也可一并加入，让每个 .d 文件都包含一个完赖的规则。一旦我们完成这个工作，接下来，我们就要把这些自动生成的规则放进我们的主Makefile中。我们可以使用Makefile的“include”命令，来引入别的Makefile文件（前面讲过），例如：
#+begin_src makefile
sources = foo.c bar.c

include $(sources:.c=.d)
#+END_SRC
上述语句中的 $(sources:.c=.d) 中的 .c=.d 的意思是做一个替换，把变量 $(sources) 所有 .c 的字串都替换成 .d ，关于这个“替换”的内容，在后面我会有更为详细的讲述。当然，你得注意次序，因为include是按次序来载入文件，最先载入的 .d 文件中的目标会成为默认目标。
** 命令的书写
每条规则中的命令和操作系统Shell的命令行是一致的。make会一按顺序一条一条的执行命令，每条命令的开头必须以 Tab 键开头，除非，命令是紧跟在依赖规则后面的分号后的。在命令行之间中的空格或是空行会被忽略，但是如果该空格或空行是以Tab键开头的，那么make会认为其是一个空命令。

我们在UNIX下可能会使用不同的Shell，但是make的命令默认是被 /bin/sh ——UNIX的标准Shell 解释执行的。除非你特别指定一个其它的Shell。Makefile中， # 是注释符，很像C/C++中的 // ，其后的本行字符都被注释。
** 显示命令
通常，make会把其要执行的命令行在命令执行前输出到屏幕上。当我们用 @ 字符在命令行前，那么，这个命令将不被make显示出来，最具代表性的例子是，我们用这个功能来向屏幕显示一些信息。如:
#+begin_src makefile
@echo 正在编译XXX模块......
#+END_SRC
当make执行时，会输出“正在编译XXX模块……”字串，但不会输出命令，如果没有“@”，那么，make将输出:
#+begin_src makefile
echo 正在编译XXX模块......
正在编译XXX模块......
#+END_SRC
如果make执行时，带入make参数 -n 或 --just-print ，那么其只是显示命令，但不会执行命令，这个功能很有利于我们调试我们的Makefile，看看我们书写的命令是执行起来是什么样子的或是什么顺序的。

而make参数 -s 或 --silent 或 --quiet 则是全面禁止命令的显示。
** 命令执行
当依赖目标新于目标时，也就是当规则的目标需要被更新时，make会一条一条的执行其后的命令。需要注意的是，如果你要让上一条命令的结果应用在下一条命令时，你应该使用分号分隔这两条命令。比如你的第一条命令是cd命令，你希望第二条命令得在cd之后的基础上运行，那么你就不能把这两条命令写在两行上，而应该把这两条命令写在一行上，用分号分隔。如：

示例一：
#+begin_src makefile
exec:
    cd /home/hchen
    pwd
#+END_SRC
示例二：
#+begin_src makefile
exec:
    cd /home/hchen; pwd
#+END_SRC

当我们执行 make exec 时，第一个例子中的cd没有作用，pwd会打印出当前的Makefile目录，而第二个例子中，cd就起作用了，pwd会打印出“/home/hchen”。

make一般是使用环境变量SHELL中所定义的系统Shell来执行命令，默认情况下使用UNIX的标准Shell——/bin/sh来执行命令。
但在MS-DOS下有点特殊，因为MS-DOS下没有SHELL环境变量，当然你也可以指定。
如果你指定了UNIX风格的目录形式，首先，make会在SHELL所指定的路径中找寻命令解释器，
如果找不到，其会在当前盘符中的当前目录中寻找，
如果再找不到，其会在PATH环境变量中所定义的所有路径中寻找。
MS-DOS中，如果你定义的命令解释器没有找到，其会给你的命令解释器加上诸如 .exe 、 .com 、 .bat 、 .sh 等后缀。
** 命令出错
每当命令运行完后，make会检测每个命令的返回码，如果命令返回成功，那么make会执行下一条命令，当规则中所有的命令成功返回后，这个规则就算是成功完成了。如果一个规则中的某个命令出错了（命令退出码非零），那么make就会终止执行当前规则，这将有可能终止所有规则的执行。

有些时候，命令的出错并不表示就是错误的。例如mkdir命令，我们一定需要建立一个目录，如果目录不存在，那么mkdir就成功执行，万事大吉，如果目录存在，那么就出错了。我们之所以使用mkdir的意思就是一定要有这样的一个目录，于是我们就不希望mkdir出错而终止规则的运行。

为了做到这一点，忽略命令的出错，我们可以在Makefile的命令行前加一个减号 - （在Tab键之后），标记为不管命令出不出错都认为是成功的。如：
#+begin_src makefile
clean:
    -rm -f *.o
#+END_SRC
还有一个全局的办法是，给make加上 -i 或是 --ignore-errors 参数，那么，Makefile中所有命令都会忽略错误。而如果一个规则是以 .IGNORE 作为目标的，那么这个规则中的所有命令将会忽略错误。这些是不同级别的防止命令出错的方法，你可以根据你的不同喜欢设置。

还有一个要提一下的make的参数的是 -k 或是 --keep-going ，这个参数的意思是，如果某规则中的命令出错了，那么就终止该规则的执行，但继续执行其它规则。
** 嵌套执行make
在一些大的工程中，我们会把我们不同模块或是不同功能的源文件放在不同的目录中，我们可以在每个目录中都书写一个该目录的Makefile，这有利于让我们的Makefile变得更加地简洁，而不至于把所有的东西全部写在一个Makefile中，这样会很难维护我们的Makefile，这个技术对于我们模块编译和分段编译有着非常大的好处。

例如，我们有一个子目录叫subdir，这个目录下有个Makefile文件，来指明了这个目录下文件的编译规则。那么我们总控的Makefile可以这样书写：
#+begin_src makefile
subsystem:
    cd subdir && $(MAKE)
#+END_SRC
其等价于：
#+begin_src makefile
subsystem:
    $(MAKE) -C subdir
#+END_SRC
定义$(MAKE)宏变量的意思是，也许我们的make需要一些参数，所以定义成一个变量比较利于维护。这两个例子的意思都是先进入“subdir”目录，然后执行make命令。

我们把这个Makefile叫做“总控Makefile”，总控Makefile的变量可以传递到下级的Makefile中（如果你显示的声明），但是不会覆盖下层的Makefile中所定义的变量，除非指定了 -e 参数。

如果你要传递变量到下级Makefile中，那么你可以使用这样的声明:
#+begin_src makefile
export <variable ...>;
#+END_SRC
如果你不想让某些变量传递到下级Makefile中，那么你可以这样声明:
#+begin_src makefile
unexport <variable ...>;
#+END_SRC
如：

示例一：
#+begin_src makefile
export variable = value
#+END_SRC
其等价于：
#+begin_src makefile
variable = value
export variable
#+END_SRC
其等价于：
#+begin_src makefile
export variable := value
#+END_SRC
其等价于：
#+begin_src makefile
variable := value
export variable
#+END_SRC
示例二：
#+begin_src makefile
export variable += value
#+END_SRC
其等价于：
#+begin_src makefile
variable += value
export variable
#+END_SRC
如果你要传递所有的变量，那么，只要一个export就行了。后面什么也不用跟，表示传递所有的变量。

需要注意的是，有两个变量，一个是 SHELL ，一个是 MAKEFLAGS ，这两个变量不管你是否export，其总是要传递到下层 Makefile中，特别是 MAKEFLAGS 变量，其中包含了make的参数信息，
如果我们执行“总控Makefile”时有make参数或是在上层 Makefile中定义了这个变量，那么 MAKEFLAGS 变量将会是这些参数，并会传递到下层Makefile中，这是一个系统级的环境变量。

但是make命令中的有几个参数并不往下传递，它们是 -C , -f , -h, -o 和 -W （有关Makefile参数的细节将在后面说明），如果你不想往下层传递参数，那么，你可以这样来：
#+begin_src makefile
subsystem:
    cd subdir && $(MAKE) MAKEFLAGS=
#+END_SRC
如果你定义了环境变量 MAKEFLAGS ，那么你得确信其中的选项是大家都会用到的，如果其中有 -t , -n 和 -q 参数，那么将会有让你意想不到的结果，或许会让你异常地恐慌。

还有一个在“嵌套执行”中比较有用的参数， -w 或是 --print-directory 会在make的过程中输出一些信息，让你看到目前的工作目录。
比如，如果我们的下级make目录是“/home/hchen/gnu/make”，如果我们使用 make -w 来执行，那么当进入该目录时，我们会看到:
#+begin_src makefile
make: Entering directory `/home/hchen/gnu/make'.
#+END_SRC
而在完成下层make后离开目录时，我们会看到:
#+begin_src makefile
make: Leaving directory `/home/hchen/gnu/make'
#+END_SRC
当你使用 -C 参数来指定make下层Makefile时， -w 会被自动打开的。如果参数中有 -s （ --slient ）或是 --no-print-directory ，那么， -w 总是失效的。
** 定义命令包
如果Makefile中出现一些相同命令序列，那么我们可以为这些相同的命令序列定义一个变量。定义这种命令序列的语法以 define 开始，以 endef 结束，如:
#+begin_src makefile
define run-yacc
yacc $(firstword $^)
mv y.tab.c $@
endef
#+END_SRC
这里，“run-yacc”是这个命令包的名字，其不要和Makefile中的变量重名。在 define 和 endef 中的两行就是命令序列。这个命令包中的第一个命令是运行Yacc程序，因为Yacc程序总是生成“y.tab.c”的文件，所以第二行的命令就是把这个文件改改名字。还是把这个命令包放到一个示例中来看看吧。
#+begin_src makefile
foo.c : foo.y
    $(run-yacc)
#+END_SRC
我们可以看见，要使用这个命令包，我们就好像使用变量一样。在这个命令包的使用中，命令包“run-yacc”中的 $^ 就是 foo.y ， $@ 就是 foo.c （有关这种以 $ 开头的特殊变量，我们会在后面介绍），make在执行命令包时，命令包中的每个命令会被依次独立执行。
** 使用条件判断
使用条件判断，可以让make根据运行时的不同情况选择不同的执行分支。条件表达式可以是比较变量的值，或是比较变量和常量的值。
*** 示例
下面的例子，判断 $(CC) 变量是否 gcc ，如果是的话，则使用GNU函数编译目标。
#+begin_src makefile
libs_for_gcc = -lgnu
normal_libs =

foo: $(objects)
ifeq ($(CC),gcc)
    $(CC) -o foo $(objects) $(libs_for_gcc)
else
    $(CC) -o foo $(objects) $(normal_libs)
endif

#+END_SRC
可见，在上面示例的这个规则中，目标 foo 可以根据变量 $(CC) 值来选取不同的函数库来编译程序。

我们可以从上面的示例中看到三个关键字： ifeq 、 else 和 endif 。 ifeq 的意思表示条件语句的开始，并指定一个条件表达式，表达式包含两个参数，以逗号分隔，表达式以圆括号括起。 else 表示条件表达式为假的情况。 endif 表示一个条件语句的结束，任何一个条件表达式都应该以 endif 结束。

当我们的变量 $(CC) 值是 gcc 时，目标 foo 的规则是：
#+begin_src makefile
foo: $(objects)
    $(CC) -o foo $(objects) $(libs_for_gcc)

#+END_SRC
而当我们的变量 $(CC) 值不是 gcc 时（比如 cc ），目标 foo 的规则是：
#+begin_src makefile
foo: $(objects)
    $(CC) -o foo $(objects) $(normal_libs)

#+END_SRC
当然，我们还可以把上面的那个例子写得更简洁一些：
#+begin_src makefile
libs_for_gcc = -lgnu
normal_libs =

ifeq ($(CC),gcc)
    libs=$(libs_for_gcc)
else
    libs=$(normal_libs)
endif

foo: $(objects)
    $(CC) -o foo $(objects) $(libs)

#+END_SRC
*** 语法
条件表达式的语法为:
#+begin_src makefile
<conditional-directive>
<text-if-true>
endif

#+END_SRC
以及:
#+begin_src makefile
<conditional-directive>
<text-if-true>
else
<text-if-false>
endif

#+END_SRC
其中 <conditional-directive> 表示条件关键字，如 ifeq 。这个关键字有四个。

第一个是我们前面所见过的 ifeq
#+begin_src makefile
ifeq (<arg1>, <arg2>)
ifeq '<arg1>' '<arg2>'
ifeq "<arg1>" "<arg2>"
ifeq "<arg1>" '<arg2>'
ifeq '<arg1>' "<arg2>"

#+END_SRC
比较参数 arg1 和 arg2 的值是否相同。当然，参数中我们还可以使用make的函数。如:
#+begin_src makefile
ifeq ($(strip $(foo)),)
<text-if-empty>
endif

#+END_SRC
这个示例中使用了 strip 函数，如果这个函数的返回值是空（Empty），那么 <text-if-empty> 就生效。

第二个条件关键字是 ifneq 。语法是：
#+begin_src makefile
ifneq (<arg1>, <arg2>)
ifneq '<arg1>' '<arg2>'
ifneq "<arg1>" "<arg2>"
ifneq "<arg1>" '<arg2>'
ifneq '<arg1>' "<arg2>"

#+END_SRC
其比较参数 arg1 和 arg2 的值是否相同，如果不同，则为真。和 ifeq 类似。

第三个条件关键字是 ifdef 。语法是：
#+begin_src makefile
ifdef <variable-name>

#+END_SRC
如果变量 <variable-name> 的值非空，那到表达式为真。否则，表达式为假。当然， <variable-name> 同样可以是一个函数的返回值。注意， ifdef 只是测试一个变量是否有值，其并不会把变量扩展到当前位置。还是来看两个例子：

示例一：
#+begin_src makefile
bar =
foo = $(bar)
ifdef foo
    frobozz = yes
else
    frobozz = no
endif

#+END_SRC
示例二：
#+begin_src makefile
foo =
ifdef foo
    frobozz = yes
else
    frobozz = no
endif

#+END_SRC
第一个例子中， $(frobozz) 值是 yes ，第二个则是 no。

第四个条件关键字是 ifndef 。其语法是：
#+begin_src makefile
ifndef <variable-name>

#+END_SRC
这个我就不多说了，和 ifdef 是相反的意思。

在 <conditional-directive> 这一行上，多余的空格是被允许的，但是不能以 Tab 键作为开始（不然就被认为是命令）。而注释符 # 同样也是安全的。 else 和 endif 也一样，只要不是以 Tab 键开始就行了。

特别注意的是，make是在读取Makefile时就计算条件表达式的值，并根据条件表达式的值来选择语句，所以，你最好不要把自动化变量（如 $@ 等）放入条件表达式中，因为自动化变量是在运行时才有的。

而且为了避免混乱，make不允许把整个条件语句分成两部分放在不同的文件中。
** 隐含规则
*** 使用隐含规则
如果要使用隐含规则生成你需要的目标，你所需要做的就是不要写出这个目标的规则。那么，make会试图去自动推导产生这个目标的规则和命令，如果 make可以自动推导生成这个目标的规则和命令，那么这个行为就是隐含规则的自动推导。当然，隐含规则是make事先约定好的一些东西。例如，我们有下面的一个Makefile：
#+begin_src makefile
foo : foo.o bar.o
    cc –o foo foo.o bar.o $(CFLAGS) $(LDFLAGS)

#+END_SRC
我们可以注意到，这个Makefile中并没有写下如何生成 foo.o 和 bar.o 这两目标的规则和命令。因为make的“隐含规则”功能会自动为我们自动去推导这两个目标的依赖目标和生成命令。

make会在自己的“隐含规则”库中寻找可以用的规则，如果找到，那么就会使用。如果找不到，那么就会报错。在上面的那个例子中，make调用的隐含规则是，把 .o 的目标的依赖文件置成 .c ，并使用C的编译命令 cc –c $(CFLAGS)  foo.c 来生成 foo.o 的目标。也就是说，我们完全没有必要写下下面的两条规则：
#+begin_src makefile
foo.o : foo.c
    cc –c foo.c $(CFLAGS)
bar.o : bar.c
    cc –c bar.c $(CFLAGS)

#+END_SRC
因为，这已经是“约定”好了的事了，make和我们约定好了用C编译器 cc 生成 .o 文件的规则，这就是隐含规则。

当然，如果我们为 .o 文件书写了自己的规则，那么make就不会自动推导并调用隐含规则，它会按照我们写好的规则忠实地执行。

还有，在make的“隐含规则库”中，每一条隐含规则都在库中有其顺序，越靠前的则是越被经常使用的，所以，这会导致我们有些时候即使我们显示地指定了目标依赖，make也不会管。如下面这条规则（没有命令）：
#+begin_src makefile
foo.o : foo.p

#+END_SRC
依赖文件 foo.p （Pascal程序的源文件）有可能变得没有意义。如果目录下存在了 foo.c 文件，那么我们的隐含规则一样会生效，并会通过 foo.c 调用C的编译器生成 foo.o 文件。因为，在隐含规则中，Pascal的规则出现在C的规则之后，所以，make找到可以生成 foo.o 的C的规则就不再寻找下一条规则了。如果你确实不希望任何隐含规则推导，那么，你除了写依赖规则，还要吧把命令也写出来、
*** 隐含规则一览
这里我们将讲述所有预先设置（也就是make内建）的隐含规则，如果我们不明确地写下规则，那么，make就会在这些规则中寻找所需要规则和命令。
当然，我们也可以使用make的参数 -r 或 --no-builtin-rules 选项来取消所有的预设置的隐含规则。

当然，即使是我们指定了 -r 参数，某些隐含规则还是会生效，因为有许多的隐含规则都是使用了“后缀规则”来定义的，所以，只要隐含规则中有 “后缀列表”（也就是系统定义在目标 .SUFFIXES 的依赖目标），那么隐含规则就会生效。
默认的后缀列表是：.out, .a, .ln, .o, .c, .cc, .C, .p, .f, .F, .r, .y, .l, .s, .S, .mod, .sym, .def, .h, .info, .dvi, .tex, .texinfo, .texi, .txinfo, .w, .ch .web, .sh, .elc, .el。

还是先来看一看常用的隐含规则吧。
**** 编译C程序的隐含规则

<n>.o 的目标的依赖目标会自动推导为 <n>.c ，并且其生成命令是 $(CC) –c $(CPPFLAGS) $(CFLAGS)
**** 编译C++程序的隐含规则

<n>.o 的目标的依赖目标会自动推导为 <n>.cc 或是 <n>.C ，并且其生成命令是 $(CXX) –c $(CPPFLAGS) $(CXXFLAGS) 。（建议使用 .cc 作为C++源文件的后缀，而不是 .C ）
**** 编译Pascal程序的隐含规则
**** <n>.o 的目标的依赖目标会自动推导为 <n>.p ，并且其生成命令是 $(PC) –c  $(PFLAGS) 
**** 编译Fortran/Ratfor程序的隐含规则

<n>.o 的目标的依赖目标会自动推导为 <n>.r 或 <n>.F 或 <n>.f ，并且其生成命令是:

.f $(FC) –c  $(FFLAGS)

.F $(FC) –c  $(FFLAGS) $(CPPFLAGS)

.f $(FC) –c  $(FFLAGS) $(RFLAGS)
**** 预处理Fortran/Ratfor程序的隐含规则

<n>.f 的目标的依赖目标会自动推导为 <n>.r 或 <n>.F 。这个规则只是转换Ratfor 或有预处理的Fortran程序到一个标准的Fortran程序。其使用的命令是：

.F $(FC) –F $(CPPFLAGS) $(FFLAGS)

.r $(FC) –F $(FFLAGS) $(RFLAGS)
**** 编译Modula-2程序的隐含规则

<n>.sym 的目标的依赖目标会自动推导为 <n>.def ，并且其生成命令是： $(M2C) $(M2FLAGS) $(DEFFLAGS) 。 <n>.o 的目标的依赖目标会自动推导为 <n>.mod ，并且其生成命令是： $(M2C) $(M2FLAGS) $(MODFLAGS) 。
**** 汇编和汇编预处理的隐含规则

<n>.o 的目标的依赖目标会自动推导为 <n>.s ，默认使用编译器 as ，并且其生成命令是： $ (AS) $(ASFLAGS) 。 <n>.s 的目标的依赖目标会自动推导为 <n>.S ，默认使用C预编译器 cpp ，并且其生成命令是： $(AS) $(ASFLAGS) 。
**** 链接Object文件的隐含规则

<n> 目标依赖于 <n>.o ，通过运行C的编译器来运行链接程序生成（一般是 ld ），其生成命令是： $(CC) $(LDFLAGS) <n>.o $(LOADLIBES) $(LDLIBS) 。这个规则对于只有一个源文件的工程有效，同时也对多个Object文件（由不同的源文件生成）的也有效。
例如如下规则:
#+begin_src makefile
x : y.o z.o
#+END_SRC
并且 x.c 、 y.c 和 z.c 都存在时，隐含规则将执行如下命令:
#+begin_src makefile
cc -c x.c -o x.o
cc -c y.c -o y.o
cc -c z.c -o z.o
cc x.o y.o z.o -o x
rm -f x.o
rm -f y.o
rm -f z.o

#+END_SRC
如果没有一个源文件（如上例中的x.c）和你的目标名字（如上例中的x）相关联，那么，你最好写出自己的生成规则，不然，隐含规则会报错的。
**** Yacc C程序时的隐含规则

<n>.c 的依赖文件被自动推导为 n.y （Yacc生成的文件），其生成命令是： $(YACC) $(YFALGS) 。（“Yacc”是一个语法分析器，关于其细节请查看相关资料）
**** Lex C程序时的隐含规则

<n>.c 的依赖文件被自动推导为 n.l （Lex生成的文件），其生成命令是： $(LEX) $(LFALGS) 。（关于“Lex”的细节请查看相关资料）
**** Lex Ratfor程序时的隐含规则

<n>.r 的依赖文件被自动推导为 n.l （Lex生成的文件），其生成命令是： $(LEX) $(LFALGS) 。
**** 从C程序、Yacc文件或Lex文件创建Lint库的隐含规则

<n>.ln （lint生成的文件）的依赖文件被自动推导为 n.c ，其生成命令是： $(LINT) $(LINTFALGS) $(CPPFLAGS) -i 。对于 <n>.y 和 <n>.l 也是同样的规则。
*** 隐含规则使用的变量
在隐含规则中的命令中，基本上都是使用了一些预先设置的变量。你可以在你的makefile中改变这些变量的值，或是在make的命令行中传入这些值，或是在你的环境变量中设置这些值，无论怎么样，只要设置了这些特定的变量，那么其就会对隐含规则起作用。当然，你也可以利用make的 -R 或 --no–builtin-variables 参数来取消你所定义的变量对隐含规则的作用。

例如，第一条隐含规则——编译C程序的隐含规则的命令是 $(CC) –c $(CFLAGS) $(CPPFLAGS) 。Make默认的编译命令是 cc ，如果你把变量 $(CC) 重定义成 gcc ，把变量 $(CFLAGS) 重定义成 -g ，那么，隐含规则中的命令全部会以 gcc –c -g $(CPPFLAGS) 的样子来执行了。

我们可以把隐含规则中使用的变量分成两种：一种是命令相关的，如 CC ；一种是参数相的关，如 CFLAGS 。下面是所有隐含规则中会用到的变量：

**** 关于命令的变量
AR : 函数库打包程序。默认命令是 ar

AS : 汇编语言编译程序。默认命令是 as

CC : C语言编译程序。默认命令是 cc

CXX : C++语言编译程序。默认命令是 g++

CO : 从 RCS文件中扩展文件程序。默认命令是 co

CPP : C程序的预处理器（输出是标准输出设备）。默认命令是 $(CC) –E

FC : Fortran 和 Ratfor 的编译器和预处理程序。默认命令是 f77

GET : 从SCCS文件中扩展文件的程序。默认命令是 get

LEX : Lex方法分析器程序（针对于C或Ratfor）。默认命令是 lex

PC : Pascal语言编译程序。默认命令是 pc

YACC : Yacc文法分析器（针对于C程序）。默认命令是 yacc

YACCR : Yacc文法分析器（针对于Ratfor程序）。默认命令是 yacc –r

MAKEINFO : 转换Texinfo源文件（.texi）到Info文件程序。默认命令是 makeinfo

TEX : 从TeX源文件创建TeX DVI文件的程序。默认命令是 tex

TEXI2DVI : 从Texinfo源文件创建军TeX DVI 文件的程序。默认命令是 texi2dvi

WEAVE : 转换Web到TeX的程序。默认命令是 weave

CWEAVE : 转换C Web 到 TeX的程序。默认命令是 cweave

TANGLE : 转换Web到Pascal语言的程序。默认命令是 tangle

CTANGLE : 转换C Web 到 C。默认命令是 ctangle

RM : 删除文件命令。默认命令是 rm –f

**** 关于命令参数的变量
下面的这些变量都是相关上面的命令的参数。如果没有指明其默认值，那么其默认值都是空。

ARFLAGS : 函数库打包程序AR命令的参数。默认值是 rv

ASFLAGS : 汇编语言编译器参数。（当明显地调用 .s 或 .S 文件时）

CFLAGS : C语言编译器参数。

CXXFLAGS : C++语言编译器参数。

COFLAGS : RCS命令参数。

CPPFLAGS : C预处理器参数。（ C 和 Fortran 编译器也会用到）。

FFLAGS : Fortran语言编译器参数。

GFLAGS : SCCS “get”程序参数。

LDFLAGS : 链接器参数。（如： ld ）

LFLAGS : Lex文法分析器参数。

PFLAGS : Pascal语言编译器参数。

RFLAGS : Ratfor 程序的Fortran 编译器参数。

YFLAGS : Yacc文法分析器参数。

*** 隐含规则链
有些时候，一个目标可能被一系列的隐含规则所作用。
例如，一个 .o 的文件生成，可能会是先被 Yacc的[.y]文件先成 .c ，然后再被C的编译器生成。我们把这一系列的隐含规则叫做“隐含规则链”。

在上面的例子中，如果文件 .c 存在，那么就直接调用C的编译器的隐含规则，如果没有 .c 文件，但有一个 .y 文件，那么Yacc的隐含规则会被调用，生成 .c 文件，然后，再调用C编译的隐含规则最终由 .c 生成 .o 文件，达到目标。

我们把这种 .c 的文件（或是目标），叫做中间目标。不管怎么样，make会努力自动推导生成目标的一切方法，不管中间目标有多少，其都会执着地把所有的隐含规则和你书写的规则全部合起来分析，努力达到目标，所以，有些时候，可能会让你觉得奇怪，怎么我的目标会这样生成？怎么我的 makefile发疯了？

在默认情况下，对于中间目标，它和一般的目标有两个地方所不同：第一个不同是除非中间的目标不存在，才会引发中间规则。第二个不同的是，只要目标成功产生，那么，产生最终目标过程中，所产生的中间目标文件会被以 rm -f 删除。

通常，一个被makefile指定成目标或是依赖目标的文件不能被当作中介。然而，你可以明显地说明一个文件或是目标是中介目标，你可以使用伪目标 .INTERMEDIATE 来强制声明。（如： .INTERMEDIATE : mid ）

你也可以阻止make自动删除中间目标，要做到这一点，你可以使用伪目标 .SECONDARY 来强制声明（如： .SECONDARY : sec ）。你还可以把你的目标，以模式的方式来指定（如： %.o ）成伪目标 .PRECIOUS 的依赖目标，以保存被隐含规则所生成的中间文件。

在“隐含规则链”中，禁止同一个目标出现两次或两次以上，这样一来，就可防止在make自动推导时出现无限递归的情况。

Make会优化一些特殊的隐含规则，而不生成中间文件。如，从文件 foo.c 生成目标程序 foo ，按道理，make会编译生成中间文件 foo.o ，然后链接成 foo ，但在实际情况下，这一动作可以被一条 cc 的命令完成（ cc –o foo foo.c ），于是优化过的规则就不会生成中间文件。

*** 定义模式规则
你可以使用模式规则来定义一个隐含规则。一个模式规则就好像一个一般的规则，只是在规则中，目标的定义需要有 % 字符。 % 的意思是表示一个或多个任意字符。在依赖目标中同样可以使用 % ，只是依赖目标中的 % 的取值，取决于其目标。

有一点需要注意的是， % 的展开发生在变量和函数的展开之后，变量和函数的展开发生在make载入 Makefile时，而模式规则中的 % 则发生在运行时。

**** 模式规则介绍
模式规则中，至少在规则的目标定义中要包含 % ，否则，就是一般的规则。目标中的 % 定义表示对文件名的匹配， % 表示长度任意的非空字符串。例如： %.c 表示以 .c 结尾的文件名（文件名的长度至少为3），而 s.%.c 则表示以 s. 开头， .c 结尾的文件名（文件名的长度至少为5）。

如果 % 定义在目标中，那么，目标中的 % 的值决定了依赖目标中的 % 的值，也就是说，目标中的模式的 % 决定了依赖目标中 % 的样子。例如有一个模式规则如下：
#+begin_src makefile
%.o : %.c ; <command ......>;

#+END_SRC
其含义是，指出了怎么从所有的 .c 文件生成相应的 .o 文件的规则。如果要生成的目标是 a.o b.o ，那么 %c 就是 a.c b.c 。

一旦依赖目标中的 % 模式被确定，那么，make会被要求去匹配当前目录下所有的文件名，一旦找到，make就会规则下的命令，所以，在模式规则中，目标可能会是多个的，如果有模式匹配出多个目标，make就会产生所有的模式目标，此时，make关心的是依赖的文件名和生成目标的命令这两件事。

**** 模式规则示例
下面这个例子表示了,把所有的 .c 文件都编译成 .o 文件.
#+begin_src makefile
%.o : %.c
    $(CC) -c $(CFLAGS) $(CPPFLAGS) $< -o $@

#+END_SRC
其中， $@ 表示所有的目标的挨个值， $< 表示了所有依赖目标的挨个值。这些奇怪的变量我们叫“自动化变量”，后面会详细讲述。

下面的这个例子中有两个目标是模式的：
#+begin_src makefile
%.tab.c %.tab.h: %.y
    bison -d $<

#+END_SRC
这条规则告诉make把所有的 .y 文件都以 bison -d <n>.y 执行，然后生成 <n>.tab.c 和 <n>.tab.h 文件。（其中， <n> 表示一个任意字符串）。
如果我们的执行程序 foo 依赖于文件 parse.tab.o 和 scan.o ，并且文件 scan.o 依赖于文件 parse.tab.h 。
如果 parse.y 文件被更新了，那么根据上述的规则， bison -d parse.y 就会被执行一次，于是， parse.tab.o 和 scan.o 的依赖文件就齐了。
（假设， parse.tab.o 由 parse.tab.c 生成，和 scan.o 由 scan.c 生成，而 foo 由 parse.tab.o 和 scan.o 链接生成，而且 foo 和其 .o 文件的依赖关系也写好，那么，所有的目标都会得到满足）

**** 自动化变量
在上述的模式规则中，目标和依赖文件都是一系例的文件，那么我们如何书写一个命令来完成从不同的依赖文件生成相应的目标？因为在每一次的对模式规则的解析时，都会是不同的目标和依赖文件。

自动化变量就是完成这个功能的。在前面，我们已经对自动化变量有所提涉，相信你看到这里已对它有一个感性认识了。所谓自动化变量，就是这种变量会把模式中所定义的一系列的文件自动地挨个取出，直至所有的符合模式的文件都取完了。这种自动化变量只应出现在规则的命令中。

下面是所有的自动化变量及其说明：

$@ : 表示规则中的目标文件集。在模式规则中，如果有多个目标，那么， $@ 就是匹配于目标中模式定义的集合。

$% : 仅当目标是函数库文件中，表示规则中的目标成员名。例如，如果一个目标是 foo.a(bar.o) ，那么， $% 就是 bar.o ， $@ 就是 foo.a 。如果目标不是函数库文件（Unix下是 .a ，Windows下是 .lib ），那么，其值为空。

$< : 依赖目标中的第一个目标名字。如果依赖目标是以模式（即 % ）定义的，那么 $< 将是符合模式的一系列的文件集。注意，其是一个一个取出来的。

$? : 所有比目标新的依赖目标的集合。以空格分隔。

$^ : 所有的依赖目标的集合。以空格分隔。如果在依赖目标中有多个重复的，那么这个变量会去除重复的依赖目标，只保留一份。

$+ : 这个变量很像 $^ ，也是所有依赖目标的集合。只是它不去除重复的依赖目标。

$* : 这个变量表示目标模式中 % 及其之前的部分。如果目标是 dir/a.foo.b ，并且目标的模式是 a.%.b ，那么， $* 的值就是 dir/foo 。这个变量对于构造有关联的文件名是比较有效。如果目标中没有模式的定义，那么 $* 也就不能被推导出，但是，如果目标文件的后缀是make所识别的，那么 $* 就是除了后缀的那一部分。例如：如果目标是 foo.c ，因为 .c 是make所能识别的后缀名，所以， $* 的值就是 foo 。这个特性是GNU make的，很有可能不兼容于其它版本的make，所以，你应该尽量避免使用 $* ，除非是在隐含规则或是静态模式中。如果目标中的后缀是make所不能识别的，那么 $* 就是空值。

当你希望只对更新过的依赖文件进行操作时， $? 在显式规则中很有用，例如，假设有一个函数库文件叫 lib ，其由其它几个object文件更新。那么把object文件打包的比较有效率的Makefile规则是：
#+begin_src makefile
lib : foo.o bar.o lose.o win.o
    ar r lib $?

#+END_SRC
在上述所列出来的自动量变量中。四个变量（ $@ 、 $< 、 $% 、 $* ）在扩展时只会有一个文件，而另三个的值是一个文件列表。这七个自动化变量还可以取得文件的目录名或是在当前目录下的符合模式的文件名，只需要搭配上 D 或 F 字样。这是GNU make中老版本的特性，在新版本中，我们使用函数 dir 或 notdir 就可以做到了。 D 的含义就是Directory，就是目录， F 的含义就是File，就是文件。

下面是对于上面的七个变量分别加上 D 或是 F 的含义：

$(@D)
表示 $@ 的目录部分（不以斜杠作为结尾），如果 $@ 值是 dir/foo.o ，那么 $(@D) 就是 dir ，而如果 $@ 中没有包含斜杠的话，其值就是 . （当前目录）。

$(@F)
表示 $@ 的文件部分，如果 $@ 值是 dir/foo.o ，那么 $(@F) 就是 foo.o ， $(@F) 相当于函数 $(notdir $@) 。

$(*D), $(*F)
和上面所述的同理，也是取文件的目录部分和文件部分。对于上面的那个例子， $(*D) 返回 dir ，而 $(*F) 返回 foo

$(%D), $(%F)
分别表示了函数包文件成员的目录部分和文件部分。这对于形同 archive(member) 形式的目标中的 member 中包含了不同的目录很有用。

$(<D), $(<F)
分别表示依赖文件的目录部分和文件部分。

$(^D), $(^F)
分别表示所有依赖文件的目录部分和文件部分。（无相同的）

$(+D), $(+F)
分别表示所有依赖文件的目录部分和文件部分。（可以有相同的）

$(?D), $(?F)
分别表示被更新的依赖文件的目录部分和文件部分。

最后想提醒一下的是，对于 $< ，为了避免产生不必要的麻烦，我们最好给 $ 后面的那个特定字符都加上圆括号，比如， $(<) 就要比 $< 要好一些。

还得要注意的是，这些变量只使用在规则的命令中，而且一般都是“显式规则”和“静态模式规则”（参见前面“书写规则”一章）。其在隐含规则中并没有意义。

**** 模式的匹配
一般来说，一个目标的模式有一个有前缀或是后缀的 % ，或是没有前后缀，直接就是一个 % 。因为 % 代表一个或多个字符，所以在定义好了的模式中，我们把 % 所匹配的内容叫做“茎”，例如 %.c 所匹配的文件“test.c”中“test”就是“茎”。因为在目标和依赖目标中同时有 % 时，依赖目标的“茎”会传给目标，当做目标中的“茎”。

当一个模式匹配包含有斜杠（实际也不经常包含）的文件时，那么在进行模式匹配时，目录部分会首先被移开，然后进行匹配，成功后，再把目录加回去。在进行“茎”的传递时，我们需要知道这个步骤。例如有一个模式 e%t ，文件 src/eat 匹配于该模式，于是 src/a 就是其“茎”，如果这个模式定义在依赖目标中，而被依赖于这个模式的目标中又有个模式 c%r ，那么，目标就是 src/car 。（“茎”被传递）

**** 重载内建隐含规则
你可以重载内建的隐含规则（或是定义一个全新的），例如你可以重新构造和内建隐含规则不同的命令，如：

%.o : %.c
    $(CC) -c $(CPPFLAGS) $(CFLAGS) -D$(date)
你可以取消内建的隐含规则，只要不在后面写命令就行。如：

%.o : %.s
同样，你也可以重新定义一个全新的隐含规则，其在隐含规则中的位置取决于你在哪里写下这个规则。朝前的位置就靠前。

*** 老式风格的“后缀规则”
后缀规则是一个比较老式的定义隐含规则的方法。后缀规则会被模式规则逐步地取代。因为模式规则更强更清晰。为了和老版本的Makefile兼容，GNU make同样兼容于这些东西。后缀规则有两种方式：“双后缀”和“单后缀”。

双后缀规则定义了一对后缀：目标文件的后缀和依赖目标（源文件）的后缀。如 .c.o 相当于 %o : %c 。单后缀规则只定义一个后缀，也就是源文件的后缀。如 .c 相当于 % : %.c 。

后缀规则中所定义的后缀应该是make所认识的，如果一个后缀是make所认识的，那么这个规则就是单后缀规则，而如果两个连在一起的后缀都被make所认识，那就是双后缀规则。例如： .c 和 .o 都是make所知道。因而，如果你定义了一个规则是 .c.o 那么其就是双后缀规则，意义就是 .c 是源文件的后缀， .o 是目标文件的后缀。如下示例：

.c.o:
    $(CC) -c $(CFLAGS) $(CPPFLAGS) -o $@ $<
后缀规则不允许任何的依赖文件，如果有依赖文件的话，那就不是后缀规则，那些后缀统统被认为是文件名，如：

.c.o: foo.h
    $(CC) -c $(CFLAGS) $(CPPFLAGS) -o $@ $<
这个例子，就是说，文件 .c.o 依赖于文件 foo.h ，而不是我们想要的这样：

%.o: %.c foo.h
    $(CC) -c $(CFLAGS) $(CPPFLAGS) -o $@ $<
后缀规则中，如果没有命令，那是毫无意义的。因为他也不会移去内建的隐含规则。

而要让make知道一些特定的后缀，我们可以使用伪目标 .SUFFIXES 来定义或是删除，如：

.SUFFIXES: .hack .win
把后缀 .hack 和 .win 加入后缀列表中的末尾。

.SUFFIXES:              # 删除默认的后缀
.SUFFIXES: .c .o .h   # 定义自己的后缀
先清除默认后缀，后定义自己的后缀列表。

make的参数 -r 或 -no-builtin-rules 也会使用得默认的后缀列表为空。而变量 SUFFIXE 被用来定义默认的后缀列表，你可以用 .SUFFIXES 来改变后缀列表，但请不要改变变量 SUFFIXE 的值。

*** 隐含规则搜索算法
比如我们有一个目标叫 T。下面是搜索目标T的规则的算法。请注意，在下面，我们没有提到后缀规则，原因是，所有的后缀规则在Makefile被载入内存时，会被转换成模式规则。如果目标是 archive(member) 的函数库文件模式，那么这个算法会被运行两次，第一次是找目标T，如果没有找到的话，那么进入第二次，第二次会把 member 当作T来搜索。

把T的目录部分分离出来。叫D，而剩余部分叫N。（如：如果T是 src/foo.o ，那么，D就是 src/ ，N就是 foo.o ）

创建所有匹配于T或是N的模式规则列表。

如果在模式规则列表中有匹配所有文件的模式，如 % ，那么从列表中移除其它的模式。

移除列表中没有命令的规则。

对于第一个在列表中的模式规则：

推导其“茎”S，S应该是T或是N匹配于模式中 % 非空的部分。

计算依赖文件。把依赖文件中的 % 都替换成“茎”S。如果目标模式中没有包含斜框字符，而把D加在第一个依赖文件的开头。

测试是否所有的依赖文件都存在或是理当存在。（如果有一个文件被定义成另外一个规则的目标文件，或者是一个显式规则的依赖文件，那么这个文件就叫“理当存在”）

如果所有的依赖文件存在或是理当存在，或是就没有依赖文件。那么这条规则将被采用，退出该算法。

如果经过第5步，没有模式规则被找到，那么就做更进一步的搜索。对于存在于列表中的第一个模式规则：

如果规则是终止规则，那就忽略它，继续下一条模式规则。

计算依赖文件。（同第5步）

测试所有的依赖文件是否存在或是理当存在。

对于不存在的依赖文件，递归调用这个算法查找他是否可以被隐含规则找到。

如果所有的依赖文件存在或是理当存在，或是就根本没有依赖文件。那么这条规则被采用，退出该算法。

如果没有隐含规则可以使用，查看 .DEFAULT 规则，如果有，采用，把 .DEFAULT 的命令给T使用。

一旦规则被找到，就会执行其相当的命令，而此时，我们的自动化变量的值才会生成。
** 使用make更新函数库文件
函数库文件也就是对Object文件（程序编译的中间文件）的打包文件。在Unix下，一般是由命令 ar 来完成打包工作。

*** 函数库文件的成员
一个函数库文件由多个文件组成。你可以用如下格式指定函数库文件及其组成:

archive(member)
这个不是一个命令，而一个目标和依赖的定义。一般来说，这种用法基本上就是为了 ar 命令来服务的。如:

foolib(hack.o) : hack.o
    ar cr foolib hack.o
如果要指定多个member，那就以空格分开，如:

foolib(hack.o kludge.o)
其等价于:

foolib(hack.o) foolib(kludge.o)
你还可以使用Shell的文件通配符来定义，如:

foolib(*.o)
*** 函数库成员的隐含规则
当make搜索一个目标的隐含规则时，一个特殊的特性是，如果这个目标是 a(m) 形式的，其会把目标变成 (m) 。于是，如果我们的成员是 %.o 的模式定义，并且如果我们使用 make foo.a(bar.o) 的形式调用Makefile时，隐含规则会去找 bar.o 的规则，如果没有定义 bar.o 的规则，那么内建隐含规则生效，make会去找 bar.c 文件来生成 bar.o ，如果找得到的话，make执行的命令大致如下:

cc -c bar.c -o bar.o
ar r foo.a bar.o
rm -f bar.o
还有一个变量要注意的是 $% ，这是专属函数库文件的自动化变量，有关其说明请参见“自动化变量”一节。
*** 函数库文件的后缀规则
你可以使用“后缀规则”和“隐含规则”来生成函数库打包文件，如：

.c.a:
    $(CC) $(CFLAGS) $(CPPFLAGS) -c $< -o $*.o
    $(AR) r $@ $*.o
    $(RM) $*.o
其等效于：

(%.o) : %.c
    $(CC) $(CFLAGS) $(CPPFLAGS) -c $< -o $*.o
    $(AR) r $@ $*.o
    $(RM) $*.o
*** 注意事项
在进行函数库打包文件生成时，请小心使用make的并行机制（ -j 参数）。如果多个 ar 命令在同一时间运行在同一个函数库打包文件上，就很有可以损坏这个函数库文件。所以，在make未来的版本中，应该提供一种机制来避免并行操作发生在函数打包文件上。

但就目前而言，你还是应该不要尽量不要使用 -j 参数。
** 参考文献
[[https://seisman.github.io/how-to-write-makefile/introduction.html][跟我一起写Makefile]]

* mkdir
Linux mkdir（英文全拼：make directory）命令用于创建目录。

语法:
mkdir [-p] dirName

参数说明：
-p 确保目录名称存在，不存在的就建一个。

* mount
#+begin_src bash
mount [-t vfstype] [-o options] device dir
#+END_SRC
1.-t vfstype 指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。
常用类型有：
- 光盘或光盘镜像：iso9660
- DOS fat16文件系统：msdos
- Windows 9x fat32文件系统：vfat
- Windows NT ntfs文件系统：ntfs
- Mount Windows文件网络共享：smbfs
- UNIX(LINUX) 文件网络共享：nfs

2.-o options 主要用来描述设备或档案的挂接方式。
常用的参数有：
- loop：用来把一个文件当成硬盘分区挂接上系统
- ro：采用只读方式挂接设备
- rw：采用读写方式挂接设备
- iocharset：指定访问文件系统所用字符集

3.device 要挂接(mount)的设备。

4.dir设备在系统上的挂接点(mount point)。
* objdump
objdump命令是Linux下的反汇编目标文件或者可执行文件的命令，它以一种可阅读的格式让你更多地了解二进制文件可能带有的附加信息。

参数选项

--archive-headers 
-a 
显示档案库的成员信息,类似ls -l将lib*.a的信息列出。 

-b bfdname 
--target=bfdname 
指定目标码格式。这不是必须的，objdump能自动识别许多格式，比如： 

objdump -b oasys -m vax -h fu.o 
显示fu.o的头部摘要信息，明确指出该文件是Vax系统下用Oasys编译器生成的目标文件。objdump -i将给出这里可以指定的目标码格式列表。 

-C 
--demangle 
将底层的符号名解码成用户级名字，除了去掉所开头的下划线之外，还使得C++函数名以可理解的方式显示出来。 

--debugging 
-g 
显示调试信息。企图解析保存在文件中的调试信息并以C语言的语法显示出来。仅仅支持某些类型的调试信息。有些其他的格式被readelf -w支持。 

-e 
--debugging-tags 
类似-g选项，但是生成的信息是和ctags工具相兼容的格式。 

--disassemble 
-d 
从objfile中反汇编那些特定指令机器码的section。 

-D 
--disassemble-all 
与 -d 类似，但反汇编所有section. 

--prefix-addresses 
反汇编的时候，显示每一行的完整地址。这是一种比较老的反汇编格式。 

-EB 
-EL 
--endian={big|little} 
指定目标文件的小端。这个项将影响反汇编出来的指令。在反汇编的文件没描述小端信息的时候用。例如S-records.

-f 
--file-headers 
显示objfile中每个文件的整体头部摘要信息。 

-h 
--section-headers 
--headers 
显示目标文件各个section的头部摘要信息。 

-H 
--help 
简短的帮助信息。 

-i 
--info 
显示对于 -b 或者 -m 选项可用的架构和目标格式列表。 

-j name
--section=name 
仅仅显示指定名称为name的section的信息 

-l
--line-numbers 
用文件名和行号标注相应的目标代码，仅仅和-d、-D或者-r一起使用使用-ld和使用-d的区别不是很大，在源码级调试的时候有用，要求编译时使用了-g之类的调试编译选项。 

-m machine 
--architecture=machine 
指定反汇编目标文件时使用的架构，当待反汇编文件本身没描述架构信息的时候(比如S-records)，这个选项很有用。可以用-i选项列出这里能够指定的架构.

--reloc 
-r 
显示文件的重定位入口。如果和-d或者-D一起使用，重定位部分以反汇编后的格式显示出来。 

--dynamic-reloc 
-R 
显示文件的动态重定位入口，仅仅对于动态目标文件意义，比如某些共享库。 

-s 
--full-contents 
显示指定section的完整内容。默认所有的非空section都会被显示。 

-S 
--source 
尽可能反汇编出源代码，尤其当编译的时候指定了-g这种调试参数时，效果比较明显。隐含了-d参数。 

--show-raw-insn 
反汇编的时候，显示每条汇编指令对应的机器码，如不指定--prefix-addresses，这将是缺省选项。 

--no-show-raw-insn 
反汇编时，不显示汇编指令的机器码，如不指定--prefix-addresses，这将是缺省选项。 

--start-address=address 
从指定地址开始显示数据，该选项影响-d、-r和-s选项的输出。 

--stop-address=address 
显示数据直到指定地址为止，该项影响-d、-r和-s选项的输出。 

-t 
--syms 
显示文件的符号表入口。类似于nm -s提供的信息 

-T 
--dynamic-syms 
显示文件的动态符号表入口，仅仅对动态目标文件意义，比如某些共享库。它显示的信息类似于 nm -D|--dynamic 显示的信息。 

-V 
--version 
版本信息 

--all-headers 
-x 
显示所可用的头信息，包括符号表、重定位入口。-x 等价于-a -f -h -r -t 同时指定。 

-z 
--disassemble-zeroes 
一般反汇编输出将省略大块的零，该选项使得这些零块也被反汇编。 

@file 
可以将选项集中到一个文件中，然后使用这个@file选项载入。

复制
关于符号表字段下面直接只介绍部分常用的：

.text：已编译程序的机器代码。

.rodata：只读数据，比如printf语句中的格式串和开关（switch）语句的跳转表。

.data：已初始化的全局C变量。局部C变量在运行时被保存在栈中，既不出现在.data中，也不出现在.bss节中。

.bss：未初始化的全局C变量。在目标文件中这个节不占据实际的空间，它仅仅是一个占位符。目标文件格式区分初始化和未初始化变量是为了空间效率在：在目标文件中，未初始化变量不需要占据任何实际的磁盘空间。

.symtab：一个符号表（symbol table），它存放在程序中被定义和引用的函数和全局变量的信息。一些程序员错误地认为必须通过-g选项来编译一个程序，得到符号表信息。实际上，每个可重定位目标文件在.symtab中都有一张符号表。然而，和编译器中的符号表不同，.symtab符号表不包含局部变量的表目。

.rel.text：当链接噐把这个目标文件和其他文件结合时，.text节中的许多位置都需要修改。一般而言，任何调用外部函数或者引用全局变量的指令都需要修改。另一方面调用本地函数的指令则不需要修改。注意，可执行目标文件中并不需要重定位信息，因此通常省略，除非使用者显式地指示链接器包含这些信息。

.rel.data：被模块定义或引用的任何全局变量的信息。一般而言，任何已初始化全局变量的初始值是全局变量或者外部定义函数的地址都需要被修改。

.debug：一个调试符号表，其有些表目是程序中定义的局部变量和类型定义，有些表目是程序中定义和引用的全局变量，有些是原始的C源文件。只有以-g选项调用编译驱动程序时，才会得到这张表。

.line：原始C源程序中的行号和.text节中机器指令之间的映射。只有以-g选项调用编译驱动程序时，才会得到这张表。

.strtab：一个字符串表，其内容包括.symtab和.debug节中的符号表，以及节头部中的节名字。字符串表就是以null结尾的字符串序列。

使用举例：

反汇编应用程序

objdump -d  main.o  

显示文件头信息 

objdump -f main.o

显示制定section段信息(comment段)

objdump -s -j .comment main.o
** 参考文献
[[https://cloud.tencent.com/developer/article/1494510][objdump命令解析- 云+社区 - 腾讯云]]
* OpenCV
** C++接口
先安装依赖
#+BEGIN_SRC bash
sudo apt-get install build-essential
sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev
sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev
#+END_SRC
然后，将压缩包解压，我下载我是opencv3.4.3版本，所以最后解压出来的文件夹就是opencv-3.4.3，接着，先用命令行进入该文件夹，然后执行命令，如下所示：
#+BEGIN_SRC bash
cd ~/opencv-3.4.3  # 进入opencv文件夹
mkdir build # 创建build文件夹
cd build # 进入build文件夹

#cmake指令，如果没有特殊要求建议就选择默认的就可以
#注意，后面的两个点千万不能省，代表了上级目录
cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local ..  
make -j7 # 多线程执行make任务

# 最后一步，安装库文件
sudo make install

#安装完成
#+END_SRC
** python接口
方法一：用pip命令即可
#+BEGIN_SRC bash
pip install opencv-python #安装opencv
pip install opencv-contrib-python #安装opencv的contrib扩展包
#+END_SRC

方法二：用conda安装
#+BEGIN_SRC bash
conda search opencv #查询一下conda里面可以安装的opencv
conda install opencv=3.1.0 #根据你想要安装的opencv版本（以3.1.0为例），输入指令
#+END_SRC
* od 命令
od（Octal Dump）命令用于将指定文件内容以八进制、十进制、十六进制、浮点格式或 ASCII 编码字符方式显示，通常用于显示或查看文件中不能直接显示在终端的字符。od 命令系统默认的显示方式是八进制。

常见的文件为文本文件和二进制文件。od 命令主要用来查看保存在二进制文件中的值，按照指定格式解释文件中的数据并输出，不管是 IEEE754 格式的浮点数还是 ASCII 码，od 命令都能按照需求输出它们的值。

命令格式: od [OPTION]... [FILE]...

参数：
#+BEGIN_EXAMPLE
-A RADIX
--address-radix=RADIX
	选择以何种基数表示地址偏移
-j BYTES
--skip-bytes=BYTES
	跳过指定数目的字节
-N BYTES
--read-bytes=BYTES
	输出指定字节数
-S [BYTES]
--strings[=BYTES]
	输出长度不小于指定字节数的字符串，BYTES 缺省为 3
-v
--output-duplicates
	输出时不省略重复的数据
-w [BYTES]
--width[=BYTES]
	设置每行显示的字节数，BYTES 缺省为 32 字节
-t TYPE
--format=TYPE
	指定输出格式，格式包括 a、c、d、f、o、u 和 x，各含义如下：
  	a：具名字符；比如换行符显示为 nl
  	c：可打印字符或反斜杠表示的转义字符；比如换行符显示为 \n
 	d[SIZE]：SIZE 字节组成一个有符号十进制整数。SIZE 缺省为 sizeof(int)
 	f[SIZE]：SIZE 字节组成一个浮点数。SIZE 缺省为 sizeof(double)
  	o[SIZE]：SIZE 字节组成一个八进制整数。SIZE 缺省为 sizeof(int)
  	u[SIZE]：SIZE 字节组成一个无符号十进制整数。SIZE 缺省为 sizeof(int)
  	x[SIZE]：SIZE 字节组成一个十六进制整数。SIZE 缺省为 sizeof(int)
  	SIZE 可以为数字，也可以为大写字母。如果 TYPE 是 [doux] 中的一个，那么 SIZE 可以为 C  = sizeof(char)，S = sizeof(short)，I = sizeof(int)，L = sizeof(long)。如果 TYPE 是 f，那么 SIZE 可以为 F = sizeof(float)，D = sizeof(double) ，L = sizeof(long double)
--help
	在线帮助
--version
	显示版本信息
#+END_EXAMPLE

* Pytorch安装
首先安装Anaconda.

然后进入pytorch官网,根据自己的情况进行选择,之后最下方红线位置就会显示你应该输入的安装命令

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-06-02_17-33-39.png @ 2020-06-02 17:33:46
[[file:%E5%AE%89%E8%A3%85Pytorch/2020-06-02_17-33-46_Snipaste_2020-06-02_17-33-39.png]]


将得到的命令粘贴到终端窗口中运行即可.

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20180117102533147.png @ 2020-06-02 17:28:16
[[file:%E5%AE%89%E8%A3%85Pytorch/2020-06-02_17-28-16_20180117102533147.png]]

验证pytorch是否安装成功:

terminal内输入python，进入python环境
然后输入下面的命令:
#+BEGIN_SRC bash
import torch
import torchvision
#+END_SRC

不报错的话就说明pytorch安装成功了，如图所示
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-06-02_17-29-56.png @ 2020-06-02 17:30:06
[[file:%E5%AE%89%E8%A3%85Pytorch/2020-06-02_17-30-06_Snipaste_2020-06-02_17-29-56.png]]

如果下载速度很慢的话，可以选择pip方式下载安装,如下图所示

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2020-06-02_17-26-40.png @ 2020-06-02 17:31:43
[[file:%E5%AE%89%E8%A3%85Pytorch/2020-06-02_17-31-43_Snipaste_2020-06-02_17-26-40.png]]

一般这个pip方式会比较快吧，但如果很不幸，你的pip方式也遇到网络很差的问题，那就下载文件吧……

在浏览器里新建任务，下载链接就是图中命令位置里的链接: [[https://download.............]]  把这个文件下载下来，实在不行去windows上面用迅雷下载这个文件，下载好了之后copy到你的Linux系统上去，然后找到文件位置，直接pip install 文件名

* python安装
以下为在 Unix & Linux 平台上安装 Python 的简单步骤：
- 打开 WEB 浏览器访问 https://www.python.org/downloads/source/
- 选择适用于 Unix/Linux 的源码压缩包。
- 下载及解压压缩包 Python-3.x.x.tgz，3.x.x 为你下载的对应版本号。
- 如果你需要自定义一些选项修改 Modules/Setup

以 Python3.6.1 版本为例：
#+begin_src bash
tar -zxvf Python-3.6.1.tgz
cd Python-3.6.1
./configure
make && make install
#+END_SRC
检查 Python3 是否正常可用：
#+begin_src python
>>> python3 -V
Python 3.6.1
#+END_SRC
** no module named zlib错误的解决方法
#+begin_src bash
sudo apt-get install zlib1g-dev
#+END_SRC
* pip的用法
#+BEGIN_SRC bash
sudo pip3 --no-cache-dir --default-timeout=100 install -i https://pypi.tuna.tsinghua.edu.cn/simple torch
#+END_SRC
--no-cache-dir：pip下载库时会占用缓存，当文件过大会导致错误，这个选项可以避免该错误
#+BEGIN_SRC bash
pip install -r requirements.txt
#+END_SRC
-r表示从requirements.txt读取安装列表
浙大内部镜像：http://mirrors.aliyun.com/
** 换源
清华源

临时使用
#+begin_src bash
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package
#+END_SRC
注意，simple 不能少, 是 https 而不是 http

设为默认
升级 pip 到最新的版本 (>=10.0.0) 后进行配置：
#+begin_src bash
pip install pip -U
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
#+END_SRC
如果您到 pip 默认源的网络连接较差，临时使用本镜像站来升级 pip：
#+begin_src bash
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pip -U
#+END_SRC

** 安装包
#+begin_src bash
pip install SomePackage              # 最新版本
pip install SomePackage==1.0.4       # 指定版本
pip install 'SomePackage>=1.0.4'     # 最小版本
#+END_SRC
比如我要安装 Django。用以下的一条命令就可以，方便快捷。

pip install Django==1.7

** 升级包
pip install --upgrade SomePackage

升级指定的包，通过使用==, >=, <=, >, < 来指定一个版本号。
** 卸载包
pip uninstall SomePackage
** 搜索包
pip search SomePackage
** 显示安装包信息
pip show 
** 查看指定包的详细信息
pip show -f SomePackage
** 列出已安装的包
pip list
** 查看可升级的包
pip list -o
** 显示版本和路径
pip --version
** 获取帮助
pip --help
** 升级 pip
#+BEGIN_EXAMPLE
如果升级命令出现问题 ，可以使用以下命令：
sudo easy_install --upgrade pip
#+END_EXAMPLE
*** Linux 或 macOS
#+begin_src bash
pip install --upgrade pip    # python2.x
pip3 install --upgrade pip   # python3.x
#+END_SRC 
*** Windows 平台升级：
#+begin_src bash
python -m pip install -U pip   # python2.x
python -m pip3 install -U pip    # python3.x
#+END_SRC
** socket.timeout: The read operation timed out解决方案
解决方法：pip --default-timeout=100 install -U tensorflow
* paste 命令
Linux paste 命令用于合并文件的列。

paste 指令会把每个文件以列对列的方式，一列列地加以合并。

语法
paste [-s][-d <间隔字符>][--help][--version][文件...]
参数：
- -d<间隔字符>或--delimiters=<间隔字符> 　用指定的间隔字符取代跳格字符。
- -s或--serial 　串列进行而非平行处理。
- --help 　在线帮助。
- --version 　显示帮助信息。
- [文件…] 指定操作的文件路径

** 实例
使用paste指令将文件"file"、"testfile"、"testfile1"进行合并，输入如下命令：
#+begin_src bash
paste file testfile testfile1 #合并指定文件的内容 
#+END_SRC
但是，在执行以上命令之前，首先使用"cat"指令对3个文件内容进行查看，显示如下所示：
#+begin_src bash
$ cat file                  #file文件的内容  
xiongdan 200  
lihaihui 233  
lymlrl 231  
$ cat testfile              #testfile文件的内容  
liangyuanm  ss  
$ cat testfile1             #testfile1文件的内容  
huanggai 56  
zhixi 73 
#+END_SRC
当合并指令"$ paste file testfile testfile1"执行后，程序界面中将显示合并后的文件内容，如下所示：
#+BEGIN_EXAMPLE
xiongdan 200  
lihaihui 233  
lymlrl 231  
liangyuanm  ss  
huanggai 56  
zhixi 73  
#+END_EXAMPLE

若使用paste指令的参数"-s"，则可以将一个文件中的多行数据合并为一行进行显示。例如，将文件"file"中的3行数据合并为一行数据进行显示，输入如下命令
#+begin_src bash
$ paste -s file             #合并指定文件的多行数据
#+END_SRC
上面的命令执行后，显示的数据内容如下所示：
#+BEGIN_EXAMPLE
xiongdan 200 lihaihui 233 lymlrl 231 
#+END_EXAMPLE

注意：参数"-s"只是将testfile文件的内容调整显示方式，并不会改变原文件的内容格式。
* PS1设置
** bash
#+BEGIN_EXAMPLE
\d ：代表日期，格式为weekday month date，例如："Mon Aug1"
\H：完整的主机名称。例如：我的机器名称为：fc4.linux，则这个名称就是fc4.linux
\h ：仅取主机的第一个名字，如上例，则为fc4，.linux则被省略 
\t ：显示时间为24小时格式，如：HH：MM：SS 
\T ：显示时间为12小时格式 
\A ：显示时间为24小时格式：HH：MM 
\u ：当前用户的账号名称
\v ：BASH的版本信息
\w ：完整的工作目录名称。家目录会以 ~代替
\W ：利用basename取得工作目录名称，所以只会列出最后一个目录
\# ：下达的第几个命令
\$ ：提示字符，如果是root时，提示符为：# ，普通用户则为：$
#+END_EXAMPLE
*** 颜色
 颜色代码格式：\[\e[F;Bm\]，F表示字体颜色，B表示背景颜色

 We will use the \e special character at the beginning and an m at the end to indicate that what follows is a color sequence.

 For example, the following PS1 will cause the prompt to appear in yellow underlined text with red background:
 #+begin_src bash
 PS1="\e[41;4;33m[\u@\h \W]$ "
 #+END_SRC

 #+DOWNLOADED: screenshot @ 2021-12-10 16:58:01
 [[file:images/linux%E7%AC%94%E8%AE%B0/PS1%E8%AE%BE%E7%BD%AE/2021-12-10_16-58-01_screenshot.png]]

 注意在$符号输出之后，我们还要重置颜色为透明，也就是\[\e[0m\]，这样你输入的命令就不会受之前颜色设置的影响。
 例如：PS1="\e[42;0;32m\u@\h:\w\$\e[0m"
 颜色表如下：
 | Text Format        | Foreground (text) color | Background color |
 |--------------------+-------------------------+------------------|
 | 0: normal text     | 30: Black               | 40: Black        |
 | 1: bold            | 31: Red                 | 41: Red          |
 | 4: Underlined text | 32: Green               | 42: Green        |
 |                    | 33: Yellow              | 43: Yellow       |
 |                    | 34: Blue                | 44: Blue         |
 |                    | 35: Purple              | 45: Purple       |
 |                    | 36: Cyan                | 46: Cyan         |
 |                    | 37: White               | 47: White        |
**** 参考文章
 [[https://www.tecmint.com/customize-bash-colors-terminal-prompt-linux/][How to Customize Bash Colors and Content in Linux Terminal Prompt]]
*** What does "${debian_chroot:+($debian_chroot)}" do in my terminal prompt?
 The important part to answer this question is this snippet from /etc/bash.bashrc:
 #+begin_src bash
 if [ -z "$debian_chroot" ] && [ -r /etc/debian_chroot ]; then
     debian_chroot=$(cat /etc/debian_chroot)
 fi
 #+END_SRC
 It means if the variable $debian_chroot is empty and the file /etc/debian_chroot exists and is readable the variable is set to the content of the file.

 Now what is this for? The file /etc/debian_chroot is when you have a chrooted debian system inside another debian system (ubuntu is based on debian). So this is for a better overview. To distinguish whether you are in the chroot or not.

 When you have a chroot of another system for example in /srv/nfs4/netboot/ you can set a name for this chroot in /srv/nfs4/netboot/etc/debian_chroot (in my case it's a nfs4 pxe netboot drive):
 #+begin_src bash
 user@host:~# echo "netboot" >/srv/nfs4/netboot/etc/debian_chroot
 #+END_SRC
 And then when you chroot inside:
 #+begin_src bash
 chroot /srv/nfs4/netboot/
 #+END_SRC

 Your prompt looks like this:
 #+begin_src bash
 (netboot)user@host:~#
 #+END_SRC

** zsh
oh-my-zsh终端用户名设置

先查看一下当前使用的主题：
#+begin_src bash
echo $ZSH_THEME
#+END_SRC
查看自己主题，我的是robbyrussell

之后进入oh-my-zsh的主题目录，修改主题设置
#+begin_src bash
cd ~/.oh-my-zsh/themes
vim robbyrussell.zsh-theme
#+END_SRC
可以看到
#+begin_src bash
local ret_status="%(?:%{$fg_bold[green]%}➜ :%{$fg_bold[red]%}➜ )"
PROMPT='${ret_status} %{$fg[cyan]%}%~%{$reset_color%} $(git_prompt_info)'

ZSH_THEME_GIT_PROMPT_PREFIX="%{$fg_bold[blue]%}git:(%{$fg[red]%}"
ZSH_THEME_GIT_PROMPT_SUFFIX="%{$reset_color%} "
ZSH_THEME_GIT_PROMPT_DIRTY="%{$fg[blue]%}) %{$fg[yellow]%}✗"
ZSH_THEME_GIT_PROMPT_CLEAN="%{$fg[blue]%})"
#+END_SRC
PROMPT就是设置显示的用户名

由于oh_my_zsh时常会有版本更新，为了避免我们修改的跟更新的版本有冲突，建议不要修改robbyrussell.zsh-theme，而是将其拷贝出来，命名为自己的主题文件，比如叫做myrobbyrussell.zsh-theme，然后只对myrobbyrussell.zsh-theme进行修改。

修改后将 ~/.zshrc 中的
#+begin_src bash
ZSH_THEME="robbyrussell"
#+END_SRC
改为
#+begin_src bash
ZSH_THEME="myrobbyrussell"
#+END_SRC
这样就能避免冲突了。

#+BEGIN_EXAMPLE
%T	系统时间（时：分）
%*	系统时间（时：分：秒）
%D	系统日期（年-月-日）
%n	用户名称（即：当前登陆终端的用户的名称，和whami命令输出相同）
%B - %b	开始到结束使用粗体打印
%U - %u	开始到结束使用下划线打印
%d	你当前的工作目录
%~	你目前的目录相对于～的相对路径
%M	计算机的主机名
%m	计算机的主机名（在第一个句号之前截断）
%l	你当前的tty
#+END_EXAMPLE
* QEMU
“Qemu”是一个广泛使用的开源计算机模拟器和虚拟机。"

当作为模拟器时，可以在一种架构（如x86 PC）下运行另一种架构（如ARM）下的操作系统和程序。通过使用动态转换，它可以获得非常好的性能。

作为虚拟机时，QEMU可以使用其他虚拟机管理程序（如 Xen 或 KVM）来使用CPU扩展（HVM）进行虚拟化，通过在主机CPU上直接执行客户机代码来获得接近于宿主机的性能。
** 参考文章
[[https://wiki.archlinux.org/title/QEMU_(%25E7%25AE%2580%25E4%25BD%2593%25E4%25B8%25AD%25E6%2596%2587)][QEMU (简体中文)]]
* rsync
** 简介
rsync 是一个常用的 Linux 应用程序，用于文件同步。

它可以在本地计算机与远程计算机之间，或者两个本地目录之间同步文件（但不支持两台远程计算机之间的同步）。它也可以当作文件复制工具，替代cp和mv命令。

它名称里面的r指的是 remote，rsync 其实就是"远程同步"（remote sync）的意思。与其他文件传输工具（如 FTP 或 scp）不同，rsync 的最大特点是会检查发送方和接收方已有的文件，仅传输有变动的部分（默认规则是文件大小或修改时间有变动）。
** 安装方法
如果本机或者远程计算机没有安装 rsync，可以用下面的命令安装。
#+begin_src bash
# Debian
$ sudo apt-get install rsync

# Red Hat
$ sudo yum install rsync

# Arch Linux
$ sudo pacman -S rsync
#+END_SRC
注意，传输的双方都必须安装 rsync。
** 参数
-a、--archive参数表示存档模式，保存所有的元数据，比如修改时间（modification time）、权限、所有者等，并且软链接也会同步过去。

--append参数指定文件接着上次中断的地方，继续传输。

--append-verify参数跟--append参数类似，但会对传输完成后的文件进行一次校验。如果校验失败，将重新发送整个文件。

-b、--backup参数指定在删除或更新目标目录已经存在的文件时，将该文件更名后进行备份，默认行为是删除。更名规则是添加由--suffix参数指定的文件后缀名，默认是~。

--backup-dir参数指定文件备份时存放的目录，比如--backup-dir=/path/to/backups。

--bwlimit参数指定带宽限制，默认单位是 KB/s，比如--bwlimit=100。

-c、--checksum参数改变rsync的校验方式。默认情况下，rsync 只检查文件的大小和最后修改日期是否发生变化，如果发生变化，就重新传输；使用这个参数以后，则通过判断文件内容的校验和，决定是否重新传输。

--delete参数删除只存在于目标目录、不存在于源目标的文件，即保证目标目录是源目标的镜像。

-e参数指定使用 SSH 协议传输数据。

--exclude参数指定排除不进行同步的文件，比如--exclude="*.iso"。

--exclude-from参数指定一个本地文件，里面是需要排除的文件模式，每个模式一行。

--existing、--ignore-non-existing参数表示不同步目标目录中不存在的文件和目录。

-h参数表示以人类可读的格式输出。

-h、--help参数返回帮助信息。

-i参数表示输出源目录与目标目录之间文件差异的详细情况。

--ignore-existing参数表示只要该文件在目标目录中已经存在，就跳过去，不再同步这些文件。

--include参数指定同步时要包括的文件，一般与--exclude结合使用。

--link-dest参数指定增量备份的基准目录。

-m参数指定不同步空目录。

--max-size参数设置传输的最大文件的大小限制，比如不超过200KB（--max-size='200k'）。

--min-size参数设置传输的最小文件的大小限制，比如不小于10KB（--min-size=10k）。

-n参数或--dry-run参数模拟将要执行的操作，而并不真的执行。配合-v参数使用，可以看到哪些内容会被同步过去。

-P参数是--progress和--partial这两个参数的结合。

--partial参数允许恢复中断的传输。不使用该参数时，rsync会删除传输到一半被打断的文件；使用该参数后，传输到一半的文件也会同步到目标目录，下次同步时再恢复中断的传输。一般需要与--append或--append-verify配合使用。

--partial-dir参数指定将传输到一半的文件保存到一个临时目录，比如--partial-dir=.rsync-partial。一般需要与--append或--append-verify配合使用。

--progress参数表示显示进展。

-r参数表示递归，即包含子目录。

--remove-source-files参数表示传输成功后，删除发送方的文件。

--size-only参数表示只同步大小有变化的文件，不考虑文件修改时间的差异。

--suffix参数指定文件名备份时，对文件名添加的后缀，默认是~。

-u、--update参数表示同步时跳过目标目录中修改时间更新的文件，即不同步这些有更新的时间戳的文件。

-v参数表示输出细节。-vv表示输出更详细的信息，-vvv表示输出最详细的信息。

--version参数返回 rsync 的版本。

-z参数指定同步时压缩数据。
** 参数详解
*** -r
本机使用 rsync 命令时，可以作为cp和mv命令的替代方法，将源目录同步到目标目录。
#+begin_src bash
$ rsync -r source destination
#+END_SRC
上面命令中，-r表示递归，即包含子目录。注意，-r是必须的，否则 rsync 运行不会成功。source目录表示源目录，destination表示目标目录。

如果有多个文件或目录需要同步，可以写成下面这样。
#+begin_src bash
$ rsync -r source1 source2 destination
#+END_SRC
上面命令中，source1、source2都会被同步到destination目录。
*** -a 参数
-a参数可以替代-r，除了可以递归同步以外，还可以同步元信息（比如修改时间、权限等）。由于 rsync 默认使用文件大小和修改时间决定文件是否需要更新，所以-a比-r更有用。下面的用法才是常见的写法。
#+begin_src bash
$ rsync -a source destination
#+END_SRC
目标目录destination如果不存在，rsync 会自动创建。执行上面的命令后，源目录source被完整地复制到了目标目录destination下面，即形成了destination/source的目录结构。

如果只想同步源目录source里面的内容到目标目录destination，则需要在源目录后面加上斜杠。
#+begin_src bash
$ rsync -a source/ destination
#+END_SRC
上面命令执行后，source目录里面的内容，就都被复制到了destination目录里面，并不会在destination下面创建一个source子目录。
*** -n 参数
如果不确定 rsync 执行后会产生什么结果，可以先用-n或--dry-run参数模拟执行的结果。
#+begin_src bash
$ rsync -anv source/ destination
#+END_SRC
上面命令中，-n参数模拟命令执行的结果，并不真的执行命令。
*** -v
-v参数是将结果输出到终端，这样就可以看到哪些内容会被同步。
*** --delete 参数
默认情况下，rsync 只确保源目录的所有内容（明确排除的文件除外）都复制到目标目录。它不会使两个目录保持相同，并且不会删除文件。如果要使得目标目录成为源目录的镜像副本，则必须使用--delete参数，这将删除只存在于目标目录、不存在于源目录的文件。
#+begin_src bash
$ rsync -av --delete source/ destination
#+END_SRC
上面命令中，--delete参数会使得destination成为source的一个镜像。
*** --exclude 参数
有时，我们希望同步时排除某些文件或目录，这时可以用--exclude参数指定排除模式。
#+begin_src bash
$ rsync -av --exclude='*.txt' source/ destination
# 或者
$ rsync -av --exclude '*.txt' source/ destination
#+END_SRC
上面命令排除了所有 TXT 文件。

注意，rsync 会同步以"点"开头的隐藏文件，如果要排除隐藏文件，可以这样写--exclude=".*"。

如果要排除某个目录里面的所有文件，但不希望排除目录本身，可以写成下面这样。
#+begin_src bash
$ rsync -av --exclude 'dir1/*' source/ destination
#+END_SRC
多个排除模式，可以用多个--exclude参数。
#+begin_src bash
$ rsync -av --exclude 'file1.txt' --exclude 'dir1/*' source/ destination
#+END_SRC
多个排除模式也可以利用 Bash 的大扩号的扩展功能，只用一个--exclude参数。
#+begin_src bash
$ rsync -av --exclude={'file1.txt','dir1/*'} source/ destination
#+END_SRC
如果排除模式很多，可以将它们写入一个文件，每个模式一行，然后用--exclude-from参数指定这个文件。
#+begin_src bash
$ rsync -av --exclude-from='exclude-file.txt' source/ destination
#+END_SRC
*** --include 参数
--include参数用来指定必须同步的文件模式，往往与--exclude结合使用。
#+begin_src bash
$ rsync -av --include="*.txt" --exclude='*' source/ destination
#+END_SRC
上面命令指定同步时，排除所有文件，但是会包括 TXT 文件。
** 远程同步
*** SSH 协议
rsync 除了支持本地两个目录之间的同步，也支持远程同步。它可以将本地内容，同步到远程服务器。
#+begin_src bash
$ rsync -av source/ username@remote_host:destination
#+END_SRC
也可以将远程内容同步到本地。
#+begin_src bash
$ rsync -av username@remote_host:source/ destination
#+END_SRC
rsync 默认使用 SSH 进行远程登录和数据传输。

由于早期 rsync 不使用 SSH 协议，需要用-e参数指定协议，后来才改的。所以，下面-e ssh可以省略。
#+begin_src bash
$ rsync -av -e ssh source/ user@remote_host:/destination
#+END_SRC
但是，如果 ssh 命令有附加的参数，则必须使用-e参数指定所要执行的 SSH 命令。
#+begin_src bash
$ rsync -av -e 'ssh -p 2234' source/ user@remote_host:/destination
#+END_SRC
上面命令中，-e参数指定 SSH 使用2234端口。
*** rsync 协议
除了使用 SSH，如果另一台服务器安装并运行了 rsync 守护程序，则也可以用rsync://协议（默认端口873）进行传输。具体写法是服务器与目标目录之间使用双冒号分隔::。
#+begin_src bash
$ rsync -av source/ 192.168.122.32::module/destination
#+END_SRC
注意，上面地址中的module并不是实际路径名，而是 rsync 守护程序指定的一个资源名，由管理员分配。

如果想知道 rsync 守护程序分配的所有 module 列表，可以执行下面命令。
#+begin_src bash
$ rsync rsync://192.168.122.32
#+END_SRC
rsync 协议除了使用双冒号，也可以直接用rsync://协议指定地址。
#+begin_src bash
$ rsync -av source/ rsync://192.168.122.32/module/destination
#+END_SRC

** 增量备份
rsync 的最大特点就是它可以完成增量备份，也就是默认只复制有变动的文件。

除了源目录与目标目录直接比较，rsync 还支持使用基准目录，即将源目录与基准目录之间变动的部分，同步到目标目录。

具体做法是，第一次同步是全量备份，所有文件在基准目录里面同步一份。以后每一次同步都是增量备份，只同步源目录与基准目录之间有变动的部分，将这部分保存在一个新的目标目录。这个新的目标目录之中，也是包含所有文件，但实际上，只有那些变动过的文件是存在于该目录，其他没有变动的文件都是指向基准目录文件的硬链接。

--link-dest参数用来指定同步时的基准目录。
#+begin_src bash
$ rsync -a --delete --link-dest /compare/path /source/path /target/path
#+END_SRC
上面命令中，--link-dest参数指定基准目录/compare/path，然后源目录/source/path跟基准目录进行比较，找出变动的文件，将它们拷贝到目标

目录/target/path。那些没变动的文件则会生成硬链接。这个命令的第一次备份时是全量备份，后面就都是增量备份了。

下面是一个脚本示例，备份用户的主目录。
#+begin_src bash
#!/bin/bash

# A script to perform incremental backups using rsync

set -o errexit
set -o nounset
set -o pipefail

readonly SOURCE_DIR="${HOME}"
readonly BACKUP_DIR="/mnt/data/backups"
readonly DATETIME="$(date '+%Y-%m-%d_%H:%M:%S')"
readonly BACKUP_PATH="${BACKUP_DIR}/${DATETIME}"
readonly LATEST_LINK="${BACKUP_DIR}/latest"

mkdir -p "${BACKUP_DIR}"

rsync -av --delete \
  "${SOURCE_DIR}/" \
  --link-dest "${LATEST_LINK}" \
  --exclude=".cache" \
  "${BACKUP_PATH}"

rm -rf "${LATEST_LINK}"
ln -s "${BACKUP_PATH}" "${LATEST_LINK}"
#+END_SRC
上面脚本中，每一次同步都会生成一个新目录${BACKUP_DIR}/${DATETIME}，并将软链接${BACKUP_DIR}/latest指向这个目录。下一次备份时，就将${BACKUP_DIR}/latest作为基准目录，生成新的备份目录。最后，再将软链接${BACKUP_DIR}/latest指向新的备份目录。
** 参考文章
[[https://www.ruanyifeng.com/blog/2020/08/rsync.html][rsync 用法教程]]
* rm
-f, --force    忽略不存在的文件，从不给出提示。
-i, --interactive 进行交互式删除
-r, -R, --recursive   指示rm将参数中列出的全部目录和子目录均递归地删除。
-v, --verbose    详细显示进行的步骤
   --help     显示此帮助信息并退出
   --version  输出版本信息并退出
* rz和sz命令使用
rz命令是方便从windows传文件到Linux，在windows下通过连接工具进入linux系统，cd到自己需要的目录，命令行输入rz，然后回车，之后会弹出一个选择框，选择我们需要上传的文件，然后add，最后上传就好了。

#: rz 

sz命令反过来，是从Linux传输文件到windows，同样Linux下我们需要传的文件所在目录，命令行输入sz，后面跟上需要传输的文件命，可以是一个文件，也可以跟多个文件名，同时传多个文件，然后回车，就可以传文件了。默认情况文件传到windows的用户下载目录下。

#: sz filename1 filename2 filename3

这两个命令传输传输小文件很方便也很快，但是遇到大文件经常需要很久，甚至传了一部分然后中断了，这时就需要nc命令出场了，传输大文件也非常快。
* sed
sed采用的是流编辑模式，最明显的特点是，在 sed 处理数据之前，需要预先提供一组规则，sed 会按照此规则来编辑数据。

sed 会根据脚本命令来处理文本文件中的数据，这些命令要么从命令行中输入，要么存储在一个文本文件中，此命令执行数据的顺序如下：
- 每次仅读取一行内容；
- 根据提供的规则命令匹配并修改数据。注意，sed 默认不会直接修改源文件数据，而是会将数据复制到缓冲区中，修改也仅限于缓冲区中的数据；
- 将执行结果输出。

当一行数据匹配完成后，它会继续读取下一行数据，并重复这个过程，直到将文件中所有数据处理完毕。

sed 命令的基本格式如下：
#+begin_src bash
[root@localhost ~]# sed [选项] [脚本命令] 文件名
#+END_SRC

该命令常用的选项及含义，如下表所示。
| 选项            | 含义                                                                                                                        |
|-----------------+-----------------------------------------------------------------------------------------------------------------------------|
| -e 脚本命令     | 该选项会将其后跟的脚本命令添加到已有的命令中。                                                                              |
| -f 脚本命令文件 | 该选项会将其后文件中的脚本命令添加到已有的命令中。                                                                          |
| -n              | 默认情况下，sed 会在所有的脚本指定执行完毕后，会自动输出处理后的内容，而该选项会屏蔽启动输出，需使用 print 命令来完成输出。 |
| -i              | 此选项会直接修改源文件，要慎用。                                                                                            |

** sed选项
*** -n
sed 会将模式空间里的行经过处理后输出到标准输出，这是默认的处理方式。也就是说，除非你使用“d”来删除此行，否则经过“模式空间”处理的行都是会被输出到标准输出（屏幕）上的。

使用-n 选项只输出处理成功的行
#+begin_src bash
#还是先来看看原文件的内容
[roc@roclinux ~]$ cat roc.txt
1
2
3
4
5
 
#仔细看, 输出中出现了两个“4”
[roc@roclinux ~]$ sed ‘/4/p’ roc.txt
1
2
3
4
4
5
[roc@roclinux ~]$ sed -n '/4/p' roc.txt
4
#+END_SRC

** 特殊符号
*** $
数据流末尾
*** &
& 字符，在 sed 命令中，它表示的是“之前被匹配的部分”
#+begin_src bash
#按照惯例, 先展示文件的内容
[roc@roclinux ~]$ cat mysed.txt
Beijing
London
 
#我们使用到了&符号, 大家试着猜一猜它的作用
[roc@roclinux ~]$ sed 's/B.*/&2008/' mysed.txt
Beijing2008
London

[roc@roclinux 20160229]$ sed 's/Bei/&2008/' mysed.txt
Bei2008jing
London
#+END_SRC
*** （）括号
“sed 的预存储技术”，也就是命令中被“（”和“）”括起来的内容会被依次暂存起来，存储到 \1、\2、...里面。这样你就可以使用‘\N’形式来调用这些预存储的内容了。
#+begin_src bash
[roc@roclinux ~]$ echo "hello world" | sed 's/\(hello\).*/world \1/'
world hello
#+END_SRC

来继续看一个例子，我们希望只在每行的第一个和最后一个 Beijing 后面加上 2008 字符串，言下之意就是，除了每行的第一个和最后一个 2008 之外，这一行中间出现的 Beijing 后面就不要加 2008 啦。这个需求，真的是很复杂很个性化，但 sed 命令仍然可以很好地满足：
#先看下文件内容, 第一行中出现了4个Beijing
#+begin_src bash
[roc@roclinux ~]$ cat mysed.txt
Beijing Beijing Beijing Beijing
London London London London
 
[roc@roclinux ~]$ sed 's/\(Beijing\)\(.*\)\(Beijing\)/\12008\2\32008/' mysed.txt
Beijing2008 Beijing Beijing Beijing2008
London London London London
#+END_SRC
这个例子中我们再次使用了预存储技术，存储了三项内容，分别代表第一个 Beijing、中间的内容、最后的 Beijing。而针对\1和\3，我们在其后面追加了 2008 这个字符串。
** sed脚本命令
*** sed s 寻找并替换
此命令的基本格式为：
#+begin_src bash
[address]s/pattern/replacement/flags
#+END_SRC
其中，address 表示指定要操作的具体行，pattern 指的是需要替换的内容，replacement 指的是要替换的新内容。

关于指定具体操作行（address）的用法，这里先不做解释，文章后续会对其做详细介绍。

此命令中常用的 flags 标记如表 2 所示。

表 2 sed s命令flags标记及功能
| flags 标记 | 功能                                                                                                                                |
|------------+-------------------------------------------------------------------------------------------------------------------------------------|
| n          | 1~512 之间的数字，表示指定要替换的字符串出现第几次时才进行替换，例如，一行中有 3 个 A，但用户只想替换第二个 A，这是就用到这个标记； |
| g          | 对数据中所有匹配到的内容进行替换，如果没有 g，则只会在第一次匹配成功时做替换操作。例如，一行数据中有 3 个 A，则只会替换第一个 A；   |
| p          | 会打印与替换命令中指定的模式匹配的行。此标记通常与 -n 选项一起使用。                                                                |
| w file     | 将缓冲区中的内容写到指定的 file 文件中；                                                                                            |
| &          | 用正则表达式匹配的内容进行替换；                                                                                                    |
| \n         | 匹配第 n 个子串，该子串之前在 pattern 中用 \(\) 指定。                                                                              |
| \          | 转义（转义替换部分包含：&、\ 等）。                                                                                                 |

比如，可以指定 sed 用新文本替换第几处模式匹配的地方：
#+begin_src bash
[root@localhost ~]# sed 's/test/trial/2' data4.txt
This is a test of the trial script.
This is the second test of the trial script.
#+END_SRC

可以看到，使用数字 2 作为标记的结果就是，sed 编辑器只替换每行中第 2 次出现的匹配模式。

如果要用新文件替换所有匹配的字符串，可以使用 g 标记：
#+begin_src bash
[root@localhost ~]# sed 's/test/trial/g' data4.txt
This is a trial of the trial script.
This is the second trial of the trial script.
#+END_SRC
我们知道，-n 选项会禁止 sed 输出，但 p 标记会输出修改过的行，将二者匹配使用的效果就是只输出被替换命令修改过的行，例如：
#+begin_src bash
[root@localhost ~]# cat data5.txt
This is a test line.
This is a different line.
[root@localhost ~]# sed -n 's/test/trial/p' data5.txt
This is a trial line.
#+END_SRC
w 标记会将匹配后的结果保存到指定文件中，比如：
#+begin_src bash
[root@localhost ~]# sed 's/test/trial/w test.txt' data5.txt
This is a trial line.
This is a different line.
[root@localhost ~]#cat test.txt
This is a trial line.
#+END_SRC
在使用 s 脚本命令时，替换类似文件路径的字符串会比较麻烦，需要将路径中的正斜线进行转义，例如：
#+begin_src bash
[root@localhost ~]# sed 's/\/bin\/bash/\/bin\/csh/' /etc/passwd
#+END_SRC
*** sed d 删除行
此命令的基本格式为：
[address]d

如果需要删除文本中的特定行，可以用 d 脚本命令，它会删除指定行中的所有内容。但使用该命令时要特别小心，如果你忘记指定具体行的话，文件中的所有内容都会被删除，举个例子：
#+begin_src bash
[root@localhost ~]# cat data1.txt
The quick brown fox jumps over the lazy dog
The quick brown fox jumps over the lazy dog
The quick brown fox jumps over the lazy dog
The quick brown fox jumps over the lazy dog
[root@localhost ~]# sed 'd' data1.txt
#什么也不输出，证明成了空文件
#+END_SRC
当和指定地址一起使用时，删除命令显然能发挥出大的功用。可以从数据流中删除特定的文本行。

address 的具体写法后续会做详细介绍，这里举几个例子：

- 通过行号指定，比如删除 data6.txt 文件内容中的第 3 行：
#+begin_src bash
[root@localhost ~]# cat data6.txt
This is line number 1.
This is line number 2.
This is line number 3.
This is line number 4.
[root@localhost ~]# sed '3d' data6.txt
This is line number 1.
This is line number 2.
This is line number 4.
#+END_SRC
- 或者通过特定行区间指定，比如删除 data6.txt 文件内容中的第 2、3行：
#+begin_src bash
[root@localhost ~]# sed '2,3d' data6.txt
This is line number 1.
This is line number 4.
#+END_SRC
- 也可以使用两个文本模式来删除某个区间内的行，但这么做时要小心，你指定的第一个模式会“打开”行删除功能，第二个模式会“关闭”行删除功能，因此，sed 会删除两个指定行之间的所有行（包括指定的行），例如：
#+begin_src bash
[root@localhost ~]#sed '/1/,/3/d' data6.txt
#删除第 1~3 行的文本数据
This is line number 4.
#+END_SRC
- 或者通过特殊的文件结尾字符，比如删除 data6.txt 文件内容中第 3 行开始的所有的内容：
#+begin_src bash
[root@localhost ~]# sed '3,$d' data6.txt
This is line number 1.
This is line number 2.
#+END_SRC
在此强调，在默认情况下 sed 并不会修改原始文件，这里被删除的行只是从 sed 的输出中消失了，原始文件没做任何改变。
*** sed a 和 i 插入行
a 命令表示在指定行的后面附加一行，i 命令表示在指定行的前面插入一行，这里之所以要同时介绍这 2 个脚本命令，因为它们的基本格式完全相同，如下所示：
#+begin_src bash
[address]a（或 i）\新文本内容
#+END_SRC
下面分别就这 2 个命令，给读者举几个例子。比如说，将一个新行插入到数据流第三行前，执行命令如下：
#+begin_src bash
[root@localhost ~]# sed '3i\
> This is an inserted line.' data6.txt
This is line number 1.
This is line number 2.
This is an inserted line.
This is line number 3.
This is line number 4.
#+END_SRC
再比如说，将一个新行附加到数据流中第三行后，执行命令如下：
#+begin_src bash
[root@localhost ~]# sed '3a\
> This is an appended line.' data6.txt
This is line number 1.
This is line number 2.
This is line number 3.
This is an appended line.
This is line number 4.
#+END_SRC
如果你想将一个多行数据添加到数据流中，只需对要插入或附加的文本中的每一行末尾（除最后一行）添加反斜线即可，例如：
#+begin_src bash
[root@localhost ~]# sed '1i\
> This is one line of new text.\
> This is another line of new text.' data6.txt
This is one line of new text.
This is another line of new text.
This is line number 1.
This is line number 2.
This is line number 3.
This is line number 4.
#+END_SRC
可以看到，指定的两行都会被添加到数据流中。
*** sed c 替换整行
c 命令表示将指定行中的所有内容，替换成该选项后面的字符串。该命令的基本格式为：
#+begin_src bash
[address]c\用于替换的新文本
#+END_SRC
举个例子：
#+begin_src bash
[root@localhost ~]# sed '3c\
> This is a changed line of text.' data6.txt
This is line number 1.
This is line number 2.
This is a changed line of text.
This is line number 4.
#+END_SRC
在这个例子中，sed 编辑器会修改第三行中的文本，其实，下面的写法也可以实现此目的：
#+begin_src bash
[root@localhost ~]# sed '/number 3/c\
> This is a changed line of text.' data6.txt
This is line number 1.
This is line number 2.
This is a changed line of text.
This is line number 4.
#+END_SRC
*** sed y 替换单个字符
y 转换命令是唯一可以处理单个字符的 sed 脚本命令，其基本格式如下：
#+begin_src bash
[address]y/inchars/outchars/
#+END_SRC
转换命令会对 inchars 和 outchars 值进行一对一的映射，即 inchars 中的第一个字符会被转换为 outchars 中的第一个字符，第二个字符会被转换成 outchars 中的第二个字符...这个映射过程会一直持续到处理完指定字符。如果 inchars 和 outchars 的长度不同，则 sed 会产生一条错误消息。

举个简单例子：
#+begin_src bash
[root@localhost ~]# sed 'y/123/789/' data8.txt
This is line number 7.
This is line number 8.
This is line number 9.
This is line number 4.
This is line number 7 again.
This is yet another line.
This is the last line in the file.
#+END_SRC
可以看到，inchars 模式中指定字符的每个实例都会被替换成 outchars 模式中相同位置的那个字符。

转换命令是一个全局命令，也就是说，它会文本行中找到的所有指定字符自动进行转换，而不会考虑它们出现的位置，再打个比方：
#+begin_src bash
[root@localhost ~]# echo "This 1 is a test of 1 try." | sed 'y/123/456/'
This 4 is a test of 4 try.
#+END_SRC
sed 转换了在文本行中匹配到的字符 1 的两个实例，我们无法限定只转换在特定地方出现的字符。
*** sed p 打印命令
p 命令表示搜索符号条件的行，并输出该行的内容，此命令的基本格式为：
#+begin_src bash
[address]p
#+END_SRC
p 命令常见的用法是打印包含匹配文本模式的行，例如：
#+begin_src bash
[root@localhost ~]# cat data6.txt
This is line number 1.
This is line number 2.
This is line number 3.
This is line number 4.
[root@localhost ~]# sed -n '/number 3/p' data6.txt
This is line number 3.
[root@localhost ~]# sed '/number 2/p' data6.txt 
This is line number 1.
This is line number 2.
This is line number 2.
This is line number 3.
This is line number 4.
#+END_SRC
可以看到，用 -n 选项和 p 命令配合使用，我们可以禁止输出其他行，只打印包含匹配文本模式的行。

如果不加-n 选项，原有的内容也会打印出来

如果需要在修改之前查看行，也可以使用打印命令，比如与替换或修改命令一起使用。可以创建一个脚本在修改行之前显示该行，如下所示：
#+begin_src bash
[root@localhost ~]# sed -n '/3/{
> p
> s/line/test/p
> }' data6.txt
This is line number 3.
This is test number 3.
#+END_SRC
sed 命令会查找包含数字 3 的行，然后执行两条命令。首先，脚本用 p 命令来打印出原始行；然后它用 s 命令替换文本，并用 p 标记打印出替换结果。输出同时显示了原来的行文本和新的行文本。
*** sed w 将当前文件指定行写入另一个文件
w 命令用来将文本中指定行的内容写入文件中，此命令的基本格式如下：
#+begin_src bash
[address]w filename
#+END_SRC
这里的 filename 表示文件名，可以使用相对路径或绝对路径，但不管是哪种，运行 sed 命令的用户都必须有文件的写权限。

下面的例子是将数据流中的前两行打印到一个文本文件中：
#+begin_src bash
[root@localhost ~]# sed '1,2w test.txt' data6.txt
This is line number 1.
This is line number 2.
This is line number 3.
This is line number 4.
[root@localhost ~]# cat test.txt
This is line number 1.
This is line number 2.
#+END_SRC
当然，如果不想让行直接输出，可以用 -n 选项，再举个例子：
#+begin_src bash
[root@localhost ~]# cat data11.txt
Blum, R       Browncoat
McGuiness, A  Alliance
Bresnahan, C  Browncoat
Harken, C     Alliance
[root@localhost ~]# sed -n '/Browncoat/w Browncoats.txt' data11.txt
cat Browncoats.txt
Blum, R       Browncoat
Bresnahan, C  Browncoat
#+END_SRC
可以看到，通过使用 w 脚本命令，sed 可以实现将包含文本模式的数据行写入目标文件。
*** sed r 将另一个文件的内容写入当前文件的指定位置
r 命令用于将一个独立文件的数据插入到当前数据流的指定位置，该命令的基本格式为：
#+begin_src bash
[address]r filename
#+END_SRC
sed 命令会将 filename 文件中的内容插入到 address 指定行的后面，比如说：
#+begin_src bash
[root@localhost ~]# cat data12.txt
This is an added line.
This is the second added line.
[root@localhost ~]# sed '3r data12.txt' data6.txt
This is line number 1.
This is line number 2.
This is line number 3.
This is an added line.
This is the second added line.
This is line number 4.
#+END_SRC
如果你想将指定文件中的数据插入到数据流的末尾，可以使用 $ 地址符，例如：
#+begin_src bash
[root@localhost ~]# sed '$r data12.txt' data6.txt
This is line number 1.
This is line number 2.
This is line number 3.
This is line number 4.
This is an added line.
This is the second added line.
#+END_SRC
*** sed q 停止sed操作
q 命令的作用是使 sed 命令在第一次匹配任务结束后，退出 sed 程序，不再进行对后续数据的处理。

比如：
#+begin_src bash
[root@localhost ~]# sed '2q' test.txt
This is line number 1.
This is line number 2.
#+END_SRC
可以看到，sed 命令在打印输出第 2 行之后，就停止了，是 q 命令造成的，再比如：
#+begin_src bash
[root@localhost ~]# sed '/number 1/{ s/number 1/number 0/;q; }' test.txt
This is line number 0.
#+END_SRC
使用 q 命令之后，sed 命令会在匹配到 number 1 时，将其替换成 number 0，然后直接退出。
*** sed n 将下一行内容移入缓存空间，替换当前内容
n 选项可以将下一行内容移入缓存空降，替换掉当前缓存空间的内容；而N 选项是将下一行内容添加进当前缓存空间，原有的缓存空间内容并不会清空
#+begin_src bash
cs144@cs144vm:~/test$ cat mysed.txt 
Beijing 2003
Beijing 2004
Beijing 2005
Beijing 2006
Beijing 2007
cs144@cs144vm:~/test$ sed -n '/200/{n;y/eijing/EIIJNG/;p}' mysed.txt 
BEIIING 2004
BEIIING 2006
cs144@cs144vm:~/test$ sed -n '/200/{N;y/eijing/EIIJNG/;p}' mysed.txt 
BEIIING 2003
BEIIING 2004
BEIIING 2005
BEIIING 2006

#+END_SRC

** sed 脚本命令的寻址方式
前面在介绍各个脚本命令时，我们一直忽略了对 address 部分的介绍。对各个脚本命令来说，address 用来表明该脚本命令作用到文本中的具体行。

默认情况下，sed 命令会作用于文本数据的所有行。如果只想将命令作用于特定行或某些行，则必须写明 address 部分，表示的方法有以下 2 种：
1. 以数字形式指定行区间；
2. 用文本模式指定具体行区间。

以上两种形式都可以使用如下这 2 种格式，分别是：
#+begin_src bash
[address]脚本命令
#+END_SRC
或者
#+begin_src bash
address {
    多个脚本命令
}
#+END_SRC
以上两种形式在前面例子中都有具体实例，因此这里不再做过多赘述。
*** 以数字形式指定行区间
当使用数字方式的行寻址时，可以用行在文本流中的行位置来引用。sed 会将文本流中的第一行编号为 1，然后继续按顺序为接下来的行分配行号。

在脚本命令中，指定的地址可以是单个行号，或是用起始行号、逗号以及结尾行号指定的一定区间范围内的行。这里举一个 sed 命令作用到指定行号的例子：
#+begin_src bash
[root@localhost ~]#sed '2s/dog/cat/' data1.txt
The quick brown fox jumps over the lazy dog
The quick brown fox jumps over the lazy cat
The quick brown fox jumps over the lazy dog
The quick brown fox jumps over the lazy dog
#+END_SRC
可以看到，sed 只修改地址指定的第二行的文本。下面的例子中使用了行地址区间：
#+begin_src bash
[root@localhost ~]# sed '2,3s/dog/cat/' data1.txt
The quick brown fox jumps over the lazy dog
The quick brown fox jumps over the lazy cat
The quick brown fox jumps over the lazy cat
The quick brown fox jumps over the lazy dog
#+END_SRC
在此基础上，如果想将命令作用到文本中从某行开始的所有行，可以用特殊地址——美元符（$）：
#+begin_src bash
[root@localhost ~]# sed '2,$s/dog/cat/' data1.txt
The quick brown fox jumps over the lazy dog
The quick brown fox jumps over the lazy cat
The quick brown fox jumps over the lazy cat
The quick brown fox jumps over the lazy cat
#+END_SRC
*** 用文本模式指定行区间
sed 允许指定文本模式来过滤出命令要作用的行，格式如下：
#+begin_src bash
/pattern/command
#+END_SRC
注意，必须用正斜线将要指定的 pattern 封起来，sed 会将该命令作用到包含指定文本模式的行上。

举个例子，如果你想只修改用户 demo 的默认 shell，可以使用 sed 命令，执行命令如下：
#+begin_src bash
[root@localhost ~]# grep demo /etc/passwd
demo:x:502:502::/home/Samantha:/bin/bash
[root@localhost ~]# sed '/demo/s/bash/csh/' /etc/passwd
root:x:0:0:root:/root:/bin/bash
...
demo:x:502:502::/home/demo:/bin/csh
...
#+END_SRC
虽然使用固定文本模式能帮你过滤出特定的值，就跟上面这个用户名的例子一样，但其作用难免有限，因此，sed 允许在文本模式使用正则表达式指明作用的具体行。正则表达式允许创建高级文本模式匹配表达式来匹配各种数据。这些表达式结合了一系列通配符、特殊字符以及固定文本字符来生成能够匹配几乎任何形式文本的简练模式。

关于正则表达式，本节不做过多介绍.这里仅给读者提供一个简单示例：
#+begin_src bash
[root@localhost ~]# cat test.txt
<html>
<title>First Wed</title>
<body>
h1Helloh1
h2Helloh2
h3Helloh3
</body>
</html>
#使用正则表示式给所有第一个的h1、h2、h3添加<>，给第二个h1、h2、h3添加</>
[root@localhost ~]# cat sed.sh
/h[0-9]/{
    s//\<&\>/1
    s//\<\/&\>/2
}
[root@localhost ~]# sed -f sed.sh test.txt
<h1>Hello</h1>
<h2>Hello</h2>
<h3>Hello</h3>
#+END_SRC
*** /xxx/,/yyy/定位行范围
#+begin_src bash
#文件内容展示一下
[roc@roclinux ~]$ cat mysed.txt
Beijing 2003
Beijing 2004
Beijing 2005
Beijing 2006
Beijing 2007
Beijing 2008
Beijing 2007
 
#我们想展示匹配了2005的行和2007的行之间的内容
[roc@roclinux ~]$ sed -n ’/2005/,/2007/p’ mysed.txt
Beijing 2005
Beijing 2006
Beijing 2007
#+END_SRC
我们使用/2005/来匹配行范围的首行，用/2008/来匹配行范围的尾行。可以看到，在匹配尾行时，只要遇到第一个符合要求的行，就会停止，而不会再继续向后匹配了。所以，sed 命令只是匹配到了第一个 2007，并没有匹配到第二个 2007。

** sed 多行命令
默认情况下，sed会基于换行符的位置，将数据分成行，sed 会根据定义好的脚本命令一次处理一行数据。

但是，有时我们需要对跨多行的数据执行特定操作。比如说，在文本中查找一串字符串"abcdergalskgjalskgjl" ，它很有可能出现在两行中，每行各包含其中一部分。这时，如果用普通的 sed 编辑器命令来处理文本，就不可能发现这种被分开的情况。

幸运的是，sed 命令的设计人员已经考虑到了这种情况，并设计了对应的解决方案。sed 包含了三个可用来处理多行文本的特殊命令，分别是：
- Next 命令（N）：将数据流中的下一行加进来创建一个多行组来处理。
- Delete（D）：删除多行组中的一行。
- Print（P）：打印多行组中的一行。

注意，以上命令的缩写，都为大写
*** N 多行操作命令
N 命令会将下一行文本内容添加到缓冲区已有数据之后（之间用换行符分隔），从而使前后两个文本行同时位于缓冲区中，sed 命令会将这两行数据当成一行来处理。

下面这个例子演示的 N 命令的功能：
#+begin_src bash
[root@localhost ~]# cat data2.txt
This is the header line.
This is the first data line.
This is the second data line.
This is the last line.
[root@localhost ~]# sed '/first/{ N ; s/\n/ / }' data2.txt
This is the header line.
This is the first data line. This is the second data line.
This is the last line.
#+END_SRC
在这个例子中，sed 命令查找含有单词 first 的那行文本。找到该行后，它会用 N 命令将下一行合并到那行，然后用替换命令 s 将换行符替换成空格。结果是，文本文件中的两行在 sed 的输出中成了一行。

如果要在数据文件中查找一个可能会分散在两行中的文本短语，如何实现呢？这里给大家一个实例：
#+begin_src bash
[root@localhost ~]# cat data3.txt
On Tuesday, the Linux System
Administrator's group meeting will be held.
All System Administrators should attend.
Thank you for your attendance.
[root@localhost ~]# sed 'N ; s/System Administrator/Desktop User/' data3.txt
On Tuesday, the Linux Desktop User's group meeting will be held.
All Desktop Users should attend.
Thank you for your attendance.
#+END_SRC
用 N 命令将发现第一个单词的那行和下一行合并后，即使短语内出现了换行，你仍然可以找到它，这是因为，替换命令在 System 和 Administrator之间用了通配符（.）来匹配空格和换行符这两种情况。但当它匹配了换行符时，它就从字符串中删掉了换行符，导致两行合并成一行。这可能不是你想要的。

要解决这个问题，可以在 sed 脚本中用两个替换命令，一个用来匹配短语出现在多行中的情况，一个用来匹配短语出现在单行中的情况，比如：
#+begin_src bash
[root@localhost ~]# sed 'N
> s/System\nAdministrator/Desktop\nUser/
> s/System Administrator/Desktop User/
> ' data3.txt
On Tuesday, the Linux Desktop
User's group meeting will be held.
All Desktop Users should attend.
Thank you for your attendance.
#+END_SRC
第一个替换命令专门查找两个单词间的换行符，并将它放在了替换字符串中。这样就能在第一个替换命令专门在两个检索词之间寻找换行符，并将其纳入替换字符串。这样就允许在新文本的同样位置添加换行符了。

但这个脚本中仍有个小问题，即它总是在执行 sed 命令前将下一行文本读入到缓冲区中，当它到了后一行文本时，就没有下一行可读了，此时 N 命令会叫 sed 程序停止，这就导致，如果要匹配的文本正好在最后一行中，sed 命令将不会发现要匹配的数据。

解决这个 bug 的方法是，将单行命令放到 N 命令前面，将多行命令放到 N 命令后面，像这样：
#+begin_src bash
[root@localhost ~]# sed '
> s/System\nAdministrator/Desktop\nUser/
> N
> s/System Administrator/Desktop User/
> ' data3.txt
On Tuesday, the Linux Desktop
User's group meeting will be held.
All Desktop Users should attend.
Thank you for your attendance.
#+END_SRC
现在，查找单行中短语的替换命令在数据流的后一行也能正常工作，多行替换命令则会负责短语出现在数据流中间的情况。
*** D 多行删除命令
sed 不仅提供了单行删除命令（d），也提供了多行删除命令 D，其作用是只删除缓冲区中的第一行，也就是说，D 命令将缓冲区中第一个换行符（包括换行符）之前的内容删除掉。

比如说：
#+begin_src bash
[root@localhost ~]# cat data4.txt
On Tuesday, the Linux System
Administrator's group meeting will be held.
All System Administrators should attend.
[root@localhost ~]# sed 'N ; /System\nAdministrator/D' data4.txt
Administrator's group meeting will be held.
All System Administrators should attend.
#+END_SRC
文本的第二行被 N 命令加到了缓冲区(如果不加N是匹配不到东西的)，因此 sed 命令第一次匹配就是成功，而 D 命令会将缓冲区中第一个换行符之前（也就是第一行）的数据删除，所以，得到了如上所示的结果。

下面的例子中，它会删除数据流中出现在第一行前的空白行：
#+begin_src bash
[root@localhost ~]# cat data5.txt

This is the header line.
This is a data line.

This is the last line.
[root@localhost ~]# sed '/^$/{N ; /header/D}' data5.txt
This is the header line.
This is a data line.

This is the last line.
#+END_SRC
sed会查找空白行，然后用 N 命令来将下一文本行添加到缓冲区。此时如果缓冲区的内容中含有单词 header，则 D 命令会删除缓冲区中的第一行。
*** P 多行打印命令
同 d 和 D 之间的区别一样，P（大写）命令和单行打印命令 p（小写）不同，对于具有多行数据的缓冲区来说，它只会打印缓冲区中的第一行，也就是首个换行符之前的所有内容。

例如，test.txt 文件中的内容如下：
#+begin_src bash
[root@localhost ~]# cat test.txt
aaa
bbb
ccc
ddd
eee
fff
#+END_SRC
下面对 test.txt 文件中的内容分别用 p 命令和 P 命令后，产生的输出信息的对比。
#+begin_src bash
[root@localhost ~]# sed '/.*/N;P' test.txt
aaa
aaa
bbb
ccc
ccc
ddd
eee
eee
fff

[root@localhost ~]# sed -n '/.*/N;P' test.txt
aaa
ccc
eee

[root@localhost ~]# sed '/.*/N;p' test.txt
aaa
bbb
aaa
bbb
ccc
ddd
ccc
ddd
eee
fff
eee
fff

[root@localhost ~]# sed -n '/.*/N;p' test.txt
aaa
bbb
ccc
ddd
eee
fff
#+END_SRC
注意：N 将后面行加入缓存区后就不会处理后面行了，处理完将直接处理第三行内容
** sed 保持空间
前面我们一直说，sed 命令处理的是缓冲区中的内容，其实这里的缓冲区，应称为模式空间。值得一提的是，模式空间并不是 sed 命令保存文件的唯一空间。sed 还有另一块称为保持空间的缓冲区域，它可以用来临时存储一些数据。

下表列出了 5 条可用来操作保持空间的命令。

| 命令 | 功能                             |
|------+----------------------------------|
| h    | 将模式空间中的内容复制到保持空间 |
| H    | 将模式空间中的内容附加到保持空间 |
| g    | 将保持空间中的内容复制到模式空间 |
| G    | 将保持空间中的内容附加到模式空间 |
| x    | 交换模式空间和保持空间中的内容   |
通常，在使用 h 或 H 命令将字符串移动到保持空间后，最终还要用 g、G 或 x 命令将保存的字符串移回模式空间。保持空间最直接的作用是，一旦我们将模式空间中所有的文件复制到保持空间中，就可以清空模式空间来加载其他要处理的文本内容。

由于有两个缓冲区域，下面的例子中演示了如何用 h 和 g 命令来将数据在 sed 缓冲区之间移动。
#+begin_src bash
[root@localhost ~]# cat data2.txt
This is the header line.
This is the first data line.
This is the second data line.
This is the last line.
[root@localhost ~]# sed -n '/first/ {h ; p ; n ; p ; g ; p }' data2.txt
This is the first data line.
This is the second data line.
This is the first data line.
#+END_SRC
这个例子的运行过程是这样的：
- sed脚本命令用正则表达式过滤出含有单词first的行；
- 当含有单词 first 的行出现时，h 命令将该行放到保持空间；
- p 命令打印模式空间也就是第一个数据行的内容；
- n 命令提取数据流中的下一行（This is the second data line），并将它放到模式空间；
- p 命令打印模式空间的内容，现在是第二个数据行；
- g 命令将保持空间的内容（This is the first data line）放回模式空间，替换当前文本；
- p 命令打印模式空间的当前内容，现在变回第一个数据行了。
** sed改变指定流程
*** b 分支命令
通常，sed 程序的执行过程会从第一个脚本命令开始，一直执行到最后一个脚本命令（D 命令是个例外，它会强制 sed 返回到脚本的顶部，而不读取新的行）。sed 提供了 b 分支命令来改变命令脚本的执行流程，其结果与结构化编程类似。

b 分支命令基本格式为：
#+begin_src bash
[address]b [label]
#+END_SRC
其中，address 参数决定了哪些行的数据会触发分支命令，label 参数定义了要跳转到的位置。

需要注意的是，如果没有加 label 参数，跳转命令会跳转到脚本的结尾，比如：
#+begin_src bash
[root@localhost ~]# cat data2.txt
This is the header line.
This is the first data line.
This is the second data line.
This is the last line.
[root@localhost ~]# sed '{2,3b ; s/This is/Is this/ ; s/line./test?/}' data2.txt
Is this the header test?
This is the first data line.
This is the second data line.
Is this the last test?
#+END_SRC
可以看到，因为 b 命令未指定 label 参数，因此数据流中的第2行和第3行并没有执行那两个替换命令。

如果我们不想直接跳到脚本的结尾，可以为 b 命令指定一个标签（也就是格式中的 label，最多为 7 个字符长度）。在使用此该标签时，要以冒号开始（比如 :label2），并将其放到要跳过的脚本命令之后。这样，当 sed 命令匹配并处理该行文本时，会跳过标签之前所有的脚本命令，但会执行标签之后的脚本命令。

比如说：
#+begin_src bash
[root@localhost ~]# sed '{/first/b jump1 ; s/This is the/No jump on/
> :jump1
> s/This is the/Jump here on/}' data2.txt
No jump on header line
Jump here on first data line
No jump on second data line
No jump on last line
#+END_SRC
在这个例子中，如果文本行中出现了 first，程序的执行会直接跳到 jump1 标签之后的脚本行。如果分支命令的模式没有匹配，sed 会继续执行所有的脚本命令。

b 分支命令除了可以向后跳转，还可以向前跳转，例如：
#+begin_src bash
[root@localhost ~]# echo "This, is, a, test, to, remove, commas." | sed -n '{
> :start
> s/,//1p
> /,/b start
> }'
This is, a, test, to, remove, commas.
This is a, test, to, remove, commas.
This is a test, to, remove, commas.
This is a test to, remove, commas.
This is a test to remove, commas.
This is a test to remove commas.
#+END_SRC
在这个例子中，当缓冲区中的行内容中有逗号时，脚本命令就会一直循环执行，每次迭代都会删除文本中的第一个逗号，并打印字符串，直至内容中没有逗号。
*** t 测试命令
类似于 b 分支命令，t 命令也可以用来改变 sed 脚本的执行流程。t 测试命令会根据 s 替换命令的结果，如果匹配并替换成功，则脚本的执行会跳转到指定的标签；反之，t 命令无效。

测试命令使用与分支命令相同的格式：
#+begin_src bash
[address]t [label]
#+END_SRC
跟分支命令一样，在没有指定标签的情况下，如果 s 命令替换成功，sed 会跳转到脚本的结尾（相当于不执行任何脚本命令）。例如：
#+begin_src bash
[root@localhost ~]# sed '{
> s/first/matched/
> t
> s/This is the/No match on/
> }' data2.txt
No match on header line
This is the matched data line
No match on second data line
No match on last line
#+END_SRC
此例中，第一个替换命令会查找模式文本 first，如果匹配并替换成功，命令会直接跳过后面的替换命令；反之，如果第一个替换命令未能匹配成功，第二个替换命令就会被执行。

再举个例子：
#+begin_src bash
[root@localhost ~]#  echo "This, is, a, test, to, remove, commas. " | sed -n '{
> :start
> s/,//1p
> t start
> }'
This is, a, test, to, remove, commas.
This is a, test, to, remove, commas.
This is a test, to, remove, commas.
This is a test to, remove, commas.
This is a test to remove, commas.
This is a test to remove commas.
#+END_SRC
** 给文件的行编号
#+begin_src bash
# 行号和内容分开
sed '=' data

# 行号和内容在同一行
sed '=' data | sed 'N;s/\n/: /'
#+END_SRC

* split
split 的作用很好描述，就是将文件按照一定规则进行拆分。一般情况下，我们可以按照文件大小来进行拆分，如果是文本文件的话，还可以按照行数来进行拆分，默认是 1000 行作为一个拆分单位。

默认情况下，分割后的文件的名称会以 x 作为前缀，以 aa、ab、ac 这样的双字母格式作为后缀，形成 xaa、xab 这样的名称格式。

我们来一起看看 split 的命令格式：
~split [-b ][-C ][-][-l ][要切割的文件][输出文件名前缀][-a ]~

最常用的选项，都在这里了：
- -b<字节>：指定按多少字节进行拆分，也可以指定 K、M、G、T 等单位。
- -<行数>或-l<行数>：指定每多少行要拆分成一个文件。
- 输出文件名前缀：设置拆分后的文件的名称前缀，split 会自动在前缀后加上编号，默认从 aa 开始。
- -a<后缀长度>：默认的后缀长度是 2，也就是按 aa、ab、ac 这样的格式依次编号。
** 例子
闲言少叙，我们现在就来介绍拆分的方法。先使用 dd 命令来生成一个 700MB 文件来作为我们的拆分对象：
#+BEGIN_SRC bash
[root@roclinux ~]$ dd if=/dev/zero bs=1024 count=700000 of=king_of_ring.avi
700000+0 records in
700000+0 records out
716800000 bytes (717 MB) copied, 12.9189 s, 55.5 MB/s
 
[root@roclinux ~]$  ls -l king_of_ring.avi
-rw-r--r-- 1 root root 716800000 Apr 12 13:01 king_of_ring.avi
#+END_SRC
美国大片的文件大小是 700MB，而我手边仅有的两个优盘，都是 512MB 大小的。我打算把文件以 400MB 作为一个拆分单位，来进行拆分。这里使用到了 split 的-b选项，来指定每个拆分文件的大小：
#+BEGIN_SRC bash
[root@roclinux ~]$ split -b 400M king_of_ring.avi
 
[root@roclinux ~]$ ls -l
total 1400008
-rw-r--r-- 1 root root 716800000 Apr 12 13:01 king_of_ring.avi
-rw-r--r-- 1 root root 419430400 Apr 12 13:04 xaa
-rw-r--r-- 1 root root 297369600 Apr 12 13:04 xab
#+END_SRC
看！分身完毕！咦，怎么多出了 xaa 和 xab 两个文件，这么奇怪的名字？

是的，你没看错，在没有明确指定拆分后文件的命名方式的情况下，split 会默认采用 x 字符作为文件前缀，采用类似 aa、ab、ac 的字符串依次作为文件后缀。于是，就出现了我们上面看到的 xaa、xab 了。

从文件大小来看，如我们所愿，电影文件的确被切割成了一个 400MB 的文件、一个 300MB 的文件，终于可以装到两个优盘里了。
** 切分后的合并
使用 cat 命令将拆分文件 xaa 和 xab 合并成一个文件，可以看出合并后的文件和源文件的大小是一致的：
#+BEGIN_SRC bash
[root@roclinux ~]$ cat xaa xab > king_of_ring_merge.avi
 
[root@roclinux ~]$ ls -l
total 2100012
-rw-r--r-- 1 root root 716800000 Apr 12 13:01 king_of_ring.avi
-rw-r--r-- 1 root root 716800000 Apr 12 13:07 king_of_ring_merge.avi
-rw-r--r-- 1 root root 419430400 Apr 12 13:04 xaa
-rw-r--r-- 1 root root 297369600 Apr 12 13:04 xab
#+END_SRC
对了，如果是在 Windows 下的话，我们要先运行 cmd，然后用 copy 命令来进行文件的合并：
#+BEGIN_SRC bash
copy /b xaa + xab king_of_ring.avi
#+END_SRC
格式上和 Linux 有些区别，但原理是一样的。
** 设置拆分文件的名称前缀
上面例子中，我们没有指定拆分文件的名称前缀，结果拆分后的文件名都是 aa、ab 这样的名称，这样的名称既不达意也不美观。

下面的例子，我们尝试以 king_of_ring_part_ 作为拆分后文件的名称前缀：
#+BEGIN_SRC bash
#我们指定了king_of_ring_part_前缀

[root@roclinux ~]$ split -b 400m king_of_ring.avi king_of_ring_part_
 
#可以看到, 文件名的可读性提高了很多
[root@roclinux ~]$ ls -l king*
-rw-r--r-- 1 root root 716800000 Feb 25 18:29 king_of_ring.avi
-rw-r--r-- 1 root root 419430400 Feb 25 19:24 king_of_ring_part_aa
-rw-r--r-- 1 root root 297369600 Feb 25 19:24 king_of_ring_part_ab
#+END_SRC

文件名的可读性是不是提高了不少，从文件名称就可以看出来是美国大片的拆分文件啦。
** 设置数字后缀
如果大家看不惯以 aa、ab 这种字母作为文件后缀，我们还可以通过-d选项来指定数字形式的文件后缀：
#+BEGIN_SRC bash
#使用了-d选项
[root@roclinux ~]$ split -b 400m -d king_of_ring.avi king_of_ring_part_
 
#后缀从原来的aa、ab变成了00、01
[root@roclinux ~]$ ls -l king*
-rw-r--r-- 1 root root 716800000 Feb 25 18:29 king_of_ring.avi
-rw-r--r-- 1 root root 419430400 Feb 25 19:24 king_of_ring_part_00
-rw-r--r-- 1 root root 297369600 Feb 25 19:24 king_of_ring_part_01
#+END_SRC
** 按照行数进行拆分
前面我们讲的是按照文件大小（如 400MB）进行文件拆分的方法，但是并非所有情况都适合于用文件大小作为拆分单元。比如，我们希望把 /etc/passwd 文件按照一个文件 10 行记录的方式进行拆分，又该怎么操作呢？
#+BEGIN_SRC bash
#使用-N来指定拆分的行数,本例中为-10
[root@roclinux ~]$ split -d -10 /etc/passwd my_passwd_
 
#可以看到拆分成功
[root@roclinux ~]$ wc -l my_passwd_*
  10 my_passwd_00
  10 my_passwd_01
   5 my_passwd_02
  25 total
#+END_SRC
** 合并后的校验
需要注意的是，在通过网络来传输大文件，或者在设备之间复制大文件的时候，可能会出现传输前后数据不一致的情况。

使用 split 来拆分大文件仅仅是故事的开始，操作完毕后化零为整、完璧归赵才是完美的结局。因此需要在合并文件后进行文件的完整性校验，推荐使用 md5sum 来计算和比对前后两个大文件的 md5 值。
#+BEGIN_SRC bash
#对原先的文件计算md5值
[root@roclinux ~]$ md5sum king_of_ring.avi
eacff27bf2db99c7301383b7d8c1c07c  king_of_ring.avi
 
#对合并后的文件计算md5值, 并与原值进行比较
[root@roclinux ~]$ md5sum king_of_ring_merge.avi
eacff27bf2db99c7301383b7d8c1c07c  king_of_ring_merge.avi
#+END_SRC
如果前后一致，那么恭喜你，文件合并成功！
* script（记录终端输出）
script这个命令很强大，可以记录终端的所有输出到相应的文件中

执行script就是启动了一个子shell，所以用exit即可退出script

看例子:
#+begin_src bash
[lhd@hongdi ~]$ script
Script. started, file is typescript
[lhd@hongdi ~]$ ls
1.gtkrc-2.0 c.tar kmess-2.0alpha2.tar.gz secpanel-0.5.3-1.noarch.rpm
2009 DownZipAction.php kmesslog secpanel-0.5.4-2.noarch.rpm
[lhd@hongdi ~]$ exit
exit
Script. done, file is typescript
 
 
[lhd@hongdi ~]$ cat typescript
Script. started on 2009年02月08日 星期日 18时56分52秒
[lhd@hongdi ~]$ ls
1.gtkrc-2.0 c.tar kmess-2.0alpha2.tar.gz secpanel-0.5.3-1.noarch.rpm
2009 DownZipAction.php kmesslog secpanel-0.5.4-2.noarch.rpm
[lhd@hongdi ~]$ exit
exit
Script. done on 2009年02月08日 星期日 18时57分00秒
#+END_SRC
我们在启动script时没有指定文件名，它会自动记录到当前目录下一个名为 typescript的文件中。也可以用 -a参数 指定文件名
* ssh 相关
** ubuntu开启SS
H服务远程登录
 SSH分客户端openssh-client和openssh-server

 如果你只是想登陆别的机器的SSH只需要安装openssh-client（ubuntu有默认安装，如果没有则sudo apt-get install openssh-client），如果要使本机开放SSH服务就需要安装openssh-server。

  查看当前的ubuntu是否安装了ssh-server服务。默认只安装ssh-client服务
 dpkg -l | grep ssh

 安装ssh-server服务
 sudo apt-get install openssh-server

 再次查看安装的服务：
 dpkg -l | grep ssh

 然后确认ssh-server是否启动了：

 ps -e | grep ssh

 如果看到sshd那说明ssh-server已经启动了。
 如果没有则可以这样启动：sudo /etc/init.d/ssh start或sudo service ssh start
 配置相关：
 ssh-server配置文件位于/etc/ssh/sshd_config，在这里可以定义SSH的服务端口，默认端口是22，你可以自己定义成其他端口号，如222。（或把配置文件中的”PermitRootLogin without-password”加一个”#”号,把它注释掉，再增加一句”PermitRootLogin yes”）
 然后重启SSH服务：
 sudo /etc/init.d/ssh stop
 sudo /etc/init.d/ssh start
*** Unable to locate package openssh-server
  给一个docker的ubuntu容器安装openssh-server出现了这个问题
 #+BEGIN_SRC bash
 root@1c3148b444e2:/# apt-get install openssh-server
 Reading package lists... Done
 Building dependency tree       
 Reading state information... Done
 E: Unable to locate package openssh-server
 #+END_SRC
 原因解释：因为软件源出问题，导致无法找到或者下载软件，一般原因是刚安装的Ubuntu后没有更新软件源或者更新后但是没有sudo apt-get update，导致找不到软件。

 1 sudo apt-get update 
 （更新软件源）执行安装操作，如果不成功，执行2 

 2  sudo apt-get upgrade 
 （继续更新软件源）执行安装操作，应该能成功 
 最后执行sudo apt-get install -y openssh-server
** SSH、SCP和SFTP
*** SSH、SCP和SFTP都是SSH软件包的组成部分
 SSH 是 Secure Shell 的缩写，由 IETF 的网络小组（Network Working Group）所制定；SSH 为建立在应用层基础上的安全协议。SSH 是目前广泛采用的安全登录协议，专为远程登录会话和其他网络服务提供安全性的协议，替代以前不安全的Telnet协议。利用 SSH 协议可以有效防止远程管理过程中的信息泄露问题。

 SSH包括二个部分，服务端的SSHD（Secure Shell Daemon）和SSH客户端。我们通常所说的用SSH登录到某某主机，指的是用SSH客户端远程登录到某台主机（该主机运行了SSHD服务端程序）。

 SSH最初是UNIX系统上的一个程序，后来又迅速扩展到其他操作平台，目前几乎所有UNIX平台—包括HP-UX、Linux、AIX、Solaris、Digital UNIX、Irix，以及其他系统平台，都可运行SSH。
*** SCP和SFTP
 SCP是Secure Copy的简称，是用来与远程主机之间进行数据传输的协议，相当于经过加密的Copy命令。SCP数据传输使用 ssh协议，并且和ssh 使用相同的认证方式，提供相同的安全保证 。 根据实际需要，scp进行验证时会要求你输入密码或口令。

 SFTP=SSH File Transfer Protocol ，有时也被称作 Secure File Transfer Protocol 。SFTP是用SSH封装过的FTP协议，相当于经过加密的FTP协议，功能与FTP一样，只是传输数据经过加密。

 SFTP也有二个部分，服务端的SFTP-Server及SFTP Client。通常所说的用SFTP登录到某台主机，指的是用SFTP客户端登录到某台主机（该主机运行了SFTP-Server服务端程序）。
**** SCP和SFTP异同：

 不管SCP还是SFTP，都是SSH的功能之一，也都是使用SSH协议来传输文件的。

 不只是登录时的用户信息，相互传输的文件内容也是经过SSH加密的，所以说SCP和SFTP实现了安全的文件传输。

 SCP和CP命令相似，SFTP和FTP的使用方法也类似。SCP和SFTP的共同之处在于「使用SSH将文件加密才传输的」

 使用「WinSCP」或者「FileZilla」之类的客户端，还可以和Windows之间进行文件传输。

 SCP和SFTP的不同之处，首先就是之前提到的，SCP使用「SCP命令」，SFTP则类似「FTP处理文件」的使用方式。

 它们的不同之处还不止如此，还有「SCP比较简单，是轻量级的，SFTP的功能则比较多」。

 虽然还有很多不同之处，但二者的最大不同之处在于「SFTP在文件传输过程中中断的话，连接后还可以继续传输，但SCP不行」。

 由于各种原因导致的文件传输中断是经常讨论的话题，所以这个区别（SFTP支持断点续传，SCP则不支持）被认为是最大的区别。
*** 常见的SSH客户端
**** 图形化客户端：
 WinSCP，是一个Windows环境下使用SSH的开源图形化SFTP客户端。同时支持FTP、SCP、webdav协议。它的主要功能就是在本地与远程计算机间安全的复制文件。

 Xftp，是一个基于 MS windows 平台的功能强大的SFTP、FTP 文件传输软件。使用了 Xftp 以后，MS windows 用户能安全地在 UNIX/Linux 和 Windows PC 之间传输文件。

 FileZilla是一个免费开源的FTP软件，分为客户端版本和服务器版本，具备所有的FTP软件功能。支持FTP，SFTP(SSH File Transfer Protocol)， FTPS(FTP over SSL/TLS)等多种协议。

**** 终端工具类：
 PuTTY是一个Telnet、SSH、rlogin、纯TCP以及串行接口连接软件。PuTTY是一款开放源代码软件，使用MIT licence授权。

 Xshell 是一个强大的安全终端模拟软件，它支持SSH1, SSH2, SFTP以及Microsoft Windows 平台的TELNET 协议。
** 基本用法
ssh 最常见的用途就是登录服务器，这要求服务器安装并正在运行 SSH 服务器软件。

ssh 登录服务器的命令如下。
#+begin_src bash
$ ssh hostname
#+END_SRC
上面命令中，hostname是主机名，它可以是域名，也可能是 IP 地址或局域网内部的主机名。不指定用户名的情况下，将使用客户端的当前用户名，作为远程服务器的登录用户名。如果要指定用户名，可以采用下面的语法。
#+begin_src bash
$ ssh user@hostname
#+END_SRC
上面的命令中，用户名和主机名写在一起了，之间使用@分隔。

用户名也可以使用ssh的-l参数指定，这样的话，用户名和主机名就不用写在一起了。
#+begin_src bash
$ ssh -l username host
#+END_SRC
ssh 默认连接服务器的22端口，-p参数可以指定其他端口。
#+begin_src bash
$ ssh -p 8821 foo.com
#+END_SRC
上面命令连接服务器foo.com的8821端口。

** SSH 密钥
 基于密钥的验证机制使用了密码学中的公钥，我们只需要向服务器证明客户端持有对应的私钥，而不需要公开其私钥。
 这样您就可以避免每次登录都输入密码的麻烦了秘密就可以登录。
 不过，私钥(通常是 ~/.ssh/id_rsa 或者 ~/.ssh/id_ed25519) 等效于您的密码，所以一定要好好保存它。

 ssh秘钥登录特点：1.安全；2.免输密码。

 对于安全级别较高的服务器，建议配好ssh登录后禁掉密码登录。

 缺点：略繁琐。如果你的只是临时登录一次，那么还是密码吧。

*** 密钥登录的过程
SSH 密钥登录分为以下的步骤：

预备步骤，客户端通过ssh-keygen生成自己的公钥和私钥。

第一步，手动将客户端的公钥放入远程服务器的指定位置。

第二步，客户端向服务器发起 SSH 登录的请求。

第三步，服务器收到用户 SSH 登录的请求，发送一些随机数据给用户，要求用户证明自己的身份。

第四步，客户端收到服务器发来的数据，使用私钥对数据进行签名，然后再发还给服务器。

第五步，服务器收到客户端发来的加密签名后，使用对应的公钥解密，然后跟原始数据比较。如果一致，就允许用户登录。

*** 生成秘钥
秘钥对需要在你自己的机器上生成，然后把公钥放到服务器相应用户的~/.ssh目录

使用 ssh-keygen 命令可以生成一对密钥

执行下面命令,默认生成位置是~/.ssh

~ssh-keygen~

系统会询问你文件名和秘钥密码，可以一路回车过去，会生成两个文件：
- id_rsa 私钥
- id_rsa.pub 公钥
默认使用rsa算法，你也可以用比较详细的指令，如
~ssh-keygen -t rsa -b 1024 -f yourkeyname -C "备注"~

| 参数   | 解释                                                  |
|--------+-------------------------------------------------------|
| -b     | 采用长度1024bit的密钥对,b=bits,最长4096，不过没啥必要 |
| -t rsa | 采用rsa加密方式,t=type                                |
| -f     | 生成文件名(文件路径),f=output_keyfiles                     |
| -C     | 备注，C=comment                                       |


更多参数可运行 man ssh-keygen

输入上面的命令以后，ssh-keygen会要求用户回答一些问题。
#+begin_src bash
$ ssh-keygen -t dsa
Generating public/private dsa key pair.
Enter file in which to save the key (/home/username/.ssh/id_dsa):  press ENTER
Enter passphrase (empty for no passphrase): ********
Enter same passphrase again: ********
Your identification has been saved in /home/username/.ssh/id_dsa.
Your public key has been saved in /home/username/.ssh/id_dsa.pub.
The key fingerprint is:
14:ba:06:98:a8:98:ad:27:b5:ce:55:85:ec:64:37:19 username@shell.isp.com
#+END_SRC
上面示例中，执行ssh-keygen命令以后，会出现第一个问题，询问密钥保存的文件名，默认是~/.ssh/id_dsa文件，这个是私钥的文件名，对应的公钥文件~/.ssh/id_dsa.pub是自动生成的。用户的密钥一般都放在主目录的.ssh目录里面。

如果选择rsa算法，生成的密钥文件默认就会是~/.ssh/id_rsa（私钥）和~/.ssh/id_rsa.pub（公钥）。

接着，就会是第二个问题，询问是否要为私钥文件设定密码保护（passphrase）。这样的话，即使入侵者拿到私钥，还是需要破解密码。如果为了方便，不想设定密码保护，可以直接按回车键，密码就会为空。后面还会让你再输入一次密码，两次输入必须一致。注意，这里“密码”的英文单词是 passphrase，这是为了避免与 Linux 账户的密码单词 password 混淆，表示这不是用户系统账户的密码。

实际上这里的密码保护是为了防止别人复制私钥后拿去登录服务器。当别人用复制来的私钥在他的电脑上登录服务器时，ssh客户端会要求其输入密码来解开私钥。实际上，密码保护只能防止别人无法用私钥在他自己的电脑上登录，但他还是能够在你的电脑上登录服务器，因为你的ssh客户端已经在第一次登录的时候解开了私钥，之后登录不会再要求验证密码。参考下面的解释：
#+BEGIN_EXAMPLE
SSH uses private/public key pairs to protect your communication with the server. SSH passphrases protect your private key from being used by someone who doesn't know the passphrase. Without a passphrase, anyone who gains access to your computer has the potential to copy your private key. For example, family members, coworkers, system administrators, and hostile actors could gain access.

A secure passphrase helps keep your private key from being copied and used even if your computer is compromised.

The downside to passphrases is that you need to enter it every time you create a connection using SSH. You can temporarily cache your passphrase using ssh-agent so you don't have to enter it every time you connect.
#+END_EXAMPLE

最后，就会生成私钥和公钥，屏幕上还会给出公钥的指纹，以及当前的用户名和主机名作为注释，用来识别密钥的来源。

公钥文件和私钥文件都是文本文件，可以用文本编辑器看一下它们的内容。公钥文件的内容类似下面这样。
#+BEGIN_EXAMPLE
ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAIEAvpB4lUbAaEbh9u6HLig7amsfywD4fqSZq2ikACIUBn3GyRPfeF93l/
weQh702ofXbDydZAKMcDvBJqRhUotQUwqV6HJxqoqPDlPGUUyo8RDIkLUIPRyq
ypZxmK9aCXokFiHoGCXfQ9imUP/w/jfqb9ByDtG97tUJF6nFMP5WzhM= username@shell.isp.com
#+END_EXAMPLE
上面示例中，末尾的username@shell.isp.com是公钥的注释，用来识别不同的公钥，表示这是哪台主机（shell.isp.com）的哪个用户（username）的公钥，不是必需项。

注意，公钥只有一行。因为它太长了，所以上面分成三行显示。

生成密钥以后，建议修改它们的权限，防止其他人读取。
#+begin_src bash
$ chmod 600 ~/.ssh/id_rsa
$ chmod 600 ~/.ssh/id_rsa.pub
#+END_SRC
您可以使用 ssh-agent 或 gpg-agent ，这样就不需要每次都输入该密码了。
*** 在服务器上安装秘钥
ssh 会查询 .ssh/authorized_keys 来确认那些用户可以被允许登录。

把上一步生成的公钥发送到服务器(scp,FillZilla等)上，然后在服务器上执行下面命令

cat id_rsa.pub >> ~/.ssh/authorized_keys


或者直接在本地机器上执行命令： cat ~/.ssh/id_rsa.pub | ssh user@host "mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys"

如果支持 ssh-copy-id 的话，可以使用下面这种更简单的解决方案：

ssh-copy-id -i .ssh/id_rsa.pub foobar@remote

如果ssh-copy-id不行的话，就打开远程主机的/etc/ssh/sshd_config这个文件，检查下面几行前面"#"注释是否取掉。
#+BEGIN_EXAMPLE
RSAAuthentication yes
PubkeyAuthentication yes
AuthorizedKeysFile .ssh/authorized_keys
#+END_EXAMPLE
然后，重启远程主机的ssh服务。

如此便完成了公钥安装，有个小坑值得一提：authenrized_keys的权限必须是600或更小，否则会连接失败。

保险起见，执行下面命令
#+BEGIN_SRC bash
chmod 600 ~/.ssh/authorized_keys
chmod 700 ~/.ssh
#+END_SRC
另外，.ssh目录的owner必须是ssh登录用户，不能是root
*** config 文件的配置
ssh 默认用~/.ssh/xxx下的id_rsa私钥进行验证，如果需要用别的私钥，需要用IdentityFile指定私钥位置

在.ssh/config 配置文件下中加个密钥文件的定义

注意：IdentityFile填的是私钥文件的位置
#+BEGIN_EXAMPLE
HOST w231
    HostName 192.168.1.231
    IdentityFile ~/.ssh/xxx
#+END_EXAMPLE
*** 服务器ssh配置

 修改服务器上的ssh配置文件，位置：/etc/ssh/sshd_config
 #+BEGIN_EXAMPLE
 RSAAuthentication yes
 PubkeyAuthentication yes

 PermitRootLogin no //禁止root登录
 PasswordAuthentication yes //允许密码登录，根据你的情况设置
 #+END_EXAMPLE
 然后重启ssh服务

 service sshd restart
** SSH执行命令
 ssh 的一个经常被忽视的特性是它可以直接远程执行命令。

 ssh foobar@server ls 可以直接在用foobar的命令下执行 ls 命令。 

 想要配合管道来使用也可以， ssh foobar@server ls | grep PATTERN 会在本地查询远端 ls 的输出而 ls | ssh foobar@server grep PATTERN 会在远端对本地 ls 输出的结果进行查询。

采用这种语法执行命令时，ssh 客户端不会提供互动式的 Shell 环境，而是直接将远程命令的执行结果输出在命令行。但是，有些命令需要互动式的 Shell 环境，这时就要使用-t参数。
#+begin_src bash
# 报错
$ ssh remote.server.com emacs
emacs: standard input is not a tty

# 不报错
$ ssh -t server.example.com emacs
#+END_SRC
上面代码中，emacs命令需要一个互动式 Shell，所以报错。只有加上-t参数，ssh 才会分配一个互动式 Shell。
** 通过 SSH 复制文件
 使用 ssh 复制文件有很多方法：
 - ssh+tee, 最简单的方法是执行 ssh 命令，然后通过这样的方法利用标准输入实现 cat localfile | ssh remote_server tee serverfile。回忆一下，tee 命令会将标准输出写入到一个文件；
 - scp ：当需要拷贝大量的文件或目录时，使用scp 命令则更加方便，因为它可以方便的遍历相关路径。语法如下：scp path/to/local_file remote_host:path/to/remote_file；
 - rsync 对 scp 进行了改进，它可以检测本地和远端的文件以防止重复拷贝。它还可以提供一些诸如符号连接、权限管理等精心打磨的功能。甚至还可以基于 --partial标记实现断点续传。rsync 的语法和scp类似；
** SSH 客户端config
SSH config是Linux系统下针对SSH客户端的一个参数配置方案，可以将一些关于SSH命令的参数放到配置文件中去，执行ssh命令的时候从文件中读取，简化命令行的操作。这篇短博客记录ssh config相关的配置问题和使用方法。

SSH 参数配置有3个层次：
- 命令行参数，如-p 10086, -i /path/to/identity_file 等选项来设置SSH的端口号或认证证书位置
- 针对某个用户的配置文件，所在路径为~/.ssh/config，默认是不存在的，需要手动创建
- 针对系统所有用户的配置文件，所在路径为/etc/ssh/ssh_config

参数重要性的顺序也是1>2>3，即越近的配置重要性越高。

除了配置文件，~/.ssh目录还有一些用户个人的密钥文件和其他文件。下面是其中一些常见的文件。
- ~/.ssh/id_ecdsa：用户的 ECDSA 私钥。
- ~/.ssh/id_ecdsa.pub：用户的 ECDSA 公钥。
- ~/.ssh/id_rsa：用于 SSH 协议版本2 的 RSA 私钥。
- ~/.ssh/id_rsa.pub：用于SSH 协议版本2 的 RSA 公钥。
- ~/.ssh/identity：用于 SSH 协议版本1 的 RSA 私钥。
- ~/.ssh/identity.pub：用于 SSH 协议版本1 的 RSA 公钥。
- ~/.ssh/known_hosts：包含 SSH 服务器的公钥指纹。

这里针对~/.ssh/config文件的写法进行说明。

一个示例的文件如下：
#+BEGIN_EXAMPLE
# configuration 1
Host *
     Port 2222

# configuration 2
Host remoteserver
     HostName remote.example.com
     User neo
     Port 2112


# configuration 3
Host=aliyun
     Hostname=202.44.2.2
     User tom
#+END_EXAMPLE
主要的规则如下：
- 不同主机的配置通过Host参数来区分，一个配置文件里面可以有针对多个Host的配置
- 以#开头的是注释，会被忽略
- 同一个Host的配置内部，参数名 参数值和参数值=参数名的形式可以混用，如上例#2配置所示

Host命令的值可以使用通配符，比如Host *表示对所有主机都有效的设置，Host *.edu表示只对一级域名为.edu的主机有效的设置。它们的设置都可以被单个主机的设置覆盖。

上面代码中，Host *表示对所有主机生效，后面的Port 2222表示所有主机的默认连接端口都是2222，这样就不用在登录时特别指定端口了。这里的缩进并不是必需的，只是为了视觉上，易于识别针对不同主机的设置。

后面的Host remoteserver表示，下面的设置只对主机remoteserver生效。remoteserver只是一个别名，具体的主机由HostName命令指定，User和Port这两项分别表示用户名和端口。这里的Port会覆盖上面Host *部分的Port设置。

以后，登录remote.example.com时，只要执行ssh remoteserver命令，就会自动套用 config 文件里面指定的参数。

*** config文件常见参数
参数可以在命令行通过man ssh_config来查看
#+BEGIN_EXAMPLE
AddressFamily inet：表示只使用 IPv4 协议。如果设为inet6，表示只使用 IPv6 协议。
BindAddress 192.168.10.235：指定本机的 IP 地址（如果本机有多个 IP 地址）。
BatchMode no                              如果设为“yes”，passphrase/password（交互式输入口令）的提示将被禁止。当不能交互式输入口令的时候，这个选项对脚本文件和批处理任务十分有用
CheckHostIP yes：检查 SSH 服务器的 IP 地址是否跟公钥数据库吻合。
Ciphers blowfish,3des：指定加密算法。
Compression yes：是否压缩传输信号。
ConnectionAttempts 10：客户端进行连接时，最大的尝试次数。
ConnectTimeout 60：客户端进行连接时，服务器在指定秒数内没有回复，则中断连接尝试。
DynamicForward 1080：指定动态转发端口。
EscapeChar ~                              设置escape字符
ForwardAgent no                           设置连接是否经过验证代理（如果存在）转发给远程计算机。
ForwardX11 no                             设置X11连接是否被自动重定向到安全的通道和显示集（DISPLAY set）
FallBackToRsh no                      设置如果用ssh连接出现错误是否自动使用rsh
GlobalKnownHostsFile /users/smith/.ssh/my_global_hosts_file：指定全局的公钥数据库文件的位置。
Host server.example.com：指定连接的域名或 IP 地址，也可以是别名，支持通配符。Host命令后面的所有配置，都是针对该主机的，直到下一个Host命令为止。“*”表示所有的计算机。
HostKeyAlgorithms ssh-dss,ssh-rsa：指定密钥算法，优先级从高到低排列。
HostName myserver.example.com：在Host命令使用别名的情况下，HostName指定域名或 IP 地址。
IdentityFile keyfile：指定私钥文件。 默认位置是~/.ssh/id_rsa,~/ssh/id_dsa等，如果采用默认的证书，可以不用设置此参数，除非你的证书放在某个自定义的目录，那么你就需要设置该参数来指向你的证书
LocalForward 2001 localhost:143：指定本地端口转发。
LogLevel QUIET：指定日志详细程度。如果设为QUIET，将不输出大部分的警告和提示。
MACs hmac-sha1,hmac-md5：指定数据校验算法。
NumberOfPasswordPrompts 2：密码登录时，用户输错密码的最大尝试次数。
PasswordAuthentication no：指定是否支持密码登录。不过，这里只是客户端禁止，真正的禁止需要在 SSH 服务器设置。
Port 2035：指定客户端连接的 SSH 服务器端口。
PreferredAuthentications publickey,hostbased,password：指定各种登录方法的优先级。
Protocol 2：支持的 SSH 协议版本，多个版本之间使用逗号分隔。默认是22端口，同上，只有在非默认情况下才需要设置该值
PubKeyAuthentication yes：是否支持密钥登录。这里只是客户端设置，还需要在 SSH 服务器进行相应设置。
RemoteForward 2001 server:143：指定远程端口转发。
RhostsAuthentication no                   设置是否使用基于rhosts的安全验证
RhostsRSAAuthentication no            设置是否使用用RSA算法的基于rhosts的安全验证
RSAAuthentication yes                     设置是否使用RSA算法进行安全验证
SendEnv COLOR：SSH 客户端向服务器发送的环境变量名，多个环境变量之间使用空格分隔。环境变量的值从客户端当前环境中拷贝。
ServerAliveCountMax 3：如果没有收到服务器的回应，客户端连续发送多少次keepalive信号，才断开连接。该项默认值为3。
ServerAliveInterval 300：客户端建立连接后，如果在给定秒数内，没有收到服务器发来的消息，客户端向服务器发送keepalive消息。如果不希望客户端发送，这一项设为0。
StrictHostKeyChecking yes：yes表示严格检查，服务器公钥为未知或发生变化，则拒绝连接。no表示如果服务器公钥未知，则加入客户端公钥数据库，如果公钥发生变化，不改变客户端公钥数据库，输出一条警告，依然允许连接继续进行。ask（默认值）表示询问用户是否继续进行。
TCPKeepAlive yes：客户端是否定期向服务器发送keepalive信息。
User userName：指定远程登录的账户名。
UserKnownHostsFile /users/smith/.ssh/my_local_hosts_file：指定当前用户的known_hosts文件（服务器公钥指纹列表）的位置。
UseRsh no                             设置是否在这台计算机上使用“rlogin/rsh”
VerifyHostKeyDNS yes：是否通过检查 SSH 服务器的 DNS 记录，确认公钥指纹是否与known_hosts文件保存的一致。
#+END_EXAMPLE

*** /etc/ssh/sshd_config配置文件
#+BEGIN_EXAMPLE
参数选项                                                        说明
Port 22                                                         SSH 预设使用 22 这个 port，您也可以使用多的 port ！
Protocol 2,1                                                    选择的 SSH 协议版本，可以是 1 也可以是 2 ，如果要同时支持两者，就必须要使用 2,1 这个分隔了！
ListenAddress 0.0.0.0                                           监听的主机适配卡！举个例子来说，如果您有两个 IP，分别是 192.168.0.100 及 192.168.2.20 ，那么只想要开放 192.168.0.100 时，就可以写如同下面的样式：
ListenAddress 192.168.0.100                                     只监听来自 192.168.0.100 这个 IP 的SSH联机。如果不使用设定的话，则预设所有接口均接受 SSH
PidFile /var/run/sshd.pid                                       可以放置 SSHD 这个 PID 的档案！左列为默认值
LoginGraceTime 600                                              当使用者连上 SSH server 之后，会出现输入密码的画面，在该画面中，在多久时间内没有成功连上 SSH server ，就断线！时间为秒！
Compression yes                                                 是否可以使用压缩指令？
HostKey /etc/ssh/ssh_host_key                                   SSH version 1 使用的私钥
HostKey /etc/ssh/ssh_host_rsa_key                               SSH version 2 使用的 RSA 私钥
HostKey /etc/ssh/ssh_host_dsa_key                               SSH version 2 使用的 DSA 私钥
KeyRegenerationInterval 3600                                    由前面联机的说明可以知道， version 1 会使用 server 的 Public Key ，每隔一段时间来重新建立一次！时间为秒！
ServerKeyBits 768                                               Server key 的长度！
SyslogFacility AUTH                                             当有人使用 SSH 登入系统的时候，SSH会记录信息
LogLevel INFO                                                   登录记录的等级---》全部
PermitRootLogin no                                              是否允许 root 登入！预设是允许的，但是建议设定成 no！
UserLogin no                                                    在 SSH 底下本来就不接受 login 这个程序的登入！
StrictModes yes                                                 当使用者的 host key 改变之后，Server 就不接受联机
RSAAuthentication yes                                           是否使用纯的 RSA 认证！？仅针对 version 1 ！
PubkeyAuthentication yes                                        是否允许 Public Key ？只有 version 2
AuthorizedKeysFile   .ssh/authorized_keys                       设定若要使用不需要密码登入的账号时，那么那个账号的存放档案所在档名！
RhostsAuthentication no                                         本机系统不使用 .rhosts ， .rhosts 不安全！
IgnoreRhosts yes                                                是否取消使用 ~/.ssh/.rhosts 来做为认证！
RhostsRSAAuthentication no                                      针对 version 1 ，使用 rhosts 档案在/etc/hosts.equiv配合 RSA 演算方式来进行认证！
HostbasedAuthentication no                                      这个项目与上面的项目类似，不过是给 version 2 使用的！
IgnoreUserKnownHosts no                                         是否忽略家目录内的 ~/.ssh/known_hosts 这个档案所记录的主机内容
PasswordAuthentication yes                                      密码验证当然是需要的！
PermitEmptyPasswords no                                         上面那一项如果设定为 yes 的话，这一项就最好设定为 no ，这个项目在是否允许以空的密码登入！
ChallengeResponseAuthentication yes                             挑战任何的密码认证！所以，任何 login.conf 规定的认证方式，均可适用！
PAMAuthenticationViaKbdInt yes                                  是否启用其它的 PAM 模块！启用这个模块将会导致 PasswordAuthentication 设定失效！

与Kerberos 有关的参数设定！底下不用设定
KerberosAuthentication no
KerberosOrLocalPasswd yes
KerberosTicketCleanup yes
KerberosTgtPassing no

有关在 X-Window 底下使用的相关设定
X11Forwarding yes
X11DisplayOffset 10
X11UseLocalhost yes

PrintMotd no                                                    登入后是否显示出一些信息呢？例如上次登入的时间、地点等，预设是 yes ，但是，如果为了安全，可以考虑改为 no ！
PrintLastLog yes                                                显示上次登入的信息！预设也是 yes 
KeepAlive yes                                                   一般而言，如果设定这项目的话，那么 SSH Server 会传送KeepAlive 的讯息给 Client 端，以确保两者的联机正常！在这个情况下，任何一端死掉后， SSH 可以立刻知道！而不会有僵尸程序的发生！
UsePrivilegeSeparation yes                                      使用者的权限设定项目！
MaxStartups 10                                                  同时允许几个尚未登入的联机画面
DenyUsers *                                                     设定受抵挡的使用者名称
AllowUsers *                                                    设定允许的使用者名称
#+END_EXAMPLE
** ssh 命令行配置项
ssh 命令有很多配置项，修改它的默认行为。
*** -c
-c参数指定加密算法。
#+begin_src bash
$ ssh -c blowfish,3des server.example.com
# 或者
$ ssh -c blowfish -c 3des server.example.com
#+END_SRC
上面命令指定使用加密算法blowfish或3des。

*** -C
-C参数表示压缩数据传输。
#+begin_src bash
$ ssh -C server.example.com
#+END_SRC

*** -D
-D参数指定本机的 Socks 监听端口，该端口收到的请求，都将转发到远程的 SSH 主机，又称动态端口转发，详见《端口转发》一章。
#+begin_src bash
$ ssh -D 1080 server
#+END_SRC
上面命令将本机 1080 端口收到的请求，都转发到服务器server。

*** -f
-f参数表示 SSH 连接在后台运行。

*** -F
-F参数指定配置文件。
#+begin_src bash
$ ssh -F /usr/local/ssh/other_config
#+END_SRC
上面命令指定使用配置文件other_config。

*** -i
-i参数用于指定私钥，意为“identity_file”，默认值为~/.ssh/id_dsa（DSA 算法）和~/.ssh/id_rsa（RSA 算法）。注意，对应的公钥必须存放到服务器，详见《密钥登录》一章。
#+begin_src bash
$ ssh -i my-key server.example.com
#+END_SRC
*** -l
-l参数指定远程登录的账户名。
#+begin_src bash
$ ssh -l sally server.example.com
# 等同于
$ ssh sally@server.example.com
#+END_SRC
*** -L
-L参数设置本地端口转发，详见《端口转发》一章。
#+begin_src bash
$ ssh  -L 9999:targetServer:80 user@remoteserver
#+END_SRC
上面命令中，所有发向本地9999端口的请求，都会经过remoteserver发往 targetServer 的 80 端口，这就相当于直接连上了 targetServer 的 80 端口。

*** -m
-m参数指定校验数据完整性的算法（message authentication code，简称 MAC）。
#+begin_src bash
$ ssh -m hmac-sha1,hmac-md5 server.example.com
#+END_SRC
上面命令指定数据校验算法为hmac-sha1或hmac-md5。

*** -N
-N参数用于端口转发，表示建立的 SSH 只用于端口转发，不能执行远程命令，这样可以提供安全性，详见《端口转发》一章。

*** -o
-o参数用来指定一个配置命令。
#+begin_src bash
$ ssh -o "Keyword Value"
#+END_SRC
举例来说，配置文件里面有如下内容。
#+BEGIN_EXAMPLE
User sally
Port 220
#+END_EXAMPLE
通过-o参数，可以把上面两个配置命令从命令行传入。
#+begin_src bash
$ ssh -o "User sally" -o "Port 220" server.example.com
#+END_SRC
使用等号时，配置命令可以不用写在引号里面，但是等号前后不能有空格。
#+begin_src bash
$ ssh -o User=sally -o Port=220 server.example.com
#+END_SRC

*** -p
-p参数指定 SSH 客户端连接的服务器端口。
#+begin_src bash
$ ssh -p 2035 server.example.com
#+END_SRC
上面命令连接服务器的2035端口。

*** -q

-q参数表示安静模式（quiet），不向用户输出任何警告信息。
#+begin_src bash
$ ssh –q foo.com
root’s password:
#+END_SRC
上面命令使用-q参数，只输出要求用户输入密码的提示。

*** -R

-R参数指定远程端口转发，详见《端口转发》一章。

$ ssh -R 9999:targetServer:902 local

上面命令需在跳板服务器执行，指定本地计算机local监听自己的 9999 端口，所有发向这个端口的请求，都会转向 targetServer 的 902 端口。

*** -t

-t参数在 ssh 直接运行远端命令时，提供一个互动式 Shell。

$ ssh -t server.example.com emacs

*** -v

-v参数显示详细信息。

$ ssh -v server.example.com

-v可以重复多次，表示信息的详细程度，比如-vv和-vvv。

$ ssh -vvv server.example.com
# 或者
$ ssh -v -v -v server.example.com
上面命令会输出最详细的连接信息。

*** -V

-V参数输出 ssh 客户端的版本。

$ ssh –V
ssh: SSH Secure Shell 3.2.3 (non-commercial version) on i686-pc-linux-gnu

*** -X

-X参数表示打开 X 窗口转发。

$ ssh -X server.example.com

*** -1，-2

-1参数指定使用 SSH 1 协议。

-2参数指定使用 SSH 2 协议。

$ ssh -2 server.example.com

*** -4，-6

-4指定使用 IPv4 协议，这是默认值。

$ ssh -4 server.example.com
-6指定使用 IPv6 协议。

$ ssh -6 server.example.com
** ssh 服务端配置
*** sshd简介
SSH 的架构是服务器/客户端模式，两端运行的软件是不一样的。OpenSSH 的客户端软件是 ssh，服务器软件是 sshd。本章介绍 sshd 的各种知识。

如果没有安装 sshd，可以用下面的命令安装。

#+begin_src bash
# Debian
$ sudo aptitude install openssh-server

# Red Hat
$ sudo yum install openssh-server
#+END_SRC

一般来说，sshd 安装后会跟着系统一起启动。如果当前 sshd 没有启动，可以用下面的命令启动。

#+begin_src bash
$ sshd
#+END_SRC

上面的命令运行后，如果提示“sshd re-exec requires execution with an absolute path”，就需要使用绝对路径来启动。这是为了防止有人出于各种目的，放置同名软件在$PATH变量指向的目录中，代替真正的 sshd。

#+begin_src bash
# Centos、Ubuntu、OS X
$ /usr/sbin/sshd
#+END_SRC

上面的命令运行以后，sshd 自动进入后台，所以命令后面不需要加上&。

除了直接运行可执行文件，也可以通过 Systemd 启动 sshd。

#+begin_src bash
# 启动
$ sudo systemctl start sshd.service

# 停止
$ sudo systemctl stop sshd.service

# 重启
$ sudo systemctl restart sshd.service
#+END_SRC

下面的命令让 sshd 在计算机下次启动时自动运行。

#+begin_src bash
$ sudo systemctl enable sshd.service
#+END_SRC

*** sshd 配置文件

sshd 的配置文件在/etc/ssh目录，主配置文件是sshd_config，此外还有一些安装时生成的密钥。

- /etc/ssh/sshd_config：配置文件
- /etc/ssh/ssh_host_ecdsa_key：ECDSA 私钥。
- /etc/ssh/ssh_host_ecdsa_key.pub：ECDSA 公钥。
- /etc/ssh/ssh_host_key：用于 SSH 1 协议版本的 RSA 私钥。
- /etc/ssh/ssh_host_key.pub：用于 SSH 1 协议版本的 RSA 公钥。
- /etc/ssh/ssh_host_rsa_key：用于 SSH 2 协议版本的 RSA 私钥。
- /etc/ssh/ssh_host_rsa_key.pub：用于 SSH 2 协议版本的 RSA 公钥。
- /etc/pam.d/sshd：PAM 配置文件。

注意，如果重装 sshd，上面这些密钥都会重新生成，导致客户端重新连接 ssh 服务器时，会跳出警告，拒绝连接。为了避免这种情况，可以在重装 sshd 时，先备份/etc/ssh目录，重装后再恢复这个目录。

配置文件sshd_config的格式是，每个命令占据一行。每行都是配置项和对应的值，配置项的大小写不敏感，与值之间使用空格分隔。

#+begin_src bash
Port 2034
#+END_SRC

上面的配置命令指定，配置项Port的值是2034。Port写成port也可。

配置文件还有另一种格式，就是配置项与值之间有一个等号，等号前后的空格可选。

#+begin_src bash
#Port指定 sshd 监听的端口，即客户端连接的端口，默认是22（Port 22）。出于安全考虑，可以改掉这个端口（比如Port 8822）。
Port = 2034
#+END_SRC

配置文件里面，#开头的行表示注释。

注意，注释只能放在一行的开头，不能放在一行的结尾。

#+begin_src bash
Port 2034 # 此处不允许注释
#+END_SRC

上面的写法是错误的。

另外，空行等同于注释。

sshd 启动时会自动读取默认的配置文件。如果希望使用其他的配置文件，可以用 sshd 命令的-f参数指定。

#+begin_src bash
$ sshd -f /usr/local/ssh/my_config
#+END_SRC

上面的命令指定 sshd 使用另一个配置文件my_config。

修改配置文件以后，可以用 sshd 命令的-t（test）检查有没有语法错误。

#+begin_src bash
$ sshd -t
#+END_SRC

配置文件修改以后，并不会自动生效，必须重新启动 sshd。

#+begin_src bash
$ sudo systemctl restart sshd.service
#+END_SRC

*** sshd 密钥

sshd 有自己的一对或多对密钥。它使用密钥向客户端证明自己的身份。所有密钥都是公钥和私钥成对出现，公钥的文件名一般是私钥文件名加上后缀.pub。

DSA 格式的密钥文件默认为/etc/ssh/ssh_host_dsa_key（公钥为ssh_host_dsa_key.pub），RSA 格式的密钥为/etc/ssh/ssh_host_rsa_key（公钥为ssh_host_rsa_key.pub）。如果需要支持 SSH 1 协议，则必须有密钥/etc/ssh/ssh_host_key。

如果密钥不是默认文件，那么可以通过配置文件sshd_config的HostKey配置项指定。默认密钥的HostKey设置如下。

#+begin_src bash
# HostKey for protocol version 1
# HostKey /etc/ssh/ssh_host_key

# HostKeys for protocol version 2
# HostKey /etc/ssh/ssh_host_rsa_key
# HostKey /etc/ssh/ssh_host_dsa_ke
#+END_SRC

上面命令前面的#表示这些行都是注释，因为这是默认值，有没有这几行都一样。

如果要修改密钥，就要去掉行首的#，指定其他密钥。

#+begin_src bash
HostKey /usr/local/ssh/my_dsa_key
HostKey /usr/local/ssh/my_rsa_key
HostKey /usr/local/ssh/my_old_ssh1_key
#+END_SRC

*** sshd 配置项

以下是/etc/ssh/sshd_config文件里面的配置项。

**AcceptEnv**

AcceptEnv指定允许接受客户端通过SendEnv命令发来的哪些环境变量，即允许客户端设置服务器的环境变量清单，变量名之间使用空格分隔（AcceptEnv PATH TERM）。

**AllowGroups**

AllowGroups指定允许登录的用户组（AllowGroups groupName，多个组之间用空格分隔。如果不使用该项，则允许所有用户组登录。

**AllowUsers**

AllowUsers指定允许登录的用户，用户名之间使用空格分隔（AllowUsers user1 user2），也可以使用多行AllowUsers命令指定，用户名支持使用通配符。如果不使用该项，则允许所有用户登录。该项也可以使用用户名@域名的格式（比如AllowUsers jones@example.com）。

**AllowTcpForwarding**

AllowTcpForwarding指定是否允许端口转发，默认值为yes（AllowTcpForwarding yes），local表示只允许本地端口转发，remote表示只允许远程端口转发。

**AuthorizedKeysFile**

AuthorizedKeysFile指定储存用户公钥的目录，默认是用户主目录的ssh/authorized_keys目录（AuthorizedKeysFile .ssh/authorized_keys）。

**Banner**

Banner指定用户登录后，sshd 向其展示的信息文件（Banner /usr/local/etc/warning.txt），默认不展示任何内容。

**ChallengeResponseAuthentication**

ChallengeResponseAuthentication指定是否使用“键盘交互”身份验证方案，默认值为yes（ChallengeResponseAuthentication yes）。

从理论上讲，“键盘交互”身份验证方案可以向用户询问多重问题，但是实践中，通常仅询问用户密码。如果要完全禁用基于密码的身份验证，请将PasswordAuthentication和ChallengeResponseAuthentication都设置为no。

**Ciphers**

Ciphers指定 sshd 可以接受的加密算法（Ciphers 3des-cbc），多个算法之间使用逗号分隔。

**ClientAliveCountMax**

ClientAliveCountMax指定建立连接后，客户端失去响应时，服务器尝试连接的次数（ClientAliveCountMax 8）。

**ClientAliveInterval**

ClientAliveInterval指定允许客户端发呆的时间，单位为秒（ClientAliveInterval 180）。如果这段时间里面，客户端没有发送任何信号，SSH 连接将关闭。

**Compression**

Compression指定客户端与服务器之间的数据传输是否压缩。默认值为yes（Compression yes）

**DenyGroups**

DenyGroups指定不允许登录的用户组（DenyGroups groupName）。

**DenyUsers**

DenyUsers指定不允许登录的用户（DenyUsers user1），用户名之间使用空格分隔，也可以使用多行DenyUsers命令指定。

**FascistLogging**

SSH 1 版本专用，指定日志输出全部 Debug 信息（FascistLogging yes）。

**HostKey**

HostKey指定 sshd 服务器的密钥，详见前文。

**KeyRegenerationInterval**

KeyRegenerationInterval指定 SSH 1 版本的密钥重新生成时间间隔，单位为秒，默认是3600秒（KeyRegenerationInterval 3600）。

**ListenAddress**

ListenAddress指定 sshd 监听的本机 IP 地址，即 sshd 启用的 IP 地址，默认是 0.0.0.0（ListenAddress 0.0.0.0）表示在本机所有网络接口启用。可以改成只在某个网络接口启用（比如ListenAddress 192.168.10.23），也可以指定某个域名启用（比如ListenAddress server.example.com）。

如果要监听多个指定的 IP 地址，可以使用多行ListenAddress命令。

#+begin_src bash
ListenAddress 172.16.1.1
ListenAddress 192.168.0.1
#+END_SRC

**LoginGraceTime**

LoginGraceTime指定允许客户端登录时发呆的最长时间，比如用户迟迟不输入密码，连接就会自动断开，单位为秒（LoginGraceTime 60）。如果设为0，就表示没有限制。

**LogLevel**

LogLevel指定日志的详细程度，可能的值依次为QUIET、FATAL、ERROR、INFO、VERBOSE、DEBUG、DEBUG1、DEBUG2、DEBUG3，默认为INFO（LogLevel INFO）。

**MACs**

MACs指定sshd 可以接受的数据校验算法（MACs hmac-sha1），多个算法之间使用逗号分隔。

**MaxAuthTries**

MaxAuthTries指定允许 SSH 登录的最大尝试次数（MaxAuthTries 3），如果密码输入错误达到指定次数，SSH 连接将关闭。

**MaxStartups**

MaxStartups指定允许同时并发的 SSH 连接数量（MaxStartups）。如果设为0，就表示没有限制。

这个属性也可以设为A:B:C的形式，比如MaxStartups 10:50:20，表示如果达到10个并发连接，后面的连接将有50%的概率被拒绝；如果达到20个并发连接，则后面的连接将100%被拒绝。

**PasswordAuthentication**

PasswordAuthentication指定是否允许密码登录，默认值为yes（PasswordAuthentication yes），建议改成no（禁止密码登录，只允许密钥登录）。

**PermitEmptyPasswords**

PermitEmptyPasswords指定是否允许空密码登录，即用户的密码是否可以为空，默认为yes（PermitEmptyPasswords yes），建议改成no（禁止无密码登录）。

**PermitRootLogin**

PermitRootLogin指定是否允许根用户登录，默认为yes（PermitRootLogin yes），建议改成no（禁止根用户登录）。

还有一种写法是写成prohibit-password，表示 root 用户不能用密码登录，但是可以用密钥登录。

#+begin_src bash
PermitRootLogin prohibit-password
#+END_SRC

**PermitUserEnvironment**

PermitUserEnvironment指定是否允许 sshd 加载客户端的~/.ssh/environment文件和~/.ssh/authorized_keys文件里面的environment= options环境变量设置。默认值为no（PermitUserEnvironment no）。

**Port**

Port指定 sshd 监听的端口，即客户端连接的端口，默认是22（Port 22）。出于安全考虑，可以改掉这个端口（比如Port 8822）。

配置文件可以使用多个Port命令，同时监听多个端口。

#+begin_src bash
Port 22
Port 80
Port 443
Port 8080
#+END_SRC

上面的示例表示同时监听4个端口。

**PrintMotd**

PrintMotd指定用户登录后，是否向其展示系统的 motd（Message of the day）的信息文件/etc/motd。该文件用于通知所有用户一些重要事项，比如系统维护时间、安全问题等等。默认值为yes（PrintMotd yes），由于 Shell 一般会展示这个信息文件，所以这里可以改为no。

**PrintLastLog**

PrintLastLog指定是否打印上一次用户登录时间，默认值为yes（PrintLastLog yes）。

**Protocol**

Protocol指定 sshd 使用的协议。Protocol 1表示使用 SSH 1 协议，建议改成Protocol 2（使用 SSH 2 协议）。Protocol 2,1表示同时支持两个版本的协议。

**PubKeyAuthentication**

PubKeyAuthentication指定是否允许公钥登录，默认值为yes（PubKeyAuthentication yes）。

**QuietMode**

SSH 1 版本专用，指定日志只输出致命的错误信息（QuietMode yes）。

**RSAAuthentication**

RSAAuthentication指定允许 RSA 认证，默认值为yes（RSAAuthentication yes）。

**ServerKeyBits**

ServerKeyBits指定 SSH 1 版本的密钥重新生成时的位数，默认是768（ServerKeyBits 768）。

**StrictModes**

StrictModes指定 sshd 是否检查用户的一些重要文件和目录的权限。默认为yes（StrictModes yes），即对于用户的 SSH 配置文件、密钥文件和所在目录，SSH 要求拥有者必须是根用户或用户本人，用户组和其他人的写权限必须关闭。

**SyslogFacility**

SyslogFacility指定 Syslog 如何处理 sshd 的日志，默认是 Auth（SyslogFacility AUTH）。

**TCPKeepAlive**

TCPKeepAlive指定打开 sshd 跟客户端 TCP 连接的 keepalive 参数（TCPKeepAlive yes）。

**UseDNS**

UseDNS指定用户 SSH 登录一个域名时，服务器是否使用 DNS，确认该域名对应的 IP 地址包含本机（UseDNS yes）。打开该选项意义不大，而且如果 DNS 更新不及时，还有可能误判，建议关闭。

**UseLogin**

UseLogin指定用户认证内部是否使用/usr/bin/login替代 SSH 工具，默认为no（UseLogin no）。

**UserPrivilegeSeparation**

UserPrivilegeSeparation指定用户认证通过以后，使用另一个子线程处理用户权限相关的操作，这样有利于提高安全性。默认值为yes（UsePrivilegeSeparation yes）。

**VerboseMode**

SSH 2 版本专用，指定日志输出详细的 Debug 信息（VerboseMode yes）。

**X11Forwarding**

X11Forwarding指定是否打开 X window 的转发，默认值为 no（X11Forwarding no）。

修改配置文件以后，可以使用下面的命令验证，配置文件是否有语法错误。

#+begin_src bash
$ sshd -t
#+END_SRC

新的配置文件生效，必须重启 sshd。

#+begin_src bash
$ sudo systemctl restart sshd
#+END_SRC

*** sshd 的命令行配置项

sshd 命令有一些配置项。这些配置项在调用时指定，可以覆盖配置文件的设置。

-d

-d参数用于显示 debug 信息。

#+begin_src bash
$ sshd -d
#+END_SRC

-D

-D参数指定 sshd 不作为后台守护进程运行。

#+begin_src bash
$ sshd -D
#+END_SRC

-e

-e参数将 sshd 写入系统日志 syslog 的内容导向标准错误（standard error）。

-f

-f参数指定配置文件的位置。

-h

-h参数用于指定密钥。

#+begin_src bash
$ sshd -h /usr/local/ssh/my_rsa_key
#+END_SRC

-o

-o参数指定配置文件的一个配置项和对应的值。

#+begin_src bash
$ sshd -o "Port 2034"
#+END_SRC

配置项和对应值之间，可以使用等号。

#+begin_src bash
$ sshd -o "Port = 2034"
#+END_SRC

如果省略等号前后的空格，也可以不使用引号。

#+begin_src bash
$ sshd -o Port=2034
#+END_SRC

-o参数可以多个一起使用，用来指定多个配置关键字。

-p

-p参数指定 sshd 的服务端口。

#+begin_src bash
$ sshd -p 2034
#+END_SRC

上面命令指定 sshd 在2034端口启动。

-p参数可以指定多个端口。

#+begin_src bash
$ sshd -p 2222 -p 3333
#+END_SRC

-t

-t参数检查配置文件的语法是否正确。
** SSH 端口转发
SSH 端口转发功能能够将其他 TCP 端口的网络数据通过 SSH 链接来转发，并且自动提供了相应的加密及解密服务。其实这一技术就是我们常常听说的隧道(tunnel)技术，原因是 SSH 为其他 TCP 链接提供了一个安全的通道来进行传输。

SSH端口转发也被称作SSH隧道(SSH Tunnel)，因为它们都是通过SSH登陆之后，在SSH客户端与SSH服务端之间建立了一个隧道，从而进行通信。

我们知道，FTP 协议是以明文来传递数据的。但是我们可以让 FTP 客户端和服务器通过 SSH 隧道传输数据，从而实现安全的 FTP 数据传输。

更常见的情况是我们的应用经常被各种防火墙限制。常见的有禁止访问某些网站、禁用某类软件，同时你的所有网络行为都被监控并分析！同样的通过 SSH 隧道技术我们完全可以规避这些限制。

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-19_15-42-53.png @ 2021-10-19 15:43:34
[[file:ssh_%E7%9B%B8%E5%85%B3/2021-10-19_15-43-34_Snipaste_2021-10-19_15-42-53.png]]
如上图所示，通过 SSH 的端口转发， 应用程序的客户端和应用程序的服务器端不再直接通讯，而是转发到了 SSH 客户端及 SSH 服务端来通讯。这样就可以同时实现两个目的：数据的加密传输和穿透防火墙！
在具体的使用场景中，端口转发又被细分为本地端口转发、远程端口转发、动态端口转发等。
*** 本地端口转发
[[https://unix.stackexchange.com/questions/115897/whats-ssh-port-forwarding-and-whats-the-difference-between-ssh-local-and-remot][StackOverflow 文章示意图]]：

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-19_16-38-33.png @ 2021-10-19 16:38:37
[[file:ssh_%E7%9B%B8%E5%85%B3/2021-10-19_16-38-37_Snipaste_2021-10-19_16-38-33.png]]

顾名思义，本地端口转发是将应用【application client】对于本地主机A指定端口X的访问请求转发给主机B，交由主机B对另一指定主机C的指定端口Z发起访问。

命令如下：
#+begin_src bash
ssh -L 主机A端口X:主机C:主机C端口Z username@hostname
# 简单理解为：将对A:X的访问转变成对C:Z的访问
#+END_SRC
客户端在执行端口转发命令的同时，实际上也执行了基本的连接命令。多出来的部分中，「-L」旗标表示使用「本地端口转发」选项，之后是用冒号分隔开的三个需要指定的项。原理上，主机C可以是任何能够被主机B识别到的设备，也可以是主机B自身。

当主机C在其某端口提供某服务【application server】，主机A需要使用该服务却无法直接访问主机C或该端口时，如果发现有SSH：A→B的连接，且主机B能够直接访问主机C的该端口，本地端口转发就派上用场。

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-19_15-48-49.png @ 2021-10-19 15:48:54
[[file:ssh_%E7%9B%B8%E5%85%B3/2021-10-19_15-48-54_Snipaste_2021-10-19_15-48-49.png]]

此时，访问请求在主机A一侧发生，可以来自于主机A自身，也可以是其他与A连接的设备。图中Host A或Host B的阴影指代主机A或主机B一侧的网络系统。

实际上ssh本地端口转发命令的「-L」旗标后可以填写四个参数，完整格式为：
#+begin_src bash
ssh -L [收听接口:]收听端口:目标主机:目标端口 username@hostname
#+END_SRC
username@hostname 是 SSH 服务器所在的主机

命令中方括号内的部分，即第一个参数可以不写；它的默认值一般是0.0.0.0，意味着SSH隧道会收听所有接口，接受来自任何地址的应用访问请求并进行转发。而如果在此处填写了绑定地址（bind address），SSH隧道连接就会只处理来自绑定地址的应用请求，而对其他地址发来的请求置之不理；如同在（真实世界的）隧道入口设立哨卡，只对白名单牌号的车辆放行。例如在此处填写127.0.0.1，即可实现只有来自主机A本机的应用请求才被SSH隧道转发的效果。

需留意，收听接口是站在主机A的视角上去规定允许与A连接的设备，解决「能够使用SSH端口转发的应用请求从何处来」的问题，类似防火墙的入站；收听端口则依旧是主机A上的那个端口X，不能够跑到别的主机上去。

类似地，远程端口转发和动态端口转发也具有「收听接口」这一可不指明的参数，下文不再赘述。从安全或控制流量的角度，规定绑定地址是一项实用的功能。
**** 实例场景
***** 场景1 
主机B与主机C处于同一内网中，主机B能够与外界联系而主机C不能。这时不处于内网中的主机A如果想要访问主机C，就可以通过SSH连接主机B＋端口转发来进行。

台式机B上运行着虚拟机C，虚拟机使用虚拟机软件搭建的虚拟网络与宿主主机B相连接，但在主机B以外无法直接访问该虚拟网络。想要通过SSH，用与台式机B处于同一WiFi下的笔记本A来远程控制虚拟机C，（在A上）执行端口转发命令：
#+begin_src bash
ssh -L 22022:10.0.2.15:22 desktop_user@192.168.1.11	# cmd.1-1
#+END_SRC
其中，22022号端口是随便选的一个没被占用的端口；192.168.1.11是台式机B在WiFi中的IP；desktop_user是主机B上的用户名；10.0.2.15是虚拟机C在主机B为其搭建的虚拟网络中的IP；22号端口是默认的SSH端口。已知virtual_user是虚拟机C上的用户名，这时在笔记本A上执行应用的访问请求命令：
#+begin_src bash
ssh -p 22022 virtual_user@localhost	# cmd.1-2
#+END_SRC
我们在笔记本A上以SSH协议访问本机（localhost）的22022号端口，这个请求就像通过了隧道（SSH隧道）一样抵达台式机B，台式机B则把这个请求变为对虚拟机C的22号端口的访问，并为A返回结果。其中，使用「-p」旗标是为了访问主机A的特定端口而不是SSH默认的22号端口；由于我们在主机A上执行命令，A管自己叫localhost，假如在其他主机上执行则需相应地改为主机A的域名或IP等他们对A的称呼。

cmd.1-2中我们是将SSH当作普通应用使用的。参考Fig.1，cmd.1-1在A与B之间建立SSH隧道，此时A上的SSH客户端和B上的SSH服务器对应图中的SSH Client和SSH Server；cmd.1-2则表达应用的访问请求，此时A上的SSH客户端和C上的SSH服务器对应图中的application client和application server。

以上cmd.1-1和cmd.1-2合起来实际是想（在A上）进行：
#+begin_src bash
ssh -p 22 virtual_user@10.0.2.15	# cmd.1-3
#+END_SRC
当然，如果这cmd.1-3能被成功执行的话，就不需要端口转发了。
***** 场景2
防火墙阻止了主机A对主机B一些端口的连接，但主机B仍有部分端口是对主机A开放的。这时主机A如果需要访问主机B上被防火墙阻挡的端口，就可以通过SSH连接主机B＋端口转发来进行。需注意，这时所谓的主机C就是主机B。
某某云的云服务器B默认的防火墙设置仅开放了22号端口，其他入方向的访问都被屏蔽了。我们为云服务器B安装了桌面环境，现在想要在自己的计算机A上，通过VNC远程控制云服务器B的桌面。（在A上）执行端口转发命令：

ssh -L 5920:localhost:5901 cloud_user@server.example.com	# cmd.2-1
因为C就是B自己，所以C的位置填localhost；5920随便选；5901是云服务器B上VNC服务进程收听的端口；cloud_user是B上的用户名；http://server.example.com 是B的域名，换成公网IP也行。

下面在计算机A上打开RealVNC VNC Viewer（VNC客户端），输入VNC服务器地址：
#+begin_src bash
localhost:20
#+END_SRC
20=5920−5900，这是采用5901到5999之间端口时RealVNC的特殊设定。开始使用优雅（或许吧）的GUI来操作云服务器吧！

*** 远程端口转发
#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-19_16-44-15.png @ 2021-10-19 16:44:19
[[file:ssh_%E7%9B%B8%E5%85%B3/2021-10-19_16-44-19_Snipaste_2021-10-19_16-44-15.png]]
当主机C在其某端口提供某服务，主机B需要使用该服务却无法直接访问主机C或该端口时，如果发现有SSH：A→B的连接，且主机A能够直接访问主机C的该端口，远程端口转发就派上用场。

远程端口转发的结构如下图所示：

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-19_16-45-42.png @ 2021-10-19 16:45:46
[[file:ssh_%E7%9B%B8%E5%85%B3/2021-10-19_16-45-46_Snipaste_2021-10-19_16-45-42.png]]

需注意，此时访问请求在主机B一侧发生，而SSH连接的方向却没有变化，仍是由A到B的。因此「本地与远程端口转发互为镜像」的说法并不完全准确；严格意义上的镜像，SSH连接也要变为由B到A，那时则应该是在B上采用本地端口转发。可以看出，采取哪种端口转发主要取决于SSH连接建立的方向。

与本地端口转发的流动方向相反，远程端口转发是将对于远程主机B指定端口Y的访问请求转发给主机A，交由主机A对另一指定主机C的指定端口Z发起访问。命令如下：
#+begin_src bash
ssh -R 主机B端口Y:主机C:主机C端口Z username@hostname
# 简单理解为：将对B:Y的访问转变成对C:Z的访问
#+END_SRC
username@hostname不变，因为我们仍然以从主机A对主机B发起SSH连接为基础；「-R」旗标表示使用「远程端口转发」选项，之后是用冒号分隔开的三个需要指定的项。原理上，主机C可以是任何能够被主机A识别到的设备，也可以是主机A自身。

**** 示例场景

***** 场景1
主机A与主机C处于同一内网中，主机A能够与外界联系而主机C不能。这时（在主机A上）如果想让不处于内网中的主机B访问主机C，就可以通过SSH连接主机B＋端口转发来进行。
台式机A上运行着虚拟机C，虚拟机使用虚拟机软件搭建的虚拟网络与宿主主机A相连接，但在主机A以外无法直接访问该虚拟网络。想要通过SFTP，用与台式机A处于同一WiFi下的笔记本B来向虚拟机C传输文件，（在A上）执行端口转发命令：
#+begin_src bash
ssh -R 22122:10.0.2.16:22 laptop_user@192.168.1.233	# cmd.3-1
#+END_SRC
其中，22122号端口是随便选的一个没被占用的端口；192.168.1.233是笔记本B在WiFi中的IP；laptop_user是主机B上的用户名；10.0.2.16是虚拟机C在主机A为其搭建的虚拟网络中的IP；22号端口是默认的SFTP端口。已知virtual_user是虚拟机C上的用户名，这时在笔记本B上执行应用的访问请求命令：
#+begin_src bash
sftp -P 22122 virtual_user@localhost	# cmd.3-2
#+END_SRC
请注意这是一条运行在B上的应用命令；B上的SFTP客户端这时充当Fig.2中的application client。此处localhost是主机B对自己的称呼。对B的22122号端口的访问被转发至A，A访问C，即10.0.2.16的22号端口并将结果返回给B。于是B就通过远程端口转发成功访问了C上的SFTP服务器。

以上cmd.3-1和cmd.3-2合起来实际是想（在B上）进行：
#+begin_src bash
sftp -P 22 virtual_user@10.0.2.15	# cmd.3-3
#+END_SRC
当然，这cmd.3-3也是不能被直接成功执行的。

***** 场景2
处于内网之中的主机A可以访问公网，但不具有公网IP；公网中的主机B无法找到A，但为A开放各个端口的访问（A可以直接连接B，反之则不行）。这时A想要让B访问自己，就可以通过SSH连接主机B＋端口转发来进行。需注意，这时所谓的主机C就是主机A。

注意：OpenSSH服务器对于远程端口转发的设定，默认只接受远程主机B本机上的应用发起的请求。想要从其他连接到B的设备发起请求，需将「sshd_config」文件中「GatewayPorts」选项后的「no」修改为「yes」。

手头上计算机A运行着http服务，但A没有公网IP，其他设备不能使用该服务。恰好云服务器B有公网IP（甚至域名），便于被访问。在不将http服务迁移至云服务器B的前提下，可以使用SSH端口转发使其他设备通过访问B的方式访问A上的http服务。（在A上）执行端口转发命令：
#+begin_src bash
ssh -R 80:localhost:80 cloud_user@server.example.com	# cmd.4-1
#+END_SRC

这时C便是A自己（localhost）；80号端口是http默认端口，为简便两个都用默认；cloud_user还是B上的用户名；http://server.example.com 还是B的域名。

接下来在其他设备上打开浏览器，输入地址：
#+BEGIN_EXAMPLE
http://server.example.com/
#+END_EXAMPLE
于是大家可以通过访问 http://server.example.com 来访问本地计算机A提供的http服务了。

*** 动态端口转发
动态转发指的是，本机与 SSH 服务器之间创建了一个加密连接，然后本机内部针对某个端口的通信，都通过这个加密连接转发。它的一个使用场景就是，访问所有外部网站，都通过 SSH 转发。

动态转发需要把本地端口绑定到 SSH 服务器。至于 SSH 服务器要去访问哪一个网站，完全是动态的，取决于原始通信，所以叫做动态转发。

#+begin_src bash
ssh -D local-port username@hostname -N
#+END_SRC
上面命令中，-D表示动态转发，local-port是本地端口，username@hostname是 SSH 服务器，-N表示这个 SSH 连接只进行端口转发，不登录远程 Shell，不能执行远程命令，只能充当隧道。

举例来说，如果本地端口是2121，那么动态转发的命令就是下面这样。
#+begin_src bash
$ ssh -D 2121 tunnel-host -N
#+END_SRC
注意，这种转发采用了 SOCKS5 协议。访问外部网站时，需要把 HTTP 请求转成 SOCKS5 协议，才能把本地端口的请求转发出去。

下面是 SSH 隧道建立后的一个使用实例。
#+begin_src bash
$ curl -x socks5://localhost:2121 http://www.example.com
#+END_SRC
上面命令中，curl 的-x参数指定代理服务器，即通过 SOCKS5 协议的本地2121端口，访问http://www.example.com。

如果经常使用动态转发，可以将设置写入 SSH 客户端的用户个人配置文件（~/.ssh/config）。
#+BEGIN_EXAMPLE
DynamicForward tunnel-host:local-port
#+END_EXAMPLE

动态端口转发可以把本地主机A上运行的SSH客户端转变成一个SOCKS代理服务器；实际上它是一种特殊的本地端口转发，或者说叫它「动态本地端口转发」更科学。这个动态，就动在这种转发不规定目标地址（主机C）和目标端口（端口Z），而是去读取应用发起的请求，从请求中获取目标信息。

这里有一个问题：之前使用固定的端口转发时，应用的访问请求都是指向被转发的那个端口X的，但现在应用的访问请求必须指向目标，以指定动态端口转发的目标。可如果不指向端口X，如何让数据走SSH隧道呢？这就要求我们在系统或应用（浏览器等）中设置一个使用SOCKS5协议、服务器为localhost、端口为X的代理，利用代理使请求走端口X。

这样应用的请求就从X进入隧道，抵达B后其中的目标信息被解析出来，B访问目标后再将结果通过隧道返回给A。比如在开启代理的A上的浏览器中访问http://zhihu.com ，经过端口转发，相当于是B在帮A访问 http://zhihu.com。 

**** 端口转发的停止
SSH端口转发完全基于基本的SSH连接，因此，通过在远程终端上执行exit命令、暴力关闭本地终端窗口、远程主机B关机、本地主机A关机等可以切断SSH连接的方式，即可停止SSH端口转发。就是这样。
*** 参考文章
[[https://wangdoc.com/ssh/port-forwarding.html][SSH 端口转发]]
[[https://solitum.net/posts/an-illustrated-guide-to-ssh-tunnels/][An Illustrated Guide to SSH Tunnels]]
** 开机自启动ssh
这个方法对docker容器无效

设置开机自启动
#+begin_src bash
sudo systemctl enable ssh
#+END_SRC
关闭ssh开机自动启动命令
#+begin_src bash
sudo systemctl disable ssh
#+END_SRC

** 关于公钥指纹
如果是第一次登录对方主机，系统会出现如下图的提示
#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-26_16-49-04.png @ 2021-11-26 16:49:17
[[file:ssh_%E7%9B%B8%E5%85%B3/2021-11-26_16-49-17_Snipaste_2021-11-26_16-49-04.png]]

这段话的意思是，无法确认host主机的真实性，只知道它的公钥指纹，问你还想继续连接吗？

在上面这段文字后面，输入yes，就可以将当前服务器的指纹也储存在本机~/.ssh/known_hosts文件中，并显示下面的提示。以后再连接的时候，就不会再出现警告了。

#+BEGIN_EXAMPLE
Warning: Permanently added 'foo.com (192.168.121.111)' (RSA) to the list of known hosts
#+END_EXAMPLE

所谓"公钥指纹"，是指公钥长度较长（这里采用RSA算法，长达1024位），很难比对，所以对其进行MD5计算，将它变成一个128位的指纹。上例中是20:42:b3:d6:79:dc:79:ec:26:1a:54:8c:72:b7:a7:e3，再进行比较，就容易多了。

很自然的一个问题就是，用户怎么知道远程主机的公钥指纹应该是多少？回答是没有好办法，远程主机必须在自己的网站上贴出公钥指纹，以便用户自行核对。

下面的命令可以查看某个公钥的指纹:
#+begin_src bash
$ ssh-keygen -l -f /etc/ssh/ssh_host_ecdsa_key.pub
256 da:24:43:0b:2e:c1:3f:a1:84:13:92:01:52:b4:84:ff   (ECDSA)
#+END_SRC
上面的例子中，ssh-keygen -l -f命令会输出公钥/etc/ssh/ssh_host_ecdsa_key.pub的指纹。

ssh 会将本机连接过的所有服务器公钥的指纹，都储存在本机的~/.ssh/known_hosts文件中。每次连接服务器时，通过该文件判断是否为陌生主机（陌生公钥）。

服务器指纹可以防止有人恶意冒充远程主机。如果服务器的密钥发生变更（比如重装了 SSH 服务器），客户端再次连接时，就会发生公钥指纹不吻合的情况。这时，客户端就会中断连接，并显示一段警告信息。
#+BEGIN_EXAMPLE
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that the RSA host key has just been changed.
The fingerprint for the RSA key sent by the remote host is
77:a5:69:81:9b:eb:40:76:7b:13:04:a9:6c:f4:9c:5d.
Please contact your system administrator.
Add correct host key in /home/me/.ssh/known_hosts to get rid of this message.
Offending key in /home/me/.ssh/known_hosts:36
#+END_EXAMPLE
上面这段文字的意思是，该主机的公钥指纹跟~/.ssh/known_hosts文件储存的不一样，必须处理以后才能连接。这时，你需要确认是什么原因，使得公钥指纹发生变更，到底是恶意劫持，还是管理员变更了 SSH 服务器公钥。

如果新的公钥确认可以信任，需要继续执行连接，你可以执行下面的命令，将原来的公钥指纹从~/.ssh/known_hosts文件删除。
#+begin_src bash
$ ssh-keygen -R hostname
#+END_SRC
上面命令中，hostname是发生公钥变更的主机名。

除了使用上面的命令，你也可以手工修改known_hosts文件，将公钥指纹删除。

删除了原来的公钥指纹以后，重新执行 ssh 命令连接远程服务器，将新的指纹加入known_hosts文件，就可以顺利连接了。
** ssh-keygen命令：生成密钥
密钥登录时，首先需要生成公钥和私钥。OpenSSH 提供了一个工具程序ssh-keygen命令，用来生成密钥。

ssh-keygen的命令行配置项，主要有下面这些。

*** -b

-b参数指定密钥的二进制位数。这个参数值越大，密钥就越不容易破解，但是加密解密的计算开销也会加大。

一般来说，-b至少应该是1024，更安全一些可以设为2048或者更高。

*** -C

-C参数可以为密钥文件指定新的注释，格式为username@host。

下面命令生成一个4096位 RSA 加密算法的密钥对，并且给出了用户名和主机名。

$ ssh-keygen -t rsa -b 4096 -C "your_email@domain.com"

*** -f

-f参数指定生成的私钥文件。

$ ssh-keygen -t dsa -f mykey
上面命令会在当前目录生成私钥文件mykey和公钥文件mykey.pub。

*** -F

-F参数检查某个主机名是否在known_hosts文件里面。

$ ssh-keygen -F example.com

*** -N

-N参数用于指定私钥的密码（passphrase）。

$ ssh-keygen -t dsa -N secretword

*** -p

-p参数用于重新指定私钥的密码（passphrase）。它与-N的不同之处在于，新密码不在命令中指定，而是执行后再输入。ssh 先要求输入旧密码，然后要求输入两遍新密码。

*** -R

-R参数将指定的主机公钥指纹移出known_hosts文件。

$ ssh-keygen -R example.com

*** -t

-t参数用于指定生成密钥的加密算法，一般为dsa或rsa
** ssh-copy-id 命令：自动上传公钥
OpenSSH 自带一个ssh-copy-id命令，可以自动将公钥拷贝到远程服务器的~/.ssh/authorized_keys文件。如果~/.ssh/authorized_keys文件不存在，ssh-copy-id命令会自动创建该文件。

用户在本地计算机执行下面的命令，就可以把本地的公钥拷贝到服务器。
#+begin_src bash
$ ssh-copy-id -i key_file user@host
#+END_SRC
上面命令中，-i参数用来指定公钥文件，user是所要登录的账户名，host是服务器地址。如果省略用户名，默认为当前的本机用户名。执行完该命令，公钥就会拷贝到服务器。

注意，公钥文件可以不指定路径和.pub后缀名，ssh-copy-id会自动在~/.ssh目录里面寻找。
#+begin_src bash
$ ssh-copy-id -i id_rsa user@host
#+END_SRC
上面命令中，公钥文件会自动匹配到~/.ssh/id_rsa.pub。

ssh-copy-id会采用密码登录，系统会提示输入远程服务器的密码。

注意，ssh-copy-id是直接将公钥添加到authorized_keys文件的末尾。如果authorized_keys文件的末尾不是一个换行符，会导致新的公钥添加到前一个公钥的末尾，两个公钥连在一起，使得它们都无法生效。所以，如果authorized_keys文件已经存在，使用ssh-copy-id命令之前，务必保证authorized_keys文件的末尾是换行符（假设该文件已经存在）。
** ssh-agent 命令和ssh-add 命令
*** 基本用法
私钥设置了密码以后，每次使用都必须输入密码，有时让人感觉非常麻烦。比如，连续使用scp命令远程拷贝文件时，每次都要求输入密码。

ssh-agent命令就是为了解决这个问题而设计的，它让用户在整个 Bash 对话（session）之中，只在第一次使用 SSH 命令时输入密码，然后将私钥保存在内存中，后面都不需要再输入私钥的密码了。

第一步，使用下面的命令新建一次命令行对话。
#+begin_src bash
$ ssh-agent bash
#+END_SRC
上面命令中，如果你使用的命令行环境不是 Bash，可以用其他的 Shell 命令代替。比如zsh和fish。

如果想在当前对话启用ssh-agent，可以使用下面的命令。
#+begin_src bash
$ eval `ssh-agent`
#+END_SRC
上面命令中，ssh-agent会先自动在后台运行，并将需要设置的环境变量输出在屏幕上，类似下面这样。
#+begin_src bash
$ ssh-agent
SSH_AUTH_SOCK=/tmp/ssh-barrett/ssh-22841-agent; export SSH_AUTH_SOCK;
SSH_AGENT_PID=22842; export SSH_AGENT_PID;
echo Agent pid 22842;
#+END_SRC
eval命令的作用，就是运行上面的ssh-agent命令的输出，设置环境变量。

第二步，在新建的 Shell 对话里面，使用ssh-add命令添加默认的私钥（比如~/.ssh/id_rsa，或~/.ssh/id_dsa，或~/.ssh/id_ecdsa，或~/.ssh/id_ed25519）。
#+begin_src bash
$ ssh-add
Enter passphrase for /home/you/.ssh/id_dsa: ********
Identity added: /home/you/.ssh/id_dsa (/home/you/.ssh/id_dsa)
#+END_SRC

上面例子中，添加私钥时，会要求输入密码。以后，在这个对话里面再使用密钥时，就不需要输入私钥的密码了，因为私钥已经加载到内存里面了。

如果添加的不是默认私钥，ssh-add命令需要显式指定私钥文件。
#+begin_src bash
$ ssh-add my-other-key-file
#+END_SRC
上面的命令中，my-other-key-file就是用户指定的私钥文件。

第三步，使用 ssh 命令正常登录远程服务器。
#+begin_src bash
$ ssh remoteHost
#+END_SRC
上面命令中，remoteHost是远程服务器的地址，ssh 使用的是默认的私钥。这时如果私钥设有密码，ssh 将不再询问密码，而是直接取出内存里面的私钥。

如果要使用其他私钥登录服务器，需要使用 ssh 命令的-i参数指定私钥文件。
#+begin_src bash
$ ssh –i OpenSSHPrivateKey remoteHost
#+END_SRC
最后，如果要退出ssh-agent，可以直接退出子 Shell（按下 Ctrl + d），也可以使用下面的命令。
#+begin_src bash
$ ssh-agent -k
#+END_SRC
*** ssh-add命令
ssh-add命令用来将私钥加入ssh-agent，它有如下的参数。

**** -d

-d参数从内存中删除指定的私钥。

$ ssh-add -d name-of-key-file

**** -D

-D参数从内存中删除所有已经添加的私钥。

$ ssh-add -D

**** -l

-l参数列出所有已经添加的私钥。

$ ssh-add -l
** 关闭ssh密码登录
为了安全性，启用密钥登录之后，最好关闭服务器的密码登录。

对于 OpenSSH，具体方法就是打开服务器 sshd 的配置文件/etc/ssh/sshd_config，将PasswordAuthentication这一项设为no。
#+BEGIN_EXAMPLE
PasswordAuthentication no
#+END_EXAMPLE

修改配置文件以后，不要忘了重新启动 sshd，否则不会生效。
** known_hosts文件
当远程主机的公钥被接受以后，它就会被保存在文件$HOME/.ssh/known_hosts之中。下次再连接这台主机，系统就会认出它的公钥已经保存在本地了，从而跳过警告部分，直接提示输入密码。

每个SSH用户都有自己的known_hosts文件，此外系统也有一个这样的文件，通常是/etc/ssh/ssh_known_hosts，保存一些对所有用户都可信赖的远程主机的公钥。

** authorized_keys文件
生成密钥以后，公钥必须上传到服务器，才能使用公钥登录。

OpenSSH 规定，用户公钥保存在服务器的~/.ssh/authorized_keys文件。你要以哪个用户的身份登录到服务器，密钥就必须保存在该用户主目录的~/.ssh/authorized_keys文件。只要把公钥添加到这个文件之中，就相当于公钥上传到服务器了。每个公钥占据一行。如果该文件不存在，可以手动创建。

用户可以手动编辑该文件，把公钥粘贴进去，也可以在本机计算机上，执行下面的命令。
#+begin_src bash
$ cat ~/.ssh/id_rsa.pub | ssh user@host "mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys"
#+END_SRC
上面示例中，user@host要替换成你所要登录的用户名和主机名。

注意，authorized_keys文件的权限要设为644，即只有文件所有者才能写。如果权限设置不对，SSH 服务器可能会拒绝读取该文件。
#+begin_src bash
$ chmod 644 ~/.ssh/authorized_keys
#+END_SRC
只要公钥上传到服务器，下次登录时，OpenSSH 就会自动采用密钥登录，不再提示输入密码。
#+begin_src bash
$ ssh -l username shell.isp.com
Enter passphrase for key '/home/you/.ssh/id_dsa': ************
Last login: Mon Mar 24 02:17:27 2014 from ex.ample.com
shell.isp.com>
#+END_SRC
上面例子中，SSH 客户端使用私钥之前，会要求用户输入密码（passphrase），用来解开私钥。
** 加密参数
SSH 连接的握手阶段，客户端必须跟服务端约定加密参数集（cipher suite）。

加密参数集包含了若干不同的加密参数，它们之间使用下划线连接在一起，下面是一个例子。
#+BEGIN_EXAMPLE
TLS_RSA_WITH_AES_128_CBC_SHA
#+END_EXAMPLE
它的含义如下。
#+BEGIN_EXAMPLE
TLS：加密通信协议
RSA：密钥交换算法
AES：加密算法
128：加密算法的强度
CBC：加密算法的模式
SHA：数字签名的 Hash 函数
#+END_EXAMPLE
下面是一个例子，客户端向服务器发出的握手信息。
#+BEGIN_EXAMPLE
Handshake protocol: ClientHello
    Version: TLS 1.2
    Random
        Client time: May 22, 2030 02:43:46 GMT
        Random bytes: b76b0e61829557eb4c611adfd2d36eb232dc1332fe29802e321ee871
    Session ID: (empty)
    Cipher Suites
        Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256”
        Suite: TLS_DHE_RSA_WITH_AES_128_GCM_SHA256
        Suite: TLS_RSA_WITH_AES_128_GCM_SHA256
        Suite: TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA
        Suite: TLS_DHE_RSA_WITH_AES_128_CBC_SHA
        Suite: TLS_RSA_WITH_AES_128_CBC_SHA
        Suite: TLS_RSA_WITH_3DES_EDE_CBC_SHA
        Suite: TLS_RSA_WITH_RC4_128_SHA
    Compression methods
        Method: null
    Extensions
        Extension: server_name
            Hostname: www.feistyduck.com
        Extension: renegotiation_info
        Extension: elliptic_curves
            Named curve: secp256r1
            Named curve: secp384r1
        Extension: signature_algorithms
            Algorithm: sha1/rsa
            Algorithm: sha256/rsa
            Algorithm: sha1/ecdsa
            Algorithm: sha256/ecdsa”
#+END_EXAMPLE
上面的握手信息（ClientHello）之中，Cipher Suites字段就是客户端列出可选的加密参数集，服务器在其中选择一个自己支持的参数集。

服务器选择完毕之后，向客户端发出回应。
#+BEGIN_EXAMPLE
Handshake protocol: ServerHello
    Version: TLS 1.2
    Random
        Server time: Mar 10, 2059 02:35:57 GMT”
        Random bytes: 8469b09b480c1978182ce1b59290487609f41132312ca22aacaf5012
    Session ID: 4cae75c91cf5adf55f93c9fb5dd36d19903b1182029af3d527b7a42ef1c32c80
    Cipher Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
    Compression method: null
    Extensions
        Extension: server_name
        Extension: renegotiation_info”
#+END_EXAMPLE
上面的回应信息（ServerHello）中，Cipher Suite字段就是服务器最终选定的加密参数。
** SSH 证书登录
SSH 是服务器登录工具，一般情况下都采用密码登录或密钥登录。

但是，SSH 还有第三种登录方法，那就是证书登录。某些情况下，它是更合理、更安全的登录方法，本文就介绍这种登录方法。
*** 非证书登录的缺点

密码登录和密钥登录，都有各自的缺点。

密码登录需要输入服务器密码，这非常麻烦，也不安全，存在被暴力破解的风险。

密钥登录需要服务器保存用户的公钥，也需要用户保存服务器公钥的指纹。这对于多用户、多服务器的大型机构很不方便，如果有员工离职，需要将他的公钥从每台服务器删除。

*** 证书登录是什么？

证书登录就是为了解决上面的缺点而设计的。它引入了一个证书颁发机构（Certificate Authority，简称 CA），对信任的服务器颁发服务器证书，对信任的用户颁发用户证书。

登录时，用户和服务器不需要提前知道彼此的公钥，只需要交换各自的证书，验证是否可信即可。

证书登录的主要优点有两个：（1）用户和服务器不用交换公钥，这更容易管理，也具有更好的可扩展性。（2）证书可以设置到期时间，而公钥没有到期时间。针对不同的情况，可以设置有效期很短的证书，进一步提高安全性。

*** 证书登录的流程

SSH 证书登录之前，如果还没有证书，需要生成证书。具体方法是：（1）用户和服务器都将自己的公钥，发给 CA；（2）CA 使用服务器公钥，生成服务器证书，发给服务器；（3）CA 使用用户的公钥，生成用户证书，发给用户。

有了证书以后，用户就可以登录服务器了。整个过程都是 SSH 自动处理，用户无感知。

第一步，用户登录服务器时，SSH 自动将用户证书发给服务器。

第二步，服务器检查用户证书是否有效，以及是否由可信的 CA 颁发。证实以后，就可以信任用户。

第三步，SSH 自动将服务器证书发给用户。

第四步，用户检查服务器证书是否有效，以及是否由信任的 CA 颁发。证实以后，就可以信任服务器。

第五步，双方建立连接，服务器允许用户登录。

*** 生成 CA 的密钥

证书登录的前提是，必须有一个 CA，而 CA 本质上就是一对密钥，跟其他密钥没有不同，CA 就用这对密钥去签发证书。

虽然 CA 可以用同一对密钥签发用户证书和服务器证书，但是出于安全性和灵活性，最好用不同的密钥分别签发。所以，CA 至少需要两对密钥，一对是签发用户证书的密钥，假设叫做user_ca，另一对是签发服务器证书的密钥，假设叫做host_ca。

使用下面的命令，生成user_ca。

#+begin_src bash
# 生成 CA 签发用户证书的密钥
$ ssh-keygen -t rsa -b 4096 -f ~/.ssh/user_ca -C user_ca
#+END_SRC

上面的命令会在~/.ssh目录生成一对密钥：user_ca（私钥）和user_ca.pub（公钥）。

这个命令的各个参数含义如下。

- -t rsa：指定密钥算法 RSA。
- -b 4096：指定密钥的位数是4096位。安全性要求不高的场合，这个值可以小一点，但是不应小于1024。
- -f ~/.ssh/user_ca：指定生成密钥的位置和文件名。
- -C user_ca：指定密钥的识别字符串，相当于注释，可以随意设置。

使用下面的命令，生成host_ca。

#+begin_src bash
# 生成 CA 签发服务器证书的密钥
$ ssh-keygen -t rsa -b 4096 -f host_ca -C host_ca
#+END_SRC

上面的命令会在~/.ssh目录生成一对密钥：host_ca（私钥）和host_ca.pub（公钥）。

现在，~/.ssh目录应该至少有四把密钥。

- ~/.ssh/user_ca
- ~/.ssh/user_ca.pub
- ~/.ssh/host_ca
- ~/.ssh/host_ca.pub

*** CA 签发服务器证书

有了 CA 以后，就可以签发服务器证书了。

签发证书，除了 CA 的密钥以外，还需要服务器的公钥。一般来说，SSH 服务器（通常是sshd）安装时，已经生成密钥/etc/ssh/ssh_host_rsa_key了。如果没有的话，可以用下面的命令生成。

#+begin_src bash
$ sudo ssh-keygen -f /etc/ssh/ssh_host_rsa_key -b 4096 -t rsa
#+END_SRC

上面命令会在/etc/ssh目录，生成ssh_host_rsa_key（私钥）和ssh_host_rsa_key.pub（公钥）。然后，需要把服务器公钥ssh_host_rsa_key.pub，复制或上传到 CA 所在的服务器。

上传以后，CA 就可以使用密钥host_ca为服务器的公钥ssh_host_rsa_key.pub签发服务器证书。

#+begin_src bash
$ ssh-keygen -s host_ca -I host.example.com -h -n host.example.com -V +52w ssh_host_rsa_key.pub
#+END_SRC

上面的命令会生成服务器证书ssh_host_rsa_key-cert.pub（服务器公钥名字加后缀-cert）。这个命令各个参数的含义如下。

- -s：指定 CA 签发证书的密钥。
- -I：身份字符串，可以随便设置，相当于注释，方便区分证书，将来可以使用这个字符串撤销证书。
- -h：指定该证书是服务器证书，而不是用户证书。
- -n host.example.com：指定服务器的域名，表示证书仅对该域名有效。如果有多个域名，则使用逗号分隔。用户登录该域名服务器时，SSH 通过证书的这个值，分辨应该使用哪张证书发给用户，用来证明服务器的可信性。
- -V +52w：指定证书的有效期，这里为52周（一年）。默认情况下，证书是永远有效的。建议使用该参数指定有效期，并且有效期最好短一点，最长不超过52周。
- ssh_host_rsa_key.pub：服务器公钥。

生成证书以后，可以使用下面的命令，查看证书的细节。

#+begin_src bash
$ ssh-keygen -L -f ssh_host_rsa_key-cert.pub
#+END_SRC

最后，为证书设置权限。

#+begin_src bash
$ chmod 600 ssh_host_rsa_key-cert.pub
#+END_SRC

*** CA 签发用户证书

下面，再用 CA 签发用户证书。这时需要用户的公钥，如果没有的话，客户端可以用下面的命令生成一对密钥。

#+begin_src bash
$ ssh-keygen -f ~/.ssh/user_key -b 4096 -t rsa
#+END_SRC

上面命令会在~/.ssh目录，生成user_key（私钥）和user_key.pub（公钥）。

然后，将用户公钥user_key.pub，上传或复制到 CA 服务器。接下来，就可以使用 CA 的密钥user_ca为用户公钥user_key.pub签发用户证书。

#+begin_src bash
$ ssh-keygen -s user_ca -I user@example.com -n user -V +1d user_key.pub
#+END_SRC

上面的命令会生成用户证书user_key-cert.pub（用户公钥名字加后缀-cert）。这个命令各个参数的含义如下。

- -s：指定 CA 签发证书的密钥
- -I：身份字符串，可以随便设置，相当于注释，方便区分证书，将来可以使用这个字符串撤销证书。
- -n user：指定用户名，表示证书仅对该用户名有效。如果有多个用户名，使用逗号分隔。用户以该用户名登录服务器时，SSH 通过这个值，分辨应该使用哪张证书，证明自己的身份，发给服务器。
- -V +1d：指定证书的有效期，这里为1天，强制用户每天都申请一次证书，提高安全性。默认情况下，证书是永远有效的。
- user_key.pub：用户公钥。

生成证书以后，可以使用下面的命令，查看证书的细节。

#+begin_src bash
$ ssh-keygen -L -f user_key-cert.pub
#+END_SRC

最后，为证书设置权限。

#+begin_src bash
$ chmod 600 user_key-cert.pub
#+END_SRC

*** 服务器安装证书

CA 生成服务器证书ssh_host_rsa_key-cert.pub以后，需要将该证书发回服务器，可以使用下面的scp命令，将证书拷贝过去。

#+begin_src bash
$ scp ~/.ssh/ssh_host_rsa_key-cert.pub root@host.example.com:/etc/ssh/
#+END_SRC

然后，将下面一行添加到服务器配置文件/etc/ssh/sshd_config。

#+begin_src bash
HostCertificate /etc/ssh/ssh_host_rsa_key-cert.pub
#+END_SRC

上面的代码告诉 sshd，服务器证书是哪一个文件。

重新启动 sshd。

#+begin_src bash
$ sudo systemctl restart sshd.service
# 或者
$ sudo service sshd restart
#+END_SRC

*** 服务器安装 CA 公钥

为了让服务器信任用户证书，必须将 CA 签发用户证书的公钥user_ca.pub，拷贝到服务器。

#+begin_src bash
$ scp ~/.ssh/user_ca.pub root@host.example.com:/etc/ssh/
#+END_SRC

上面的命令，将 CA 签发用户证书的公钥user_ca.pub，拷贝到 SSH 服务器的/etc/ssh目录。

然后，将下面一行添加到服务器配置文件/etc/ssh/sshd_config。

#+begin_src bash
TrustedUserCAKeys /etc/ssh/user_ca.pub
#+END_SRC

上面的做法是将user_ca.pub加到/etc/ssh/sshd_config，这会产生全局效果，即服务器的所有账户都会信任user_ca签发的所有用户证书。

另一种做法是将user_ca.pub加到服务器某个账户的~/.ssh/authorized_keys文件，只让该账户信任user_ca签发的用户证书。具体方法是打开~/.ssh/authorized_keys，追加一行，开头是@cert-authority principals="..."，然后后面加上user_ca.pub的内容，大概是下面这个样子。

#+begin_src bash
@cert-authority principals="user" ssh-rsa AAAAB3Nz...XNRM1EX2gQ==
#+END_SRC

上面代码中，principals="user"指定用户登录的服务器账户名，一般就是authorized_keys文件所在的账户。

重新启动 sshd。

#+begin_src bash
$ sudo systemctl restart sshd.service
# 或者
$ sudo service sshd restart
#+END_SRC

至此，SSH 服务器已配置为信任user_ca签发的证书。

*** 客户端安装证书

客户端安装用户证书很简单，就是从 CA 将用户证书user_key-cert.pub复制到客户端，与用户的密钥user_key保存在同一个目录即可。

*** 客户端安装 CA 公钥

为了让客户端信任服务器证书，必须将 CA 签发服务器证书的公钥host_ca.pub，加到客户端的/etc/ssh/ssh_known_hosts文件（全局级别）或者~/.ssh/known_hosts文件（用户级别）。

具体做法是打开ssh_known_hosts或known_hosts文件，追加一行，开头为@cert-authority *.example.com，然后将host_ca.pub文件的内容（即公钥）粘贴在后面，大概是下面这个样子。

#+begin_src bash
@cert-authority *.example.com ssh-rsa AAAAB3Nz...XNRM1EX2gQ==
#+END_SRC

上面代码中，*.example.com是域名的模式匹配，表示只要服务器符合该模式的域名，且签发服务器证书的 CA 匹配后面给出的公钥，就都可以信任。如果没有域名限制，这里可以写成*。如果有多个域名模式，可以使用逗号分隔；如果服务器没有域名，可以用主机名（比如host1,host2,host3）或者 IP 地址（比如11.12.13.14,21.22.23.24）。

然后，就可以使用证书，登录远程服务器了。

#+begin_src bash
$ ssh -i ~/.ssh/user_key user@host.example.com
#+END_SRC

上面命令的-i参数用来指定用户的密钥。如果证书与密钥在同一个目录，则连接服务器时将自动使用该证书。

*** 废除证书

废除证书的操作，分成用户证书的废除和服务器证书的废除两种。

服务器证书的废除，用户需要在known_hosts文件里面，修改或删除对应的@cert-authority命令的那一行。

用户证书的废除，需要在服务器新建一个/etc/ssh/revoked_keys文件，然后在配置文件sshd_config添加一行，内容如下。

#+begin_src bash
RevokedKeys /etc/ssh/revoked_keys
#+END_SRC

revoked_keys文件保存不再信任的用户公钥，由下面的命令生成。

#+begin_src bash
$ ssh-keygen -kf /etc/ssh/revoked_keys -z 1 ~/.ssh/user1_key.pub
#+END_SRC

上面命令中，-z参数用来指定用户公钥保存在revoked_keys文件的哪一行，这个例子是保存在第1行。

如果以后需要废除其他的用户公钥，可以用下面的命令保存在第2行。

#+begin_src bash
$ ssh-keygen -ukf /etc/ssh/revoked_keys -z 2 ~/.ssh/user2_key.pub
#+END_SRC

*** 参考链接

- [SSH Emergency Access](https://smallstep.com/blog/ssh-emergency-access/), Carl Tashian
- [Using OpenSSH Certificate Authentication](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/deployment_guide/sec-using_openssh_certificate_authentication), Red Hat Enterprise Linux Deployment Guide
- [How to SSH Properly](https://gravitational.com/blog/how-to-ssh-properly/), Gus Luxton
** 配置vscode 远程开发+ 免密登录
1. 先在window上安装ssh，然后在cmd上用命令ssh-keygen生成密钥对

2. 在vscode中安装remote development 插件

3. 将公钥添加到服务器上的~/.ssh/authorized_keys文件里

4. 在vscode中（或者直接在本地路径打开）打开.ssh/config文件

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-26_18-57-54.png @ 2021-11-26 18:57:59
[[file:ssh_%E7%9B%B8%E5%85%B3/2021-11-26_18-57-59_Snipaste_2021-11-26_18-57-54.png]]
5. 修改.ssh/config文件：加入IdentityFile的路径（也就是私钥在本机的所在位置）,

注意这里的Windows路径要加双引号，如果是linux路径，则不用加双引号

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-26_18-58-21.png @ 2021-11-26 18:58:25
[[file:ssh_%E7%9B%B8%E5%85%B3/2021-11-26_18-58-25_Snipaste_2021-11-26_18-58-21.png]]

** 获取SSH登陆用户的IP地址
一、找sshd进程
#+begin_src bash
ps -ef|grep sshd
root      1693     1  0 Aug13 ?        00:00:00 /usr/sbin/sshd   #父进程号是1的是系统服务进程
root     12598  1693  2 14:59 ?        00:00:00 /usr/sbin/sshd   #有这个进程说明有SSHD远程客户登录
root     12638  1693  0 15:02 ?        00:00:00 /usr/sbin/sshd   #这是第二个登录用户
root     12633 12600  0 14:59 pts/0    00:00:00 grep sshd
#+END_SRC

二、根据登录上来的进程号找到用户进程
#+begin_src bash
#ps -ef|grep 12598
root     12598  1693  0 14:59 ?        00:00:00 /usr/sbin/sshd
root     12600 12598  0 14:59 pts/0    00:00:00 -bash            #第一个用户，居然还是用root登录的。

#ps -ef|grep 12633
root     12638  1693  0 15:02 ?        00:00:00 /usr/sbin/sshd
root     12640 12638  0 15:02 pts/1    00:00:00 -bash            #第二个用户
#+END_SRC

三、根据bash进程的终端号pts/?来确定来源
#+begin_src bash
[root@redhat root]# who -a|grep pts/1
root     + pts/1        Aug 14 15:02 00:03       12640 (192.168.0.123)   #登录来源IP192.168.0.123
[root@redhat root]# who -a|grep pts/0
root     + pts/0        Aug 14 14:59   .         12600 (192.168.0.123)   #登录来源IP192.168.0.123
#+END_SRC
** ssh Connection refused的解决方法
 没装openssh_server 或者openssh_client，解决方法:sudo apt-get install openssh_server openssh_client
 #+BEGIN_EXAMPLE
 The ssh binary, the SSH client, is provided by the openssh-client package, which is installed on your system.

 The ssh service runs the SSH server, provided by the openssh-server package, which isn’t installed on your system.

 The ssh package is a meta-package which installs both the client and the server.
 #+END_EXAMPLE
 开启openssh服务: sudo /etc/init.d/ssh start

 验证是否开启服务: ps -e | grep ssh

 如果有输出 sshd 证明已经开启ssh服务

** git bash关于debug1: send_pubkey_test: no mutual signature algorithm 的解决方法
相关讨论：[[https://github.com/golang/go/issues/37278][x/crypto/ssh: support RSA SHA-2 host key signatures #37278]]
#+BEGIN_EXAMPLE
The new version of Git for Windows uses OpenSSH version 8.8, which uses THE RSA-SHA2 algorithm by default. However, the Golang SSH library used by Gitee uses the RSA-SHA1 algorithm, so the public key authentication fails.

Add the following information to the config file under ~/.ssh/ :

Host gitee.com 
HostkeyAlgorithms +ssh-rsa 
PubkeyAcceptedAlgorithms +ssh-rsa
#+END_EXAMPLE

** SSH连接报错:Permission denied, please try again.的解决方法
当使用 SSH 登录云服务器 ECS （Elastic Compute Server） Linux 服务器时，如果是 root 用户，即便正确输入了密码，也会出现类似如下错误信息。

Permission denied, please try again.

SSH 服务器拒绝了密码，请再试一次。

但非root用户可以正常登录，而且root用户通过 管理终端 登录也正常。

这是因为服务端SSH 服务配置了禁止root用户登录策略。

修改/etc/ssh/sshd_config 中:
PermitRootLogin no

为：PermitRootLogin yes

并重启ssh服务：service sshd restart
** SSH-Key authentication fails
root用户无法用密钥登录的解决方法

ssh -vvv root@10.0.12.28 查看连接详细信息，发现出现
#+begin_src bash
debug1: Offering public key: /c/Users/123/.ssh/id_rsa RSA SHA256:FBIuIq5fVuKYh0ncLnXiJryP/K+58ysh2fGdwVveU1I
debug3: send packet: type 50
debug2: we sent a publickey packet, wait for reply
debug3: receive packet: type 51
#+END_SRC
这个是root用户目录下的.ssh文件夹以及authorized_keys文件的权限没有正确设置的原因

*** 参考文章
[[https://superuser.com/questions/1137438/ssh-key-authentication-fails][SSH-Key authentication fails]]
** 参考文章
[[https://www.jianshu.com/p/1e793e386beb][ssh配置文件详解]]
[[https://wangdoc.com/ssh/client.html][ssh教程]]
* scp
scp可以用于从远程复制文件到本地、从本地复制文件到服务器，也可以在两个远程服务器之间传输文件

（1）-c

-c参数用来指定文件拷贝数据传输的加密算法。

$ scp -c blowfish some_file your_username@remotehost.edu:~
上面代码指定加密算法为blowfish。

（2）-C

-C参数表示是否在传输时压缩文件。

$ scp -c blowfish -C local_file your_username@remotehost.edu:~
（3）-F

-F参数用来指定 ssh_config 文件，供 ssh 使用。

$ scp -F /home/pungki/proxy_ssh_config Label.pdf root@172.20.10.8:/root
（4）-i

-i参数用来指定密钥。

$ scp -vCq -i private_key.pem ~/test.txt root@192.168.1.3:/some/path/test.txt
（5）-l

-l参数用来限制传输数据的带宽速率，单位是 Kbit/sec。对于多人分享的带宽，这个参数可以留出一部分带宽供其他人使用。

$ scp -l 80 yourusername@yourserver:/home/yourusername/* .
上面代码中，scp命令占用的带宽限制为每秒 80K 比特位，即每秒 10K 字节。

（6）-p

-p参数用来保留修改时间（modification time）、访问时间（access time）、文件状态（mode）等原始文件的信息。

$ scp -p ~/test.txt root@192.168.1.3:/some/path/test.txt
（7）-P

-P参数用来指定远程主机的 SSH 端口。如果远程主机使用默认端口22，可以不用指定，否则需要用-P参数在命令中指定。

$ scp -P 2222 user@host:directory/SourceFile TargetFile
（8）-q

-q参数用来关闭显示拷贝的进度条。

$ scp -q Label.pdf mrarianto@202.x.x.x:.
（9）-r

-r参数表示是否以递归方式复制目录。

（10）-v

-v参数用来显示详细的输出。

$ scp -v ~/test.txt root@192.168.1.3:/root/help2356.txt
* sftp
sftp是 SSH 提供的一个客户端应用程序，主要用来安全地访问 FTP。因为 FTP 是不加密协议，很不安全，sftp就相当于将 FTP 放入了 SSH。

下面的命令连接 FTP 主机。
#+begin_src bash
$ sftp username@hostname
#+END_SRC
执行上面的命令，会要求输入 FTP 的密码。密码验证成功以后，就会出现 FTP 的提示符sftp>，下面是一个例子。
#+begin_src bash
$ sftp USER@penguin.example.com
USER@penguin.example.com's password:
Connected to penguin.example.com.
sftp>
#+END_SRC
FTP 的提示符下面，就可以输入各种 FTP 命令了，这部分完全跟传统的 FTP 用法完全一样。
- ls [directory]：列出一个远程目录的内容。如果没有指定目标目录，则默认列出当前目录。
- cd directory：从当前目录改到指定目录。
- mkdir directory：创建一个远程目录。
- rmdir path：删除一个远程目录。
- put localfile [remotefile]：本地文件传输到远程主机。
- get remotefile [localfile]：远程文件传输到本地。
- help：显示帮助信息。
- bye：退出 sftp。
- quit：退出 sftp。
- exit：退出 sftp。
* tar命令
- *.Z compress 程序压缩的文件；
- *.zip zip 程序压缩的文件；
- *.gz gzip 程序压缩的文件；
- *.bz2 bzip2 程序压缩的文件；
- *.xz xz 程序压缩的文件；
- *.tar tar 程序打包的数据，并没有压缩过；
- *.tar.gz tar 程序打包的文件，其中并且经过 gzip 的压缩
- *.tar.bz2 tar 程序打包的文件，其中并且经过 bzip2 的压缩
- *.tar.xz tar 程序打包的文件，其中并且经过 xz 的压缩

** 常见的压缩指令
*** gzip, zcat/zmore/zless/zgrep
 目前 gzip 可以解开 compress, zip 与 gzip 等软件所压缩的文件

 当你使用 gzip 进行压缩时，在预设的状态下原本的文件会被压缩成为 .gz 的档名，源文件就不再存在了。
*** bzip2, bzcat/bzmore/bzless/bzgrep
*** xz, xzcat/xzmore/xzless/xzgrep

** 命令格式
#+begin_src bash
tar [-z|-j|-J] [cv] [-f 待建立的新檔名] filename... <==打包与压缩
tar [-z|-j|-J] [tv] [-f 既有的 tar 檔名] <==察看檔名
tar [-z|-j|-J] [xv] [-f 既有的 tar 檔名] [-C 目录] <==解压缩
#+END_SRC
** 选项与参数：
- -c ：建立打包文件，可搭配 -v 来察看过程中被打包的档名(filename)
- -t ：察看打包文件的内容含有哪些档名，重点在察看『档名』就是了；
- -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开。 

特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。

- -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz
- -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2
- -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz。 

特别留意， -z, -j, -J 不可以同时出现在一串指令列中

- -v ：在压缩/解压缩的过程中，将正在处理的文件名显示出来！用-tv 选项查看打包文件时，详细的文件权限/属性都会被列出来。
- -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) 。如果把-jcvf filename写成-jvfc filename ，会导致产生的档名变成 c 
- -C 目录 ：指定解压目录。这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。
- -p(小写) ：保留备份数据的原本权限与属性，常用于备份(-c)重要的配置文件
- -P(大写) ：保留绝对路径，亦即允许备份数据中含有根目录存在之意；
- --exclude=FILE：在压缩的过程中，不要将 FILE 打包！
** 仅解开单一文件的方法
#+begin_src bash
# 1. 先找到我们要的档名，假设解开 shadow 文件好了：
[root@study ~]# tar -jtv -f /root/etc.tar.bz2 | grep 'shadow'
---------- root/root 721 2015-06-17 00:20 etc/gshadow
---------- root/root 1183 2015-06-17 00:20 etc/shadow-
---------- root/root 1210 2015-06-17 00:20 etc/shadow <==这是我们要的！
---------- root/root 707 2015-06-17 00:20 etc/gshadow-
# 先搜寻重要的档名！其中那个 grep 是『撷取』关键词的功能！我们会在第三篇说明！
# 这里您先有个概念即可！那个管线 | 配合 grep 可以撷取关键词的意思！
# 2. 将该文件解开！语法与实际作法如下：
[root@study ~]# tar -jxv -f 打包檔.tar.bz2 待解开档名
[root@study ~]# tar -jxv -f /root/etc.tar.bz2 etc/shadow
etc/shadow
[root@study ~]# ll etc
total 4
----------. 1 root root 1210 Jun 17 00:20 shadow
# 很有趣！此时只会解开一个文件而已！不过，重点是那个档名！你要找到正确的档名。
# 在本例中，你不能写成 /etc/shadow ！因为记录在 etc.tar.bz2 内的并没有 / 之故！
#+END_SRC
** tar命令解压时去除目录结构
指定去除目录结构，使用--strip-components N

如：压缩文件 file.tar 中文件信息为 three/two/one/file.txt

1、（去除第一层目录 three）运行以下命令：
tar -xvf file.tar --strip-components 1

最终结果为：
two/one/file.txt

2、（去除三层目录 three、two、one、）运行以下命令：
tar -xvf file.tar --strip-components 3

解压结果为：
file.txt
* TensorFlow
用pip命令
#+BEGIN_SRC bash
pip install tensorflow
#+END_SRC
* tee命令
Linux tee命令用于读取标准输入的数据，并将其内容输出成文件。

tee指令会从标准输入设备读取数据，将其内容输出到标准输出设备，同时保存成文件。

语法：
tee [-ai][--help][--version][文件...]

参数：
- -a或--append 　附加到既有文件的后面，而非覆盖它．
- -i或--ignore-interrupts 　忽略中断信号。
- --help 　在线帮助。
- --version 　显示版本信息。
** 实例
使用指令"tee"将用户输入的数据同时保存到文件"file1"和"file2"中，输入如下命令：
#+begin_src bash
$ tee file1 file2                   #在两个文件中复制内容 
#+END_SRC
以上命令执行后，将提示用户输入需要保存到文件的数据，如下所示：
#+BEGIN_EXAMPLE
My Linux                        #提示用户输入数据  
My Linux                        #输出数据，进行输出反馈  
#+END_EXAMPLE
此时，可以分别打开文件"file1"和"file2"，查看其内容是否均是"My Linux"即可判断指令"tee"是否执行成功。
* tmux
** sessions, windows and panes介绍
摘抄 github 上 tmux 项目的 wiki 之 Getting-Started#summary-of-terms 属于解释表，如下：
| Term           | Description                                                                          |
|----------------+--------------------------------------------------------------------------------------|
| Client         | Attaches a tmux session from an outside terminal such as xterm(1)                    |
| Session        | Groups one or more windows together                                                  |
| Window         | Groups one or more panes together, linked to one or more sessions                    |
| Pane           | Contains a terminal and running program, appears in one window                       |
| Active pane    | The pane in the current window where typing is sent; one per window                  |
| Current window | The window in the attached session where typing is sent; one per session             |
| Last window    | The previous current window                                                          |
| Session name   | The name of a session, defaults to a number starting from zero                       |
| Window list    | The list of windows in a session in order by number                                  |
| Window name    | The name of a window, defaults to the name of the running program in the active pane |
| Window index   | The number of a window in a session’s window list                                    |
| Window layout  | The size and position of the panes in a window                                       |
*** pane
每一个运行在 tmux 里的终端都属于一个 pane，它是一个矩形区域。

因为 tmux 里一个终端仅仅被显示在一个 pane 里，术语 pane 常常用于表示 tmux 里所有的 pane。

一个 pane 仅属于一个 window ，一个 window 可以有多个 pane，多个 pane 共同填满整个 window 区域，达到多个 pane 允许同时被看到。

pane 间用线分隔，这条线叫 pane border 。

每个 window 里有一个 active pane，active pane 是输入的文本发送的地方，是用指令定位到 window 后默认的 pane 。

active pane 的 pane border 是绿色的。
*** window
一个 window 通常会占据整个 tmux 终端，不过 window 大小是可以设置的。

一个 window 里的所有 pane 的大小、位置信息叫做 window layout。

每个 window 都有一个名称，默认情况 tmux 会自动命名一个，用户可以修改。

window 名称不用全局唯一，鉴定出一个 window 用的是 session 和 window index，不是用 window 名称。
*** session
一个 session 可以包含多个 window，一个 window 也可以属于多个 session，大多数时候一个 window 仅属于一个 session 。

一个 window 是某个 session 的一部分，那么可以说此 window 链接到这个 session。

同属一个 session 的 window 有个数字，叫做 window index 。当一个 window 属于多个 session 时，同样的 window 有多个不同的 window index 值。

一个 session 的 window 列表是此 session 下所有 window 的列表，并且 window 按照 window index 排序。

每个 session 有个 current window 。当依附到 session 时展现的 window 叫 current window，也是用指定选中 session 时默认的 window 。

如果 current window 改变了，前一个 current window 改叫做 last window 。

多个客户端可以依附到同一个 session 上，

session 没有 index 但是有名次，session 的名称必须唯一。
** 会话
每个会话都是一个独立的工作区，其中包含一个或多个窗口

tmux 开始一个新的会话
tmux new -s NAME -n window_name 以指定名称开始一个新的会话,-s指定会话名，-n指定窗口名
tmux new 'emacs ~/.tmux.conf' 或者 tmux new -- emacs ~/.tmux.conf 启动其他程序替代启动 shell
tmux ls 列出当前所有会话
tmux kill-session -t <session_name> 删除指定名字的会话
tmux list-keys 列出您拥有的所有自定义键绑定。
tmux a 重新连接最后一个会话。您也可以通过 -t 来指定具体的会话（例如tmux attach -t 0）
tmux rename-session -t 0 database 对指定的会话由0改名为database
tmux attach -d -t <session_name>. The “-d” option causes the session to be detached from its previous terminal. Without this option, the session will be attached to both its previous terminal and the new terminal
<C-b> d 将当前会话分离
<C-b> $ 重命名会话
<C-b> s 会话切换
<C-b> w 会显示 session 和 session 下的 window ，并选中 session 下的 current window 。
exit 退出会话
** 窗口 
相当于编辑器或是浏览器中的标签页，从视觉上将一个会话分割为多个部分

<C-b> c 创建一个新的窗口，使用 <C-d>关闭
tmux neww -n Window1 创建一个新的名为Window1的窗口
<C-b> N 跳转到第 N 个窗口，注意每个窗口都是有编号的
<C-b> p 切换到前一个窗口
<C-b> n 切换到下一个窗口
<C-b> , 重命名当前窗口
<C-b> 0	切换到0号窗口. 窗口1-9操作类似.
<C-b> w 列出当前所有窗口
<C-b> ? 列出所有快捷键

** 面板
像 vim 中的分屏一样，面板使我们可以在一个屏幕里显示多个 shell

<C-b> " 水平分割
<C-b> % 垂直分割
<C-b> <方向> 切换到指定方向的面板，<方向> 指的是键盘上的方向键
<C-b> z 切换当前面板的缩放（即切换全屏）
<C-b> [ 开始往回卷动屏幕。您可以按下空格键来开始选择，回车键复制选中的部分。type Ctrl-space to begin selection, then move the cursor to make our selection. Finally, we type Alt-w to copy the selected text.
<C-b> ] 粘贴
<C-b> <C-arrow> Resize pane by 1 character
<C-b> <Alt-arrow> Resize pane by 5 characters
<C-b> x	Destroy current pane
<C-b> <空格> 在不同的面板排布间切换
exit or <C-b> d 退出面板
<C-b> t 在当前面板显示一个数字时钟，按q可以退出
<C-b> ! 将当前面板变成一个新的窗口
** 状态栏
当一个客户端依附到 session 时，客户端会在屏幕底部显示一个状态栏，默认是绿色的，并展示下面信息：
- 左边是依附到的 session 名称
- 中间是依附到 session 的 window 列表，有 window 的 index，比如 0:ksh 。
- 右边是用引号括起 pane 的 title （默认情况是运行 tmux 的主机名）和时间日期。
- 如果打开的 window 太多，< 和 > 符号会加在 window 列表的两边。
- window 列表里，current window 会加 * ，last window 会加 - 。
#+DOWNLOADED: screenshot @ 2021-12-10 09:37:39
[[file:images/linux%E7%AC%94%E8%AE%B0/tmux/2021-12-10_09-37-39_screenshot.png]]
** 帮助键
每个默认的 tmux 快捷键都有个简单的描述来帮助记忆这些快捷键用来做什么的。

C-b ? 键可以查看快捷键列表。

C-b ? 进入 view 模式，显示文本。pane 进入 view 模式后有自己独立的快捷键就不需要使用前缀键了。pane view 模式的快捷键和 emacs 很像。Up、Down、C-Up，C-Down 用于上下滚动，q 退出 pane 的 view 模式；右上角显示总行数和当前行数。

或者，tmux lsk -N | more 也可以查看相同的快捷键列表。

C-b / 查看单个键的描述信息，按下 C-b / 之后，一个提示符出现在终端底部，输入要查看的 key 就会显示它的描述，比如按下 C-b / 在按下 ? 会显示 “C-b ? List key bindings” 。
** 完全的退出 tmux
当 tmux 里没有 session 、window、pane 时 tmux server 自动退出。

也可以手动退出正在运行的 tmux server，进入命令提示符（C-b :）输入 :kill-server 即可。
** tree mode(选择 session, window, panen)
tmux 有种模式叫 tree mode，允许在一个 tree 上浏览、依附、杀掉、选择 session 、window 、pane ；允许选择多个 session 、window 、pane 打上标签，然后同时向它们发送命令。

有两种进入 tree mode 的快捷键：
- C-b s 仅显示 session，并选中依附上的 session 。
- C-b w 会显示 session 和 session 下的 window ，并选中 session 下的 current window 。

tree mode 把窗口分割成两部分，上半部分是列出 session 、window 、pane 信息的 tree ，下半部分是光标选中部分的预览。对于选中的 session 会尽可能显示每个 window 的 active pane ，选中的 window 会尽可能显示每个 pane ，选中的 pane 会只显示选中的 pane 。

控制 tree mode 的按键不需要前缀键。
- Up ，Down 上下移动。
- Enter 改成选中的项，变成依附的 session ，current window ，active pane ，并退出 tree mode。
- Right 展开选中的项，session 展开显示 window ，window 展开显示 pane 。
- Left 收起选中的项。
- q 退出 tree mode 。
- t 键给项打标签，再按一次 t 取消标签。被打上标签的项加粗显示，名称后面有 * 号。
- T 一次取消所有项的标签。
- X 一次性杀掉所有打上标签的项。
- : 进入提示符模式，输入的命令会一次性应用到所有被打标签的项上。
- tree 里的每项前面都有个序号，在行首，用括号包围着。按下序号对应的按键会立即选中此项，相当于按下 enter 键。前十个项是 0-9 按键，后面的使用 M-a 到 M-z 。
** client mode(分离其他客户端)
C-b D (C-b S-d) 快捷键可以获得客户端列表，进入 client mode 。

每个 client 会显示在上半部分的列表里，会显示客户端的名字、依附到的 session 、大小、最后使用的时间和日期。下半部分会预览选中的客户端。

在 client mode 里移动和打标签的快捷键和 tree mode 一样。但是有区别：
- Enter 分离选中的 client 。
- d 分离选中的 client 和 enter 一样。
- D 分离被打标签了的 clients 。
- x 分离选中的 client 并尝试 kill 启动此 client 的 shell 。
- X 分离被打标签的 clients 并尝试 kill 启动这些 client 的 shell 们。
** 配置文件
位置文件位置在：
/etc/tmux.conf and ~/.tmux.conf

加载配置文件：在 tmux 窗口中，先按下 Ctrl+b 指令前缀，然后按下系统指令:，进入到命令模式后输入 source-file ~/.tmux.conf
*** 修改指令前缀
#+begin_src bash
unbind C-f # C-b 即 Ctrl+b 键，unbind 意味着解除绑定
set -g prefix C-f #
bind C-f send-prefix # 绑定 Ctrl+f 为新的指令前缀

# 从tmux v1.6版起，支持设置第二个指令前缀
set-option -g prefix2 ` # 设置一个不常用的`键作为指令前缀，按键更快些
#+END_SRC
*** 添加加载配置文件快捷指令 r#
#+begin_src bash
bind r source-file ~/.tmux.conf \; display-message "Config reloaded.."
#+END_SRC
*** 支持鼠标
- 选取文本
- 调整面板大小
- 选中并切换面板
#+begin_src bash
# 老版本：
setw -g mode-mouse on # 支持鼠标选取文本等
setw -g mouse-resize-pane on # 支持鼠标拖动调整面板的大小(通过拖动面板间的分割线)
setw -g mouse-select-pane on # 支持鼠标选中并切换面板
setw -g mouse-select-window on # 支持鼠标选中并切换窗口(通过点击状态栏窗口名称)

# v2.1及以上的版本
set-option -g mouse on
#+END_SRC
但在以上设置下，会发现无法用中键向 tmux 中复制文本，也无法将 tmux 中选择好的文本中键复制到系统其他应用程序中。

这里有一个 trick，那就是在 tmux 中不论选择还是复制时，都按住 Shift 键，你会发现熟悉的中键又回来了 ? 此外，还可以使用 Shift+Insert 快捷键将系统剪切板中的内容输入 tmux 中。 相对于 tmux 原生的选择模式（不加 shift 键），使用系统选择有个缺陷，即当一行内存在多个面板时，无法选择单个面板中的内容，这时就必须使用 tmux 自带的复制粘贴系统了。
*** 更改新增面板键
- 垂直新增面板
- 水平新增面板
#+begin_src bash
unbind '"'
bind - splitw -v -c '#{pane_current_path}' # 垂直方向新增面板，默认进入当前目录
unbind %
bind =  splitw -h -c '#{pane_current_path}' # 水平方向新增面板，默认进入当前目录
#+END_SRC
*** 面板调整大小
绑定Ctrl+hjkl键为面板上下左右调整边缘的快捷指令
#+begin_src bash
bind -r ^k resizep -U 10 # 绑定Ctrl+k为往↑调整面板边缘10个单元格
bind -r ^j resizep -D 10 # 绑定Ctrl+j为往↓调整面板边缘10个单元格
bind -r ^h resizep -L 10 # 绑定Ctrl+h为往←调整面板边缘10个单元格
bind -r ^l resizep -R 10 # 绑定Ctrl+l为往→调整面板边缘10个单元格
#+END_SRC
*** 快速面板切换
鼠标支持确实能带来很大的便捷性，特别是对于习惯了鼠标操作的tmux新手，但对于键盘爱好者而言，这不是什么好消息，对他们而言，双手不离键盘是基本素质。

虽然指令前缀加方向键可以切换面板，但方向键太远，不够快，不够Geek。没关系，我们可以将面板切换升级为熟悉的h、j、k、l键位。
#+begin_src bash
# 绑定hjkl键为面板切换的上下左右键
bind -r k select-pane -U # 绑定k为↑
bind -r j select-pane -D # 绑定j为↓
bind -r h select-pane -L # 绑定h为←
bind -r l select-pane -R # 绑定l为→
#+END_SRC
-r表示可重复按键，大概500ms之内，重复的h、j、k、l按键都将有效，完美支持了快速切换的Geek需求。

除了上下左右外， 还有几个快捷指令可以设置。
#+begin_src bash
bind -r e lastp # 选择最后一个面板
bind -r ^e last # 选择最后一个窗口

bind -r ^u swapp -U # 与前一个面板交换位置
bind -r ^d swapp -D # 与后一个面板交换位置
#+END_SRC
*** 复制模式
复制模式更改为 vi 风格
#+begin_src bash
setw -g mode-keys vi # 开启vi风格后，支持vi的C-d、C-u、hjkl等快捷键
#+END_SRC
复制模式向 vi 靠拢
- v 开始选择文本
- y 复制选中文本
- p 粘贴文本
#+begin_src bash
bind -t vi-copy v begin-selection # 绑定v键为开始选择文本
bind -t vi-copy y copy-selection # 绑定y键为复制选中文本
bind p pasteb # 绑定p键为粘贴文本（p键默认用于进入上一个窗口，不建议覆盖）
#+END_SRC

*** 设置窗口面板起始序
#+begin_src bash
set -g base-index 1 # 设置窗口的起始下标为1
set -g pane-base-index 1 # 设置面板的起始下标为1
#+END_SRC

*** 自定义状态栏
#+begin_src bash
set -g status-utf8 on # 状态栏支持utf8
set -g status-interval 1 # 状态栏刷新时间
set -g status-justify left # 状态栏列表左对齐
setw -g monitor-activity on # 非当前窗口有内容更新时在状态栏通知

set -g status-bg black # 设置状态栏背景黑色
set -g status-fg yellow # 设置状态栏前景黄色
set -g status-style "bg=black, fg=yellow" # 状态栏前景背景色

set -g status-left "#[bg=#FF661D] ❐ #S " # 状态栏左侧内容
set -g status-right 'Continuum status: #{continuum_status}' # 状态栏右侧内容
set -g status-left-length 300 # 状态栏左边长度300
set -g status-right-length 500 # 状态栏左边长度500

set -wg window-status-format " #I #W " # 状态栏窗口名称格式
set -wg window-status-current-format " #I:#W#F " # 状态栏当前窗口名称格式(#I：序号，#w：窗口名称，#F：间隔符)
set -wg window-status-separator "" # 状态栏窗口名称之间的间隔
set -wg window-status-current-style "bg=red" # 状态栏当前窗口名称的样式
set -wg window-status-last-style "fg=red" # 状态栏最后一个窗口名称的样式

set -g message-style "bg=#202529, fg=#91A8BA" # 指定消息通知的前景、后景色
#+END_SRC
状态栏包含 3 个组件：一个左面板，窗口列表和一个右面板。我们可以改变状态栏里左侧或右侧面板的内容，这需要使用一个文本和变量的组合。表1（状态栏变量）列出了状态栏里可能用到的变量。

表1 - 状态栏变量
| 变量             | 描述                          |
| #H               | 本地主机的主机名              |
| #h               | 本地主机的主机名，没有 domain |
| #F               | 当前窗口的标签                |
| #I               | 当前窗口的索引                |
| #P               | 当前面板的索引                |
| #S               | 当前会话的名称                |
| #T               | 当前窗口的标题                |
| #W               | 当前窗口的名称                |
| ##               | 一个 # 符号                   |
| #(shell-command) | shell 命令的第一行输出        |
| #[attributes]    | 要改变的颜色或属性            |

例如，如果想要在左侧显示当前 tmux 会话的名称，就需要使用 set-option -g status-left 选项，后面跟着 #S 值，就像这样：
#+begin_src bash
set -g status-left "#S"
#+END_SRC
还可以通过设置前景色让它显示地更明显，像这样：
#+begin_src bash
set -g status-left "#[fg=green]#S"
#+END_SRC
可以向状态栏里添加任何想要的属性和条目。为了便于展示，我们修改了左侧的状态栏，让它显示绿色的会话名称，黄色的窗口编号，以及蓝绿色的当前面板。配置如下：
#+begin_src bash
set -g status-left "#[fg=green]#S #[fg=yellow]#I #[fg=cyan]#P"
#+END_SRC
也可以向状态栏里添加任意文字。我们现在添加一些文字，让会话、窗口和面板显示地更突出，像这样：
#+begin_src bash
set -g status-left-length 40
set -g status-left "#[fg=green]Session: #S #[fg=yellow]#I #[fg=cyan]#P"
#+END_SRC
我们设置了 status-left-length 选项因为指定的输出对默认长度来说太长了，所以我们让那个区域更宽一些。

还可以配置右侧的状态栏。现在我们向它添加当前日期和时间：
#+begin_src bash
set -g status-right "#[fg=cyan]%d %b %R"
#+END_SRC
这样配置的日期格式是“13-Jan 13:45”，你可以让它显示任意你想要的格式，可以使用许多编程语言通用的 strftime() 时间格式化机制。

在状态栏里开启 UTF-8 支持是个不错的注意，尤其是如果你特别喜欢使用这些字符。
#+begin_src bash
set -g status-utf8 on
#+END_SRC
还可以更进一步，通过使用 #(shell-command) 变量把 shell 命令加入到状态栏中，在状态栏显示该命令的返回结果。

**** 让状态栏实时更新信息
我们已经把当前时间和一些其它动态信息添加到了状态栏，这时需要告诉 tmux 这些信息的刷新周期。默认配置下，tmux 会每 15 秒刷新一次状态栏。可以通过使用 set-option -g status-interval 命令后面加上刷新周期（以秒为单位，译者注）来指定 tmux 的刷新时间，就像这样：
#+begin_src bash
set -g status-interval 60
#+END_SRC
这样就会让 tmux 每 60 秒刷新一次状态栏。注意，如果你在状态栏里添加了 shell 命令，这些命令也会在每次状态栏刷新时执行一遍，所以要注意不要加载太多资源密集型的脚本。

**** 让窗口列表居中显示
我们还能控制窗口列表显示的位置。默认的，窗口列表是靠左对齐的，通过简单的配置就可以让窗口列表在左右面板之间居中显示：
#+begin_src bash
set -g status-justify centre
#+END_SRC
这样配置就会让窗口列表居中显示。创建新窗口时，窗口列表会相应地变换位置，让整个窗口列表显示在状态栏正中间。

**** 窗口活动通知
同样的，我们希望如果当前会话的其他窗口里有一些事件发生时我们能够注意到这些事件，那么我们就可以快速响应那个窗口。可以通过增加一个可视化的通知（visual notification）实现这个功能，像这样：
#+begin_src bash
setw -g monitor-activity on
set -g visual-activity on
#+END_SRC
现在呢，如果其它窗口里有一些活动，它就会使用蓝绿色的背景色突出显示，就像这里的 webserver 窗口：

#+DOWNLOADED: screenshot @ 2021-12-09 16:02:25
[[file:images/linux%E7%AC%94%E8%AE%B0/tmux/2021-12-09_16-02-25_screenshot.png]]

*** 参考文章
[[https://www.cnblogs.com/zuoruining/p/11074367.html][Tmux 配置：打造最适合自己的终端复用工具]]
[[http://louiszhai.github.io/2017/09/30/tmux/#%25E8%2587%25AA%25E5%25AE%259A%25E4%25B9%2589%25E7%258A%25B6%25E6%2580%2581%25E6%25A0%258F][Tmux使用手册]]
[[https://www.kancloud.cn/kancloud/tmux/62463][《tmux: Productive Mouse-Free Development》中文翻译]]
** tmux 选项
tmux 的选项有几种类型，如下：
- server options，影响整个 tmux server 。
- session options，影响一个或者所有的 session 。
- window options，影响一个或者所有 window 。
- pane options，影响一个或者所有 pane 。
- user options，tmux 没有使用这个类型的选项，但是保留了这个选项。

session 和 window 的选项由 2 部分组成，一部分是全局的，另一部分每个 session 、window 独有的，全局的叫 global set，独有的叫 session set 和 window set 。若一个选项没有在 session set 或者 window set 里出现，那么会用 global set 里项。pane 的选项和它们很相似，除非 window 的项也被检查时。（最后一句话原文是 Pane options are similar except the window options are also checked，我没深究 window options are also checked 时 pane option 会怎么样，猜测可能是以 window 的 option 项为主）
*** 显示选项
选项可以用 show-options 指令列出。
- -g 标志参数列出 global options 。
- -s 标志参数列出 server options.tmux show -s
- -g -w 列出 global window options 。tmux show -wg
- 单个 option 的值可以用 show-option 指令加上此 option 的名字显示出来。给了 option 的名字时，就不用加 -s、-w 标志参数了。tmux show -g status 列出 status 选项的值。
*** 改变选项
set-option 指令用来设置和取消设置选项。set-option 指令不用指定 -s -w 标志参数，它可以根据 option name 找到 option 。-g 参数是需要的，用来设置 global session 或者 global window 的选项，对于 server 选项它不起作用。
- set -g status off 关闭 status 。
- set -s default-terminal 'tmux-256color' 设置 default-terminal 值。
- -u 标志参数 unset option，取消某个设置会恢复它的默认值。set -gu status
*** 内嵌的 style
内嵌的 style 包含在 #[] 里面。每个内嵌的 style 会改变接下来的文本风格，直到遇到下一个内嵌的 style 或者文本结尾。

例如，在 status-left 里把文本 foreground 改成 red 把 background 改成 blue ，set -g status-left 'default #[fg=red] red #[fg=blue] blue' 。这样设置也需要增加 status-left-length 值，set -g status-left-length 100 。

内嵌的 style 可以使用条件判断。下面设置 status-left 在按下前缀键时 P 用红色显示，否则是默认的 style 。
#+begin_src bash
set -g status-left '#{?client_prefix,#[bg=red],}P#[default] [#{session_name}] '
#+END_SRC

*** set-option [-aFgopqsuUw] [-t target-pane] option value
(alias: set)
Set a pane option with -p, a window option with -w, a server option with -s, otherwise a session option. If the option is not a user option, -w or -s may be unnecessary - tmux will infer the type from the option name, assuming -w for pane options. If -g is given, the global session or window option is set.
-F expands formats in the option value. The -u flag unsets an option, so a session inherits the option from the global options (or with -g, restores a global option to the default). -U unsets an option (like -u) but if the option is a pane option also unsets the option on any panes in the window. value depends on the option and may be a number, a string, or a flag (on, off, or omitted to toggle).

The -o flag prevents setting an option that is already set and the -q flag suppresses errors about unknown or ambiguous options.

With -a, and if the option expects a string or a style, value is appended to the existing setting. For example:
#+begin_src bash
set -g status-left "foo"
set -ag status-left "bar"
#+END_SRC
Will result in ‘foobar’. And:
#+begin_src bash
set -g status-style "bg=red"
set -ag status-style "fg=blue"
#+END_SRC
Will result in a red background and blue foreground. Without -a, the result would be the default background and a blue foreground.
*** show-options [-AgHpqsvw] [-t target-pane] [option]
(alias: show)
Show the pane options (or a single option if option is provided) with -p, the window options with -w, the server options with -s, otherwise the session options. If the option is not a user option, -w or -s may be unnecessary - tmux will infer the type from the option name, assuming -w for pane options. Global session or window options are listed if -g is used. -v shows only the option value, not the name. If -q is set, no error will be returned if option is unset. -H includes hooks (omitted by default). -A includes options inherited from a parent set of options, such options are marked with an asterisk.
*** 参考文章
[[http://man.openbsd.org/cgi-bin/man.cgi/OpenBSD-current/man1/tmux.1?query=tmux%2526sec=1][官方文档]]
** 退出 session, window, pane
退出 window
- C-b & 会出现提示符让确认是否退出 window ，所有的 pane 也同时退出。
- C-b : 和 kill-window 组合使用退出 window 。

退出 pane
- C-b x 退出 active pane 。
- C-b : 和 kill-pane ，用命令退出 pane 。

退出 session
- C-b : 和 kill-session ，退出 session ，window 同时退出，客户端分离。
** 重命名 session 和 window
重命名 session
- 快捷键使用 C-b $ 出现提示符，为依附的 session 重命名。
- 命令使用 C-b : 和 rename-session 。

重命名 window
- 快捷键使用 C-b , 出现提示符，为 current window 重命名。
- 命令使用 C-b : 和 rename-window 。
** 快捷键
tmux 里绑定快捷键使用 bind-key 命令，解除快捷键使用 unbind-key 命令。每个快捷键都属于有名称的表里，默认有 4 张被命名的表。
- root 表，表里的快捷键不需要和前缀键一起使用。
- prefix 表，表里的键需要和前缀键使用。
- copy-mode 表，copy mode 里使用的快捷键，emacs 风格。
- copy-mode-vi 表，copy mode 里使用的快捷键，vi 风格。

所有的快捷键，或者单个表里的快捷键都可以使用 list-keys 来列出。默认展示 bind-key 的一系列键。
- -T 参数指定表名。
    - tmux lsk -Tprefix
- -N 参数列出帮助信息。
    - tmux lsk -Tprefix -N

unbind-key 用来删除一个快捷键，和 bind-key 一样，有 -T 、-n 标志参数。重新设置快捷键动作时，无需删除此快捷键，bind-key 会自动覆盖已有的快捷键。只有在完全删除一个快捷键时才使用 unbind-key 。
** 自动保存tmux会话 关机重启再也不怕
参考文章：https://zhuanlan.zhihu.com/p/146544540
** Tmux不管怎么改配置文件，都不产生变化的解决方法
这个主要是由于Tmux的后台缓存机制造成的。我就犯了个大错误：甚至删了Tmux、重装Tmux、重启电脑，都没达成。
Tmux会有一个叫Tmux-server的东西。只要把它kill，重启tmux就OK了：
#+begin_src bash
tmux kill-server -a
#+END_SRC
** 复制模式
tmux中操作文本，自然离不开复制模式，通常使用复制模式的步骤如下：
- 输入 `+[ 进入复制模式
- 按下 空格键 开始复制，移动光标选择复制区域
- 按下 回车键 复制选中文本并退出复制模式
- 按下 `+] 粘贴文本

查看复制模式默认的快捷键风格：
#+begin_src bash
tmux show-window-options -g mode-keys # mode-keys emacs
#+END_SRC
默认情况下，快捷键为emacs风格。

为了让复制模式更加方便，我们可以将快捷键设置为熟悉的vi风格，如下：
#+begin_src bash
setw -g mode-keys vi # 开启vi风格后，支持vi的C-d、C-u、hjkl等快捷键

set-window-option -g mode-keys vi #可以设置为vi或emacs
#+END_SRC
tmux复制模式的命令表：
| Command                     | emacs(1) | vi(1)  | Description                                                                     |
|-----------------------------+----------+--------+---------------------------------------------------------------------------------|
| begin-selection             | C-Space  | Space  | Start selection                                                                 |
| cancel                      | q        | q      | Exit copy mode                                                                  |
| clear-selection             | C-g      | Escape | Clear selection                                                                 |
| copy-pipe                   |          |        | Copy and pipe to the command in the first argument                              |
| copy-selection-and-cancel   | M-w      | Enter  | Copy the selection and exit copy mode                                           |
| cursor-down                 | Down     | j      | Move the cursor down                                                            |
| cursor-left                 | Left     | h      | Move the cursot left                                                            |
| cursor-right                | Right    | l      | Move the cursor right                                                           |
| cursor-up                   | Up       | k      | Move the cursor up                                                              |
| end-of-line                 | C-e      | $      | Move the cursor to the end of the line                                          |
| history-bottom              | M->      | G      | Move to the bottom of the history                                               |
| history-top                 | M-<      | g      | Move to the top of the history                                                  |
| middle-line                 | M-r      | M      | Move to middle line                                                             |
| next-word-end               | M-f      | e      | Move to the end of the next word                                                |
| page-down                   | PageDown | C-f    | Page down                                                                       |
| page-up                     | PageUp   | C-b    | Page up                                                                         |
| previous-word               | M-b      | b      | Move to the previous word                                                       |
| rectangle-toggle            | R        | v      | Toggle rectangle selection                                                      |
| search-again                | n        | n      | Repeat the last search                                                          |
| search-backward             |          | ?      | Search backwards, the first argument is the search term                         |
| search-backward-incremental | C-r      |        | Search backwards incrementally, usually used with the -i flag to command-prompt |
| search-forward              |          | /      | Search forwards, the first argument is the search term                          |
| search-forward-incremental  | C-s      |        | Search forwards incrementally                                                   |
| search-reverse              | N        | N      | Repeat the last search but reverse the direction                                |
| start-of-line               | C-a      | 0      | Move to the start of the line                                                   |
*** 自定义复制和选择快捷键了快捷键外，复制模式的启用、选择、复制、粘贴等按键也可以向vi风格靠拢。
#+begin_src bash
bind Escape copy-mode # 绑定esc键为进入复制模式
bind -t vi-copy v begin-selection # 绑定v键为开始选择文本
bind -t vi-copy y copy-selection # 绑定y键为复制选中文本
bind p pasteb # 绑定p键为粘贴文本（p键默认用于进入上一个窗口，不建议覆盖）
#+END_SRC
以上，绑定 v、y两键的设置只在tmux v2.4版本以下才有效，对于v2.4及以上的版本，绑定快捷键需要使用 -T 选项，发送指令需要使用 -X 选项，请参考如下设置：
#+begin_src bash
bind -T copy-mode-vi v send-keys -X begin-selection
bind -T copy-mode-vi y send-keys -X copy-selection-and-cancel
#+END_SRC
** 参考文章
[[https://missing.csail.mit.edu/2020/command-line/][Command-line Environment]]
[[http://linuxcommand.org/lc3_adv_termmux.php][Terminal Multiplexers]]
[[https://www.ruanyifeng.com/blog/2019/10/tmux.html][Tmux 使用教程]]
[[http://longed.top/index.php/2020/10/11/tmux-tutorial/#fu_zhi_zhan_tie][走进神奇的 TMUX]]
* tr命令
Linux tr 命令用于转换或删除文件中的字符。

tr 指令从标准输入设备读取数据，经过字符串转译后，将结果输出到标准输出设备。

语法
#+begin_src bash
tr [-cdst][--help][--version][第一字符集][第二字符集]  
tr [OPTION]…SET1[SET2] 
#+END_SRC
参数说明：
- -c, --complement：反选设定字符。也就是符合 SET1 的部份不做处理，不符合的剩余部份才进行转换
- -d, --delete：删除指令字符
- -s, --squeeze-repeats：缩减连续重复的字符成指定的单个字符
- -t, --truncate-set1：削减 SET1 指定范围，使之与 SET2 设定长度相等
- --help：显示程序用法信息
- --version：显示程序本身的版本信息

字符集合的范围：
- \NNN 八进制值的字符 NNN (1 to 3 为八进制值的字符)
- \\ 反斜杠
- \a Ctrl-G 铃声
- \b Ctrl-H 退格符
- \f Ctrl-L 走行换页
- \n Ctrl-J 新行
- \r Ctrl-M 回车
- \t Ctrl-I tab键
- \v Ctrl-X 水平制表符
- CHAR1-CHAR2 ：字符范围从 CHAR1 到 CHAR2 的指定，范围的指定以 ASCII 码的次序为基础，只能由小到大，不能由大到小。
- [CHAR*] ：这是 SET2 专用的设定，功能是重复指定的字符到与 SET1 相同长度为止
- [CHAR*REPEAT] ：这也是 SET2 专用的设定，功能是重复指定的字符到设定的 REPEAT 次数为止(REPEAT 的数字采 8 进位制计算，以 0 为开始)
- [:alnum:] ：所有字母字符与数字
- [:alpha:] ：所有字母字符
- [:blank:] ：所有水平空格
- [:cntrl:] ：所有控制字符
- [:digit:] ：所有数字
- [:graph:] ：所有可打印的字符(不包含空格符)
- [:lower:] ：所有小写字母
- [:print:] ：所有可打印的字符(包含空格符)
- [:punct:] ：所有标点字符
- [:space:] ：所有水平与垂直空格符
- [:upper:] ：所有大写字母
- [:xdigit:] ：所有 16 进位制的数字
- [=CHAR=] ：所有符合指定的字符(等号里的 CHAR，代表你可自订的字符)
** 实例
将文件testfile中的小写字母全部转换成大写字母，此时，可使用如下命令：
#+begin_src bash
cat testfile |tr a-z A-Z 
#+END_SRC
testfile文件中的内容如下：
#+begin_src bash
$ cat testfile         #testfile原来的内容  
Linux networks are becoming more and more common, 
but scurity is often an overlooked  
issue. Unfortunately, in today’s environment all networks 
are potential hacker targets,  
fro0m tp-secret military research networks to small home LANs.  
Linux Network Securty focuses on securing Linux in a 
networked environment, where the  
security of the entire network needs to be considered
rather than just isolated machines.  
It uses a mix of theory and practicl techniques to 
teach administrators how to install and  
use security applications, as well as how the 
applcations work and why they are necesary. 
#+END_SRC
使用 tr 命令大小写转换后，得到如下输出结果：
#+begin_src bash
$ cat testfile | tr a-z A-Z #转换后的输出  
LINUX NETWORKS ARE BECOMING MORE AND MORE COMMON, BUT SCURITY IS OFTEN AN OVERLOOKED  
ISSUE. UNFORTUNATELY, IN TODAY’S ENVIRONMENT ALL NETWORKS ARE POTENTIAL HACKER TARGETS,  
FROM TP-SECRET MILITARY RESEARCH NETWORKS TO SMALL HOME LANS.  
LINUX NETWORK SECURTY FOCUSES ON SECURING LINUX IN A NETWORKED ENVIRONMENT, WHERE THE  
SECURITY OF THE ENTIRE NETWORK NEEDS TO BE CONSIDERED RATHER THAN JUST ISOLATED MACHINES.  
IT USES A MIX OF THEORY AND PRACTICL TECHNIQUES TO TEACH ADMINISTRATORS HOW TO INSTALL AND  
USE SECURITY APPLICATIONS, AS WELL AS HOW THE APPLCATIONS WORK AND WHY THEY ARE NECESARY. 
#+END_SRC
大小写转换，也可以通过[:lower][:upper]参数来实现。例如使用如下命令：
#+begin_src bash
cat testfile |tr [:lower:] [:upper:] 
#+END_SRC
输出结果如下：
#+begin_src bash
$ cat testfile | tr [:lower:] [:upper:] #转换后的输出  
LINUX NETWORKS ARE BECOMING MORE AND MORE COMMON, BUT SCURITY IS OFTEN AN OVERLOOKED  
ISSUE. UNFORTUNATELY, IN TODAY’S ENVIRONMENT ALL NETWORKS ARE POTENTIAL HACKER TARGETS,  
FROM TP-SECRET MILITARY RESEARCH NETWORKS TO SMALL HOME LANS.  
LINUX NETWORK SECURTY FOCUSES ON SECURING LINUX IN A NETWORKED ENVIRONMENT, WHERE THE  
SECURITY OF THE ENTIRE NETWORK NEEDS TO BE CONSIDERED RATHER THAN JUST ISOLATED MACHINES.  
IT USES A MIX OF THEORY AND PRACTICL TECHNIQUES TO TEACH ADMINISTRATORS HOW TO INSTALL AND  
USE SECURITY APPLICATIONS, AS WELL AS HOW THE APPLCATIONS WORK AND WHY THEY ARE NECESARY. 
#+END_SRC
** 参考文章
[[https://www.runoob.com/linux/linux-comm-tr.html][Linux tr命令 - 菜鸟教程]]
* ubuntu-18.04 设置开机启动python脚本
关于xxx.service文件的写法，可参考这篇文章：[[https://blog.csdn.net/superjunenaruto/article/details/105890739][ubuntu 18.04 python脚本 开机自启]]
** 配置开机启动
ubuntu-18.04没有rc.local，不能通过rc.local来设置开机启动脚本，可以自己建一个。

*** 建立rc-local.service文件
#+begin_src bash
sudo vi /etc/systemd/system/rc-local.service
#+END_SRC
然后将下面内容复制进去
#+begin_src bash
[Unit]
Description=/etc/rc.local Compatibility
ConditionPathExists=/etc/rc.local
 
[Service]
Type=forking
ExecStart=/etc/rc.local start
TimeoutSec=0
StandardOutput=tty
RemainAfterExit=yes
SysVStartPriority=99
 
[Install]
WantedBy=multi-user.target
#+END_SRC
 
*** 创建文件rc.local
sudo vi /etc/rc.local

然后将下面内容复制进去
#+begin_src bash
#!/bin/sh -e
#
# rc.local
#
# This script is executed at the end of each multiuser runlevel.
# Make sure that the script will "exit 0" on success or any other
# value on error.
#
# In order to enable or disable this script just change the execution
# bits.
#
# By default this script does nothing.
#后台启动python工程，结果重定向到log.txt
/usr/bin/nohup /usr/bin/python start_person.py > log.txt 2>&1 &
exit 0
#+END_SRC

*** 给rc.local加上权限

sudo chmod +x /etc/rc.local

重启看看日志就可以.
** 解决python启动无法导入第三方模块问题
看日志发现导入模块失败，但是本地直接执行脚本没有任何问题

如 ImportError: No module named torch

分析

1. 在本地进入python交互环境，查看python搜索路径
#+begin_src bash
nvidia@nvidia-desktop:~/data/projects/dt_product$ python
Python 2.7.15+ (default, Nov 27 2018, 23:36:35)
[GCC 7.3.0] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import sys
>>> sys.path
['', '/home/nvidia/.local/lib/python2.7/site-packages', '/home/nvidia/data/projects/dt_product', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-aarch64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/gtk-2.0']
>>>
#+END_SRC
2. 新建一个python脚本设置为开机启动，在此脚本中获取python搜索路径
#+begin_src python
!test.py

import sys
print sys.path
#+END_SRC
重新启动环境查看启动重定向得日志 log.txt ：
#+BEGIN_EXAMPLE
['/home/nvidia/data/projects/dt_product', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-aarch64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/gtk-2.0']
#+END_EXAMPLE

解决方法：

一种是安装时注意下路径，一种是把这个路径加到环境变量中。

加入环境变量网上搜有几种，但是试了都不行，因为启动python项目时改环境变量未生效，所以最简单得就是在启动python项目前加入该变量。

打开刚才创建得启动python项目得文件，启动前加入环境变量

~sudo vi /etc/rc.local~

#+begin_src bash
#!/bin/sh -e
#
# rc.local
#
# This script is executed at the end of each multiuser runlevel.
# Make sure that the script will "exit 0" on success or any other
# value on error.
#
# In order to enable or disable this script just change the execution
# bits.
#
# By default this script does nothing.

#加入环境变量
export PYTHONPATH=/home/nvidia/.local/lib/python2.7/site-packages:$PYTHONPATH

#后台启动python工程，结果重定向到log.txt
/usr/bin/nohup /usr/bin/python start_person.py > log.txt 2>&1 &
exit 0
#+END_SRC

* ubuntu 设置root默认密码（初始密码）
ubuntu安装好后，root初始密码（默认密码）不知道，需要设置。

1、先用安装时候的用户登录进入系统

2、输入：sudo passwd  按回车

3、输入新密码，重复输入密码，最后提示passwd：password updated sucessfully

此时已完成root密码的设置

4、输入：su root

切换用户到root
* unzip命令
Linux unzip命令用于解压缩zip文件

语法:
#+begin_src bash
unzip [-cflptuvz][-agCjLMnoqsVX][-P <密码>][.zip文件][文件][-d <目录>][-x <文件>] 或 unzip [-Z]
#+END_SRC
参数：
#+BEGIN_EXAMPLE
-c 将解压缩的结果显示到屏幕上，并对字符做适当的转换。
-f 更新现有的文件。
-l 显示压缩文件内所包含的文件。
-p 与-c参数类似，会将解压缩的结果显示到屏幕上，但不会执行任何的转换。
-t 检查压缩文件是否正确。
-u 与-f参数类似，但是除了更新现有的文件外，也会将压缩文件中的其他文件解压缩到目录中。
-v 执行是时显示详细的信息。
-z 仅显示压缩文件的备注文字。
-a 对文本文件进行必要的字符转换。
-b 不要对文本文件进行字符转换。
-C 压缩文件中的文件名称区分大小写。
-j 不处理压缩文件中原有的目录路径。
-L 将压缩文件中的全部文件名改为小写。
-M 将输出结果送到more程序处理。
-n 解压缩时不要覆盖原有的文件。
-o 不必先询问用户，unzip执行后覆盖原有文件。
-P<密码> 使用zip的密码选项。
-q 执行时不显示任何信息。
-s 将文件名中的空白字符转换为底线字符。
-V 保留VMS的文件版本信息。
-X 解压缩时同时回存文件原来的UID/GID。
[.zip文件] 指定.zip压缩文件。
[文件] 指定要处理.zip压缩文件中的哪些文件。
-d<目录> 指定文件解压缩后所要存储的目录。
-x<文件> 指定不要处理.zip压缩文件中的哪些文件。
-Z unzip -Z等于执行zipinfo指令。
#+END_EXAMPLE

* Vmware虚拟机
NAT模式下，多个虚拟机都是连接到同一个网关地址
** ubuntu终端放大缩小
<Ctrl> + <->减小字号
<Ctrl> + <Shift> + <+>增大字号
终端设置
1、打开终端；
2、Edit→Profile Preferences
3、在General里最后一行设置默认大小

Ubuntu快速安装模式中，root用户默认不设置密码，可执行下面的命令为root设置密码
#+BEGIN_SRC bash
sudo passwd
#+END_SRC
** 硬盘扩容
1.扩展虚拟机硬盘大小（关机状态才能扩容）

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20171225160128418.png @ 2020-06-02 19:14:00
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-06-02_19-14-00_20171225160128418.png]]

2.安装修改文件大小的软件，此软件和Window上的DiskGenius用法相似。
#+BEGIN_SRC bash
sudo apt-get install gparted
#+END_SRC

打开gparted
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20171225160133078.png @ 2020-06-02 19:15:12
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-06-02_19-15-12_20171225160133078.png]]

3.和DiskGenius相同，只有相邻的空间时没有被分配的才能扩展空间大小，所以我们先删除/dev/sda2，保存修改（点击变成绿色的对号）。

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20171225160136902.png @ 2020-06-02 19:17:46
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-06-02_19-17-46_20171225160136902.png]]

4./dev/sda1之后的空间都是未分配的空间，我们可以把鼠标放在/dev/sda1，右键
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20171225160140891.png @ 2020-06-02 19:18:34
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-06-02_19-18-34_20171225160140891.png]]

鼠标拖动改变大小，或者直接在New size对应的文本框输入大小。预留部分空间给我们在第三步删除的交换分区。
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20171225160144569.png @ 2020-06-02 19:27:49
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-06-02_19-27-49_20171225160144569.png]]

5.鼠标放在剩余的未分配的空间，创建交换分区，保存修改。
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20171225160148123.png @ 2020-06-02 19:28:52
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-06-02_19-28-52_20171225160148123.png]]

6.sudo fdisk -l，/dev/sda1空间从40G扩展到了58G
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20171225160152672.png @ 2020-06-02 19:39:22
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-06-02_19-39-22_20171225160152672.png]]
** 解决VM Workstation安装VMware Tools显示灰色的办法
解决办法如下：
1.关闭虚拟机；

2.在虚拟机设置分别设置CD/DVD、CD/DVD2和软盘为自动检测三个步骤；

3.再重启虚拟机，灰色字即点亮。

如果上述步骤不行，就执行方法二：
1.关闭虚拟机；

2.在虚拟机将CD/DVD设置为VMware安装目录中的linux.iso（如ubuntu-16.04.5-desktop-i386.iso）、CD/DVD2设置和CD/DVD一样也是可以的（这里建议设置为自动检测，因为设置为自动检测也是可以的）和和软盘为自动检测三个步骤；

3.再重启虚拟机，灰色字即点亮。
** VMware 虚拟机如何连接网络
*** 一、首先查看自己的虚拟机服务有没有开启，选择电脑里面的服务查看；
1.计算机点击右键选择管理

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170103235137966.jpg @ 2020-10-29 15:25:51
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-25-51_20170103235137966.jpg]]

2.进入管理选择VM开头的服务如果没有开启的话就右键开启

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170103235345453.jpg @ 2020-10-29 15:26:15
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-26-15_20170103235345453.jpg]]
*** 二、虚拟机服务开启后就查看本地网络虚拟机的网卡启动没有
1.电脑右下角网络标志右键进入网络和共享中心
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170103235608659%20%281%29.jpg @ 2020-10-29 15:27:21
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-27-21_20170103235608659%2520%25281%2529.jpg]]

2.点击更改适配器，查看虚拟机的虚拟网卡启动没有，没有启动的话右键点击启动

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170103235743739.jpg @ 2020-10-29 15:27:39
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-27-39_20170103235743739.jpg]]

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170103235832318.jpg @ 2020-10-29 15:28:33
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-28-33_20170103235832318.jpg]]


3.网卡开启后设置ip地址，此处设置的ip和本机的ip没有关系，设置成你虚拟机里面运行的计算机需要的ip地址网段

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170104000142275.jpg @ 2020-10-29 15:28:43
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-28-43_20170104000142275.jpg]]
*** 三、此时你的本机设置完成了，该设置虚拟机了
1.打开虚拟机，选择你使用的操作系统打开详情页选择网络适配器，选择NAT模式并选择启动时连接，如下图；
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170104000401889.jpg @ 2020-10-29 15:29:52
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-29-52_20170104000401889.jpg]]
2.选择完后点击虚拟机页面上的编辑进入虚拟网络编辑器

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170104001142827.jpg @ 2020-10-29 15:30:06
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-30-06_20170104001142827.jpg]]

3.进来后会出现这个窗口，选择右下角更改设置，使用管理员进行修改
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170104001303104.jpg @ 2020-10-29 15:30:17
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-30-17_20170104001303104.jpg]]

4.更改完成后，更改下方的ip地址，此处的ip地址段和你在本机网络虚拟网卡（二-3）里面设置的ip要在一个网段里面,本机设置的是ip地址，而在此处设置的是ip网段
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170104001424828.jpg @ 2020-10-29 15:31:20
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-31-20_20170104001424828.jpg]]

5.选择DHCP,进行设置你的虚拟机分配虚拟计算机的ip地址范围

#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170104001826299.jpg @ 2020-10-29 15:31:41
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-31-41_20170104001826299.jpg]]

6.设置完DHCP后进行网关的设置，选择NAT设置，设置你虚拟计算机的网关地址。
#+DOWNLOADED: file:F%3A/org/%E5%9B%BE%E7%89%87/20170104001943940.jpg @ 2020-10-29 15:31:52
[[file:%E8%99%9A%E6%8B%9F%E6%9C%BA/2020-10-29_15-31-52_20170104001943940.jpg]]
*** 四、这时候，必要条件就已经配合结束了，开启虚拟计算机，进入IPv4的设置。
填写ip地址，IP地址要在你在虚拟机DHCP分配的ip地址（三-5）范围内
填写网关，就是在上面设置虚拟机网关的地址（三-6）
DNS服务器可以设置114.114.114.144 8.8.8.8 等。
*** 五、这时候基本就可以进行网络连接了，打开网页试一下，如果还连接不上，查看是否是哪一步没有设置对，在就重新启动虚拟计算机的网络。
** 修改时区和时间
~timedatectl set-timezone Asia/Shanghai~
* vim
[[https://vim.rtorr.com/lang/zh_cn][Vim Cheat Sheet]]
** 移动光标
h 或 向左箭头键(←) 光标向左移动一个字符
j 或 向下箭头键(↓) 光标向下移动一个字符
k 或 向上箭头键(↑) 光标向上移动一个字符
l 或 向右箭头键(→) 光标向右移动一个字符
[Ctrl] + [f] 屏幕『向下』移动一页，相当于 [Page Down]按键 (常用)
[Ctrl] + [b] 屏幕『向上』移动一页，相当于 [Page Up] 按键 (常用)
[Ctrl] + [d] 屏幕『向下』移动半页
[Ctrl] + [u] 屏幕『向上』移动半页
+ 光标移动到非空格符的下一列
- 光标移动到非空格符的上一列
n<space> 那个 n 表示『数字』，例如 20 。按下数字后再按空格键，光标会向右移动这一列的 n个字符。例如 20<space> 则光标会向后面移动 20 个字符距离。
0 或功能键[Home] 这是数字『 0 』：移动到这一列的最前面字符处 (常用)
$ 或功能键[End] 移动到这一列的最后面字符处(常用)
H 光标移动到这个屏幕的最上方那一列的第一个字符
M 光标移动到这个屏幕的中央那一列的第一个字符
L 光标移动到这个屏幕的最下方那一列的第一个字符
G 移动到这个文件的最后一列(常用)
nG n 为数字。移动到这个文件的第 n 列。例如 20G 则会移动到这个文件的第 20 列(可配合 :set nu)
gg 移动到这个文件的第一列，相当于 1G 啊！ (常用)
n<Enter> n 为数字。光标向下移动 n 列(常用)
** 搜寻与替换
r, R 进入取代模式(Replace mode)：r 只会取代光标所在的那一个字符一次；R 会一直取代光标所在的文字，直到按下 ESC为止；(常用)
/word 向光标之下寻找一个名称为 word 的字符串。例如要在文件内搜寻 vbird 这个字符串，就输入 /vbird 即可！ (常用)
?word 向光标之上寻找一个字符串名称为 word 的字符串。
n 这个 n 是英文按键。代表『重复前一个搜寻的动作』。举例来说， 如果刚刚我们执行/vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为vbird 的字符串！
N 这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 /vbird后，按下 N 则表示『向上』搜寻 vbird 。使用 /word 配合 n 及 N 是非常有帮助的！可以让你重复的找到一些你搜寻的关键词！
:n1,n2s/word1/word2/g n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则：『:100,200s/vbird/VBIRD/g』。(常用)
:1,$s/word1/word2/g 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用)。这个1和$都可以空着的，例如,$表示从当前行到最后一行
:1,$s/word1/word2/gc 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用)
:%s/word1/word2/g 全文替换
*** 关于替换\n换行符的问题
详细内容可参考文章:https://vim.fandom.com/wiki/Search_and_replace

字符串查找时，”\n” 是换行，”\r” 是回车，也就是经常会看到的 ^M。

字符串替换时，”\r” 是换行，’\n” 是空字符（0x00）。

When searching:
#+BEGIN_EXAMPLE
., *, \, [, ^, and $ are metacharacters.
+, ?, |, &, {, (, and ) must be escaped to use their special function.
\/ is / (use backslash + forward slash to search for forward slash)
\t is tab, \s is whitespace (space or tab)
\n is newline, \r is CR (carriage return = Ctrl-M = ^M)
After an opening [, everything until the next closing ] specifies a /collection. Character ranges can be represented with a -; for example a letter a, b, c, or the number 1 can be matched with [1a-c]. Negate the collection with [^ instead of [; for example [^1a-c] matches any character except a, b, c, or 1.
\{#\} is used for repetition. /foo.\{2\} will match foo and the two following characters. The \ is not required on the closing } so /foo.\{2} will do the same thing.
\(foo\) makes a backreference to foo. Parenthesis without escapes are literally matched. Here the \ is required for the closing \).
#+END_EXAMPLE
When replacing:
#+BEGIN_EXAMPLE
\r is newline, \n is a null byte (0x00).
\& is ampersand (& is the text that matches the search pattern).
\0 inserts the text matched by the entire pattern
\1 inserts the text of the first backreference. \2 inserts the second backreference, and so on.
#+END_EXAMPLE

**** 参考文章
[[http://www.leakon.com/archives/830/comment-page-1][VIM 替换 \n 换行符]]
** 删除、复制与粘贴
x, X 在一列字当中，x 为向后删除一个字符 (相当于 [del] 按键)， X 为向前删除一个字符(相当于 [backspace] 亦即是退格键) (常用)
nx n 为数字，连续向后删除 n 个字符。举例来说，我要连续删除 10 个字符， 『10x』。
dd 删除游标所在的那一整列(常用)
ndd n 为数字。删除光标所在的向下 n 列，例如 20dd 则是删除 20 列 (常用)
d1G 删除光标所在到第一列的所有数据
dG 删除光标所在到最后一列的所有数据
d$ 删除游标所在处，到该列的最后一个字符
d0 那个是数字的 0 ，删除游标所在处，到该列的最前面一个字符
yy 复制游标所在的那一列(常用)
nyy n 为数字。复制光标所在的向下 n 列，例如 20yy 则是复制 20 列(常用)
y1G 复制光标所在列到第一列的所有数据
yG 复制光标所在列到最后一列的所有数据
y0 复制光标所在的那个字符到该列行首的所有数据
y$ 复制光标所在的那个字符到该列行尾的所有数据
p, P p 为将已复制的数据在光标下一列贴上，P 则为贴在游标上一列！ 举例来说，我目前光标在第 20 列，且已经复制了 10 列数据。则按下 p 后， 那 10 列数据会贴在原本的 20列之后，亦即由 21 列开始贴。但如果是按下 P 呢？ 那么原本的第 20 列会被推到变成 30 列。 (常用)
J 将光标所在列与下一列的数据结合成同一列
c 重复删除多个数据，例如向下删除 10 列，[ 10cj ]
** 重复撤销
u 复原前一个动作。(常用)
[Ctrl]+r 重做上一个动作。(常用)
. 不要怀疑！这就是小数点！意思是重复前一个动作的意思。 如果你想要重复删除、重复贴上等等动作，按下小数点『.』就好了！ (常用)
** 插入
i, I 进入插入模式(Insert mode)：i 为『从目前光标所在处插入』， I 为『在目前所在列的第一个非空格符处开始插入』。(常用)
a, A 进入插入模式(Insert mode)：a 为『从目前光标所在的下一个字符处开始插入』， A 为『从光标所在列的最后一个字符处开始插入』。(常用)
o, O 进入插入模式(Insert mode)：这是英文字母 o 的大小写。o 为『在目前光标所在的下一列处插入新的一列』； O 为在目前光标所在处的上一列插入新的一列！(常用)
** 指令列模式的储存、离开等指令
:w 将编辑的数据写入硬盘文件中(常用)
:w! 若文件属性为『只读』时，强制写入该文件。不过，到底能不能写入， 还是跟你对该文件的文件权限有关啊！
:q 离开 vi (常用)
:q! 若曾修改过文件，又不想储存，使用 ! 为强制离开不储存文件。
:wq 储存后离开，若为 :wq! 则为强制储存后离开 (常用)
ZZ 这是大写的 Z 喔！若文件没有更动，则不储存离开，若文件已经被更动过，则储存后离开！
:w [filename] 将编辑的数据储存成另一个文件（类似另存新档）
:r [filename] 在编辑的数据中，读入另一个文件的数据。亦即将 『filename』 这个文件内容加到游标所在列后面
:n1,n2 w [filename] 将 n1 到 n2 的内容储存成 filename 这个文件。
:! command 暂时离开 vi 到指令列模式下执行 command 的显示结果！例如『:! ls /home』即可在 vi 当中察看 /home 底下以 ls 输出的文件信息！
** vim 环境的变更
:set nu 显示行号，设定之后，会在每一列的前缀显示该列的行号
:set nonu 与 set nu 相反，为取消行号！
** 区块选择(Visual Block)
v 字符选择，会将光标经过的地方反白选择！
V 列选择，会将光标经过的列反白选择！
[Ctrl]+v 区块选择，可以用长方形的方式选择资料
y 将反白的地方复制起来
d 将反白的地方删除掉
p 将刚刚复制的区块，在游标所在处贴上！
** 环境设定
vim 的环境设定参数有很多，如果你想要知道目前的设定值，可以在一般指令模式时输入:set all  来查阅

:set nu与:set nonu 就是设定与取消行号啊！
:set hlsearch与:set nohlsearch hlsearch 就是 high light search(高亮度搜寻)。 这个就是设定是否将搜寻的字符串反白的设定值。默认值是 hlsearch
:set autoindent与:set noautoindent 是否自动缩排？autoindent 就是自动缩排。
:set backup 是否自动储存备份档？一般是 nobackup 的， 如果设定 backup 的话，那么当你更动任何一个文件时，则源文件会被另存成一个档名为 filename~ 的文件。 举例来说，我们编辑 hosts ，设定 :set backup ，那么当更动 hosts 时，在同目录下，就会产生 hosts~文件名的文件，记录原始的 hosts 文件内容
:set ruler 还记得我们提到的右下角的一些状态栏说明吗？ 这个 ruler 就是在显示或不显示该设定值的啦！
:set showmode 这个则是，是否要显示 --INSERT-- 之类的字眼在左下角的状态栏。
:set backspace=(012) 一般来说， 如果我们按下 i 进入编辑模式后，可以利用退格键 (backspace) 来删除任意字符的。 但是，某些 distribution 则不许如此。此时，我们就可以透过 backspace来设定啰～ 当 backspace 为 2 时，就是可以删除任意值；0 或 1 时，仅可删除刚刚输入的字符，而无法删除原本就已经存在的文字了！
:set all 显示目前所有的环境参数设定值。
:set 显示与系统默认值不同的设定参数， 一般来说就是你有自行变动过的设定参数啦！
:syntax on与:syntax off 是否依据程序相关语法显示不同颜色？ 举例来说，在编辑一个纯文本档时，如果开头是以 # 开始，那么该列就会变成蓝色。 如果你懂得写程序，那么这个 :syntax on 还会主动的帮你除错呢！但是， 如果你仅是编写纯文本文件，要避免颜色对你的屏幕产生的干扰，则可以取消这个设定 。
:set bg=dark与:set bg=light 可用以显示不同的颜色色调，预设是『 light 』。如果你常常发现批注的字体深蓝色实在很不容易看， 那么这里可以设定为 dark 喔！试看看，会有不同的样式呢！
** 让Vim显示dos下的^M符号
参考文章：[[https://stackoverflow.com/questions/21228946/why-does-vim-sometimes-show-m-and-sometimes-not-even-if-they-are-there][Why does vim sometimes show ^M and sometimes not (even if they are there)?]]

When opening a file, vim tries to detect if it's a MS-DOS/Windows or a unix file. If all lines are terminated by \r\n, it's probably a DOS file, if only some of them are, vim may assume unix as well. If the file format is set to DOS, vim ignores \r when reading the file, and shows [dos] in the status line directly after reading the file.

我的理解是vim将文件当做dos文件，所以将\r\n作为换行符显示。在这里，vim正确地显示了文件的内容，但是其他的软件未必会正确地将\r\n为换行符，所以用其他的软件读取改文件，就会出现错误。为了能让vim将^M显示出来，只要将文件编码格式设置为unix即可。

vim中键入： :e ++ff=unix 即可让vim显示^M

设置文件编码格式:set ff=unix 然后保存:wq ，即可去除^M

The :e ++ff=unix command tells Vim to read the file again, forcing unix file format. 

To learn about ff, execute ":help ff"
* vscode 相关
** vscode的sftp
 ps：SFTP目前不能处理中文文件
*** SFTP原理
 SFTP原理是这样的：首先本地要有一个项目文件夹，同时远程也有一个项目文件夹，然后通过配置文件来同步二者。
 SFTP可以查看远程项目所有文件，但不能直接操作，必须操作本地项目文件，再同步到远程项目。

 现在我们本地和远程均有一个文件夹“sftpFolder”，用VsCode打开本地文件夹“sftpFolder”，然后执行 ctrl+shift+p ，搜索 SFTP:Config ，回车后，会生成一个“.vscode/sftp.json”，这个就是配置文件。
 同时，如下图左侧会多了一个“远程目录”。

 #+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-13_10-46-50.png @ 2021-10-13 10:47:31
 [[file:vs_code%E7%9A%84sftp/2021-10-13_10-47-31_Snipaste_2021-10-13_10-46-50.png]]
*** SFTP配置

 #+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-10-13_10-47-59.png @ 2021-10-13 10:48:26
 [[file:vs_code%E7%9A%84sftp/2021-10-13_10-48-26_Snipaste_2021-10-13_10-47-59.png]]
 配置文件不能写注释，所以这里说明一下其中几个属性：
 - uploadOnSave：本地更新文件保存会自动同步到远程文件（不会同步重命名文件和删除文件）
 - downloadOnOpen：从远程服务器下载打开的文件
 - ignore：忽略的文件（匹配的文件不会同步）
 - watcher：监听器（可以重命名文件和删除文件）
     - autoUpload：文件变更会自动同步（修改和重命名）
     - autoDelete：文件删除会自动同步
** 调试配置文件
VSCode 内置了对 Node.js 的调试支持，如果你需要调试其他语言如 C++、PHP、Python 等，可以在 VSCode 的插件市场安装对应的插件。

name：调试界面下拉选择项的名称

type：必填项，调试类型，是vscode用于计算调试代码需要用哪个扩展

request，必填项，有两种类型，分别是 launch 和 attach，前者的意思就是 VSCode 会打开这个程序然后进入调试，后者的意思是你已经打开了程序，然后接通程序的内部调试协议进行调试，

program，程序的启动入口

mode：可以设置为auto，debug，remote，test，exeC中的一个

program：调试程序的路径（绝对路径）

env：调试时使用的环境变量。例如：{"ENVNAME"："ENVVALUE"}

envFile：包含环境变量文件的绝对路径，在env中设置的属性会覆盖envFi1e中的配置

args：启动时的命令行参数

showLog：布尔值，是否将调试信息输出

logOutput：配置调试输出的组件（debugger，gdbwire，lldbout，debuglineerr，rpc），使用,分隔，showLog设置为true时}此项配置生效

buildFlags：构建go程序时传给go编译器的标志

remotePath：远程调试程序的绝对路径，当mode设置为remote时有效

runtimeExecutable，使用什么命令启动
*** Variables Reference
**** Predefined variables
The following predefined variables are supported:
- ${workspaceFolder} - the path of the folder opened in VS Code
- ${workspaceFolderBasename} - the name of the folder opened in VS Code without any slashes (/)
- ${file} - the current opened file
- ${fileWorkspaceFolder} - the current opened file's workspace folder
- ${relativeFile} - the current opened file relative to workspaceFolder
- ${relativeFileDirname} - the current opened file's dirname relative to workspaceFolder
- ${fileBasename} - the current opened file's basename
- ${fileBasenameNoExtension} - the current opened file's basename with no file extension
- ${fileDirname} - the current opened file's dirname
- ${fileExtname} - the current opened file's extension
- ${cwd} - the task runner's current working directory on startup
- ${lineNumber} - the current selected line number in the active file
- ${selectedText} - the current selected text in the active file
- ${execPath} - the path to the running VS Code executable
- ${defaultBuildTask} - the name of the default build task
- ${pathSeparator} - the character used by the operating system to separate components in file paths
***** Predefined variables examples#
Supposing that you have the following requirements:
1. A file located at /home/your-username/your-project/folder/file.ext opened in your editor;
2. The directory /home/your-username/your-project opened as your root workspace.
So you will have the following values for each variable:
- ${workspaceFolder} - /home/your-username/your-project
- ${workspaceFolderBasename} - your-project
- ${file} - /home/your-username/your-project/folder/file.ext
- ${fileWorkspaceFolder} - /home/your-username/your-project
- ${relativeFile} - folder/file.ext
- ${relativeFileDirname} - folder
- ${fileBasename} - file.ext
- ${fileBasenameNoExtension} - file
- ${fileDirname} - /home/your-username/your-project/folder
- ${fileExtname} - .ext
- ${lineNumber} - line number of the cursor
- ${selectedText} - text selected in your code editor
- ${execPath} - location of Code.exe
- ${pathSeparator} - / on macOS or linux, \ on Windows

#+BEGIN_EXAMPLE
Tip: Use IntelliSense inside string values for tasks.json and launch.json to get a full list of predefined variables.
#+END_EXAMPLE

*** 参考文章
[[https://www.barretlee.com/blog/2019/03/18/debugging-in-vscode-tutorial/][VSCode 调试中 launch.json 配置不完全指南]]
* watch命令
watch是一个非常实用的命令，基本所有的Linux发行版都带有这个小工具，如同名字一样，watch可以帮你监测一个命令的运行结果，省得你一遍遍的手动运行。在Linux下，watch是周期性的执行下个程序，并全屏显示执行结果。你可以拿他来监测你想要的一切命令的结果变化，比如 tail 一个 log 文件，ls 监测某个文件的大小变化。

命令格式：
watch[参数][命令]

命令参数：
- -n或--interval  watch缺省每2秒运行一下程序，可以用-n或-interval来指定间隔的时间。
- -d或--differences  用-d或--differences 选项watch 会高亮显示变化的区域。 而-d=cumulative选项会把变动过的地方(不管最近的那次有没有变动)都高亮显示出来。
- -t 或-no-title  会关闭watch命令在顶部的时间间隔,命令，当前时间的输出。
- -h, --help 查看帮助文档

FreeBSD和Linux下watch命令的不同，在Linux下，watch是周期性的执行下个程序，并全屏显示执行结果，如：watch -n 1 -d netstat -ant，而在FreeBSD下的watch命令是查看其它用户的正在运行的操作，watch允许你偷看其它terminal正在做什么，该命令只能让超级用户使用。
** 实例
#+begin_src bash
watch -n 1 -d netstat -ant       # 命令：每隔一秒高亮显示网络链接数的变化情况
watch -n 1 -d 'pstree|grep http' # 每隔一秒高亮显示http链接数的变化情况。 后面接的命令若带有管道符，需要加''将命令区域归整。
watch 'netstat -an | grep:21 | \ grep<模拟攻击客户机的IP>| wc -l' # 实时查看模拟攻击客户机建立起来的连接数
watch -d 'ls -l|grep scf'       # 监测当前目录中 scf' 的文件的变化
watch -n 10 'cat /proc/loadavg' # 10秒一次输出系统的平均负载
watch uptime
watch -t uptime
watch -d -n 1 netstat -ntlp
watch -d 'ls -l | fgrep goface'     # 监测goface的文件
watch -t -differences=cumulative uptime
watch -n 60 from            # 监控mail
watch -n 1 "df -i;df"       # 监测磁盘inode和block数目变化情况
#+END_SRC
* wget
** 介绍
Linux系统中的wget是一个下载文件的工具，它用在命令行下。对于Linux用户是必不可少的工具，我们经常要下载一些软件或从远程服务器恢复备份到本地服务器。wget支持HTTP，HTTPS和FTP协议，可以使用HTTP代理。所谓的自动下载是指，wget可以在用户退出系统的之后在后台执行。这意味这你可以登录系统，启动一个wget下载任务，然后退出系统，wget将在后台执行直到任务完成，相对于其它大部分浏览器在下载大量数据时需要用户一直的参与，这省去了极大的麻烦。

wget 可以跟踪HTML页面上的链接依次下载来创建远程服务器的本地版本，完全重建原始站点的目录结构。这又常被称作”递归下载”。在递归下载的时候，wget 遵循Robot Exclusion标准(/robots.txt). wget可以在下载的同时，将链接转换成指向本地文件，以方便离线浏览。

wget 非常稳定，它在带宽很窄的情况下和不稳定网络中有很强的适应性.如果是由于网络的原因下载失败，wget会不断的尝试，直到整个文件下载完毕。如果是服务器打断下载过程，它会再次联到服务器上从停止的地方继续下载。这对从那些限定了链接时间的服务器上下载大文件非常有用。

用于从网络上下载资源，没有指定目录，下载资源回默认为当前目录。wget虽然功能强大，但是使用起来还是比较简单：

wget优点：
1）支持断点下传功能；这一点，也是网络蚂蚁和FlashGet当年最大的卖点，现在，Wget也可以使用此功能，那些网络不是太好的用户可以放心了；

2）同时支持FTP和HTTP下载方式；尽管现在大部分软件可以使用HTTP方式下载，但是，有些时候，仍然需要使用FTP方式下载软件；

3）支持代理服务器；对安全强度很高的系统而言，一般不会将自己的系统直接暴露在互联网上，所以，支持代理是下载软件必须有的功能；

4）设置方便简单；可能，习惯图形界面的用户已经不是太习惯命令行了，但是，命令行在设置上其实有更多的优点，最少，鼠标可以少点很多次，也不要担心是否错点鼠标；

5）程序小，完全免费；程序小可以考虑不计，因为现在的硬盘实在太大了；完全免费就不得不考虑了，即使网络上有很多所谓的免费软件，但是，这些软件的广告却不是我们喜欢的。
** 参数
*** 启动参数：
-V, –version 显示wget的版本后退出

-h, –help 打印语法帮助

-b, –background 启动后转入后台执行

-e, –execute=COMMAND 执行`.wgetrc’格式的命令，wgetrc格式参见/etc/wgetrc或~/.wgetrc
*** 记录和输入文件参数：

-o, –output-file=FILE 把记录写到FILE文件中

-a, –append-output=FILE 把记录追加到FILE文件中

-d, –debug 打印调试输出

-q, –quiet 安静模式(没有输出)

-v, –verbose 冗长模式(这是缺省设置)

-nv, –non-verbose 关掉冗长模式，但不是安静模式

-i, –input-file=FILE 下载在FILE文件中出现的URLs

-F, –force-html 把输入文件当作HTML格式文件对待

-B, –base=URL 将URL作为在-F -i参数指定的文件中出现的相对链接的前缀

–sslcertfile=FILE 可选客户端证书

–sslcertkey=KEYFILE 可选客户端证书的KEYFILE

–egd-file=FILE 指定EGD socket的文件名
*** 下载参数：

–bind-address=ADDRESS 指定本地使用地址(主机名或IP，当本地有多个IP或名字时使用)

-t, –tries=NUMBER 设定最大尝试链接次数(0 表示无限制).

-O –output-document=FILE 把文档写到FILE文件中

-nc, –no-clobber 不要覆盖存在的文件或使用.#前缀

-c, –continue 接着下载没下载完的文件

–progress=TYPE 设定进程条标记

-N, –timestamping 不要重新下载文件除非比本地文件新

-S, –server-response 打印服务器的回应

–spider 不下载任何东西

-T, –timeout=SECONDS 设定响应超时的秒数

-w, –wait=SECONDS 两次尝试之间间隔SECONDS秒

–waitretry=SECONDS 在重新链接之间等待1…SECONDS秒

–random-wait 在下载之间等待0…2*WAIT秒

-Y, –proxy=on/off 打开或关闭代理

-Q, –quota=NUMBER 设置下载的容量限制

–limit-rate=RATE 限定下载输率
*** 目录参数：

-nd –no-directories 不创建目录

-x, –force-directories 强制创建目录

-nH, –no-host-directories 不创建主机目录

-P, –directory-prefix=PREFIX 将文件保存到目录 PREFIX/…

–cut-dirs=NUMBER 忽略 NUMBER层远程目录

*** HTTP 选项参数：

–http-user=USER 设定HTTP用户名为 USER.

–http-passwd=PASS 设定http密码为 PASS

-C, –cache=on/off 允许/不允许服务器端的数据缓存 (一般情况下允许)

-E, –html-extension 将所有text/html文档以.html扩展名保存

–ignore-length 忽略 `Content-Length’头域

–header=STRING 在headers中插入字符串 STRING

–proxy-user=USER 设定代理的用户名为 USER

–proxy-passwd=PASS 设定代理的密码为 PASS

–referer=URL 在HTTP请求中包含 `Referer: URL’头

-s, –save-headers 保存HTTP头到文件

-U, –user-agent=AGENT 设定代理的名称为 AGENT而不是 Wget/VERSION

–no-http-keep-alive 关闭 HTTP活动链接 (永远链接)

–cookies=off 不使用 cookies

–load-cookies=FILE 在开始会话前从文件 FILE中加载cookie

–save-cookies=FILE 在会话结束后将 cookies保存到 FILE文件中
*** FTP 选项参数：

-nr, –dont-remove-listing 不移走 `.listing’文件

-g, –glob=on/off 打开或关闭文件名的 globbing机制

–passive-ftp 使用被动传输模式 (缺省值).

–active-ftp 使用主动传输模式

–retr-symlinks 在递归的时候，将链接指向文件(而不是目录)
*** 递归下载参数：

-r, –recursive 递归下载－－慎用!

-l, –level=NUMBER 最大递归深度 (inf 或 0 代表无穷)

–delete-after 在现在完毕后局部删除文件

-k, –convert-links 转换非相对链接为相对链接

-K, –backup-converted 在转换文件X之前，将之备份为 X.orig

-m, –mirror 等价于 -r -N -l inf -nr

-p, –page-requisites 下载显示HTML文件的所有图片

递归下载中的包含和不包含(accept/reject)：

-A, –accept=LIST 分号分隔的被接受扩展名的列表

-R, –reject=LIST 分号分隔的不被接受的扩展名的列表

-D, –domains=LIST 分号分隔的被接受域的列表

–exclude-domains=LIST 分号分隔的不被接受的域的列表

–follow-ftp 跟踪HTML文档中的FTP链接

–follow-tags=LIST 分号分隔的被跟踪的HTML标签的列表

-G, –ignore-tags=LIST 分号分隔的被忽略的HTML标签的列表

-H, –span-hosts 当递归时转到外部主机

-L, –relative 仅仅跟踪相对链接

-I, –include-directories=LIST 允许目录的列表

-X, –exclude-directories=LIST 不被包含目录的列表

-np, –no-parent 不要追溯到父目录

wget -S –spider url 不下载只显示过程
* xargs（用来给命令传递参数）
xargs（英文全拼： eXtended ARGuments）是给命令传递参数的一个过滤器，也是组合多个命令的一个工具。

xargs 可以将管道或标准输入（stdin）数据转换成命令行参数，也能够从文件的输出中读取数据。

xargs 也可以将单行或多行文本输入转换为其他格式，例如多行变单行，单行变多行。

xargs 默认的命令是 echo，这意味着通过管道传递给 xargs 的输入将会包含换行和空白，不过通过 xargs 的处理，换行和空白将被空格取代。

xargs 是一个强有力的命令，它能够捕获一个命令的输出，然后传递给另外一个命令。

之所以能用到这个命令，关键是由于很多命令不支持|管道来传递参数，而日常工作中有有这个必要，所以就有了 xargs 命令，例如：
#+begin_src bash
find /sbin -perm +700 |ls -l       #这个命令是错误的
find /sbin -perm +700 |xargs ls -l   #这样才是正确的
#+END_SRC

命令格式：
somecommand |xargs -item  command
** 参数
-a file 从文件中读入作为 stdin
-e flag ，注意有的时候可能会是-E，flag必须是一个以空格分隔的标志，当xargs分析到含有flag这个标志的时候就停止。
-p 当每次执行一个argument的时候询问一次用户。
-n num 后面加次数，表示命令在执行的时候一次用的argument的个数，默认是用所有的。
-t 表示先打印命令，然后再执行。
-i 或者是-I，这得看linux支持了，将xargs的每项名称，一般是一行一行赋值给 {}，可以用 {} 代替。
-r no-run-if-empty 当xargs的输入为空的时候则停止xargs，不用再去执行了。
-s num 命令行的最大字符数，指的是 xargs 后面那个命令的最大命令行字符数。
-L num 从标准输入一次读取 num 行送给 command 命令。
-l 同 -L。
-d delim 分隔符，默认的xargs分隔符是回车，argument的分隔符是空格，这里修改的是xargs的分隔符。
-x exit的意思，主要是配合-s使用。。
-P 修改最大的进程数，默认是1，为0时候为as many as it can ，这个例子我没有想到，应该平时都用不到的吧。
-0 不仅可以将分隔符从默认的空格变成 NULL，还会将单引号、双引号、反斜线等统统默认为是普通字符
** 参数使用例子
*** -E
我们正在处理一份日志文件 country.list 中的内容，将日志文件中的字符以空行作为分隔符依次 echo 出来，一旦遇到 korea 便终止退出：
#+BEGIN_SRC bash
[roc@roclinux ~]$ echo "china usa korea japan" > country.list
 
[roc@roclinux ~]$ cat country.list
china usa korea japan
 
[roc@roclinux ~]$ cat country.list | xargs -E 'korea' echo
china usa
#+END_SRC
*** -p
#+BEGIN_SRC bash
[roc@roclinux ~]$ find . -type f |xargs -p rm -f
rm -f ./china.txt ./usa.txt ./japan.txt ?...n
#+END_SRC
*** -n
#+BEGIN_SRC bash
[roc@roclinux 20160408]$ find . -type f |xargs -p -n 1 rm -f
rm -f ./china.txt ?...n
rm -f ./usa.txt ?...y
rm -f ./japan.txt ?...n
#+END_SRC
** 与管道的区别
管道可以实现：将前面的标准输出作为后面的“标准输入”。

管道无法实现：将前面的标准输出作为后面的“命令参数”。

** 注意事项
xargs 的标准输入中出现的“换行符、空格、制表符”都将被空格取代。下面来看一个带有换行符的例子：
#+BEGIN_SRC bash
#我们准备好了带有换行的标准输入
[roc@roclinux ~]$ echo -e "china.txt\njapan.txt"
china.txt
japan.txt
 
#可见, 换行符和空格的作用一样
[roc@roclinux ~]$ echo -e "china.txt\njapan.txt" | xargs cat
hello beijing
hello tokyo
#+END_SRC
* xmodmap
First, generate the current keycode map:

xmodmap -pke > .Xmodmap

To detect what the keycode is for specific key, use command:
$ xev -event keyboard

Now save the file, and activate the key map:
$ xmodmap .Xmodmap

[[https://dev.to/0xbf/remap-keys-in-the-keyboard-in-ubuntu-5a36][Remap keys in the keyboard in Ubuntu]]
* zsh
** 安装zsh
如果你用 Redhat Linux，执行：sudo yum install zsh

如果你用 Ubuntu Linux，执行：sudo apt-get install zsh

安装完成后设置当前用户使用 zsh：chsh -s /bin/zsh，根据提示输入当前用户的密码就可以了。
** 安装oh my zsh
安装「oh my zsh」可以自动安装也可以手动安装。

自动安装：
#+begin_src bash
wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | sh
#+END_SRC
手动安装：
#+begin_src bash
git clone git://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh
cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc
#+END_SRC

都不复杂，安装完成之后退出当前会话重新打开一个终端窗口，你就可以见到这个彩色的提示了：

#+DOWNLOADED: screenshot @ 2021-12-11 10:24:52
[[file:images/linux%E7%AC%94%E8%AE%B0/zsh/2021-12-11_10-24-52_screenshot.png]]
** 启动文件顺序 
对于Zsh，其不同方式的执行顺序如下所示。【请注意，如果不存在〜/.zshrc，则zsh似乎也会读取〜/.profile】

#+DOWNLOADED: screenshot @ 2021-12-11 10:04:54
[[file:images/linux%E7%AC%94%E8%AE%B0/%E5%90%AF%E5%8A%A8%E6%96%87%E4%BB%B6/2021-12-11_10-04-54_screenshot.png]]
** autojump安装
安装[[https://github.com/wting/autojump][autojump官网]]的安装方式，先安装autojump
#+begin_src bash
# 下载插件autojump到/.oh-my-zsh/custom目录中
git clone https://gitee.com/null_454_5218/autojump.git
# 解压缩后进入目录，执行install.py
./install.py
#+END_SRC
配置zshrc文件
#+begin_src bash
vi ~/.zshrc
#在配置结尾处或者参考上面处添加此行命令
plugins=(
  git
  zsh-autosuggestions
  autojump
)
#+END_SRC

除了还要在~/.zshrc中配置plugins，还要将下面这句写到zshrc文件中：
#+begin_src bash
[[ -s ~/.autojump/etc/profile.d/autojump.sh ]] && . ~/.autojump/etc/profile.d/autojump.sh
#+END_SRC
*** 参考文章
[[https://zhuanlan.zhihu.com/p/19556676][终极 Shell——ZSH]]
[[https://my.oschina.net/u/4264517/blog/4522246][搞机： oh-my-zsh + autojump + screen 让你的终端起飞]]
** 参考文章
[[https://zhuanlan.zhihu.com/p/19556676][终极 Shell——ZSH]]
* 统计文件个数
统计当前文件夹下文件的个数，包括子文件夹里的
#+BEGIN_SRC bash
ls -lR|grep "^-"|wc -l
#+END_SRC
统计文件夹下目录的个数，包括子文件夹里的
#+BEGIN_SRC bash
ls -lR|grep "^d"|wc -l
#+END_SRC
统计当前文件夹下文件的个数
#+BEGIN_SRC bash
ls -l |grep "^-"|wc -l
#+END_SRC
统计当前文件夹下目录的个数
#+BEGIN_SRC bash
ls -l |grep "^d"|wc -l
#+END_SRC
附：
统计输出信息的行数
#+BEGIN_SRC bash
wc -l
#+END_SRC
将长列表输出信息过滤一部分，只保留一般文件，如果只保留目录就是 ^d
#+BEGIN_SRC bash
grep "^-"
#+END_SRC
* 查看GPU使用情况
Nvidia自带了一个nvidia-smi的命令行工具，会显示显存使用情况：
#+BEGIN_SRC bash
>>>nvidia-smi
#+END_SRC
nvidia-smi是nvidia 的系统管理界面 ，其中smi是System management interface的缩写，它可以收集各种级别的信息，查看显存使用情况。此外, 可以启用和禁用 GPU 配置选项 (如 ECC 内存功能)。
** 常用的Nvidia-smi指令
*** nvidia-smi
【功能】 显示出当前GPU的所有基础信息。

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_09-36-20.png @ 2021-11-22 09:36:35
[[file:%E6%9F%A5%E7%9C%8BGPU%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5/2021-11-22_09-36-35_Snipaste_2021-11-22_09-36-20.png]]

解释相关参数含义：
- GPU：本机中的GPU编号
- Name：GPU 类型
- Persistence-M：
- Fan：风扇转速
- Temp：温度，单位摄氏度
- Perf：表征性能状态，从P0到P12，P0表示最大性能，P12表示状态最小性能
- Pwr:Usage/Cap：能耗表示
- Bus-Id：涉及GPU总线的相关信息；
- Disp.A：Display Active，表示GPU的显示是否初始化
- Memory-Usage：显存使用率
- Volatile GPU-Util：浮动的GPU利用率
- Uncorr. ECC：关于ECC的东西
- Compute M.：计算模式
- Processes 显示每块GPU上每个进程所使用的显存情况。
*** nvidia-smi -L 命令
【功能】 列出所有可用的 NVIDIA 设备

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_09-37-45.png @ 2021-11-22 09:37:49
[[file:%E6%9F%A5%E7%9C%8BGPU%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5/2021-11-22_09-37-49_Snipaste_2021-11-22_09-37-45.png]]
*** nvidia-smi topo --matrix 命令
【功能】查看系统拓扑

【说明】 要正确地利用更先进的 NVIDIA GPU 功能 (如 GPUDirect)，使用系统拓扑正确配置往往是至关重要的。该拓扑指的是 PCI Express 设备 (GPUs, InfiniBand HCAs, storage controllers, 等) 如何互相连接以及如何连接到系统的CPU。如果使用不正确的拓扑, 某些功能可能会减慢甚至停止工作

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_09-38-18.png @ 2021-11-22 09:38:23
[[file:%E6%9F%A5%E7%9C%8BGPU%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5/2021-11-22_09-38-23_Snipaste_2021-11-22_09-38-18.png]]
*** nvidia-smi -q -d CLOCK 命令
【功能】查看当前的 GPU 时钟速度、默认时钟速度和最大可能的时钟速度

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_09-38-47.png @ 2021-11-22 09:38:52
[[file:%E6%9F%A5%E7%9C%8BGPU%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5/2021-11-22_09-38-52_Snipaste_2021-11-22_09-38-47.png]]
*** nvidia-smi -q -d SUPPORTED_CLOCKS 
【功能】显示每个 GPU 的可用时钟速度列表

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_09-39-20.png @ 2021-11-22 09:39:25
[[file:%E6%9F%A5%E7%9C%8BGPU%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5/2021-11-22_09-39-25_Snipaste_2021-11-22_09-39-20.png]]
*** nvidia-smi vgpu
【功能】 查看当前vGPU的状态信息：
#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_09-39-47.png @ 2021-11-22 09:39:52
[[file:%E6%9F%A5%E7%9C%8BGPU%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5/2021-11-22_09-39-52_Snipaste_2021-11-22_09-39-47.png]]

【补充说明】 虚拟图形处理单元（vGPU）是在虚拟桌面上渲染图形的一个组件。倘若没有此组件，显示如下：

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_09-40-08.png @ 2021-11-22 09:40:12
[[file:%E6%9F%A5%E7%9C%8BGPU%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5/2021-11-22_09-40-12_Snipaste_2021-11-22_09-40-08.png]]
*** nvidia-smi vgpu -p 
【功能】循环显示虚拟桌面中应用程序对GPU资源的占用情况

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_09-40-41.png @ 2021-11-22 09:40:47
[[file:%E6%9F%A5%E7%9C%8BGPU%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5/2021-11-22_09-40-47_Snipaste_2021-11-22_09-40-41.png]]
*** nvidia-smi -q
【功能】 查看当前所有GPU的信息，也可以通过参数i指定具体的GPU。
#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_09-41-04.png @ 2021-11-22 09:41:09
[[file:%E6%9F%A5%E7%9C%8BGPU%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5/2021-11-22_09-41-09_Snipaste_2021-11-22_09-41-04.png]]

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_09-41-31.png @ 2021-11-22 09:41:36
[[file:%E6%9F%A5%E7%9C%8BGPU%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5/2021-11-22_09-41-36_Snipaste_2021-11-22_09-41-31.png]]

通过nvidia-smi -q 我们可以获取以下有用的信息：
- 系统中的GPU的基本信息
- GPU的SN号、VBIOS、PN号等信息：
- GPU的总线、PCI-E总线倍速、风扇转速等信息：
- 补充： PCI是Peripheral Component Interconnect(外设部件互连标准)的缩写，它是目前个人电脑中使用最为广泛的接口，几乎所有的主板产品上都带有这种插槽。


#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_09-42-08.png @ 2021-11-22 09:42:13
[[file:%E6%9F%A5%E7%9C%8BGPU%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5/2021-11-22_09-42-13_Snipaste_2021-11-22_09-42-08.png]]

GPU的显存、BAR1、所有资源利用率、ECC模式等信息：

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_09-42-27.png @ 2021-11-22 09:42:32
[[file:%E6%9F%A5%E7%9C%8BGPU%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5/2021-11-22_09-42-32_Snipaste_2021-11-22_09-42-27.png]]
*** 参考文章
[[https://blog.csdn.net/C_chuxin/article/details/82993350][Nvidia-smi简介及常用指令及其参数说明]]
* 解压缩命令
#+BEGIN_SRC bash
#压缩
tar -jcv -f filename.tar.bz2 要被压缩的文件或目录名称
#查询
tar -jtv -f filename.tar.bz2
#解压缩
tar -jxv -f filename.tar.bz2 -C 欲解压缩的目录
#+END_SRC

* 搭建c语言环境
** gcc和g++的区别
gcc 最开始的时候是 GNU C Compiler, 如你所知，就是一个c编译器。但是后来因为这个项目里边集成了更多其他不同语言的编译器，GCC就代表 the GNU Compiler Collection，所以表示一堆编译器的合集。

g++则是GCC的c++编译器。现在你在编译代码时调用的gcc，已经不是当初那个c语言编译器了，更确切的说他是一个驱动程序，根据代码的后缀名来判断调用c编译器还是c++编译器 (g++)。比如你的代码后缀是*.c，他会调用c编译器还有linker去链接c的library。如果你的代码后缀是cpp, 他会调用g++编译器，当然library call也是c++版本的。
** 安装gcc和g++
#+BEGIN_SRC bash
sudo apt-get install gcc
sudo apt-get install gcc
#+END_SRC
* 设置合上笔记本盖子不休眠的方法
编辑下列文件：sudo gedit /etc/systemd/logind.conf
#+BEGIN_EXAMPLE
#HandlePowerKey按下电源键后的行为，默认power off
#HandleSleepKey 按下挂起键后的行为，默认suspend
#HandleHibernateKey按下休眠键后的行为，默认hibernate
#HandleLidSwitch合上笔记本盖后的行为，默认suspend（改为ignore；即合盖不休眠）在原文件中，还要去掉前面的#
#+END_EXAMPLE
最后重启服务

service systemd-logind restart
* 设置默认启动方式（图形界面或命令行界面）
在root用户权限下：

查看当前启动模式

systemctl get-default

更改模式命令：

systemctl set-default graphical.target由命令行模式更改为图形界面模式

systemctl set-default multi-user.target由图形界面模式更改为命令行模式

跟以前使用的linux版本一样，编辑 vi /etc/inittab 文件，修改系统初始化方式
* 设置Linux在未登录账号情况下自动连接wifi
期望机器能在通电进入系统后，即使没有登录账号也能自动连接wifi。可以使用Linux的网络管理工具的命令：

nmctl device wifi connect [ssid wifi名字] password [wifi密码]

* 查看Linux系统架构类型的方法
** uname 命令
uname -a 命令可以直接显示 Linux 系统架构的命令，安几乎可以工作在所有 Linux/Unix 系统当中。

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_16-26-12.png @ 2021-11-22 16:26:17
[[file:%E6%9F%A5%E7%9C%8BLinux%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%96%B9%E6%B3%95/2021-11-22_16-26-17_Snipaste_2021-11-22_16-26-12.png]]
** dpkg 命令
dpkg 的命令可用于查看 Debian/ Ubuntu 操作系统是 32 位还是 64 位，此命令只适用于基于 Debian 和 Ubuntu 的 Linux 发行版。

在终端中执行如下命令：
#+begin_src bash
dpkg --print-architecture
#+END_SRC

如果当前 Linux 是 64 位则输出 amd64，是 32 位则会输出 i386。
** getconf 命令
getconf 命令主要用于显示系统变量配置，我们也可使用如下参数来查看 Linux 系统架构：
#+begin_src bash
getconf LONG_BIT
#+END_SRC
** arch 命令
arch 命令主要用于显示操作系统架构类型，与 uname -m 命令非常类似。如果输出 x86_64 则表示为 64 位系统，如果输出 i686 或 i386 则表示为 32 位系统。

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_16-27-10.png @ 2021-11-22 16:27:15
[[file:%E6%9F%A5%E7%9C%8BLinux%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%96%B9%E6%B3%95/2021-11-22_16-27-14_Snipaste_2021-11-22_16-27-10.png]]
** file 命令
file 命令可以配合 /sbin/init 这个特殊参数来查看系统架构类型（/sbin/init 在 Ubuntu 15.10 里面是链接到 /lib/systemd/systemd 的）：
#+begin_src bash
file /sbin/init
#+END_SRC

#+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-11-22_16-27-37.png @ 2021-11-22 16:27:42
[[file:%E6%9F%A5%E7%9C%8BLinux%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%96%B9%E6%B3%95/2021-11-22_16-27-42_Snipaste_2021-11-22_16-27-37.png]]

** 参考文章
[[https://www.sysgeek.cn/find-out-linux-system-32-or-64-bit/][查看Linux系统架构类型的5条常用命令]]
* 启动文件
** bash
bash检查的启动文件取决于你启动bash shell的方式。启动bash shell有3种方式：
- 登录时作为默认登录shell
- 作为非登录shell的交互式shell
- 作为运行脚本的非交互shell

对于Bash，它们的工作方式如下。读取适当的列。执行A，然后执行B，然后执行C，依此类推。B1，B2，B3表示仅执行找到的那些文件中的第一个。

#+DOWNLOADED: screenshot @ 2021-12-11 10:03:53
[[file:images/linux%E7%AC%94%E8%AE%B0/%E5%90%AF%E5%8A%A8%E6%96%87%E4%BB%B6/2021-12-11_10-03-53_screenshot.png]]

*** 登录shell
 当你登录Linux系统时，bash shell会作为登录shell启动。登录shell会从5个不同的启动文件里读取命令：
 - /etc/profile
 - $HOME/.bash_profile
 - $HOME/.bashrc
 - $HOME/.bash_login
 - $HOME/.profile

 /etc/profile文件是系统上默认的bash shell的主启动文件。系统上的每个用户登录时都会执行这个启动文件

 /etc/profile里的for循环会迭代/etc/profile.d目录下的所有文件。

 $HOME目录下的启动文件都起着同一个作用：提供一个用户专属的启动文件来定义该用户所用到的环境变量。大多数Linux发行版只用这四个启动文件中的一到两个：
 - $HOME/.bash_profile
 - $HOME/.bashrc
 - $HOME/.bash_login
 - $HOME/.profile

 shell会按照按照下列顺序，运行第一个被找到的文件，余下的则被忽略：
 - $HOME/.bash_profile
 - $HOME/.bash_login
 - $HOME/.profile

 注意，这个列表中并没有$HOME/.bashrc文件。这是因为该文件通常通过其他文件运行的。
*** 交互式 shell 进程
 如果你的bash shell不是登录系统时启动的（比如是在命令行提示符下敲入bash时启动），那么你启动的shell叫作交互式shell。

 如果bash是作为交互式shell启动的，它就不会访问/etc/profile文件，只会检查用户HOME目录中的.bashrc文件。
*** 非交互式 shell
 系统执行shell脚本时用的就是非交互式shell。它没有命令行提示符。

 当shell启动一个非交互式shell进程时，它会检查环境变量BASH_ENV来查看要执行的启动文件。如果有指定的文件，shell会执行该文件里的命令，这通常包括shell脚本变量设置
** zsh 
对于Zsh，其不同方式的执行顺序如下所示。【请注意，如果不存在〜/.zshrc，则zsh似乎也会读取〜/.profile】

#+DOWNLOADED: screenshot @ 2021-12-11 10:04:54
[[file:images/linux%E7%AC%94%E8%AE%B0/%E5%90%AF%E5%8A%A8%E6%96%87%E4%BB%B6/2021-12-11_10-04-54_screenshot.png]]

** 参考文章
[[https://www.xth8013.com/website/blogArticle/detail/163][Bash/Zsh启动文件加载顺序]]

* 修改某用户默认shell
** 临时修改shell
我们可以对当前使用的shell进行改变，直接调用不同sheel名称进入到shell环境中去。

使用命令cat /etc/shells得到当前系统支持的shell环境。

使用echo $SHELL可得到当前的shell环境。
** 修改用户默认的shel
在linux系统的/etc/passwd文件内是保存系统内所有用户和用户的设置。

对某用户的默认设置也在这里。

可以使用chsh命令修改某用户的默认shell：
#+begin_src bash
merle@bogon ~ $ chsh                                                                                                                
Changing shell for merle.
New shell [/bin/bash]: /bin/zsh
密码：
Shell changed.

# 再次查看该用户设置
merle@bogon ~ $ grep merle /etc/passwd                                                                                               
merle:x:1000:1000:merle:/home/merle:/bin/zsh    
#+END_SRC
也可以使用usermod命令：
#+begin_src bash
usermod -s /bin/zsh merle

# 再次查看该用户设置
merle@bogon ~ $ grep merle /etc/passwd                                                                                               
merle:x:1000:1000:merle:/home/merle:/bin/zsh    
#+END_SRC
* 解决 Windows 传入 linux 出现的 ^M 问题
通过查询得知，其问题根源是windows和linux换行符不同造成，二者区别如下表（外加了mac book）

|        | windows | linux | MacBook |
|--------+---------+-------+---------|
| 换行符 | \r\n    | \n    | \r      |
| ASCII  | 0x0d0a  | 0x0a  | 0x0d    |

其中：
- "\r"在ASCII中表示“换行（LF）”
- "\n"在ASCII中表示“回车（CR）”

Dos、Windows 格式的文件，用 0D 0A (CR+LF)作为换行符，而Unix 的则是以0A(LF) 作为换行符，所以dos 底下的文本文件到了unix的话，换行符就会多出来一个 0D(CR) 显示为 ^M

解决方法如下：

1. 使用命令dos2unix对文件进行转换

2. 使用vi的替换功能。启动vi，进入命令模式，输入以下命令:
#+BEGIN_EXAMPLE
:%s/^M$//g         # 去掉行尾的^M,注意^M 不是shift ^ +M 而是ctrl+v 加上ctrl+m,此命令必须是手动打上，不可复制
:%s/^M//g          # 去掉所有的^M
:%s/^M/[ctrl-v]+[enter]/g        # 将^M替换成回车
:%s/^M/\r/g                      # 将^M替换成回车
#+END_EXAMPLE

3. 使用sed命令：
#+begin_src bash
$ sed -e‘s/^M/\n/g’upgrade.sh
#+END_SRC
* X11转发
** X 协议原理简介
#+DOWNLOADED: screenshot @ 2022-03-05 12:48:01
[[file:images/linux%E7%AC%94%E8%AE%B0/X11%E8%BD%AC%E5%8F%91/2022-03-05_12-48-01_screenshot.png]]

Linux 本身是没有图形化界面的，所谓的图形化界面系统只不过中 Linux 下的应用程序。这一点和 Windows 不一样。Windows 从 Windows 95 开始，图形界面就直接在系统内核中实现了，是操作系统不可或缺的一部分。Linux 的图形化界面，底层都是基于 X 协议。

X 协议由 X server 和 X client 组成：

X server 管理主机上与显示相关的硬件设置（如显卡、硬盘、鼠标等），它负责屏幕画面的绘制与显示，以及将输入设置（如键盘、鼠标）的动作告知 X client。

X client (即 X 应用程序) 则主要负责事件的处理（即程序的逻辑）。

举个例子，如果用户点击了鼠标左键，因为鼠标归 X server 管理，于是 X server 就捕捉到了鼠标点击这个动作，然后它将这个动作告诉 X client，因为 X client 负责程序逻辑，于是 X client 就根据程序预先设定的逻辑（例如画一个圆），告诉 X server 说：“请在鼠标点击的位置，画一个圆”。最后，X server 就响应 X client 的请求，在鼠标点击的位置，绘制并显示出一个圆。

** 服务器x（X client）设置
首先要确保服务器的sshd打开X11转发功能

打开 /etc/sshd_config 确保下面的配置设置正确：
#+begin_src bash
X11Forwarding yes
X11DisplayOffset 10
X11UseLocalhost no
#+END_SRC
然后重启ssh服务：
#+begin_src bash
# If you are using CentOS 7, you should use
systemctl restart sshd.service
# If you are on CentOS 6, you should use the following command instead:
service sshd restart
#+END_SRC
** X server设置
*** widows下git bash的设置
**** 安装X server
***** windows
 安装vcxsrv
 #+BEGIN_EXAMPLE
 In order to use X11 forwarding, you also need to install a X server on your Windows computer. You can choose to install xming or vcxsrv. Vcxsrv is better since it is updated frequently and is free, while latest version of xming requires a license. 
 #+END_EXAMPLE
 之后打开xlaunch启动vcxsrv

 #+DOWNLOADED: screenshot @ 2021-12-12 23:00:43
 [[file:images/linux%E7%AC%94%E8%AE%B0/%E8%AE%BE%E7%BD%AEX11%E8%BD%AC%E5%8F%91/2021-12-12_23-00-43_screenshot.png]]
**** 配置ssh
 在git bash上设置变量DISPLAY：
 #+begin_src bash
 export DISPLAY=localhost:0.0
 #+END_SRC
 可以将该设置添加到用户目录下的.bash_profile文件里，这样以后每次连接服务器就会自动设置好该变量。

 使用带-X选项的ssh连接：
 #+begin_src bash
 # -v选项可以显示连接的详细信息
 ssh -X -v <user>@<server_addresss>
 #+END_SRC
 这里可能会出现下面的警告：
 #+BEGIN_EXAMPLE
 Warning: untrusted X11 forwarding setup failed: xauth key data not generated
 #+END_EXAMPLE
 如果这时候打开GUI程序，比如xclock，将会出现下面的报错：
 #+BEGIN_EXAMPLE
 Error: Can’t open display:
 #+END_EXAMPLE
 如果出现该错误，则选择用带-Y选项的ssh连接：
 #+begin_src bash
 ssh -Y -v <user>@<server_addresss>
 #+END_SRC
 运行下面的命令：
 #+begin_src bash
 xclock &
 #+END_SRC
 可以看到下面的图像：
 #+DOWNLOADED: screenshot @ 2021-12-12 23:07:10
 [[file:images/linux%E7%AC%94%E8%AE%B0/%E8%AE%BE%E7%BD%AEX11%E8%BD%AC%E5%8F%91/2021-12-12_23-07-10_screenshot.png]]

*** linux的设置
sudo vim /etc/ssh/ssh_config  修改以下配置，保存退出。
#+begin_src bash
ForwardAgent yes
ForwardX11 yes
ForwardX11Trusted yes
#+END_SRC
客服端修改完成后也需要执行对应的命令重启ssh服务
#+begin_src bash
sudo systemctl restart ssh.service
#+END_SRC
在客服端C 上执行命令
#+begin_src bash
xhost +　　//允许服务器的的x11界面连接过来
ssh -X tsfh@192.168.0.200　　　　　　//-X参数表示转发X11数据， 把用户名称tsfh 以及服务器S的ip地址替换为你自己的
#+END_SRC
现在你已经登陆了服务器，而且还有一个终端是连接的状态，和平时ssh连接没有什么区别，除了会转发X11的数据，你可以在终端里面用命令运行你想要运行的gui程序比如：firefox , google-chrome , xclock

现在我以xclock为例演示一下, xclock程序会显示一个图形时钟
#+begin_src bash
xclock
#+END_SRC
等待一小会儿 你就可以在客服端C 的桌面上看到服务器S 的xclock了


** termux设置方法
先安装VNC Viewer
*** Enabling the X11 Repository
X11 packages are available in a separate APT repository. You can enable it by running the following command:
#+begin_src bash
pkg install x11-repo
#+END_SRC
It will automatically add appropriate sources.list file and PGP key.

To disable this repository, you need to uninstall package x11-repo.
*** Setting up VNC
Server
If you decided to use VNC for graphical output, follow these instructions for properly setting up VNC server.

1. Install package `tigervnc`:
#+begin_src bash
pkg install tigervnc
#+END_SRC
2. After installation, execute this:
#+begin_src bash
vncserver -localhost
#+END_SRC
At first time, you will be prompted for setting up passwords:

You will require a password to access your desktops.
#+begin_src bash
Password:
Verify:
Would you like to enter a view-only password (y/n)? n
#+END_SRC
Note that passwords are not visible when you are typing them and maximal password length is 8 characters.

3. If everything is okay, you will see this message:
#+begin_src bash
New 'localhost:1 ()' desktop is localhost:1

Creating default startup script /data/data/com.termux/files/home/.vnc/xstartup
Creating default config /data/data/com.termux/files/home/.vnc/config
Starting applications specified in /data/data/com.termux/files/home/.vnc/xstartup
Log file is /data/data/com.termux/files/home/.vnc/localhost:1.log
#+END_SRC
It means that X (vnc) server is available on display 'localhost:1'.

4. Finally, to make programs do graphical output to the display 'localhost:1', set environment variable like shown here (yes, without specifying 'localhost'):
#+begin_src bash
export DISPLAY=":1"
#+END_SRC
You may even put this variable to your bashrc or profile so you don't have to always set it manually unless display address will be changed.
*** Client
Here will be assumed that you use this Android VNC client: VNC Viewer (developed by RealVNC Limited).

1. Determine port number on which VNC server listens. It can be calculated like this: 5900 + {display number}. So for display 'localhost:1' the port will be 5901.

2. Now open the VNC Viewer application and create a new connection with the following information (assuming that VNC port is 5901):
#+BEGIN_EXAMPLE
Address:
127.0.0.1:5901

Name:
Termux
#+END_EXAMPLE

#+DOWNLOADED: screenshot @ 2021-12-12 23:11:40
[[file:images/linux%E7%AC%94%E8%AE%B0/%E8%AE%BE%E7%BD%AEX11%E8%BD%AC%E5%8F%91/2021-12-12_23-11-40_screenshot.png]]

If you are using VNC client on a computer using the same network as the phone does, make sure you correctly start a VNC session and know the IP address of the device.

3. Now launch it. You will be prompted for password that you entered on first launch of 'vncserver'. Depending on packages you installed, you may see either entirely black screen or terminal prompt (only if 'aterm' is installed).
*** 参考文章
[[https://wiki.termux.com/wiki/Graphical_Environment#Enabling_the_X11_Repository][Graphical Environment]]
** 参考文章
[[https://jdhao.github.io/2018/03/02/Windows-connect-server-x11-with-gitbash/][Set up X11 Forwarding with Git for Windows (git-bash)]]
[[https://www.cnblogs.com/tsfh/p/9022170.html][通过ssh X11转发使用远程gui程序]]
[[https://www.jianshu.com/p/1a296191a122][什么是X11-forwarding？怎么使用？]]
* 端口
** 介绍
传输层协议，如传输控制协议（TCP）与用户资料包协议（UDP），在分组表头中，定义了来源端口号与目的端口号。

一个端口号使用16位无符号整数（unsigned integer）来表示，其范围介于0与65535之间：
- 在TCP协议中，端口号0是被保留的，不可使用。
- 1--1023 系统保留，只能由root用户使用。
- 1024---4999 由客户端程序自由分配。
- 5000---65535 由服务器端程序自由分配在UDP协议中，来源端口号是可以选择要不要填上，如果设为0，则代表没有来源端口号。

在操作系统中，一个行程可以通过网络套接字将它的输入与输出与一个特定的传输协议、一个端口、一个IP地址关系起来。这个关系动作，称为綁定（binding），在这之后，就可以通过网络提交与接收资料。

在操作系统上运行的网络软件，可以透过操作系统，利用各个不同的端口，将资料发送到网络上；操作系统也可以根据数据包的IP地址以及端口号，将这些数据包转送到符合的行程去。

虽然使用同样传输协议，但是特定的IP地址以及端口的组合，只会被綁定到单一的特定行程上。当使用同样协议的多个程序，尝试着綁定在同一个IP地址下的相同端口，就会产生一个常见的应用程序错误，这个错误有时候被称为端口冲突（port conflicts）。
** Linux 查看端口占用情况
 Linux 查看端口占用情况可以使用 lsof 和 netstat 命令。
*** lsof
 lsof(list open files)是一个列出当前系统打开文件的工具。

 lsof 查看端口占用语法格式：
 #+BEGIN_EXAMPLE
 lsof -i:端口号
 #+END_EXAMPLE

**** 实例
 查看服务器 8000 端口的占用情况：
 #+begin_src bash
 /:lsof -i:8000

 COMMAND   PID USER   FD   TYPE   DEVICE SIZE/OFF NODE NAME
 nodejs  26993 root   10u  IPv4 37999514      0t0  TCP *:8000 (LISTEN)
 #+END_SRC
 可以看到 8000 端口已经被轻 nodejs 服务占用。

 lsof -i 需要 root 用户的权限来执行，如下图：
 #+DOWNLOADED: file:E%3A/org/%E5%9B%BE%E7%89%87/Snipaste_2021-09-03_15-42-19.png @ 2021-09-03 16:14:42
 [[file:Linux_%E6%9F%A5%E7%9C%8B%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E6%83%85%E5%86%B5/2021-09-03_16-14-42_Snipaste_2021-09-03_15-42-19.png]]
 更多 lsof 的命令如下：
 #+begin_src bash
 lsof -i:8080：查看8080端口占用
 lsof abc.txt：显示开启文件abc.txt的进程
 lsof -c abc：显示abc进程现在打开的文件
 lsof -c -p 1234：列出进程号为1234的进程所打开的文件
 lsof -g gid：显示归属gid的进程情况
 lsof +d /usr/local/：显示目录下被进程开启的文件
 lsof +D /usr/local/：同上，但是会搜索目录下的目录，时间较长
 lsof -d 4：显示使用fd为4的进程
 lsof -i -U：显示所有打开的端口和UNIX domain文件
 #+END_SRC

*** netstat
 netstat -tunlp 用于显示 tcp，udp 的端口和进程等相关情况。

 netstat 查看端口占用语法格式：
 #+begin_src bash
 netstat -tunlp | grep 端口号
 #+END_SRC
 -t (tcp) 仅显示tcp相关选项
 -u (udp)仅显示udp相关选项
 -n 拒绝显示别名，能显示数字的全部转化为数字
 -l 仅列出在Listen(监听)的服务状态
 -p 显示建立相关链接的程序名

 例如查看 8000 端口的情况，使用以下命令：
 #+begin_src bash
 /:netstat -tunlp | grep 8000
 tcp        0      0 0.0.0.0:8000            0.0.0.0:*               LISTEN      26993/nodejs   
 #+END_SRC
 更多命令：
 #+begin_src bash
 netstat -ntlp   //查看当前所有tcp端口
 netstat -ntulp | grep 80   //查看所有80端口使用情况
 netstat -ntulp | grep 3306   //查看所有3306端口使用情况
 #+END_SRC
* 挂载iso
mount命令允许你将 ISO 文件挂载到文件夹中的某个指定的挂载点。

本节内容的指定，应该可以在任何 Linux 发行版上正常运行，包括 Ubuntu, Debian, 和 CentOS。

01.开始创建挂载点，它可以是你想要的任何位置：
#+begin_src bash
sudo mkdir /media/iso
#+END_SRC

02.通过mount命令将 ISO 文件挂载到挂载点。
#+begin_src bash
sudo mount /path/to/image.iso /media/iso -o loop
#+END_SRC

这里很重要的一点是-o loop选项。它告诉命令将指定 ISO 文件映射成一个回环设备，并且将这个设备挂载到指定的挂载点。

不要忘记将/path/to/image.iso替换成你自己的 ISO 文件路径。

03.查看 ISO 镜像内容，请使用ls命令
#+begin_src bash
ls /media/iso
#+END_SRC

你也可以在一个文件管理器中，直接打开并浏览 ISO 镜像里面的具体内容。

04.当镜像挂载后，通过umount命令后面加上挂载目录，就可以卸载 ISO 文件。
#+begin_src bash
sudo umount /media/iso
#+END_SRC

如果此时文件系统正在使用中，那么umount将会卸载失败。
* loop设备
什么是loop设备？
loop设备是一种伪设备，是使用文件来模拟块设备的一种技术，文件模拟成块设备后, 就像一个磁盘或光盘一样使用。在使用之前，一个 loop 设备必须要和一个文件进行连接。这种结合方式给用户提供了一个替代块特殊文件的接口。因此，如果这个文件包含有一个完整的文件系统，那么这个文件就可以像一个磁盘设备一样被 mount 起来。之所以叫loop设备（回环），其实是从文件系统这一层来考虑的，因为这种被 mount 起来的镜像文件它本身也包含有文件系统，通过loop设备把它mount起来，它就像是文件系统之上再绕了一圈的文件系统，所以称为 loop。

loop设备的使用
一般在linux中会有8个loop设备，一般是/dev/loop0~loop7，可用通过losetup -a查看所有的loop设备，如果命令没有输出就说明所有的loop设备都没有被占用，你可以按照以下步骤创建自己的loop设备。

1）创建一个文件
dd if=/dev/zero of=/var/loop.img bs=1M count=10240

2）使用losetup将文件转化为块设备
losetup /dev/loop0 /var/loop.img

3）通过lsblk查看刚刚创建的块设备
lsblk |grep loop0
losetup -a

4）当然，你也可以将这个块设备格式化并创建其他的文件系统，然后再mount到某个目录，有点多余啊，一般人不这么干。

5）要删除这个loop设备可以执行以下命令
losetup -d /dev/loop0

作者：ppdjs
链接：https://www.jianshu.com/p/add423a1f01f
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
* TexLive安装
这里以 Ubunt 18.04 LTS 版为例子进行讲解，首先下载镜像文件。然后挂载镜像，或者解压之后进入文件夹。

以root用户执行下面的命令：
#+begin_src bash
./install-tl
#+END_SRC
这会启动安装进程，如果是 GUI 模式，可以用鼠标点，如果是命令行，命令行会询问你是否安装，根据提示，输入大写的 I 就可以进行自动安装了。

安装过程大概有 12 到 30 分钟，具体的时间视情况而定，带有固态硬盘的电脑可能会快一点。整个过程中，有 3700 左右个包需要安装，请耐心等待。

配置环境变量。 安装完成之后，你需要配置一下环境变量，具体操作(注意，现在还是 root 用户)：
#+begin_src bash
echo "export MANPATH=${MANPATH}:/usr/local/texlive/2019/texmf-dist/doc/man" >> ~/.bashrc
echo "export INFOPATH=${INFOPATH}:/usr/local/texlive/2019/texmf-dist/doc/info" >> ~/.bashrc
echo "export PATH=${PATH}:/usr/local/texlive/2019/bin/x86_64-linux" >> ~/.bashrc
#+END_SRC
以上三句命令就是让系统识别 manual 手册、info 手册以及最最重要的 tex 的编译器所在位置。如果不配置上述三句话，那么你将无法使用texlive。

因为你一般是在普通用户下使用 TeX Live，所以还需要切换到普通用户下，配置一下环境变量。运行以下命令。

在当前终端中，输入 Ctrl + D，退出 root 身份。

在当前终端下，输入以下命令 (这几个命令和上面是一模一样的)：
#+begin_src bash
echo "export MANPATH=${MANPATH}:/usr/local/texlive/2019/texmf-dist/doc/man" >> ~/.bashrc
echo "export INFOPATH=${INFOPATH}:/usr/local/texlive/2019/texmf-dist/doc/info" >> ~/.bashrc
echo "export PATH=${PATH}:/usr/local/texlive/2019/bin/x86_64-linux" >> ~/.bashrc
source ~/.bashrc # 令 bashrc 生效
#+END_SRC
到此就结束了。
* 修改主机名
使用hostname命令就可以查看Linux的主机名

可以使用如下命令来修改主机名

hostnamectl set-hostname eaglezsx

也可以修改其配置文件/etc/hostname里边的内容
* 设置本地域名解析
在/etc/hosts文件中添加一句话
#+BEGIN_EXAMPLE
192.168.188.1 www.baidu.com
#+END_EXAMPLE

保存文件后再ping一下www.baidu.com就会连接到192.168.188.1了

每一行为一条记录，分成两部分，第一部分是IP，第二部分是域名。
- 一个IP后面可以跟多个域名，可以是几十个甚至上百个
- 每一行只能有一个IP，也就是说一个域名不能对应多个IP
- 如果有多行中出现相同的域名（对应的ip不一样），会按最前面的记录来解析
* gitee.com无法访问的临时解决方案
首先是否ping通: ping gitee.com

如果无法ping，则用临时解决方案：

修改/etc/hosts，添加 153.3.137.123 gitee.com

这个原因大概率是停止域名解析了。所以可以写死ip地址来进行访问
* 静态库和动态库
** 什么是库
库是写好的现有的，成熟的，可以复用的代码。现实中每个程序都要依赖很多基础的底层库，不可能每个人的代码都从零开始，因此库的存在意义非同寻常。

本质上来说库是一种可执行代码的二进制形式，可以被操作系统载入内存执行。

库有两种：静态库（.a、.lib）和动态库（.so、.dll）。

所谓静态、动态是指链接。回顾一下，将一个程序编译成可执行程序的步骤：

#+DOWNLOADED: screenshot @ 2022-05-16 16:39:28
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_16-39-28_screenshot.png]]
** 静态库
之所以成为【静态库】，是因为在链接阶段，会将汇编生成的目标文件.o与引用到的库一起链接打包到可执行文件中。
因此对应的链接方式称为静态链接。

试想一下，静态库与汇编生成的目标文件一起链接为可执行文件，那么静态库必定跟.o文件格式相似。
其实一个静态库可以简单看成是一组目标文件（.o/.obj文件）的集合，即很多目标文件经过压缩打包后形成的一个文件。
静态库特点总结：
1. 静态库对函数库的链接是放在编译时期完成的。
2. 程序在运行时与函数库再无瓜葛，移植方便。
3. 浪费空间和资源，因为所有相关的目标文件与牵涉到的函数库被链接合成一个可执行文件。

Linux下使用ar工具、Windows下vs使用lib.exe，将目标文件压缩到一起，并且对其进行编号和索引，以便于查找和检索。

一般创建静态库的步骤如图所示：

#+DOWNLOADED: screenshot @ 2022-05-16 16:41:13
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_16-41-13_screenshot.png]]
*** Linux下创建与使用静态库
下面编写一些简单的四则运算C++类，将其编译成静态库给他人用，头文件如下所示：

StaticMath.h头文件
#+begin_src bash
#pragma once
class StaticMath
{
public:
    StaticMath(void);
    ~StaticMath(void);
 
    static double add(double a, double b);//加法
    static double sub(double a, double b);//减法
    static double mul(double a, double b);//乘法
    static double div(double a, double b);//除法
    void print();
};
#+END_SRC

**** Linux静态库命名规则
Linux静态库命名规范，必须是"lib[your_library_name].a"：lib为前缀，中间是静态库名，扩展名为.a。
**** 创建静态库（.a）
Linux创建静态库过程如下：

首先，将代码文件编译成目标文件.o（StaticMath.o）
#+begin_src bash
g++ -c StaticMath.cpp
#+END_SRC
注意带参数-c，否则直接编译为可执行文件

然后，通过ar工具将目标文件打包成.a静态库文件
#+begin_src bash
ar -crv libstaticmath.a StaticMath.o
#+END_SRC
生成静态库libstaticmath.a。

#+DOWNLOADED: screenshot @ 2022-05-16 17:07:02
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_17-07-02_screenshot.png]]
大一点的项目会编写makefile文件（CMake等等工程管理工具）来生成静态库，输入多个命令太麻烦了。
**** 使用静态库
编写使用上面创建的静态库的测试代码：
#+begin_src bash
#include "StaticMath.h"
#include <iostream>
using namespace std;

int main(int argc, char* argv[])
{
    double a = 10;
    double b = 2;

    cout << "a + b = " << StaticMath::add(a, b) << endl;
    cout << "a - b = " << StaticMath::sub(a, b) << endl;
    cout << "a * b = " << StaticMath::mul(a, b) << endl;
    cout << "a / b = " << StaticMath::div(a, b) << endl;

    StaticMath sm;
    sm.print();

    system("pause");
    return 0;
}
#+END_SRC
Linux下使用静态库，只需要在编译的时候，指定静态库的搜索路径（-L选项）、指定静态库名（不需要lib前缀和.a后缀，-l选项）。
#+begin_src bash
g++ TestStaticLibrary.cpp -L../StaticLibrary -lstaticmath
#+END_SRC
- -L：表示要连接的库所在目录
- -l：指定链接时需要的动态库，编译器查找动态连接库时有隐含的命名规则，即在给出的名字前面加上lib，后面加上.a或.so来确定库的名称。

#+DOWNLOADED: screenshot @ 2022-05-16 17:09:14
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_17-09-14_screenshot.png]]
*** Windows下创建与使用静态库
**** 创建静态库（.lib）
如果是使用VS命令行生成静态库，也是分两个步骤来生成程序：

首先，通过使用带编译器选项 /c 的 Cl.exe 编译代码 (cl /c StaticMath.cpp)，创建名为“StaticMath.obj”的目标文件。

然后，使用库管理器 Lib.exe 链接代码 (lib StaticMath.obj)，创建静态库StaticMath.lib。

当然，我们一般不这么用，使用VS工程设置更方便。创建win32控制台程序时，勾选静态库类型；打开工程“属性面板”è”配置属性”è”常规”，配置类型选择静态库。

#+DOWNLOADED: screenshot @ 2022-05-16 17:14:22
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_17-14-22_screenshot.png]]
Build项目即可生成静态库。
**** 使用静态库
测试代码Linux下面的一样。有3种使用方法：

方法一：
在VS中使用静态库方法：

工程“属性面板”è“通用属性”è “框架和引用”è”添加引用”，将显示“添加引用”对话框。 “项目”选项卡列出了当前解决方案中的各个项目以及可以引用的所有库。 在“项目”选项卡中，选择 StaticLibrary。 单击“确定”。

#+DOWNLOADED: screenshot @ 2022-05-16 17:15:04
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_17-15-04_screenshot.png]]
添加StaticMath.h 头文件目录，必须修改包含目录路径。打开工程“属性面板”è”配置属性”è “C/C++”è” 常规”，在“附加包含目录”属性值中，键入StaticMath.h 头文件所在目录的路径或浏览至该目录。

#+DOWNLOADED: screenshot @ 2022-05-16 17:15:25
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_17-15-25_screenshot.png]]
编译运行OK。

#+DOWNLOADED: screenshot @ 2022-05-16 17:16:47
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_17-16-47_screenshot.png]]
如果引用的静态库不是在同一解决方案下的子工程，而是使用第三方提供的静态库lib和头文件，上面的方法设置不了。还有2中方法设置都可行。

方法二：
打开工程“属性面板”è”配置属性”è “链接器”è”命令行”，输入静态库的完整路径即可。


#+DOWNLOADED: screenshot @ 2022-05-16 17:17:36
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_17-17-36_screenshot.png]]
方法三：
- “属性面板”è”配置属性”è “链接器”è”常规”，附加依赖库目录中输入，静态库所在目录；
- “属性面板”è”配置属性”è “链接器”è”输入”，附加依赖库中输入静态库名StaticLibrary.lib。


#+DOWNLOADED: screenshot @ 2022-05-16 17:18:07
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_17-18-07_screenshot.png]]

** 动态库
为什么需要动态库，其实也是静态库的特点导致。

空间浪费是静态库的一个问题。

#+DOWNLOADED: screenshot @ 2022-05-16 16:41:30
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_16-41-30_screenshot.png]]

另一个问题是静态库对程序的更新、部署和发布页会带来麻烦。
如果静态库liba.lib更新了，所以使用它的应用程序都需要重新编译、发布给用户（对于玩家来说，可能是一个很小的改动，却导致整个程序重新下载，全量更新）。

动态库在程序编译时并不会被连接到目标代码中，而是在程序运行是才被载入。
不同的应用程序如果调用相同的库，那么在内存里只需要有一份该共享库的实例，规避了空间浪费问题。
动态库在程序运行是才被载入，也解决了静态库对程序的更新、部署和发布页会带来麻烦。用户只需要更新动态库即可，增量更新。

#+DOWNLOADED: screenshot @ 2022-05-16 16:41:52
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_16-41-52_screenshot.png]]
动态库特点总结：
1. 动态库把对一些库函数的链接载入推迟到程序运行的时期。
2. 可以实现进程之间的资源共享。（因此动态库也称为共享库）
3. 将一些程序升级变得简单。
4. 甚至可以真正做到链接载入完全由程序员在程序代码中控制（显示调用）。

Window与Linux执行文件格式不同，在创建动态库的时候有一些差异。

- 在Windows系统下的执行文件格式是PE格式，动态库需要一个DllMain函数做出初始化的入口，通常在导出函数的声明时需要有_declspec(dllexport)关键字。
- Linux下gcc编译的执行文件默认是ELF格式，不需要初始化入口，亦不需要函数做特别的声明，编写比较方便。

与创建静态库不同的是，不需要打包工具（ar、lib.exe），直接使用编译器即可创建动态库。

*** Linux下创建与使用动态库
**** linux动态库的命名规则
动态链接库的名字形式为 libxxx.so，前缀是lib，后缀名为“.so”。

针对于实际库文件，每个共享库都有个特殊的名字“soname”。在程序启动后，程序通过这个名字来告诉动态加载器该载入哪个共享库。

在文件系统中，soname仅是一个链接到实际动态库的链接。对于动态库而言，每个库实际上都有另一个名字给编译器来用。它是一个指向实际库镜像文件的链接文件（lib+soname+.so）。
**** 创建动态库（.so）
编写四则运算动态库代码：
DynamicMath.h头文件
#+begin_src bash
#pragma once

class DynamicMath
{
public:
        DynamicMath(void);
        ~DynamicMath(void);

        static double add(double a, double b);//¼Ó·¨
        static double sub(double a, double b);//¼õ·¨
        static double mul(double a, double b);//³Ë·¨
        static double div(double a, double b);//³ý·¨

        void print();
};
#+END_SRC
首先，生成目标文件，此时要加编译器选项-fpic
#+begin_src bash
g++ -fPIC -c DynamicMath.cpp
#+END_SRC
-fPIC 创建与地址无关的编译程序（pic，position independent code），是为了能够在多个应用程序间共享。
然后，生成动态库，此时要加链接器选项-shared
#+begin_src bash
g++ -shared -o libdynmath.so DynamicMath.o
#+END_SRC
-shared指定生成动态链接库。

#+DOWNLOADED: screenshot @ 2022-05-16 19:19:24
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_19-19-24_screenshot.png]]
其实上面两个步骤可以合并为一个命令：
#+begin_src bash
g++ -fPIC -shared -o libdynmath.so DynamicMath.cpp
#+END_SRC 
**** 使用动态库
编写使用动态库的测试代码：
#+begin_src bash
#include "../DynamicLibrary/DynamicMath.h"
#include <iostream>

using namespace std;
int main(int argc, char* argv[])
{
    double a = 10;
    double b = 2;

    cout << "a + b = " << DynamicMath::add(a, b) << endl;
    cout << "a - b = " << DynamicMath::sub(a, b) << endl;
    cout << "a * b = " << DynamicMath::mul(a, b) << endl;
    cout << "a / b = " << DynamicMath::div(a, b) << endl;

    DynamicMath dyn;
    dyn.print();
    return 0;
}
#+END_SRC
引用动态库编译成可执行文件（跟静态库方式一样）：
#+begin_src bash
g++ TestDynamicLibrary.cpp -L../DynamicLibrary -ldynmath
#+END_SRC

在执行的时候是如何定位共享库文件的呢？
- 当系统加载可执行代码时候，能够知道其所依赖的库的名字，但是还需要知道绝对路径。此时就需要系统动态载入器(dynamic linker/loader)。
- 对于elf格式的可执行程序，是由ld-linux.so*来完成的，它先后搜索elf文件的 DT_RPATH段—环境变量LD_LIBRARY_PATH—/etc/ld.so.cache文件列表—/lib/,/usr/lib 目录找到库文件后将其载入内存。

如何让系统能够找到它：
- 如果安装在/lib或者/usr/lib下，那么ld默认能够找到，无需其他操作。
- 如果安装在其他目录，需要将其添加到/etc/ld.so.cache文件中，步骤如下：
    - 编辑/etc/ld.so.conf文件，加入库文件所在目录的路径
    - 运行ldconfig ，该命令会重建/etc/ld.so.cache文件

我们将创建的动态库复制到/usr/lib下面，然后运行测试程序。
#+DOWNLOADED: screenshot @ 2022-05-16 19:23:15
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_19-23-15_screenshot.png]]
*** Windows下创建与使用动态库
**** 创建动态库（.dll）
与Linux相比，在Windows系统下创建动态库要稍微麻烦一些。首先，需要一个DllMain函数做出初始化的入口（创建win32控制台程序时，勾选DLL类型会自动生成这个文件）：

dllmain.cpp入口文件
#+begin_src c++
// dllmain.cpp : Defines the entry point for the DLL application.

#include "stdafx.h"
BOOL APIENTRY DllMain( HMODULE hModule,
                       DWORD  ul_reason_for_call,
                       LPVOID lpReserved
                     )
{
    switch (ul_reason_for_call)
    {
    case DLL_PROCESS_ATTACH:
    case DLL_THREAD_ATTACH:
    case DLL_THREAD_DETACH:
    case DLL_PROCESS_DETACH:
        break;
    }
    return TRUE;
}
#+END_SRC
通常在导出函数的声明时需要有_declspec(dllexport)关键字：

DynamicMath.h头文件
#+BEGIN_SRC c++
#pragma once

class DynamicMath
{
public:
    __declspec(dllexport) DynamicMath(void);
    __declspec(dllexport) ~DynamicMath(void);

    static __declspec(dllexport) double add(double a, double b);//加法
    static __declspec(dllexport) double sub(double a, double b);//减法
    static __declspec(dllexport) double mul(double a, double b);//乘法
    static __declspec(dllexport) double div(double a, double b);//除法

    __declspec(dllexport) void print();
};
#+END_SRC
生成动态库需要设置工程属性，打开工程“属性面板”è”配置属性”è”常规”，配置类型选择动态库。

#+DOWNLOADED: screenshot @ 2022-05-16 19:25:49
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_19-25-49_screenshot.png]]
Build项目即可生成动态库。
**** 使用动态库
创建win32控制台测试程序：
TestDynamicLibrary.cpp测试程序
#+BEGIN_SRC c++
#include "stdafx.h"
#include "DynamicMath.h"

#include <iostream>
using namespace std;

int _tmain(int argc, _TCHAR* argv[])
{
    double a = 10;
    double b = 2;

    cout << "a + b = " << DynamicMath::add(a, b) << endl;

    cout << "a - b = " << DynamicMath::sub(a, b) << endl;

    cout << "a * b = " << DynamicMath::mul(a, b) << endl;

    cout << "a / b = " << DynamicMath::div(a, b) << endl;
 
    DynamicMath dyn;
    dyn.print();

    system("pause");
    return 0;
}
#+END_SRC
方法一：
工程“属性面板”è“通用属性”è “框架和引用”è”添加引用”，将显示“添加引用”对话框。“项目”选项卡列出了当前解决方案中的各个项目以及可以引用的所有库。 在“项目”选项卡中，选择 DynamicLibrary。 单击“确定”。

#+DOWNLOADED: screenshot @ 2022-05-16 19:27:07
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_19-27-07_screenshot.png]]
添加DynamicMath.h 头文件目录，必须修改包含目录路径。打开工程“属性面板”è”配置属性”è “C/C++”è” 常规”，在“附加包含目录”属性值中，键入DynamicMath.h 头文件所在目录的路径或浏览至该目录。

#+DOWNLOADED: screenshot @ 2022-05-16 19:27:20
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_19-27-20_screenshot.png]]
编译运行OK。

#+DOWNLOADED: screenshot @ 2022-05-16 19:27:29
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_19-27-29_screenshot.png]]
方法二：
“属性面板”è”配置属性”è “链接器”è”常规”，附加依赖库目录中输入，动态库所在目录；

#+DOWNLOADED: screenshot @ 2022-05-16 19:27:45
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_19-27-45_screenshot.png]]
“属性面板”è”配置属性”è “链接器”è”输入”，附加依赖库中输入动态库编译出来的DynamicLibrary.lib。

#+DOWNLOADED: screenshot @ 2022-05-16 19:27:55
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_19-27-55_screenshot.png]]
这里可能大家有个疑问，动态库怎么还有一个DynamicLibrary.lib文件？即无论是静态链接库还是动态链接库，最后都有lib文件，那么两者区别是什么呢？其实，两个是完全不一样的东西。

#+DOWNLOADED: screenshot @ 2022-05-16 19:28:06
[[file:images/linux%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E5%BA%93%E5%92%8C%E5%8A%A8%E6%80%81%E5%BA%93/2022-05-16_19-28-06_screenshot.png]]
StaticLibrary.lib的大小为190KB，DynamicLibrary.lib的大小为3KB，静态库对应的lib文件叫静态库，动态库对应的lib文件叫【导入库】。实际上静态库本身就包含了实际执行代码、符号表等等，而对于导入库而言，其实际的执行代码位于动态库中，导入库只包含了地址符号表等，确保程序找到对应函数的一些基本地址信息。

** 动态库的显式调用
上面介绍的动态库使用方法和静态库类似属于隐式调用，编译的时候指定相应的库和查找路径。其实，动态库还可以显式调用。【在C语言中】，显示调用一个动态库轻而易举！

*** 在Linux下显式调用动态库
#include <dlfcn.h>，提供了下面几个接口：
- void * dlopen( const char * pathname, int mode )：函数以指定模式打开指定的动态连接库文件，并返回一个句柄给调用进程。
- void* dlsym(void* handle,const char* symbol)：dlsym根据动态链接库操作句柄(pHandle)与符号(symbol)，返回符号对应的地址。使用这个函数不但可以获取函数地址，也可以获取变量地址。
- int dlclose (void *handle)：dlclose用于关闭指定句柄的动态链接库，只有当此动态链接库的使用计数为0时,才会真正被系统卸载。
- const char *dlerror(void)：当动态链接库操作函数执行失败时，dlerror可以返回出错信息，返回值为NULL时表示操作函数执行成功。

*** 在Windows下显式调用动态库
应用程序必须进行函数调用以在运行时显式加载 DLL。为显式链接到 DLL，应用程序必须：
- 调用 LoadLibrary（或相似的函数）以加载 DLL 和获取模块句柄。
- 调用 GetProcAddress，以获取指向应用程序要调用的每个导出函数的函数指针。由于应用程序是通过指针调用 DLL 的函数，编译器不生成外部引用，故无需与导入库链接。
- 使用完 DLL 后调用 FreeLibrary。

*** 显式调用C++动态库注意点
对C++来说，情况稍微复杂。显式加载一个C++动态库的困难一部分是因为C++的name mangling；另一部分是因为没有提供一个合适的API来装载类，在C++中，您可能要用到库中的一个类，而这需要创建该类的一个实例，这不容易做到。

name mangling可以通过extern "C"解决。C++有个特定的关键字用来声明采用C binding的函数：extern "C" 。用 extern "C"声明的函数将使用函数名作符号名，就像C函数一样。因此，只有非成员函数才能被声明为extern "C"，并且不能被重载。尽管限制多多，extern "C"函数还是非常有用，因为它们可以象C函数一样被dlopen动态加载。冠以extern "C"限定符后，并不意味着函数中无法使用C++代码了，相反，它仍然是一个完全的C++函数，可以使用任何C++特性和各种类型的参数。
** 区别与联系
二者的不同点在于代码被载入的时刻不同。

静态库的代码在编译过程中已经被载入可执行程序,因此体积比较大。

动态库(共享库)的代码在可执行程序运行时才载入内存，在编译过程中仅简单的引用，因此代码体积比较小。

不同的应用程序如果调用相同的库,那么在内存中只需要有一份该动态库(共享库)的实例。

静态库和动态库的最大区别,静态情况下,把库直接加载到程序中,而动态库链接的时候,它只是保留接口,将动态库与程序代码独立,这样就可以提高代码的可复用度，和降低程序的耦合度。

静态库在程序编译时会被连接到目标代码中，程序运行时将不再需要该静态库。

动态库在程序编译时并不会被连接到目标代码中，而是在程序运行是才被载入，因此在程序运行时还需要动态库存在
** 参考文章
[[https://blog.nowcoder.net/n/8e07e78a703c413c916d0f830b8ceda7][浅析静态库和动态库的区别]]
[[https://www.cnblogs.com/skynet/p/3372855.html][C++静态库与动态库]]
* 创建用户
下面将创建用户 zyq01
1. 输入命令：sudo useradd zyq01，回车，创建用户；
2. 输入命令：ls，回车，查看用户是否创建成功（可以看到用户已经创建成功了）；
3. 输入命令：sudo passwd zyq01，回车，设置登录用户密码；
4. 输入命令：su zyq01，切换到新用户
